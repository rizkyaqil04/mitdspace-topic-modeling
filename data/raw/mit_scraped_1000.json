[
    {
        "title": "The American Institute for Manufacturing Integrated Photonics: advancing the ecosystem",
        "abstract": "The American Institute for Manufacturing Integrated Photonics (AIM Photonics) is focused on developing an end-to-end integrated photonics ecosystem in the U.S., including domestic foundry access, integrated design tools, automated packaging, assembly and test, and workforce development. This paper describes how the institute has been structured to achieve these goals, with an emphasis on advancing the integrated photonics ecosystem. Additionally, it briefly highlights several of the technological development targets that have been identified to provide enabling advances in the manufacture and application of integrated photonics.",
        "authors": [
            "Thomas L. Koch",
            "Michael Liehr",
            "Douglas Coolbaugh",
            "John E. Bowers",
            "Rod Alferness",
            "Michael Watts",
            "Lionel C Kimerling"
        ],
        "journal_conference_name": "Broadband Access Communication Technologies X",
        "publisher": "SPIE",
        "year": "2106",
        "doi": "http://hdl.handle.net/1721.1/112987",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "How do we interpret the outputs of a neural network trained on classification?",
        "abstract": "Deep neural networks are widely used for classification tasks, but the interpretation of their output activations is often unclear. This tutorial article explains\r\nhow these outputs can be understood as approximations of the Bayesian posterior.\r\nWe showed that, in theory, the loss function for classification tasks – derived by\r\nmaximum likelihood – is minimized by the Bayesian posterior. We conducted\r\nempirical studies training neural networks to classify synthetic data from a known\r\ngenerative model. In a simple classification task, the network closely approximates the theoretically derived posterior. However, a few changes in the task can\r\nmake accurate approximation much more difficult. The ability of the networks to\r\napproximate the posterior depends on multiple factors, such as the complexity of\r\nthe posterior and whether there is sufficient data for learning.",
        "authors": [
            "Yudi Xie"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "International Conference on Learning Representations",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159032",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Geospatial Trucking Industry Decarbonization Explorer (Geo-TIDE): Technical Guide and Methodology",
        "abstract": "Geo-TIDE is a public, interactive decision-support tool developed by the MIT Climate &\r\nSustainability Consortium (MCSC) to help trucking industry stakeholders identify and evaluate early opportunities for fleet and infrastructure decarbonization. By integrating public geospatial datasets such as regional freight flows, policy incentives, and spatially resolved cost and emissions models, Geo-TIDE enables data-driven decisions about where, when, and how to invest in low-carbon technologies. In this technical guide, Danika Eamer (who has led the development of Geo-TIDE) and co-authors Micah Borrero, Brooke Bao, Brilant Kasami, and Helena De Figueiredo Valente detail the tool’s functionality, showcase real-world usage scenarios, and explore the methodology behind its evolution and development.",
        "authors": [
            "Danika Eamer",
            "Micah Borrero",
            "Brooke Bao",
            "Brilant Kasami",
            "Helena De Figueiredo Valente"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159069",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Reducing Proliferation Risks with High-Assay Low-Enriched Uranium Fuels in Reactors with Coated-Particle (TRISO) Fuels",
        "abstract": "The use of graphite-matrix tri-structural-isotropic (TRISO) fuels in high-temperature reactors with high-assay low-enriched uranium (HALEU) can significantly reduce nuclear weapons proliferation risks relative to other fuels and reactor types. The HALEU fuel, with fuels containing 15% to 20% 235U enable used nuclear fuels (UNFs) with thermal neutron–spectrum burnups between 150 000 and 200 000 MWd per ton. At these high burnups, the plutonium isotopics make the direct use for nuclear weapons unattractive and the uranium isotopics unattractive as a feed to a uranium-enrichment plant. On the front end, it would require the theft of ~150 000 pebbles with uranium just under 20% 235U to create the theoretical potential to produce sufficient material for one weapon (1000 kg), which is about a 2-year supply of fuel for these reactors.\r\n\r\nThe chemical and mechanical processing requirements to convert fresh TRISO fuel to uranium metal for use in a nuclear weapon are beyond nonstate actors. Over 10 sequential chemical process steps would be required, plus uranium recovery from waste streams, to avoid large uranium losses in the conversion processes. If a nation-state wanted to make a nuclear weapon starting with HALEU fuel, they would enrich the HALEU from 19.95% to over 90% 235U, which presumes they already possess enrichment capabilities and can use any uranium feedstock. If enriched to weapons-grade 235U, 1 ton of HALEU has sufficient 235U for multiple weapons.\r\n\r\nSeparately, it is not clear if a weapon can actually be built with HALEU fuel. The fuel characteristics also reduce risks from sabotage. Consequently, we conclude that reactor safeguards for fresh HALEU TRISO fuel can be similar to those for low-enriched uranium light water reactor fuel; that is, no requirements for added security or other measures. TRISO UNF safeguards and security can be significantly relaxed relative to the requirements for other types of UNF at the reactor site.",
        "authors": [
            "Charles Forsberg",
            "Andrew Kadak"
        ],
        "journal_conference_name": "Nuclear Technology",
        "publisher": "Taylor & Francis",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159161",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "AGM aquariums and elliptic curves over arbitrary finite fields",
        "abstract": "In this paper, we define a version of the arithmetic-geometric mean (AGM) function for arbitrary finite fields F q , and study the resulting AGM graph with points ( a , b ) ∈ F q × F q and directed edges between points (a, b), ( a + b 2 , ab ) and (a, b), ( a + b 2 , - ab ) . The points in this graph are naturally associated to elliptic curves over F q in Legendre normal form, with the AGM function defining a 2-isogeny between the associated curves. We use this correspondence to prove several results on the structure, size, and multiplicity of the connected components in the AGM graph.",
        "authors": [
            "June Kayath",
            "Connor Lane",
            "Ben Neifeld",
            "Tianyu Ni",
            "Hui Xue"
        ],
        "journal_conference_name": "Research in Number Theory",
        "publisher": "Springer International Publishing",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159072",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Ordering Candidates via Vantage Points",
        "abstract": "Given an n-element set C ⊆ R d and a (sufficiently generic) k-element multiset V ⊆ R d , we can order the points in C by ranking each point c ∈ C according to the sum of the distances from c to the points of V. Let Ψ k ( C ) denote the set of orderings of C that can be obtained in this manner as V varies, and let ψ d , k max ( n ) be the maximum of | Ψ k ( C ) | as C ranges over all n-element subsets of R d . We prove that ψ d , k max ( n ) = Θ d , k ( n 2 d k ) when d ≥ 2 and that ψ 1 , k max ( n ) = Θ k ( n 4 ⌈ k / 2 ⌉ - 2 ) . As a step toward proving this result, we establish a bound on the number of sign patterns determined by a collection of functions that are sums of radicals of nonnegative polynomials; this can be understood as an analogue of a classical theorem of Warren. We also prove several results about the set Ψ ( C ) = ⋃ k ≥ 1 Ψ k ( C ) ; this includes an exact description of Ψ ( C ) when d = 1 and when C is the set of vertices of a vertex-transitive polytope.",
        "authors": [
            "Noga Alon",
            "Colin Defant",
            "Noah Kravitz",
            "Daniel G. Zhu"
        ],
        "journal_conference_name": "Combinatorica",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159170",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Leveraging Seawater Thermal Energy Storage and Heat Pumps for Coupling Electricity and Urban Heating: A Techno-Economic Analysis",
        "abstract": "This paper presents an economic assessment of seawater thermal energy storage (TES) integrated with industrial heat pumps to couple renewable electricity generation with urban district heating networks. Using Amsterdam as a case study, we develop a techno-economic model leveraging real-world data on electricity prices, heat demand, and system costs. Our findings show that large-scale TES using seawater as a storage medium significantly enhances district heating economics through energy arbitrage and operational flexibility. The optimal configuration yields a net present value (NPV) of EUR 466 million over 30 years and a payback period under 6 years. Thermal storage increases NPV by 17% compared to systems without storage, while within-day load shifting further boosts economic value by 23%. Accurate demand and price forecasting is critical, as forecasting errors can reduce NPV by 13.7%. The proposed system is scalable and well suited for coastal cities, offering a sustainable, space-efficient solution for urban decarbonization and addressing renewable energy overproduction.",
        "authors": [
            "Timur Abbiasov",
            "Aldo Bischi",
            "Manfredi Gangi",
            "Andrea Baccioli",
            "Paolo Santi",
            "Carlo Ratti"
        ],
        "journal_conference_name": "Energies",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159075",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Encourage circular practices in the supply chain",
        "abstract": "Every year 300 million tons of plastic waste are produced, and the amount of plastic produced increases with the world population. The more people there are on the planet, the more waste is produced. The concepts of circular economy are gaining popularity. Companies are looking to implement circular strategies to maximize the use of materials, reduce waste and help the environment while improving their corporate image.\r\nSince the coronavirus pandemic, digital transformation has progressed faster and faster, which has boosted digital communication. Social networks began to play a fundamental role in communication since they are an efficient means of interacting with people worldwide in real-time. Due to social networks' social impact, they can be used to influence people's decision-making.\r\nThis study aims to develop a model that encourages people to adopt recycling habits for polyethylene terephthalate (PET) bottles through social networks focused on the population of the United States. This study will use analytics tools such as the Bass Diffusion Model, and an economic analysis of the viability will be carried out to implement the proposed strategies.",
        "authors": [
            "Alejandro Jorge Goitia Polo",
            "Juan Manuel Perez Dovalo"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159025",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Buy, Rent and Sale: Chasing better cash flows",
        "abstract": "This capstone project optimizes the inventory levels of a rental car company and improves the cash-to-cash cycle. The solution approach is a Mixed Integer Linear Program (MILP) model, considering a multiple-period inventory. The model provides purchasing and selling plans and cash and vehicle flow in the system for each quarter and each type of vehicle for five years. The analytical model was created to maximize the company’s gross margin, considering revenues from renting and sales, cost buy, opportunity cost, and general cost (maintenance, holding, and others). Moreover, it considers an initial inventory and helps the company manage these assets in the best way possible to meet the demand. The result shows an optimal solution of 3.3 billion Colombian pesos (COP) for five years in the base case scenario. Afterward, a sensitivity analysis for different perspectives related to renting period, budget, depreciation rate, and exchange rate impact was carried out. From that perspective, it is possible to understand that the primary trigger to create revenue is extending the renting period. Moreover, the sponsor company can interpret how factors in the market affect the total result success and create an action plan to anticipate these risks.",
        "authors": [
            "Ana Patricia Do Couto Selem",
            "Juan Marcelo Oyarzun Rodriguez",
            "Ricardo Leon Monsalve Uribe"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159030",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Reducing food losses by improving the efficiency of the banana supply chain in the Antioquia corridor in Colombia",
        "abstract": "In 2011, the Food and Agricultural Organization of the United Nations (FAO) estimated that one-third of the food produced in the world for human consumption was lost or wasted (FAO, 2021d). Ten years later, a World Wildlife Fund (WWF) study calculated the percentage of food destined for consumption wasted along the entire chain. It reached 1.2 billion tons of food lost on farms and 931 million tons wasted at the retail, food service, and household levels, which accounts for around 40% (WWF-UK, 2021). All this wasted food could feed more than double the number of undernourished people worldwide, estimated to be between 720 and 811 million in 2020 (see figure 1) (FAO, 2021e; World Food Programme, 2020).",
        "authors": [
            "Ananthakumar Sethuramanujam",
            "Laura Natalia Fernandez Cedi"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159027",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Demand Forecasting Analysis for Pharma Retail",
        "abstract": "Demand planning is the connection between marketing, finance, and operations. In an industry like pharma retail, products do not always behave according to a regular stable baseline. In addition, marketing enrichment like promotions or price fluctuations and the impactof government regulations and patient base characteristics increase operational complexity. Moreover, more than thirty percent of changes in the forecast from one cycle to another can lead to overstock or out-of-stock due to the high production and delivery lead times.\r\nThe purpose of this project is to find a proper demand forecasting model for a selected group of stock-keeping units to improve supply processes of the most important stores of the sponsoring company, leading to further benefits such as budget purposes as a top-down analysis. Data analysis is needed for trends, seasonality, stockouts, and demand stability. Followed by the application of various forecasting models, including Machine Learning algorithms, this project provides a comparison of models to define the best baseline as a tool for the planning area to enrich to improve operational KPIs.",
        "authors": [
            "Nestor Andres Moreno Quintero",
            "Mariana Martins de Brito Sousa",
            "Waldo Mauricio Gabriel Flores Trujillo"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159023",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Determining optimal inventory positions in an urban network",
        "abstract": "Supply chain networks are becoming increasingly complex due to the aggressive growth of multiple digital trends, like the rise of e-commerce and the increased customer expectations, which have been enhanced through the pandemic over the last few years. Therefore, this study proposes a model to develop an inventory optimization strategy for a multi-tier supply chain case study in the US market, considering the supply and demand variability for local and international distribution. First, different approaches from the theoretical perspective are analyzed, from traditional inventory management to the new end-to-end perspectives. After that, details of the methodology will be explained, considering the statistical benefits of demand pooling. Finally, real numbers from a case study are applied to the methodology to measure the solution's impact, followed by the conclusions found from the study.",
        "authors": [
            "Juan Pablo Briceño Tipacti",
            "Gabriel Rocha Camargo"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159026",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Reduction of Costs and Emissions in Outbound Transport",
        "abstract": "The global food system accounts for nearly 30% of the total CO2 emissions worldwide. About 19% of that figure is due to transportation-related emissions. The main problem being addressed in this project is to identify the main drivers of CO2 emissions in outbound transportation for a major CPG food company in Antioquia, Colombia, which has declared sustainability as a major driver in their corporate strategy. By indirectly measuring CO2 emissions, a better understanding of the main drivers of emissions can be acquired. The cause-effect relationships on the distribution performance in emissions and cost to serve are in place.\r\nA comprehensive literature review of the state-of-the-art methodologies and techniques to assess CO2 emissions is part of this project, as well as a qualitative evaluation of the challenges of Antioquia’s topography. Two different methodologies have been used throughout this document to estimate CO2 emissions. A fuel-based approach and a distance-weight-based approach use CO2 equivalent units to estimate emissions at different levels of aggregation. Quantitative and spatial analysis allows us to conclude that regions that are harder to reach (low-volume municipalities located in hilly areas, irrespective of distance traveled) have a higher cost to serve and higher emissions due to an increase in transportation costs, fuel usage, difficulties to consolidate cargo and difficulties to increase vehicle usage due to the low volume sales.",
        "authors": [
            "Leonardo Amazonas Machado",
            "Ricardo Chavelas Manzo",
            "Rodrigo Silva Tourinho Nakamura"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159028",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Minimizing last-mile emissions through altitude-aware route optimization",
        "abstract": "This study introduced an exact optimization approach to solve a new special type of Travelling Salesperson Problem. This problem considers time-windows restrictions and a new objective function—the objective regards driver assessment awareness and fuel consumption. The latter is modeled as a function of variable vehicle payload and terrain elevation data. This problem can be stated as the way to find the best route to service a set of customer demands, attempting to deliver within agreed time windows, mimicking paths that are as similar as possible to good routes executed in the past by experienced drivers, allowing small alterations as to reduce duration and fuel consumption. The authors proposed an innovative mixed integer linear programming formulation and a cluster decomposition approach that reduces search space and makes the approach applicable to solving real-world-sized problems. This model was parametrized using a small-sized mockup dataset and had its applicability tested on real data. The latter consisted of a public dataset containing trips executed and evaluated by real drivers of Amazon company. The results show that it was possible to reduce in -5.7% the fuel consumption in the routes of this dataset. Since this variable is directly related to emissions and pollution, this result shows that the suggested approach offers promising prospects for improving efficiency and reducing the carbon footprint of logistics last-mile operations. To the best of the authors' knowledge, this study contributes to the literature in that it is the first to jointly tackle driver assessment awareness and fuel consumption in a route optimization problem. Thus, it is also the first to propose a mathematical formulation and solution approach for this problem.",
        "authors": [
            "Gustavo De Abreu Rodrigues",
            "Gustavo Jimenez Ruan",
            "Jenny Carolina Amores"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159029",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Fulfillment models framework for e-commerce companies",
        "abstract": "E-commerce relevance is increasing, and companies should be prepared to fulfill customers’ expectations and ensure an optimal shopping experience. Online worldwide retail sales generated 70 billion U.S. dollars in 2019, being Mexico and Brazil the main leaders for this type of channel in LATAM (Chevalier, 2020).\r\nWith the objective of being more efficient and differentiate from competitors, it is vital to have an extremely consistent and aligned supply chain that follows the company's business strategy. To achieve this new challenge, the following study aims to generate a framework decision matrix, enabling companies to support decisions of introducing fresh, dry, refrigerated, and frozen product categories based on five major warehousing trends: distribution center, fulfillment center, dark-store, micro-fulfillment center and crowdsourced warehousing solutions. To develop this project a systematic literature review combining case studies, papers, research articles and experts’ validation will be implemented with the objective of establishing a framework that can be used to ensure strategies for the e-commerce retailers, thus they are able to serve and meet customer expectations regarding product quality, optimal price, and delivery time.",
        "authors": [
            "Juan Manuel Bellido",
            "Renata Cabrini Souza e Silva",
            "Dominique Gomez de la Luz"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159022",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Causal inference improving warehouse productivity: zoned storage and killer items",
        "abstract": "E-commerce companies need help with customer service experience: faster and more frequent deliveries. Then, order fulfillment becomes critical to establish a competitive advantage.\r\nThe main objective of this project is to determine whether a new key product assortment, called a class-based scattered storage policy, improves order-picking operations in one of the main warehouses of the sponsor company. This e-commerce firm operates in an emerging market.\r\nAs mentioned earlier, this project addresses the problem by running an A/B quasi-experiment in the warehouse, showing findings directly from a real context for the first time. For this purpose, the warehouse was split into control and treatment sections during peak season when speed is most required. The effect of the proposed storage policy is studied by comparing picking productivity through a two-sample t-test. The samples are chosen using the Coarsened Exact Matching algorithm to have similar data to analyze in observable characteristics.\r\nThe result of this work indicates that the class-based scattered storage policy does not lead to an improvement in picking productivity. It can be attributed to real-context features that are presented and discussed. Additionally, strong recommendations are given to include the findings in future research.",
        "authors": [
            "David Montemurri",
            "Hebe Adriana Herrera",
            "Maria Florencia Ghiglione"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159024",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The State of Supply Chain Sustainability in Brazil",
        "abstract": "As sustainability gains importance for consumers, employees, and investors, Supply Chain Sustainability (SCS) has become an increasingly important topic for companies. The State of Supply Chain Sustainability report, a co-presentation of the MIT Center for Transportation & Logistics and the Council of Supply Chain Management Professionals, provides a clear snapshot of this subject worldwide. Although it has been increasing its range of respondents, including Spanish and Mandarin Chinese translated surveys and the original English survey, Portuguese-speaking countries have not been fully reached. This project aims to understand the state of supply chain sustainability in Brazil, the largest country in Latin America, regarding area, population, and GDP. The State of Supply Chain Sustainability 2022 survey questions were translated to Brazilian Portuguese and applied in a local survey in Brazil with specific questions to capture particularities, such as the impact of regional tax benefits in supply chain-related decision-making. Advanced statistical models were applied to guarantee the quality of the translations and compare the local results with the past results of other countries.",
        "authors": [
            "Leonardo Gonzaga Moreira Sá C Faveret",
            "Marcelo Ikaro Carvalho Mesquita Braga",
            "Rodrigo Junqueira Nogueira"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159031",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Emerging membrane technologies for sustainable lithium extraction from brines and leachates: Innovations, challenges, and industrial scalability",
        "abstract": "This perspective critically examines challenges in advancing membrane-based technologies for lithium extraction from industrial brines, salt lakes, and battery leachates. The rapidly rising deployment of electric vehicles and renewable energy systems has intensified global lithium demand, necessitating sustainable and efficient extraction methods. Traditional techniques like brine evaporation and hard rock mining are environmentally detrimental due to high water usage, ecological disruption, and significant carbon emissions, compounded by geopolitical risks from resource concentration. Emerging membrane technologies, utilizing lithium-selective ligands, biomimetic ion channels, and two-dimensional and porous materials, can potentially realize orders-of-magnitude improvements in lithium selectivity for direct lithium extraction (DLE). However, the effectiveness of DLE membranes is constrained by impurity co-extraction, environmental hazards, lack of scalability and material instability. Conventional lithium brine concentration (LBC) techniques, which complement DLE by concentrating lithium for downstream applications like battery production, face challenges in hypersaline environments, such as fouling and reduced selectivity. Advances in electrodialysis and nanofiltration with surface modifications offer promising solutions to sustain favorable monovalent selectivity under high salinity conditions. Key gaps in the current research landscape include the absence of standardized testing procedures, evaluation metrics poorly suited to hypersaline or multi-ionic environments, scalability challenges in manufacturing, and economic limitations arising from fouling and material degradation. Addressing these issues requires material characterization with representative solution compositions, the development of comprehensive evaluation frameworks, and strategies for co-extracting valuable metals to improve economic viability. A holistic focus on membrane manufacturability, material durability, and process integration is essential to unlock sustainable lithium extraction technologies that can support the global shift to clean energy.",
        "authors": [
            "Zi Hao Foo",
            "John H. Lienhard"
        ],
        "journal_conference_name": "Desalination",
        "publisher": "Elsevier BV",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158972",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Evidence-Based Nutraceuticals Derived from Antrodia cinnamomea",
        "abstract": "Antrodia cinnamomea (A. cinnamomea), a medicinal and edible mushroom endemic to Taiwan, has been traditionally valued as a health tonic. Recent studies have highlighted the diverse specialized metabolites and bioactive potential of this substance, primarily attributed to key secondary metabolites such as benzenoids, maleic and succinic acids, ubiquinone, triterpenoids, and the primary metabolite polysaccharides. These compounds exhibit a broad spectrum of pharmacological properties, including those related to antibacterial, antitumor, anti-inflammation, hepatoprotection, hypoglycaemia, and antioxidant activities, and immunomodulation and gut microbiota regulation. These findings highlight the therapeutic potential of A. cinnamomea and its potential applications in health supplements and functional foods. This review evaluated recent advancements in the cultivation, extraction, and characterization of bioactive compounds from A. cinnamomea, with a particular focus on submerged and solid-state fermentation methods. We hope to provide a comprehensive framework for promoting the efficient and scientific evidence based utilization of A. cinnamomea in novel therapeutic strategies and health-related innovations.",
        "authors": [
            "Chunyuhang Xu",
            "Qingtong Xie",
            "Chien-Liang Kuo",
            "Xin Yang",
            "Dejian Huang"
        ],
        "journal_conference_name": "Foods",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159074",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Evaluating the Impacts of Swapping on the US Decennial Census",
        "abstract": "To meet its dual burdens of providing useful statistics and ensuring privacy of individual respondents, the US Census Bureau has for decades introduced some form of \"noise\" into published statistics. Initially, they used a method known as \"swapping\" (1990-2010). In 2020, they switched to an algorithm called TopDown that ensures a form of Differential Privacy. While the TopDown algorithm has been made public, no implementation of swapping has been released and many details of the deployed swapping methodology deployed have been kept secret. Further, the Bureau has not published (even a synthetic) \"original\" dataset and its swapped version. It is therefore difficult to evaluate the effects of swapping, and to compare these effects to those of other privacy technologies. To address these difficulties we describe and implement a parameterized swapping algorithm based on Census publications, court documents, and informal interviews with Census employees. With this implementation, we characterize the impacts of swapping on a range of statistical quantities of interest. We provide intuition for the types of shifts induced by swapping and compare against those introduced by TopDown. We find that even when swapping and TopDown introduce errors of similar magnitude, the direction in which statistics are biased need not be the same across the two techniques. More broadly, our implementation provides researchers with the tools to analyze and potentially correct for the impacts of disclosure avoidance systems on the quantities they study.",
        "authors": [
            "Mar?a Ballesteros",
            "Cynthia Dwork",
            "Gary King",
            "Conlan Olson",
            "Manish Raghavan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Symposium on Computer Science and Law",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159046",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "What Constitutes a Less Discriminatory Algorithm?",
        "abstract": "Disparate impact doctrine offers an important legal apparatus for targeting discriminatory data-driven algorithmic decisions. A recent body of work has focused on conceptualizing one particular construct from this doctrine: the less discriminatory alternative, an alternative policy that reduces disparities while meeting the same business needs of a status quo or baseline policy. However, attempts to operationalize this construct in the algorithmic setting must grapple with some thorny challenges and ambiguities. In this paper, we attempt to raise and resolve important questions about less discriminatory algorithms (LDAs). How should we formally define LDAs, and how does this interact with different societal goals they might serve? And how feasible is it for firms or plaintiffs to computationally search for candidate LDAs? We find that formal LDA definitions face fundamental challenges when they attempt to evaluate and compare predictive models in the absence of held-out data. As a result, we argue that LDA definitions cannot be purely quantitative, and must rely on standards of \"reasonableness.\" We then raise both mathematical and computational constraints on firms' ability to efficiently conduct a proactive search for LDAs, but we provide evidence that these limits are \"weak\" in a formal sense. By defining LDAs formally, we put forward a framework in which both firms and plaintiffs can search for alternative models that comport with societal goals.",
        "authors": [
            "Benjamin Laufer",
            "Manish Raghavan",
            "Solon Barocas"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Symposium on Computer Science and Law",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159048",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Pluto: Authoring Semantically Aligned Text and Charts for Data-Driven Communication",
        "abstract": "Textual content (including titles, annotations, and captions) plays a central role in helping readers understand a visualization by emphasizing, contextualizing, or summarizing the depicted data. Yet, existing visualization tools provide limited support for jointly authoring the two modalities of text and visuals such that both convey semantically-rich information and are cohesively integrated. In response, we introduce Pluto, a mixed-initiative authoring system that uses features of a chart’s construction (e.g., visual encodings) as well as any textual descriptions a user may have drafted to make suggestions about the content and presentation of the two modalities. For instance, a user can begin to type out a description and interactively brush a region of interest in the chart, and Pluto will generate a relevant auto-completion of the sentence. Similarly, based on a written description, Pluto may suggest lifting a sentence out as an annotation or the visualization’s title, or may suggest applying a data transformation (e.g., sort) to better align the two modalities. A preliminary user study revealed that Pluto’s recommendations were particularly useful for bootstrapping the authoring process and helped identify different strategies participants adopt when jointly authoring text and charts. Based on study feedback, we discuss design implications for integrating interactive verification features between charts and text, offering control over text verbosity and tone, and enhancing the bidirectional flow in unified text and chart authoring tools.",
        "authors": [
            "Arjun Srinivasan",
            "Vidya Setlur",
            "Arvind Satyanarayan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|30th International Conference on Intelligent User Interfaces",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159033",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "MIND (Mixed-Initiative Next-gen Design): Workshop on Blending Agents and Direct Manipulation for Harnessing LLMs",
        "abstract": "Since the 1980s, a key debate in human-centered computing involving machine learning at IUI is between agent-driven systems and direct manipulation. The explosion of Large Language Models (LLMs), particularly auto-regressive as agents serving as chatbots, generative search, and work automation tools, has also brought with it inherent limitations. We posit that efforts to address and alleviate these LLM challenges—hallucinations, unpredictable outputs, lack of transparency, and difficulties in customization—cannot be solved through algorithmic improvements alone but require elevated mixed-initiative interface design at the heart of the IUI community. This workshop aims to bridge the gap between agent-driven automation and direct manipulation by exploring mixed-initiative interaction models that blend the strengths of both paradigms to empower end-users seeking to harness LLMs.",
        "authors": [
            "Karthik Dinakar",
            "Henry Lieberman",
            "Sonia Wu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|30th International Conference on Intelligent User Interfaces Companion",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159042",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "MemPal: Leveraging Multimodal AI and LLMs for Voice-Activated Object Retrieval in Homes of Older Adults",
        "abstract": "Older adults have increasing difficulty with retrospective memory, hindering their abilities to perform daily activities and posing stress on caregivers to ensure their wellbeing. Recent developments in Artificial Intelligence (AI) and large context-aware multimodal models offer an opportunity to create memory support systems that assist older adults with common issues like object finding. This paper discusses the development of an AI-based, wearable memory assistant, MemPal, that helps older adults with a common problem, finding lost objects at home, and presents results from tests of the system in older adults’ own homes. Using visual context from a wearable camera, the multimodal LLM system creates a real-time automated text diary of the person’s activities for memory support purposes, offering object retrieval assistance using a voice-based interface. The system is designed to support additional use cases like context-based proactive safety reminders and recall of past actions. We report on a quantitative and qualitative study with N=15 older adults within their own homes that showed improved performance of object finding with audio-based assistance compared to no aid and positive overall user perceptions on the designed system. We discuss further applications of MemPal’s design as a multi-purpose memory aid and future design guidelines to adapt memory assistants to older adults’ unique needs.",
        "authors": [
            "Natasha Maniar",
            "Samantha Chan",
            "Wazeer Zulfikar",
            "Scott Ren",
            "Christine Xu",
            "Pattie Maes"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|30th International Conference on Intelligent User Interfaces",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159037",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Coalesce: An Accessible Mixed-Initiative System for Designing Community-Centric Questionnaires",
        "abstract": "Effectively incorporating community input into civic decision-making processes is crucial for fostering inclusive governance. However, public officials often face challenges in formulating effective questions to gather meaningful insights due to constraints such as time, resources, and limited experience in questionnaire design. This paper explores the potential of leveraging large language models (LLMs) to address this challenge. We present Coalesce, a novel mixed-initiative system that utilizes LLMs to assist civic leaders in crafting tailored and impactful questions for surveys, interviews, and conversation guides. Guided by best practices in questionnaire design, Coalesce improves question readability, enhances specificity, and reduces bias. To inform our design, we conducted a formative interview study with 30 civic leaders and implemented an iterative human-centered design process involving 14 feedback sessions. We built a fully-functional system before evaluating it through a real-world user study with 16 participants who applied the platform to their own community engagement projects. Our findings show that Coalesce improved participants’ confidence in questionnaire design, supported diverse workflows, and fostered learning while raising important questions about human agency and over-reliance on AI. These insights highlight the potential for intelligent user interfaces to reshape how civic leaders engage with their communities, fostering more informed and inclusive decision-making processes.",
        "authors": [
            "Cassandra Overney",
            "Daniel Kessler",
            "Suyash Fulay",
            "Mahmood Jasim",
            "Deb Roy"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|30th International Conference on Intelligent User Interfaces",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159053",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Scientific Advancements in Gene Therapies: Opportunities for Global Regulatory Convergence",
        "abstract": "On 4 September 2024, the Reagan-Udall Foundation for the FDA (FDA Foundation) in collaboration with the Food and Drug Administration (FDA) and the Gates Foundation hosted a workshop titled “Scientific Advancements in Gene Therapies: Opportunities for Global Regulatory Convergence”. The event brought together a diverse group of experts, including international regulatory bodies, regulated industries, healthcare professionals, patients, academic researchers and global health advocates, to discuss the rapid advancements in gene therapy and the pressing need for equitable access in low-and middle-income countries (LMICs), with sickle cell disease (SCD) serving as the model disorder for the discussions. Although there has been significant progress in gene therapy, such as breakthroughs in clustered regularly interspaced short palindromic repeats (CRISPR)-based technologies and FDA-approved therapies, access to these therapies remain limited in underresourced regions. The workshop addressed critical challenges, including the high cost of therapies, regulatory gaps and barriers and ethical concerns regarding informed consent and public engagement in LMICs. This paper highlights the critical discussion points from the workshop with a focus on exploring strategies for global regulatory convergence, the role of international collaborations and the potential pathways to making gene therapies affordable and accessible to all.",
        "authors": [
            "Jimi Olaghere",
            "David A. Williams",
            "Jeremy Farrar",
            "Hildegard Büning",
            "Cecelia Calhoun",
            "Tony Ho",
            "Maneesha S. Inamdar",
            "David Liu",
            "Julie Makani",
            "Kwasi Nyarko",
            "Sol Ruiz",
            "John Tisdale",
            "Joseph M. McCune",
            "Esther Boadi",
            "Reagan-Udall Foundation for the FDA"
        ],
        "journal_conference_name": "Biomedicines",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159014",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "High-Connectivity Triazolate-Based Metal–Organic Framework for Water Harvesting",
        "abstract": "Increasing the connectivity of structural units presents a potentially valuable approach to improve hydrolytic stability in metal–organic frameworks (MOFs). We herein leverage this strategy by synthesizing the first tritopic benzotriazolate MOF, Zn5(OAc)4(TBTT)2 (H3TBTT = 2,4,6-tris(1H-benzo[d][1,2,3]triazol-5-yl)-1,3,5-triazine), which exhibits open metal sites, high connectivity, high porosity, and significant water uptake capacity. The MOF adopts a previously unknown topology with (3,6,6)-connectivity, which is supported by single-crystal electron diffraction and elemental analysis. The framework undergoes postsynthetic metal and anion exchange with NiCl2, which increases the accessible pore volume and the net hydrophilicity of the framework. With this exchange, the apparent BET surface area increases from 1994 to 3034 m2/g, and the water uptake step shifts from 56 to 33% relative humidity (RH). The high gravimetric capacity of the Ni-rich MOF, 0.98 g/g, translates to a working capacity of 0.64 g/g during a pressure swing cycle between 20 and 40% RH at 25 °C. Combining this performance with a less than 2% loss in working capacity over 100 cycles, the new material rivals the best MOF water sorbents to date.",
        "authors": [
            "Karla Ravin",
            "Patrick Sarver",
            "Bhavish Dinakar",
            "Lukáš Palatinus",
            "Peter Müller",
            "Julius Oppenheim",
            "Mircea Dincă"
        ],
        "journal_conference_name": "Journal of the American Chemical Society",
        "publisher": "American Chemical Society",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158533",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Brain Markers of Resilience to Psychosis in High-Risk Individuals: A Systematic Review and Label-Based Meta-Analysis of Multimodal MRI Studies",
        "abstract": "Background/Objectives: Most individuals who have a familial or clinical risk of developing psychosis remain free from psychopathology. Identifying neural markers of resilience in these at-risk individuals may help clarify underlying mechanisms and yield novel targets for early intervention. However, in contrast to studies on risk biomarkers, studies on neural markers of resilience to psychosis are scarce. The current study aimed to identify potential brain markers of resilience to psychosis. Methods: A systematic review of the literature yielded a total of 43 MRI studies that reported resilience-associated brain changes in individuals with an elevated risk for psychosis. Label-based meta-analysis was used to synthesize findings across MRI modalities. Results: Resilience-associated brain changes were significantly overreported in the default mode and language network, and among highly connected and central brain regions. Conclusions: These findings suggest that the DMN and language-associated areas and central brain hubs may be hotspots for resilience-associated brain changes. These neural systems are thus of key interest as targets of inquiry and, possibly, intervention in at-risk populations.",
        "authors": [
            "Guusje Collin",
            "Joshua E. Goldenberg",
            "Xiao Chang",
            "Zhenghan Qi",
            "Susan Whitfield-Gabrieli",
            "Wiepke Cahn",
            "Jijun Wang",
            "William S. Stone",
            "Matcheri S. Keshavan",
            "Martha E. Shenton"
        ],
        "journal_conference_name": "Brain Sciences",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159013",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Design and Deployment of a Self-Powered, LoRaWAN-Based IoT Environment Sensor Ensemble for Integrated Air Quality Sensing and Simulation",
        "abstract": "The goal of this study is to describe a design architecture for a self-powered IoT (Internet of Things) sensor network that is currently being deployed at various locations throughout the Dallas-Fort Worth metroplex to measure and report on Particulate Matter (PM) concentrations. This system leverages diverse low-cost PM sensors, enhanced by machine learning for sensor calibration, with LoRaWAN connectivity for long-range data transmission. Sensors are GPS-enabled, allowing precise geospatial mapping of collected data, which can be integrated with urban air quality forecasting models and operational forecasting systems. To achieve energy self-sufficiency, the system uses a small-scale solar-powered solution, allowing it to operate independently from the grid, making it both cost-effective and suitable for remote locations. This novel approach leverages multiple operational modes based on power availability to optimize energy efficiency and prevent downtime. By dynamically adjusting system behavior according to power conditions, it ensures continuous operation while conserving energy during periods of reduced supply. This innovative strategy significantly enhances performance and resource management, improving system reliability and sustainability. This IoT network provides localized real-time air quality data, which has significant public health benefits, especially for vulnerable populations in densely populated urban environments. The project demonstrates the synergy between IoT sensor data, machine learning-enhanced calibration, and forecasting methods, contributing to scientific understanding of microenvironments, human exposure, and public health impacts of urban air quality. In addition, this study emphasizes open source design principles, promoting transparency, data quality, and reproducibility by exploring cost-effective sensor calibration techniques and adhering to open data standards. The next iteration of the sensors will include edge processing for short-term air quality forecasts. This work underscores the transformative role of low-cost sensor networks in urban air quality monitoring, advancing equitable policy development and empowering communities to address pollution challenges.",
        "authors": [
            "Lakitha O. H. Wijeratne",
            "Daniel Kiv",
            "John Waczak",
            "Prabuddha Dewage",
            "Gokul Balagopal",
            "Mazhar Iqbal",
            "Adam Aker",
            "Bharana Fernando",
            "Matthew Lary",
            "Vinu Sooriyaarachchi",
            "Rittik Patra",
            "Nora Desmond",
            "Hannah Zabiepour",
            "Darren Xi",
            "Vardhan Agnihotri",
            "Seth Lee",
            "Chris Simmons",
            "David J. Lary"
        ],
        "journal_conference_name": "Air",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159012",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Learning-Augmented Competitive Algorithms for Spatiotemporal Online Allocation with Deadline Constraints",
        "abstract": "We introduce and study spatiotemporal online allocation with deadline constraints (SOAD), a new online problem motivated by emerging challenges in sustainability and energy.  In SOAD, an online player completes a workload by allocating and scheduling it on the points of a metric space $(X, d)$ while subject to a deadline $T$.  At each time step, a service cost function is revealed that represents the cost of servicing the workload at each point, and the player must irrevocably decide the current allocation of work to points.  Whenever the player moves this allocation, they incur a movement cost defined by the distance metric $d(\\cdot, \\ \\cdot)$ that captures, e.g., an overhead cost.  SOAD formalizes the open problem of combining general metrics and deadline constraints in the online algorithms literature, unifying problems such as metrical task systems and online search.  We propose a competitive algorithm for SOAD along with a matching lower bound establishing its optimality.  Our main algorithm, ST-CLIP, is a learning-augmented algorithm that takes advantage of predictions (e.g., forecasts of relevant costs) and achieves an optimal consistency-robustness trade-off.  We evaluate our proposed algorithms in a simulated case study of carbon-aware spatiotemporal workload management, an application in sustainable computing that schedules a delay-tolerant batch compute job on a distributed network of data centers.  In these experiments, we show that ST-CLIP substantially improves on heuristic baseline methods.",
        "authors": [
            "Adam Lechowicz",
            "Nicolas Christianson",
            "Bo Sun",
            "Noman Bashir",
            "Mohammad Hajiesmaili",
            "Adam Wierman",
            "Prashant Shenoy"
        ],
        "journal_conference_name": "Proceedings of the ACM on Measurement and Analysis of Computing Systems",
        "publisher": "ACM",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159050",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Secure finite-time filtering for switched fuzzy systems with scaling attacks and stochastic sensor faults",
        "abstract": "In this study, we introduce a design for robust secure finite-time mixed H ∞ and passivity filter for discrete-time switched fuzzy systems. This design effectively combats both stochastic scaling attacks and sensor failure. To be specific, the sensor signals are represented by stochastic variables with different failure rates. Also, a comprehensive model is presented to characterize the scaling attacks and it is described by the Bernoulli distributed random variable. By designing a suitable Lyapunov functional candidate and leveraging the principles of finite-time theory, we have formulated a new collection of sufficient conditions. These conditions, expressed as linear matrix inequalities, ensure that the augmented fuzzy system maintains robust stochastic finite-time boundedness, along with a predetermined mixed H ∞ and passivity performance index. Ultimately, two numerical demonstrations are provided, incorporating real-world applications from the continuous-time single-link robot arm model and the tunnel diode circuit systems, to highlight the practicality of the proposed secure filter design.",
        "authors": [
            "Murugesan Sathishkumar",
            "Maya Joby",
            "Yong-Ki Ma",
            "Selvaraj M. Anthoni",
            "Srimanta Santra"
        ],
        "journal_conference_name": "Nonlinear Dynamics",
        "publisher": "Springer Netherlands",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159070",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "OpenEarable 2.0: Open-Source Earphone Platform for Physiological Ear Sensing",
        "abstract": "Earphones have evolved from pure audio devices to \"earables\" that are capable of advanced sensing. Bespoke research devices have shown the unique sensing capabilities of the earable platform; however, they are hard to replicate and require expertise to develop in the first place. In this paper, we present OpenEarable 2.0 - an open source, unified platform that integrates a larger number of sensors for conducting comprehensive earable research. OpenEarable 2.0 works as regular binaural Bluetooth earphones and features two ultrasound capable microphones (inward/outward), a 3-axis ear canal accelerometer/bone microphone, a 9-axis head inertial measurement unit, pulse oximeter, optical temperature sensor, ear canal pressure sensor, and microSD card. These capabilities allow for the detection and measurement of 30+ phenomena on the ear that can be used across a wide range of applications in health monitoring, activity tracking, human-computer-interaction and authentication. We describe the design and development of OpenEarable 2.0 which follows best open hardware practices and achieves commercial-level wearability. We provide justification for the selection and placement of integrated sensors and include in-depth descriptions of the extensible, open source firmware and hardware that are implemented using free to use tools and frameworks. For real-time sensor control and data recording we also contribute a web-based dashboard and mobile smartphone app. The wearability and ability to sense different phenomena are validated in four studies which showcases how OpenEarable 2.0 provides accurate measurements in comparison to established gold-standard measurements. We further demonstrate that OpenEarable 2.0 can be assembled by inexperienced users, and that undergraduate students can build applications using the OpenEarable platform.",
        "authors": [
            "Tobias R?ddiger",
            "Michael K?ttner",
            "Philipp Lepold",
            "Tobias King",
            "Dennis Moschina",
            "Oliver Bagge",
            "Joseph Paradiso",
            "Christopher Clarke",
            "Michael Beigl"
        ],
        "journal_conference_name": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "publisher": "ACM",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159051",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Picto: Crafting Remote Tangible Gestures via Recordable, Replayable, and Shareable Motions",
        "abstract": "We introduce Picto, a paired tangible interface that enables intimate dyads to co-create shared kinetic messages, fostering playful remote communication beyond temporal and physical constraints. Picto’s two modular units—a knob for rotational motion and a slider for linear motion—allow users to craft personalized motions and shapes symbolizing their significant other. Presence can be conveyed in real-time or asynchronously through record, replay, and share features. Picto empowers users to express abstract ideas through iconic gestures and non-verbal cues. Using a bistable composite tape-spring structure, we developed a novel mechanism for programming dynamic shape variations and motions. Picto’s control system records, stores, and shares motion-based interactions. A user study with intimate dyads evaluates Picto’s usability and its potential as a remote story-sharing platform and ambient presence media enhanced by metaphorical and beat gestures. The results highlight its potential to enrich and sustain intimate relationships, supporting social presence across distances.",
        "authors": [
            "Kyung Yun Choi",
            "Taehee Jung",
            "Noble Harasha",
            "Hiroshi Ishii"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Nineteenth International Conference on Tangible, Embedded, and Embodied Interaction",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158328",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Performance Analysis for High-Dimensional Bell-State Quantum Illumination",
        "abstract": "Quantum illumination (QI) is an entanglement-based protocol for improving LiDAR/radar detection of unresolved targets beyond what a classical LiDAR/radar of the same average transmitted energy can do. Originally proposed by Seth Lloyd as a discrete-variable quantum LiDAR, it was soon shown that his proposal offered no quantum advantage over its best classical competitor. Continuous-variable, specifically Gaussian-state, QI has been shown to offer a true quantum advantage, both in theory and in table-top experiments. Moreover, despite its considerable drawbacks, the microwave version of Gaussian-state QI continues to attract research attention. A recent QI study by Armanpreet Pannu, Amr Helmy, and Hesham El Gamal (PHE), however, has: (i) combined the entangled state from Lloyd’s QI with the channel models from Gaussian-state QI; (ii) proposed a new positive operator-valued measurement for that composite setup; and (iii) claimed that, unlike Gaussian-state QI, PHE QI achieves the Nair–Gu lower bound on QI target-detection error probability at all noise brightnesses. PHE’s analysis was asymptotic, i.e., it presumed infinite-dimensional entanglement. The current paper works out the finite-dimensional performance of PHE QI. It shows that there is a threshold value for the entangled-state dimensionality below which there is no quantum advantage, and above which the Nair–Gu bound is approached asymptotically. Moreover, with both systems operating with error-probability exponents 1 dB lower than the Nair–Gu bound, PHE QI requires enormously higher entangled-state dimensionality than does Gaussian-state QI to achieve useful error probabilities in both high-brightness (100 photons/mode) and moderate-brightness (1 photon/mode) noise. Furthermore, neither system has an appreciable quantum advantage in low-brightness (much less than 1 photon/mode) noise.",
        "authors": [
            "Jeffrey H. Shapiro"
        ],
        "journal_conference_name": "Physics",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158998",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Choice Vectors: Streamlining Personal AI Alignment Through Binary Selection",
        "abstract": "Value alignment for AI is not “one-size-fits-all”: even polite and friendly models can still fail to represent individual user contexts and preferences, and local cultural norms. This paper presents a modular workflow for personal fine-tuning, synthesizing four core components from our previous research: (1) robust vectorization of user values and preferences, (2) a binary choice user interface (UI) approach to capturing those preferences with minimal cognitive load, (3) contrastive activation methods for steering large language models (LLMs) via difference vectors, and (4) knowledge graph integration for more auditable and structured alignment. Our approach—descended from past research on “Towards an End-to-End Personal Fine-Tuning Framework”—demonstrates how these elements can be combined to create personalized, context-rich alignment solutions. We report on user studies for the forced-choice UI, describe an experimental pipeline for deriving “control vectors”, and propose a “moral graph” method for bridging symbolic and vector-based alignment. Our findings suggest that multi-pronged personalization can significantly reduce user annotation fatigue, improve alignment fidelity, and allow for more flexible, interpretable AI behaviors.",
        "authors": [
            "Eleanor Watson",
            "Minh Nguyen",
            "Sarah Pan",
            "Shujun Zhang"
        ],
        "journal_conference_name": "Multimodal Technologies and Interactions",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158997",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The ultra-thin conception of objecthood",
        "abstract": "In his excellent book Thin Objects, Øystein Linnebo develops a conception of\r\nobjecthood that allows for thin objects: objects whose ‘existence does not\r\nmake a substantial demand on the world’ (p. 4). His proposal is premised on\r\nthe Fregean dictum that to be an object is to be the referent of a possible\r\nsingular term (p. 22). As a result, much of Linnebo’s argumentation is focused\r\non defending a ‘thin’ conception of reference, which is liberal enough to\r\nallow for thin objects. This paper is a critique of Linnebo’s conception of\r\nreference.",
        "authors": [
            "Agustín Rayo"
        ],
        "journal_conference_name": "An Interdisciplinary Journal of Philosophy",
        "publisher": "Taylor & Francis",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159049",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "SySTeC: A Symmetric Sparse Tensor Compiler",
        "abstract": "Symmetric and sparse tensors arise naturally in many domains including linear algebra, statistics, physics, chemistry, and graph theory. Symmetric tensors are equal to their transposes, so in the n-dimensional case we can save up to a factor of n! by avoiding redundant operations. Sparse tensors, on the other hand, are mostly zero, and we can save asymptotically by processing only nonzeros. Unfortunately, specializing for both symmetry and sparsity at the same time is uniquely challenging. Optimizing for symmetry requires consideration of n! transpositions of a triangular kernel, which can be complex and error prone. Considering multiple transposed iteration orders and triangular loop bounds also complicates iteration through intricate sparse tensor formats. Additionally, since each combination of symmetry and sparse tensor formats requires a specialized implementation, this leads to a combinatorial number of cases. A compiler is needed, but existing compilers cannot take advantage of both symmetry and sparsity within the same kernel. In this paper, we describe the first compiler which can automatically generate symmetry-aware code for sparse or structured tensor kernels. We introduce a taxonomy for symmetry in tensor kernels, and show how to target each kind of symmetry. Our implementation demonstrates significant speedups ranging from 1.36x for SSYMV to 30.4x for a 5-dimensional MTTKRP over the non-symmetric state of the art.",
        "authors": [
            "Radha Patel",
            "Willow Ahrens",
            "Saman Amarasinghe"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158438",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Hands-On Quantum Cryptography: Experimentation with the B92 Protocol Using Pulsed Lasers",
        "abstract": "Quantum cryptography continues to be an area of significant research and educational interest. Here, a straightforward and reliable approach to both the experimental and theoretical aspects of quantum key distribution is presented, tailored for senior undergraduate students. Focusing on illustrating the essential concepts of the B92 protocol through a combination of optical experiments and custom-developed computational tools, this work offers a thorough exploration of quantum cryptography according to the principles of the B92 protocol.",
        "authors": [
            "Sara P. Gandelman",
            "Alona Maslennikov",
            "Georgi Gary Rozenman"
        ],
        "journal_conference_name": "Photonics",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158996",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Structurally Similar Mycotoxins Aflatoxin B1 and Sterigmatocystin Trigger Different and Distinctive High-Resolution Mutational Spectra in Mammalian Cells",
        "abstract": "Aflatoxin B1 (AFB1) and sterigmatocystin (ST) are mycotoxins that pose significant threats to human and animal health owing to their mutagenic, carcinogenic, and toxic properties. They are structurally similar and widely believed to exert their biological effects via the generation of DNA-damaging epoxides at their respective terminal furan rings. Despite structural identity in the warhead portion of each toxin, this work shows that distal parts of each molecule are responsible for the distinctive mutational fingerprints seen in gptΔ C57BL/6J mouse embryo fibroblasts (MEFs). The two toxins differ structurally in the puckered cyclopentenone ring of AFB1 and in the planar xanthone functionality of ST. While both toxins mainly induce GC→TA mutations, the aforementioned differences in structure apparently trigger unique patterns of mutations, as revealed by high-resolution duplex sequencing of MEF genomes. AFB1 is more mutagenic than ST and displays its transversion mutations in a pattern with primary and secondary hotspots (underscored) in 5′-CGC-3′ and 5′-CGG-3′ contexts, respectively. ST displays a modest 5′-CGG-3′ hotspot while its other GC→TA transversions are more uniformly distributed in a pattern resembling established oxidative stress mutational spectra. This research delineates the mutational spectra of AFB1 and ST, establishing these patterns as possible early-onset biomarkers of exposure.",
        "authors": [
            "Pennapa Thongararm",
            "Marisa Chancharoen",
            "Nutchapong Suwanwong",
            "Somsak Ruchirawat",
            "Mathuros Ruchirawat",
            "Bogdan I. Fedeles",
            "Robert G. Croy",
            "John M. Essigmann"
        ],
        "journal_conference_name": "Toxins",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158993",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Influence of Religiosity on Muslim Women’s Selection of Fund Providers in Malaysia",
        "abstract": "The purpose of this study is to analyze the factors influencing the attitudes of women investors in the context of Islamic unit trust funds in Malaysia, with a focus on women&rsquo;s religiosity and on the perceived religiosity of fund providers. Using the UTAUT model, the study examines data from a survey of 263 Muslim women in Malaysia and considers seven key factors: risk aversion, religiosity, price sensitivity, and Islamic financial literacy on the side of the investing women and past performance, perceived religiosity, and perceived risk on the side of the fund providers. The findings indicate that the perceived religiosity of a fund provider has a significant and positive impact on attitude, with positive moderating effects on the women&rsquo;s own religiosity and Islamic financial literacy, and a negative moderating effect on the women&rsquo;s price sensitivity. The study also discusses the practical implications of these findings and offers recommendations for fund providers.",
        "authors": [
            "Salim Bouzekouk",
            "Fadillah Mansor"
        ],
        "journal_conference_name": "Journal of Risk and Financial Management",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158992",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Portable High-Resolution Snapshot Multispectral Imaging Device Leveraging Spatial and Spectral Features for Non-Invasive Corn Nitrogen Treatment Classification",
        "abstract": "Spectral imaging has been widely applied in plant phenotyping to assess corn leaf nitrogen status. Recent studies indicate that spatial variations within a single leaf’s multispectral image provide stronger signals for corn nitrogen estimation. However, current technologies for corn multispectral imaging cannot capture a large corn leaf segment with high-resolution and simple operation, limiting their efficiency and accuracy in nitrogen estimation. To address this gap, this study developed a proximal multispectral imaging device that can capture high-resolution snapshot multispectral images of a large segment of a single corn leaf. This device uses airflow to autonomously position and flatten the leaf to minimize the noise in images due to leaf curvature and simplify operation. Moreover, this device adopts a transmittance imaging regime by clamping the corn leaf between the camera and the lighting source to block the environmental lights and supply uniform lighting to capture high-resolution and high-precision leaf images within six seconds. A field assay was conducted to validate the effectiveness of the multispectral images captured by this device in assessing nitrogen status by classifying the nitrogen treatments applied to corn. Six nitrogen treatments were applied to 12 plots of corn fields, and 10 images were collected at each plot. By using the average vegetative index of the whole image, only one treatment was significantly different from the other five treatments, and no significant difference was observed among any other groups. However, by extracting the spatial and spectral features from the images and combining these features, the accuracy of nitrogen treatment classification improved compared to using the average index. In another analysis, by applying spatial–spectral analysis methods to the images, the nitrogen treatment classification accuracy has improved compared to using the average index. These results demonstrated the advantages of this high-resolution and high-throughput imaging device for distinguishing nitrogen treatments by facilitating spatial–spectral combined analysis for more precise classification.",
        "authors": [
            "Xuan Li",
            "Zhongzhong Niu",
            "Ana Gabriela Morales-Ona",
            "Ziling Chen",
            "Tianzhang Zhao",
            "Daniel J. Quinn",
            "Jian Jin"
        ],
        "journal_conference_name": "Sensors",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158525",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Utilization of Classification Learning Algorithms for Upper-Body Non-Cyclic Motion Prediction",
        "abstract": "This study explores two methods of predicting non-cyclic upper-body motions using classification algorithms. Exoskeletons currently face challenges with low fluency, hypothesized to be in part caused by the lag in active control innate in many leader&ndash;follower paradigms seen in today&rsquo;s systems, leading to energetic inefficiencies and discomfort. To address this, we employ k-nearest neighbor (KNN) and deep learning models to predict motion characteristics, such as magnitude and category, from surface electromyography (sEMG) signals. Data were collected from six muscles located around the elbow. The sEMG signals were processed to identify significant activation changes. Two classification approaches were utilized: a KNN algorithm that categorizes motion based on the slopes of processed sEMG signals at change points and a deep neural network employing continuous categorization. Both methods demonstrated the capability to predict future voluntary non-cyclic motions up to and beyond commonly acknowledged electromechanical delay times, with the deep learning model able to predict, with certainty at or beyond 90%, motion characteristics even prior to myoelectric activation of the muscles involved. Our findings indicate that these classification algorithms can be used to predict upper-body non-cyclic motions to potentially increase machine interfacing fluency. Further exploration into regression-based prediction models could enhance the precision of these predictions, and further work could explore their effects on fluency when utilized in a tandem or wearable robotic application.",
        "authors": [
            "Bon H. Koo",
            "Ho Chit Siu",
            "Dava J. Newman",
            "Ellen T. Roche",
            "Lonnie G. Petersen"
        ],
        "journal_conference_name": "Sensors",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158524",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Content Analysis of E-Participation Platforms in Taiwan with Topic Modeling: How to Train and Evaluate Neural Topic Models?",
        "abstract": "E-participation platforms, such as iVoting and Join in Taiwan, provide digital spaces for citizens to engage in deliberation, voting, and oversight. As a forerunner in Asia, Taiwan has implemented these platforms to enhance participatory democracy. However, there is still limited research on the specific content debated on these platforms. Utilising recent advancements in Natural Language Processing, the content of proposals that users have submitted between 2015 and 2025 is explored. In this study, a pipeline for mining text corpora scraped from these platforms in the context of political analysis is proposed. The pipeline is applied to two datasets which have different characteristics. A topic model for each of the two platforms is generated and later evaluated with OCTIS (Optimizing and Comparing Topic Models Is Simple) and compared to different baselines. Our research highlights the trade-offs between model performance and processing time, emphasizing the balance between accuracy and meaningful topic creation. By integrating a translation pipeline from Chinese to English within the text-mining process, our method also demonstrates a solid approach to overcome language barriers. Consequently, our method is adaptable to e-participation platforms in various languages, providing decision-makers with a more comprehensive tool to understand citizens’ needs and enabling the formulation of more informed and effective policies.",
        "authors": [
            "Moritz Sontheimer",
            "Jonas Fahlbusch",
            "Shuo-Yan Chou",
            "Yu-Lin Kuo"
        ],
        "journal_conference_name": "Applied Sciences",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158523",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Diffusion and Percolation: How COVID-19 Spread Through Populations",
        "abstract": "I rely on the key concepts of diffusion and percolation to characterize the sequential but overlapping phases of the spread of infection through entire populations during the first year of the COVID-19 pandemic. Data from Los Angeles County demonstrate an extended initial diffusion phase propelled by radial geographic spread, followed by percolation within hotspots fueled by the presence of multigenerational households. Data from New York City, by contrast, reveal rapid initial diffusion along a unique, extensive subway network. Subsequent percolation within multiple hotspots, similarly powered by a high density of multigenerational households, exerted a positive feedback effect that further enhanced diffusion. Data from Florida counties support the generality of the phenomenon of viral transmission from more mobile, younger individuals to less mobile, older individuals. Data from the South Brooklyn hotspot reveal the limitations of some forms of government regulation in controlling mobility patterns that were critical to the continued percolation of the viral infection. Data from a COVID-19 outbreak at the University of Wisconsin&mdash;Madison demonstrate the critical role of a cluster of off-campus bars as an attractor for the continued percolation of infection. The evidence also demonstrates the efficacy of quarantine as a control strategy when the hotspot is contained and well identified.",
        "authors": [
            "Jeffrey E. Harris"
        ],
        "journal_conference_name": "Populations",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158991",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Point-of-Care No-Specimen Diagnostic Platform Using Machine Learning and Raman Spectroscopy: Proof-of-Concept Studies for Both COVID-19 and Blood Glucose",
        "abstract": "Significance: We describe a novel, specimen-free diagnostic platform that can immediately detect both a metabolite (glucose) or an infection (COVID-19) by non-invasively using Raman spectroscopy and machine learning. Aim: Current diagnostic testing for infections and glucose monitoring requires specimens, disease-specific reagents and processing, and it increases environmental waste. We propose a new hardware&ndash;software paradigm by designing and constructing a finger-scanning hardware device to acquire Raman spectroscopy readouts which, by varying the machine learning algorithm to interpret the data, allows for diverse diagnoses. Approach: A total of 455 patients were enrolled prospectively in the COVID-19 study; 148 tested positive and 307 tested negative through nasal PCR testing conducted concurrently with testing using our viral detector. The tests were performed on both outpatients (N = 382) and inpatients (N = 73) at Holy Name Medical Center in Teaneck, NJ, between June 2021 and August 2022. Patients&rsquo; fingers were scanned using an 830 nm Raman System and then, using machine learning, processed to provide an immediate result. In a separate study between April 2023 and August 2023, measurements using the same device and scanning a finger were used to detect blood glucose levels. Using a Dexcom sensor and an Accu-Chek device as references, a cross-validation-based regression of 205 observations of blood glucose was performed with a machine learning algorithm. Results: In a five-fold cross-validation analysis (including asymptomatic patients), a machine learning classifier using the Raman spectra as input achieved a specificity for COVID-19 of 0.837 at a sensitivity of 0.80 and an area under receiver operating curve (AUROC) of 0.896. However, when the data were split by time, with training data consisting of observations before 1 July 2022 and test data consisting of observations after it, the model achieved an AUROC of 0.67, with 0.863 sensitivity at a specificity of 0.517. This decrease in AUROC may be due to substantial domain shift as the virus evolves. A similar five-fold cross-validation analysis of Raman glucose detection produces an area under precision&ndash;recall curve (AUPR) of 0.58. Conclusions: The combination of Raman spectroscopy, AI/ML, and our patient interface admitting only a patient&rsquo;s finger and using no specimen offers unprecedented flexibility in introducing new diagnostic tests or adapting existing ones. As the ML algorithm can be iteratively re-trained with new data and the software deployed to field devices remotely, it promises to be a valuable tool for detecting rapidly emerging infectious outbreaks and disease-specific biomarkers, such as glucose.",
        "authors": [
            "Allen B. Chefitz",
            "Rohit Singh",
            "Thomas Birch",
            "Yongwu Yang",
            "Arib Hussain",
            "Gabriella Chefitz"
        ],
        "journal_conference_name": "Spectroscopy Journal",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158990",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Computer Science Behind Bars: Lessons Learned from Teaching Incarcerated Students in Prisons and Jails",
        "abstract": "Educational programs for incarcerated individuals, often called \"behind bars\" initiatives, have been shown to improve participants' social and economic outcomes upon release. Since its founding in 2018, MIT's Education Justice Institute (TEJI) has offered accredited classes for incarcerated students, with an increasing focus on computer education. Our courses have been delivered both in person and remotely (e.g., via Zoom). In this poster, we share insights into the challenges present in the incarcerated education environment, and highlight how remote learning offers unique advantages to incarcerated students. We also present preliminary findings from two years of data collected across four recurring computer science courses. This poster aims to foster a dialogue with the broader computer science education community, focusing on: (i) qualitative insights gained from extensive interactions with incarcerated education systems, (ii) preliminary empirical results obtained through IRB-approved surveys, (iii) common challenges faced during data collection, and (iv) an opportunity to seek feedback and pose questions to computer science education experts.",
        "authors": [
            "Andrew Fishberg",
            "Marisa Gaetz",
            "Martin Nisser",
            "Carole Cafferty",
            "Lee Perlman",
            "Raechel Soicher",
            "Joshua Long"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158331",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The d γ / 2 -Variation of Distance Profiles in γ -Liouville Quantum Gravity",
        "abstract": "For Brownian surfaces with boundary and an interior marked point, a natural observable to consider is the distance profile, defined as the process of distances from the marked point to a variable point lying on the boundary. When the boundary is parametrized by the natural length measure on it, this distance profile turns out to be locally absolutely continuous to Brownian motion, and as a result, the boundary length measure itself has a natural interpretation as the quadratic variation process of the distance profile. In this paper, we extend this interpretation to γ -Liouville quantum gravity ( γ -LQG), a one-parameter family of models of random geometry which is known to specialize to the case of Brownian geometry for the case γ = 8 / 3 . With d γ denoting the Hausdorff dimension of γ -LQG, we show that for a γ -LQG surface with boundary, the natural boundary length measure can be interpreted (up to a constant factor) as the d γ / 2 -variation process of the distance profile from an interior point.",
        "authors": [
            "Manan Bhatia"
        ],
        "journal_conference_name": "Communications in Mathematical Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159040",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Systematic Simulations of Structural Stability, Phonon Dispersions, and Thermal Expansion in Zinc-Blende ZnO",
        "abstract": "Zinc oxide (ZnO) has recently gained considerable attention due to its exceptional properties, including higher electron mobility, good thermal conductivity, high breakdown voltage, and a relatively large exciton-binding energy. These characteristics helped engineers to develop low dimensional heterostructures (LDHs)-based advanced flexible/transparent nanoelectronics, which were then integrated into thermal management systems. Coefficients of thermal expansion α(T),\r\n phonon dispersions  ωj(q→)\r\n, and Grüneisen parameters  γj(q→)\r\n can play important roles in evaluating the suitability of materials in such devices. By adopting a realistic rigid-ion model in the quasi-harmonic approximation, this work aims to report the results of a methodical study to comprehend the structural, lattice dynamical, and thermodynamic behavior of zinc-blende (zb) ZnO. Systematic calculations of ωj(q→)\r\n, γj(q→),\r\n and α(T)\r\n have indicated negative thermal expansion (NTE) at low T. Soft transverse acoustic shear mode gammas  γTA\r\n at critical points offered major contributions to NTE. Our results of ωj(q→)\r\n at ambient pressure compare reasonably well with Raman scattering spectroscopy measurements and first-principles calculations. By adjusting the layers of materials with positive and negative thermal expansion, it is possible to create LDHs with near-zero α(T)\r\n. Such a nanostructure might experience a minimal dimensional change with T fluctuations, making it ideal for devices where precise dimensional stability is crucial.",
        "authors": [
            "Devki N. Talwar",
            "Piotr Becla"
        ],
        "journal_conference_name": "Nanomaterials",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158301",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Orbit Plane Rotation Using Aerocapture",
        "abstract": "This study investigates the feasibility of performing orbit plane rotations during aerocapture maneuvers. Three-degrees-of-freedom bounding trajectories at Mars are propagated for a range of vehicle lift-to-drag ratios 𝐿/𝐷\r\n and hyperbolic arrival velocities 𝑣∞\r\n. The results show that the maximum plane rotation achievable increases with vehicle 𝐿/𝐷\r\n and 𝑣∞\r\n. When arriving with 𝑣∞\r\n of 6 km/s, vehicles with 𝐿/𝐷\r\n of 0.25 and 1.0 can achieve plane rotations of up to 11.6 and 45.3 deg, respectively. Heat rate, heat load, and g-loading constraints identified when rotating the orbital plane are not more severe than those observed for two-dimensional aerocapture at a given 𝐿/𝐷\r\n and 𝑣∞\r\n. A direct tradeoff between the maximum plane rotation and entry corridor width exists that will affect the ability of lower 𝐿/𝐷\r\n vehicles to achieve large plane rotations. The proposed maneuver can allow the captured orbit inclination and right ascension of the ascending node to be altered in ways that are not possible using typical interplanetary orbit targeting methods. Further, the maneuver offers the possibility of deploying multiple satellites to different orbits around a target destination using a single launch or approach path.",
        "authors": [
            "Daniel C. Gochenaur",
            "Michael P. Jones",
            "Johannes J. Norheim",
            "Olivier L. de Weck"
        ],
        "journal_conference_name": "Journal of Spacecraft and Rockets",
        "publisher": "American Institute of Aeronautics and Astronautics",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158529",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "\"Why is my code slow?\" Efficiency Bugs in Student Code",
        "abstract": "While prior research has categorized common errors and code quality issues of student programmers, little attention has been paid to researching student efficiency bugs. Qualitative content analysis of 250 slow student submissions across five CS2 assignments yielded over 750 efficiency bugs. Extracting general themes resulted in an efficiency bug taxonomy with three main categories: superfluous computation, suboptimal data structure design, and suboptimal algorithm design, with 12 subcategories. Analysis of specific bug frequencies across the assignments provided insights that may inform content design for programming courses.",
        "authors": [
            "Hope Dargan",
            "Adam Gilbert-Diamond",
            "Adam Hartz",
            "Robert Miller"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158329",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Long-Term Ageing Studies on Eco-Friendly Resistive Plate Chamber Detectors",
        "abstract": "In high-energy physics, resistive plate chamber (RPC) detectors operating in avalanche mode make use of a high-performance gas mixture. Its main component, Tetrafluoroethane (C2H2F4), is classified as a fluorinated greenhouse gas. The RPC EcoGas@GIF++ collaboration is pursuing an intensive R&D on new gas mixtures for RPCs to explore eco-friendly alternatives complying with recent European regulations. The performance of different RPC detectors has been evaluated at the CERN Gamma Irradiation Facility with Tetrafluoropropene (C3H2F4)-CO2-based gas mixtures. A long-term ageing test campaign was launched in 2022, and since 2023, systematic long-term performance studies have been carried out thanks to dedicated beam tests. The results of these studies are discussed together with their future perspectives.",
        "authors": [
            "Marcello Abbrescia",
            "Giulio Aielli",
            "Reham Aly",
            "Maria Cristina Arena",
            "Mapse Barroso Ferreira",
            "Luigi Benussi",
            "Stefano Bianco",
            "Fabio Bordon",
            "Davide Boscherini",
            "Alessia Bruni",
            "Salvatore Buontempo",
            "Mattia Busato",
            "Paolo Camarri",
            "Roberto Cardarelli",
            "Liliana Congedo",
            "Marilisa De Serio",
            "Francesco Debernardis",
            "Anna Di Ciaccio",
            "Luigi Di Stante",
            "Pascal Dupieux"
        ],
        "journal_conference_name": "Particles",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158989",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Practical DB-OS Co-Design with Privileged Kernel Bypass",
        "abstract": "This paper revisits the longstanding challenge of coordinating database systems with general-purpose OS interfaces, such as POSIX, which often lack tailored support for DB requirements. Existing approaches to this DB-OS co-design struggle with limited design space, security risks, and compatibility issues. To overcome these hurdles, we propose a new co-design approach leveraging virtualization to elevate the privilege level of DB processes. Our method enables database systems to fully exploit hardware capabilities via virtualization, while minimizing the need for extensive modifications to the host OS kernel, thereby maintaining compatibility. We demonstrate the effectiveness of our approach through two novel virtual memory mechanisms tailored for database workloads: (1) an efficient snapshotting mechanism that captures memory snapshots at millisecond intervals for in-memory databases and HTAP workloads, and (2) a streamlined in-kernel buffer pool design. We introduce Libdbos, a lightweight guest kernel implementing these mechanisms. Our evaluations highlight significant improvements in latency and efficiency compared to existing snapshotting and buffer pool designs, underscoring the potential of the approach.",
        "authors": [
            "Xinjing Zhou",
            "Viktor Leis",
            "Jinming Hu",
            "Xiangyao Yu",
            "Michael Stonebraker"
        ],
        "journal_conference_name": "Proceedings of the ACM on Management of Data",
        "publisher": "ACM",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158440",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Predicting Mortality in Subarachnoid Hemorrhage Patients Using Big Data and Machine Learning: A Nationwide Study in Türkiye",
        "abstract": "Background/Objective: Subarachnoid hemorrhage (SAH) is associated with high morbidity and mortality rates, necessitating prognostic algorithms to guide decisions. Our study evaluates the use of machine learning (ML) models for predicting 1-month and 1-year mortality among SAH patients using national electronic health records (EHR) system. Methods: Retrospective cohort of 29,274 SAH patients, identified through national EHR system from January 2017 to December 2022, was analyzed, with mortality data obtained from central civil registration system in Türkiye. Variables included (n = 102) pre- (n = 65) and post-admission (n = 37) data, such as patient demographics, clinical presentation, comorbidities, laboratory results, and complications. We employed logistic regression (LR), decision trees (DTs), random forests (RFs), and artificial neural networks (ANN). Model performance was evaluated using area under the curve (AUC), average precision, and accuracy. Feature significance analysis was conducted using LR. Results: The average age was 56.23 ± 16.45 years (47.8% female). The overall mortality rate was 22.8% at 1 month and 33.3% at 1 year. One-month mortality increased from 20.9% to 24.57% (p < 0.001), and 1-year mortality rose from 30.85% to 35.55% (p < 0.001) in the post-COVID period compared to the pre-COVID period. For 1-month mortality prediction, the ANN, LR, RF, and DT models achieved AUCs of 0.946, 0.942, 0.931, and 0.916, with accuracies of 0.905, 0.901, 0.893, and 0.885, respectively. For 1-year mortality, the AUCs were 0.941, 0.927, 0.926, and 0.907, with accuracies of 0.884, 0.875, 0.861, and 0.851, respectively. Key predictors of mortality included age, cardiopulmonary arrest, abnormal laboratory results (such as abnormal glucose and lactate levels) at presentation, and pre-existing comorbidities. Incorporating post-admission features (n = 37) alongside pre-admission features (n = 65) improved model performance for both 1-month and 1-year mortality predictions, with average AUC improvements of 0.093 ± 0.011 and 0.089 ± 0.012, respectively. Conclusions: Our study demonstrates the effectiveness of ML models in predicting mortality in SAH patients using big data. LR models’ robustness, interpretability, and feature significance analysis validate its importance. Including post-admission data significantly improved all models’ performances. Our results demonstrate the utility of big data analytics in population-level health outcomes studies.",
        "authors": [
            "Taghi Khaniyev",
            "Efecan Cekic",
            "Neslihan Nisa Gecici",
            "Sinem Can",
            "Naim Ata",
            "Mustafa Mahir Ulgu",
            "Suayip Birinci",
            "Ahmet Ilkay Isikay",
            "Abdurrahman Bakir",
            "Anil Arat",
            "Sahin Hanalioglu"
        ],
        "journal_conference_name": "Journal of Clinical Medicine",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158299",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Base-Load Nuclear Reactors for Fully Dispatchable Electricity: Nuclear Air-Brayton Combined Cycles, Firebrick Heat Storage, Hydrogen Storage, and Hydrocarbon Biofuels",
        "abstract": "Three partly coupled integrated nuclear energy systems are described. These enable base-load nuclear reactors to provide fully dispatchable electricity without greenhouse-gas emissions, thus replacing gas turbines burning natural gas and batteries storing electricity. These hybrid systems link the industrial sector to the electricity sector. Firstly, electricity-to-high-temperature (1800 &deg;C) gigawatt-hour firebrick heat storage converts low-price electricity to high-temperature stored heat to provide dispatchable heat for industry and power generation. Secondly, Nuclear Air-Brayton Combined Cycles (NACC) with thermodynamic topping cycles using high-temperature stored heat or combustible fuel to provide dispatchable electricity. Peak power output can be two to five times the base-load electricity production. The heat-to-electricity efficiency of the thermodynamic topping cycles exceeds 70%. Thirdly, nuclear hydrogen production for industrial markets enables the production of dispatchable electricity where hydrogen is used for energy storage but not to produce heat and electricity. Base-load nuclear reactors send electricity to the grid and/or electrolyzers for hydrogen production depending upon electricity prices. Low-cost hydrogen storage enables us to meet steady-state industrial hydrogen demands, even though hydrogen and grid electricity production is varied. Hydrogen production for industrial uses (ammonia fertilizer, direct reduction of iron ore to iron replacing coke, cellulosic liquid hydrocarbon biofuels replacing crude oil) may exceed 20% of total energy demand and may be a massive source of dispatchable electricity. The biofuels provide storable energy when heat storage is depleted.",
        "authors": [
            "Charles Forsberg"
        ],
        "journal_conference_name": "Energies",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158300",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Engineering of Genetically Encoded Bright Near-Infrared Fluorescent Voltage Indicator",
        "abstract": "Genetically encoded voltage indicators (GEVIs) allow for the cell-type-specific real-time imaging of neuronal membrane potential dynamics, which is essential to understanding neuronal information processing at both cellular and circuit levels. Among GEVIs, near-infrared-shifted GEVIs offer faster kinetics, better tissue penetration, and compatibility with optogenetic tools, enabling all-optical electrophysiology in complex biological contexts. In our previous work, we employed the directed molecular evolution of microbial rhodopsin Archaerhodopsin-3 (Arch-3) in mammalian cells to develop a voltage sensor called Archon1. Archon1 demonstrated excellent membrane localization, signal-to-noise ratio (SNR), sensitivity, kinetics, and photostability, and full compatibility with optogenetic tools. However, Archon1 suffers from low brightness and requires high illumination intensities, which leads to tissue heating and phototoxicity during prolonged imaging. In this study, we aim to improve the brightness of this voltage sensor. We performed random mutation on a bright Archon derivative and identified a novel variant, monArch, which exhibits satisfactory voltage sensitivity (4~5% ΔF/FAP) and a 9-fold increase in basal brightness compared with Archon1. However, it is hindered by suboptimal membrane localization and compromised voltage sensitivity. These challenges underscore the need for continued optimization to achieve an optimal balance of brightness, stability, and functionality in rhodopsin-based voltage sensors.",
        "authors": [
            "Xian Xiao",
            "Aimei Yang",
            "Hanbin Zhang",
            "Demian Park",
            "Yangdong Wang",
            "Balint Szabo",
            "Edward S. Boyden",
            "Kiryl D. Piatkevich"
        ],
        "journal_conference_name": "Department of Brain and Cognitive Sciences",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158298",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Lessons from COVID-19 patient visitation restrictions: six considerations to help develop ethical patient visitor policies",
        "abstract": "Patient visitor restrictions were implemented in unprecedented ways during the COVID-19 pandemic and included bans on any visitors to dying patients and bans separating mothers from infants. These were implemented without high quality evidence they would be beneficial and the harms to patients, families and medical personnel were often immediately clear. Evidence has also accumulated finding strict visitor restrictions were accompanied by long-term individual and societal consequences. We highlight numerous examples of restrictions that were enacted during the COVID-19 pandemic, including some that continue to be in place today. We outline six specific concerns about the nature and effects of the visitor restrictions seen during the COVID-19 pandemic. These considerations may help provide both an ethical and science-based framework, through which healthcare workers, families and government entities can work towards safeguarding patient and family rights and well-being.",
        "authors": [
            "Tracy B. Høeg",
            "Benjamin Knudsen",
            "Vinay Prasad"
        ],
        "journal_conference_name": "Monash Bioethics Review",
        "publisher": "Springer International Publishing",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158289",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Taxonomy for Social Sustainability in Corporate Communication",
        "abstract": "Sustainability, or environmental, social, and governance (ESG), reports have become ubiquitous among major companies in recent years, often criticized as tools for greenwashing and met with significant backlash. While the environmental aspects of these reports are well-defined, social sustainability remains poorly understood. Through an analysis of narrative sections from six corporate sustainability reports narrative sections, we propose an initial taxonomy of constitutive social sustainability concepts reflected in corporate speech.",
        "authors": [
            "Amelia Dogan",
            "Laura Frye-Levine",
            "Ava Malysa"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158181",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Search for long-lived heavy neutral leptons in proton-proton collision events with a lepton-jet pair associated with a secondary vertex at √s = 13 TeV",
        "abstract": "A search for long-lived heavy neutral leptons (HNLs) using proton-proton collision data corresponding to an integrated luminosity of 138 fb−1 collected at s = 13 TeV with the CMS detector at the CERN LHC is presented. Events are selected with a charged lepton originating from the primary vertex associated with the proton-proton interaction, as well as a second charged lepton and a hadronic jet associated with a secondary vertex that corresponds to the semileptonic decay of a long-lived HNL. No excess of events above the standard model expectation is observed. Exclusion limits at 95% confidence level are evaluated for HNLs that mix with electron and/or muon neutrinos. Limits are presented in the mass range of 1–16.5 GeV, with excluded square mixing parameter values reaching as low as 2 × 10−7. For masses above 11 GeV, the presented limits exceed all previous results in the semileptonic decay channel, and for some of the considered scenarios are the strongest to date.",
        "authors": [
            "A. Hayrapetyan",
            "A. Tumasyan",
            "W. Adam",
            "J. W. Andrejkovic",
            "T. Bergauer",
            "S. Chatterjee",
            "K. Damanakis",
            "M. Dragicevic",
            "P. S. Hussain",
            "M. Jeitler",
            "N. Krammer",
            "A. Li",
            "D. Liko",
            "I. Mikulec",
            "J. Schieck",
            "R. Schöfbeck",
            "D. Schwarz",
            "M. Sonawane",
            "The CMS collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158277",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Protein codes promote selective subcellular compartmentalization",
        "abstract": "Cells have evolved mechanisms to distribute ~10 billion protein molecules to\r\nsubcellular compartments where diverse proteins involved in shared functions must\r\nassemble. Here, we demonstrate that proteins with shared functions share amino\r\nacid sequence codes that guide them to compartment destinations. A protein\r\nlanguage model, ProtGPS, was developed that predicts with high performance the\r\ncompartment localization of human proteins excluded from the training set.\r\nProtGPS successfully guided generation of novel protein sequences that selectively\r\nassemble in the nucleolus. ProtGPS identified pathological mutations that change\r\nthis code and lead to altered subcellular localization of proteins. Our results\r\nindicate that protein sequences contain not only a folding code, but also a\r\npreviously unrecognized code governing their distribution to diverse subcellular\r\ncompartments.",
        "authors": [
            "Henry R. Kilgore",
            "Itamar Chinn",
            "Peter G. Mikhael",
            "Ilan Mitnikov",
            "Catherine Van Dongen",
            "Guy Zylberberg",
            "Lena Afeyan",
            "Salman F. Banani",
            "Susana Wilson-Hawken",
            "Tong Ihn Lee",
            "Regina Barzilay",
            "Richard A. Young"
        ],
        "journal_conference_name": "Science",
        "publisher": "American Association for the Advancement of Science",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158180",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Association Between Medicaid Expansion and Insurance Status, Risk Group, Receipt, and Refusal of Treatment Among Men with Prostate Cancer",
        "abstract": "Simple Summary\r\nWe sought to quantify the impact of Medicaid expansion on insurance status, stage at diagnosis, time to treatment initiation, and refusal of locoregional treatment among patients with prostate cancer, the second leading cause of cancer death among men in the United States. We found that while Medicaid expansion was associated with increased insurance coverage and decreased refusal of radiation therapy, there was no significant association with earlier risk group at diagnosis, treatment within 180 days, nor refusal of locoregional therapy. Similarly, racial minorities experienced no significant changes in time to treatment initiation following Affordable Care Act implementation compared to White patients. Ultimately, more research is needed to understand how Medicaid expansion affects cancer outcomes and whether these effects are borne equitably among different populations.\r\n\r\nAbstract\r\nBackground: Although the Patient Protection and Affordable Care Act (ACA) has been associated with increased Medicaid coverage among prostate cancer patients, the association between Medicaid expansion with risk group at diagnosis, time to treatment initiation (TTI), and the refusal of locoregional treatment (LT) among patients requires further exploration. Methods: Using the National Cancer Database, we performed a retrospective cohort analysis of all patients aged 40 to 64 years diagnosed with localized prostate cancer from 2011 to 2016. Difference-in-difference (DID) analysis was used to compare changes in insurance status, risk group at diagnosis, TTI, and the refusal of LT among patients residing in Medicaid expansion versus non-expansion states. In a secondary analysis, we used DID to compare changes in the above outcomes among racial minorities versus White patients living in expansion states. Results: Of the 112,434 patients with prostate cancer in our analysis, 50,958 patients lived in Medicaid expansion states, and 61,476 patients lived in non-expansion states. In the adjusted analysis, we found that the proportion of uninsured patients (adjusted DID: −0.87%; 95% confidence interval [95% CI]: −1.28 to −0.46) and patients who refused radiation therapy (adjusted DID: −0.71%; 95% CI: −0.95 to −0.47) decreased more in expansion states compared to non-expansion states. Similarly, we observed that the racial disparity of select outcomes in expansion states narrowed, as racial minorities experienced larger absolute decreases in uninsured status and the refusal of radiation therapy (RT) regimens than White patients following ACA implementation (p < 0.01 for all). However, residence in a Medicaid expansion state was not associated with changes in risk group at diagnosis, TTI, nor the refusal of LT (p > 0.01 for all); racial disparities in TTI were also exacerbated in expansion states following ACA implementation. Conclusions: The association between Medicaid expansion and prostate cancer outcomes and disparities remains unclear. While ACA implementation was associated with increased insurance coverage and decreased refusal of RT, there was no significant association with earlier risk group at diagnosis, TTI within 180 days, or refusal of LT. Similarly, racial minorities in expansion states had larger decreases in uninsured status and the refusal of RT regimens, as well as smaller increases in intermediate-/high-risk disease at presentation than White patients following ACA implementation, but experienced no significant changes in TTI. More research is needed to understand how Medicaid expansion affects cancer outcomes and whether these effects are borne equitably among different populations.",
        "authors": [
            "Tej A. Patel",
            "Bhav Jain",
            "Edward Christopher Dee",
            "Khushi Kohli",
            "Sruthi Ranganathan",
            "James Janopaul-Naylor",
            "Brandon A. Mahal",
            "Kosj Yamoah",
            "Sean M. McBride",
            "Paul L. Nguyen",
            "Fumiko Chino",
            "Vinayak Muralidhar",
            "Miranda B. Lam",
            "Neha Vapiwala"
        ],
        "journal_conference_name": "Cancers",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158243",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Recent Progress in Flexible Piezoelectric Tactile Sensors: Materials, Structures, Fabrication, and Application",
        "abstract": "Flexible tactile sensors are widely used in aerospace, medical and health monitoring, electronic skin, human–computer interaction, and other fields due to their unique advantages, thus becoming a research hotspot. The goal is to develop a flexible tactile sensor characterized by outstanding sensitivity, extensive detection range and linearity, elevated spatial resolution, and commendable adaptability. Among several strategies like capacitive, piezoresistive, and triboelectric tactile sensors, etc., we focus on piezoelectric tactile sensors because of their self-powered nature, high sensitivity, and quick response time. These sensors can respond to a wide range of dynamic mechanical stimuli and turn them into measurable electrical signals. This makes it possible to accurately detect objects, including their shapes and textures, and for them to sense touch in real time. This work encapsulates current advancements in flexible piezoelectric tactile sensors, focusing on enhanced material properties, optimized structural design, improved fabrication techniques, and broadened application domains. We outline the challenges facing piezoelectric tactile sensors to provide inspiration and guidance for their future development.",
        "authors": [
            "Jingyao Tang",
            "Yiheng Li",
            "Yirong Yu",
            "Qing Hu",
            "Wenya Du",
            "Dabin Lin"
        ],
        "journal_conference_name": "Sensors",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158242",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Convergence to Bohmian Mechanics in a de Broglie-Like Pilot-Wave System",
        "abstract": "Bohmian mechanics supplements the quantum wavefunction with deterministic particle trajectories, offering an alternate, dynamical language for quantum theory. However, the Bohmian wavefunction evolves independently of these trajectories, and is thus unaffected by the observable properties of the system. While this property is widely assumed necessary to ensure agreement with quantum mechanics, much work has recently been dedicated to understanding classical pilot-wave systems, which feature a two-way coupling between particle and wave. These systems—including the “walking droplet” system of Couder and Fort (Couder and Fort (2006) Phys. Rev. Lett. 97:154101) and its various abstractions (Dagan and Bush (2020) CR Mecanique 348:555–571; Durey and Bush (2020) Front. Phys. 8:300; (2021) Chaos 31:033136; Darrow and Bush (2024) Symmetry 16:149)—allow us to investigate the limits of classical systems and offer a touchstone between quantum and classical dynamics. In this work, we present a general result that bridges Bohmian mechanics with this classical pilot-wave theory. Namely, Darrow and Bush ((2024) Symmetry 16:149) recently introduced a Lagrangian pilot-wave framework to study quantum-like behaviours in classical systems; with a particular choice of particle-wave coupling, they recover key dynamics hypothesised in de Broglie’s early double-solution theory (de Broglie (1970) Foundations Phys. 1:5–15). We here show that, with a different choice of coupling, their de Broglie-like system reduces exactly to single-particle Bohmian mechanics in the non-relativistic limit. Our result clarifies that, while multi-particle entanglement is impossible to replicate in general with local, classical theories, no such restriction exists for single-particle quantum mechanics. Moreover, connecting with the previous work of Darrow and Bush, our work demonstrates that de Broglie’s and Bohm’s theories can be connected naturally within a single Lagrangian framework. Finally, we present an application of the present work in developing a single-particle analogue for position measurement in a de Broglie-like setting.",
        "authors": [
            "David Darrow"
        ],
        "journal_conference_name": "Foundations of Physics",
        "publisher": "Springer US",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158274",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Data Artifacts Glossary: a community-based repository for bias on health datasets",
        "abstract": "Background The deployment of Artificial Intelligence (AI) in healthcare has the potential to transform patient care through improved diagnostics, personalized treatment plans, and more efficient resource management. However, the effectiveness and fairness of AI are critically dependent on the data it learns from. Biased datasets can lead to AI outputs that perpetuate disparities, particularly affecting social minorities and marginalized groups. Objective This paper introduces the “Data Artifacts Glossary”, a dynamic, open-source framework designed to systematically document and update potential biases in healthcare datasets. The aim is to provide a comprehensive tool that enhances the transparency and accuracy of AI applications in healthcare and contributes to understanding and addressing health inequities. Methods Utilizing a methodology inspired by the Delphi method, a diverse team of experts conducted iterative rounds of discussions and literature reviews. The team synthesized insights to develop a comprehensive list of bias categories and designed the glossary’s structure. The Data Artifacts Glossary was piloted using the MIMIC-IV dataset to validate its utility and structure. Results The Data Artifacts Glossary adopts a collaborative approach modeled on successful open-source projects like Linux and Python. Hosted on GitHub, it utilizes robust version control and collaborative features, allowing stakeholders from diverse backgrounds to contribute. Through a rigorous peer review process managed by community members, the glossary ensures the continual refinement and accuracy of its contents. The implementation of the Data Artifacts Glossary with the MIMIC-IV dataset illustrates its utility. It categorizes biases, and facilitates their identification and understanding. Conclusion The Data Artifacts Glossary serves as a vital resource for enhancing the integrity of AI applications in healthcare by providing a mechanism to recognize and mitigate dataset biases before they impact AI outputs. It not only aids in avoiding bias in model development but also contributes to understanding and addressing the root causes of health disparities.",
        "authors": [
            "Rodrigo R. Gameiro",
            "Naira L. Woite",
            "Christopher M. Sauer",
            "Sicheng Hao",
            "Chrystinne O. Fernandes",
            "Anna E. Premo",
            "Alice R. Teixeira",
            "Isabelle Resli",
            "An-Kwok I. Wong",
            "Leo A. Celi"
        ],
        "journal_conference_name": "Journal of Biomedical Science",
        "publisher": "BioMed Central",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158286",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Data Are Power: Addressing the Power Imbalance Around Community Data with the Open-Access Data4HumanRights Curriculum",
        "abstract": "Data4HumanRights’ training materials have been developed as open-source and tailored to limited-resource settings, where community data collectors often live and work. Access to training on data collection, analysis, and visualisation to support the advocacy of vulnerable groups is essential, particularly in the context of increasing human rights challenges such as land rights, adequate housing, conflicts, and climate justice. This paper provides an overview of how the training materials were co-developed with community data collectors in Nigeria and Kenya, offering insights into the fundamental principles (i.e., inclusiveness, adaptive, limited resources, and being gender- and incentive-sensitive) and the structure of the open-access training materials. The development process resulted in 28 modules, each designed to be delivered in a face-to-face format in less than one day by a local trainer. To maximize adaptivity, the training modules can be mixed and matched (e.g., as individual modules or a learning path of several modules around a specific training need). The individual modules cover a range of methods and tools that are useful to human rights work and community advocacy, e.g., documenting evictions, performing rapid needs assessments after acute crises, community profiling, and monitoring community development indicators. The training materials contain instructions for the training facilitator(s) and all necessary training materials. To ensure inclusivity, the training covers both basic and advanced topics, with most modules designed to address basic needs that can be followed using a mobile phone, thereby avoiding the need for computers or printed handouts. The training results in Nigeria and Kenya showcase applications, including mapping waste problems and addressing forced evictions. Trained community groups produced maps of waste piles to prioritize community actions, such as finding space for urban agriculture, and conducted rapid needs assessments during a massive eviction. This approach helps reduce power imbalances and empowers community groups to effectively manage and utilise their own data.",
        "authors": [
            "Monika Kuffer",
            "Dana R. Thomson",
            "Dianne Wakonyo",
            "Nicera Wanjiru Kimani",
            "Divyani Kohli-Poll Jonker",
            "Enyo Okoko",
            "Rasak Toheeb",
            "Bisola Akinmuyiwa",
            "Mohammed Zanna",
            "Dezyno Imole",
            "Andrew Maki"
        ],
        "journal_conference_name": "Department of Urban Studies and Planning",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158297",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploring the Holiday Effect on Elevated Traffic-Related Air Pollution with Hyperlocal Measurements in Chengdu, China",
        "abstract": "Traffic-related air pollutants (TRAPs) pose significant health risks in megacities, yet fixed monitoring sites often fail to capture their complexity. To characterize the TRAP concentrations which fixed sites cannot address, we employed a mobile platform to effectively capture real-time hyperlocal-scale TRAP variations in Chengdu, China. A 17-day sampling campaign was conducted covering the National Holiday of China and collected ~1.2 × 105 1 Hz paired data. We measured particle number concentration (PNC), black carbon (BC), and nitrogen oxides (NOx) across urban and rural freeway environments to assess the impact of reduced heavy-duty diesel vehicles (HDDVs) during the holiday (i.e., holiday effect). No clear impact of wind direction on TRAP concentrations was found in this study. However, substantial differences (two times) were observed when comparing non-holiday to holiday campaigns. Spearman correlations (0.21–0.56) between TRAPs persistently exceeded Pearson correlations (0.14–0.41), indicating non-linear relationships and suggesting the necessity for data transformations (e.g., logarithms) in TRAP analysis. The comparison of the background subtracted TRAPs concentrations between non-holiday and holidays, revealing approximately a 50% reduction in TRAPs across microenvironments. Among the TRAPs, NOx emerged as a reliable indicator of HDDV emissions. The study provides insights into vehicle fleet composition impacts, paving the way for enhanced exposure assessment strategies.",
        "authors": [
            "Sheng Xiang",
            "Jiaojiao Yu",
            "Yu Ting Yu",
            "Pengbo Zhao",
            "Tie Zheng",
            "Jingsong Yue",
            "Yuanyuan Yang",
            "Haobing Liu"
        ],
        "journal_conference_name": "Atmosphere",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158296",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Quantum Computing from Graphs",
        "abstract": "While stabilizer tableaus have proven exceptionally useful as a descriptive tool for additive quantum codes, they otherwise offer little guidance for concrete constructions or coding algorithm analysis. We introduce a representation of stabilizer codes as graphs with certain structures. Specifically, the graphs take a semi-bipartite form wherein input nodes map to output nodes, such that output nodes may connect to each other but input nodes may not. Intuitively, the graph’s input-output edges represent information propagation of the encoding circuit, while output-output edges represent the code’s entanglement structure. We prove that this graph representation is in bijection with tableaus and give an efficient compilation algorithm that transforms tableaus into graphs. We then show that this map is efficiently invertible, which gives a new universal recipe for code construction by way of finding graphs with sufficiently nice properties.\r\n\r\nThe graph representation gives insight into both code construction and algorithms. To the former, we argue that graphs provide a flexible platform for building codes particularly at small non-asymptotic scales. We construct as examples several constant-size codes and several infinite families codes. We also leverage graphs in a probabilistic analysis to extend the quantum Gilbert-Varshamov bound into a three-way distance-rate-weight trade-off. To the latter, we show that key coding algorithms, distance approximation, weight reduction, and decoding, are unified as instances of a single optimization game on a graph. Moreover, key code properties such as distance, weight, and encoding circuit depth, are all controlled by the graph degree. We give efficient algorithms for producing simple encoding circuits whose depths scale as twice the degree and for implementing logical diagonal and certain Clifford gates with non-constant but reduced depth. Finally, we construct a simple efficient decoding algorithm and prove a performance guarantee for a certain classes of graphs. These results give evidence that graphs are generically useful for the study of quantum computing and its practical implementations.",
        "authors": [
            "Andrey Boris Khesin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158853",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Simple Models for Complex Tropical Dynamics",
        "abstract": "Studying Earth's tropics is an essential part of understanding the climate, simulating the Earth system, and predicting the societal impacts of weather. In this thesis, we use a hierarchy of models -- including analytically tractable equations, simplified simulations, and full general circulation models -- to study tropical phenomena including the Hadley Circulation, the Inter-Tropical Convergence Zone (ITCZ), the South Asian monsoon, Pacific and ENSO seasonality, the Walker Circulation, and the modeling of the tropical energy budget. We begin with an examination of tropical SSTs and the ITCZ under warming, finding that the Hadley cells weaken and tropical SST gradients decrease in a warmer climate. The ocean's subtropical cells strengthen and transport more energy in a warmer climate, further flattening SST gradients. The ITCZ, meanwhile, increases in strength with warming because of the exponential relationship between humidity and temperature, and the presence of a dynamic ocean changes a single-ITCZ with a sinusoidal seasonal cycle to a double-ITCZ with a square wave seasonal cycle. Next, we study the ``monsoonal mode,'' an energy and precipitation anomaly triggered by the South Asian Monsoon that moves into the West Pacific during Northern Hemisphere autumn. The monsoonal mode is discussed as a possible underlying cause of the seasonality of the Pacific, i.e., that the West Pacific and ENSO both have seasonalities that favor one season despite being on the equator. To show this, ENSO seasonality is examined using simplified simulations and an energy budget of the Central-Eastern Equatorial Pacific. Similar techniques are then used to study ENSO events in warmer climates, and it is found that the Pacific zonal SST gradient and the Walker circulation, which are the sources of ENSO instability, weaken with warming, decreasing  the magnitude of ENSO events. Lastly, we assess the energy budget of CMIP6 models. It is shown that all CMIP6 models have more energy input to the deep tropics than ERA5 reanalysis, and this bias is bigger in the Southern Hemisphere. The hemispheric asymmetry in this bias can be traced back to radiation absorbed by the atmosphere, which is associated with dust (for shortwave radiation) and total column water (for longwave radiation). As a whole, this thesis demonstrates the utility of studying complex problems with simple models and deepens our understanding of Earth's tropics.",
        "authors": [
            "P.J. Tuckman"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158855",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Electrospray Thrusters in Chemical-Electric Multimode Propulsion for Small Satellites",
        "abstract": "Propulsion for small spacecraft is typically one of two modes, chemical or electric. These modes offer complementary propulsive performance: chemical propulsion provides high thrust and low specific impulse, while electric propulsion provides the inverse. As such, having access to both modes on the same spacecraft (i.e. multimode propulsion) is extremely useful. Unfortunately, the conventional propellants used by chemical and electric thrusters are highly incompatible, making this particularly difficult on small spacecraft that lack the mass, power, and volume to accommodate two separate propulsion systems. However, recent advancements in green monopropellants -- developed as less-toxic alternatives to hydrazine in chemical monopropellant thrusters -- have created a new family of ionic liquids monopropellants, making them the natural propellant for a highly compact form of electric propulsion known as electrospray thrusters. This presents a unique opportunity for a propellant to be shared between two propulsion modes, decreasing required mass and volume to be feasible for small spacecraft. This thesis examines the use of ionic liquid monopropellants in electrospray thrusters for a multimode chemical-electric propulsion system. This thesis focuses particularly on ASCENT, a high-maturity monopropellant with flight heritage in chemical thrusters.\r\n\r\nIn this work, the performance of ASCENT in the MIT ion Electrospray Propulsion System (iEPS) is extensively characterized. Experimental work includes ion plume diagnostics, indirectly and directly obtained performance estimates, temperature-dependent performance estimates, and extended duration firing behavior. Preliminary studies of similar monopropellants are also conducted to assess their use in a multimode system. To support an upcoming technology demonstration flight, a new multimode-compatible iEPS thruster tank is designed, fabricated, and validated. The integration and operation requirements for this thruster in a flight-ready system are defined. Finally, the mission benefits of an ASCENT multimode system for CubeSats are compared against current commercial options using an Earth observation mission case study.\r\n\r\nThis work finds that an iEPS thruster with ASCENT propellant has thrust of 9-15 µN, a specific impulse of 600-750 seconds, and a total efficiency of 18-22%, depending on current setpoint. We find that ASCENT is slightly volatile in high vacuum, which causes time-dependent losses in efficiency and specific impulse from gradual propellant evaporation. This volatility may also increase thruster lifetime by mitigating the risk of thruster failure by emitter flooding. This work also identified a modified version of ASCENT, created when the propellant is exposed to iron. This modified version produces a dramatically higher thrust and thrust-to-power compared to standard ASCENT. Additionally, flight-ready configurations of a multimode system are defined for 6U, 12U, and 27U CubeSats. A case study analysis found that the benefits of a chemical-electrospray multimode system are best realized at the 12U scale and above. Overall, this thesis provides critical insights on the performance, integration, and operation of electrospray thrusters with ionic liquid monopropellants. These results can then be used to enable a multimode propulsion system for small satellites.",
        "authors": [
            "Amelia R. Bruno"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158790",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Fast methods for full-wave electromagnetic solvers in MRI",
        "abstract": "High static field ( 3T) MR scanners can produce human tissue images of astounding clarity, but rely on high frequency ( 123MHz) electromagnetic radiation that generates complex in-tissue field patterns that are patient-specific and potentially harmful. Many such scanners use multiple transmitters to better control field patterns, but then adjust the transmitters based on general guidelines rather than optimizing for the specific patient, mostly because computing patient-specific fields was presumed far too slow. It was recently demonstrated that the combination of fast low-resolution tissue mapping and fast voxel-based field simulation can be used to perform a rapid patient-specific MR safety check. However, the field simulation still required several minutes, making it too slow to perform the dozens of simulations that would be needed for patient-specific optimization. In this work, we develop a set of numerical acceleration techniques that facilitate fast field simulations that bridge the gap between the performance of current state-of-art full-wave electromagnetic packages and time requirements dictated by real-time patient-specific field optimization in a clinical setting. These techniques cater to a large range of body sizes and complex coil geometries.",
        "authors": [
            "Georgy D. Guryev"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158943",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Detection and Localization of Pressure Transients in Water Distribution Systems",
        "abstract": "Water distribution systems are critical to urban water supply, but as they age they become increasingly vulnerable to bursts and leaks, leading to significant economic, social, and environmental consequences. The complexity and inaccessibility of underground pipelines present substantial challenges for their maintenance. As a result, the development of real-time monitoring systems for these systems is essential to reduce water waste and minimize adverse impacts to consumers and surrounding infrastructures. This thesis investigates the effectiveness of continuous pressure monitoring systems in detecting pipe bursts and transient events within water distribution systems. Using PTSNet, a parallel transient simulation Python package, we simulate pipe burst events at each node in a real-world system and examine the pressure-time response at all other nodes. By adding Gaussian noise to the simulation results to mimic real-world background noise, we assess the detection success of pressure signals at each node using a modified CUSUM algorithm. The correlations between detection success and three spatial metrics between the source and sensor are calculated. We show that a spatial metric, the effective number of magnitude-changing junctions along the fastest path, (NJFP), has a stronger correlation with detection success than the shortest travel path or the shortest distance. By comparing detection performance for networks with differing topologies (gridded, looped, and branched) and pipe characteristics, we discover that multiple shortest paths (MSP; where pressure waves from different paths arrive almost simultaneously at the sensor) amplify the signal due to transient interference phenomenon and enhance the detectability of transients. This effect is particularly pronounced in gridded networks. We investigate the capabilities of monitoring, from a network of fixed stations, to achieve unique localization of pressure transient events using a time-reversal back-propagation algorithm. This algorithm identifies the event source by matching the theoretical and detected arrival time differences at the sensors. A novel time differences space is constructed, representing the independent shortest time differences from locations along all the pipes to the sensors, based on network information and sensor locations. Pipe sections with unique shortest time differences are identified as uniquely localizable pipes. Effective-NJFP-based probabilities of transient detection with accurate arrival times (error < 0.1s) are derived from these simulation results. The localization performance of the sensor network is evaluated by the probability-weighted total lengths of the pipes that can be uniquely localized.\r\nWe consider sensor placement strategies aimed at maximizing the detection and localization performance of pressure monitoring sensor networks. Detection performance is defined as the total weighted pipe lengths in the network, where the weight of each pipe corresponds to its detection probability. Two problems are addressed: In order to maximize transient event detection performance when only a limited number of sensors are available, we formulate a mixed-integer programming (MIP) optimization model and employ a genetic algorithm to find solutions. The second problem involves determining the minimum number of sensors and their optimal locations to detect transient events across the entire network without a constraint on the number of available sensors. This is formulated as a minimum set cover problem, and an optimal solution is obtained using a mixed-integer linear programming solver. We focus on maximizing transient localization performance with a limited number of sensors. A genetic algorithm is applied to determine sensor locations, and the solutions obtained by this method provide significantly better localization performance than other approaches. We show differences in sensor placements for detection and localization: sensors are more evenly distributed throughout the network for detection purposes, while for localization, they are more concentrated in areas with longer pipes and simpler network structures. Finally, we present an analysis of two pressure monitoring datasets collected from a real-world water distribution system (SLG network). The first dataset consists of data from 28 sensors with a 100 Hz sampling frequency, collected over 7 to 30 days. We propose a method to identify and analyze noise levels and distributions at each sensor. Using a modified CUSUM algorithm, we detect transients and correlate them across sensors to identify events detected by multiple sensors. A transient-magnitude-based clustering method is then employed to group events based on their magnitudes, followed by a localization approach that utilizes the arrival time differences of transients between sensors. The findings indicate that noise levels in real-world monitoring data vary both spatially and temporally and are not independently normally distributed. Additionally, the arrival times detected by the modified CUSUM algorithm may not always accurately reflect the true transient arrival times due to mismatches between the signal characteristics and tuning of model parameters. Accurately identification of transient arrival time is particularly challenging for slowly changing pressure wave fronts. The second dataset includes pressure monitoring data from 7 sensors, during which 14 active transients with known source locations, times, and magnitudes were generated. We apply the modified CUSUM algorithm to detect transients at the sensors and correlate detection success with spatial metrics. The analysis confirms that the effective NJFP has the highest correlation with detection success, consistent with the simulation results. Additionally, the transient magnitude ratios between sensors and the source are found to be similar to the ratios calculated based on theoretical transmission coefficients when the source and sensor are in close proximity, suggesting that transmission coefficients can be used to estimate transient magnitudes in real networks.",
        "authors": [
            "Shiqing Liu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158880",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Information-centric Algorithms for Feature Extraction in High-Dimensional Sequential Data",
        "abstract": "Hidden Markov Models (HMMs) are a cornerstone of sequential data analysis, offering a robust framework for modeling observable events influenced by hidden internal states. With applications spanning speech recognition, video analysis, bioinformatics, and financial time series, HMMs enable the prediction and classification of raw data by leveraging their dual-layer stochastic structure: hidden Markov states and observable outputs. However, as real-world data grows increasingly high-dimensional, extracting meaningful features from observations becomes critical to reduce computational complexity while retaining relevant information.\r\n\r\nThis thesis addresses key challenges in feature extraction for high-dimensional HMMs. Current methods, such as neural networks (NNs), are widely used for nonlinear feature learning but lack mechanisms to prioritize useful features or incorporate known structural constraints. To bridge this gap, this work proposes novel algorithms to decouple representation learning from task-specific objectives and extract features aligned with predefined constraints.\r\n\r\nThe theoretical foundation, including local information geometry and Hirschfeld-Gebelein-Rényi (HGR) maximal correlation, is introduced in Chapter 2. Chapter 3 details three innovative feature extraction algorithms and their corresponding neural network architectures, highlighting their strengths and limitations. Convergence analyses and tail bounds for these methods are presented in Chapter 4. Numerical simulations validating the efficacy of the proposed approaches are provided in Chapter 5, while Chapter 6 concludes with a summary of contributions and potential future research directions.\r\n\r\nThis thesis advances the field by offering structured, constraint-aware feature extraction techniques tailored for high-dimensional sequential data, setting the stage for more effective and interpretable inference in HMMs.",
        "authors": [
            "Jiejun Jin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158923",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "System-Technology Co-Optimization of Scaled Electronics\r\nBased on Two-Dimensional Materials",
        "abstract": "Over the past 60 years, the semiconductor industry has focused on developing highly scaled electronic devices and high-density integrated circuits. However, bottlenecks have arisen recently as transistor dimensions approach the physical limits, and integration density is constrained. This thesis addresses these issues with two-dimensional (2D) materials, which includes inventing a low-temperature (< 300 °C) metal-organic chemical vapor deposition (MOCVD) method for 2D materials on 8-inch wafers, investigating extreme device scaling and multi-channel transistors. Design-Technology Co-Optimization (DTCO) and SystemTechnology Co-Optimization (STCO) are employed to rapidly model, evaluate and optimize device and circuit performance. Moreover, heterogeneous integration and monolithic 3D integration techniques are investigated, addressing challenges in integrating 2D materials with silicon complementary-metal-oxide-semiconductor (CMOS) circuits and flexible substrates. This research aims to advance high-density, high-performance electronics with low-power consumption for next-generation integrated systems.",
        "authors": [
            "Jiadi Zhu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158961",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Improved Complexity Analysis for the Proximal Bundle Algorithm Under a Novel Perspective",
        "abstract": "The proximal bundle algorithm (PBA) is a fundamental and computationally effective algorithm for solving optimization problems with non-smooth components. We investigate its convergence rate in two settings. We first focus on a composite setting where one function is smooth and the other is piecewise linear. We interpret a sequence of null steps of the PBA as a Frank-Wolfe algorithm on the Moreau envelope of the dual problem. In light of this correspondence, we first extend the linear convergence of Kelley's method on convex piecewise linear functions from the positive homogeneous to the general case. Building on this result, we propose a novel complexity analysis of PBA and derive a O (epsilon^-4/5) iteration complexity, improving upon the best known O (epsilon^-2) guarantee. This approach also unveils new insights on bundle management. We then present the first variant of the PBA for smooth objectives, achieving an accelerated convergence rate of O(epsilon^-1/2 log(epsilon^-1)), where epsilon is the desired accuracy. Our approach addresses an open question regarding the convergence guarantee of the PBA, which was previously posed in two recent papers. We interpret the PBA as a proximal point algorithm and base our proposed algorithm on an accelerated inexact proximal point scheme. Our variant introduces a novel null step test and oracle while maintaining the core structure of the original algorithm. The newly proposed oracle substitutes the traditional cutting planes with a smooth lower approximation of the true function. We show that this smooth interpolating lower model can be computed as a convex quadratic program. We finally show that Nesterov acceleration can be effectively applied when the objective is the sum of a smooth function and a piecewise linear one.",
        "authors": [
            "David Fersztand"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158820",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Towards an Artificial Neuroscience: Analytics for Language Model Interpretability",
        "abstract": "The growing deployment of neural language models demands greater understanding of their internal mechanisms. The goal of this thesis is to make progress on understanding the latent computations within large language models (LLMs) to lay the groundwork for monitoring, controlling, and aligning future powerful AI systems. We explore four areas using open source language models: concept encoding across neurons, universality of learned features and components across model initializations, presence of spatial and temporal representations, and basic dynamical systems modeling.\r\n\r\nIn Chapter 2, we adapt optimal sparse classification methods to neural network probing, allowing us to study how concepts are represented across multiple neurons. This sparse probing technique reveals both monosemantic neurons (dedicated to single concepts) and polysemantic neurons (representing multiple concepts in superposition) in full-scale LLMs confirming predictions from toy models. In Chapter 3, we identify and exhaustively catalog universal neurons across different model initializations by computing pairwise correlations of neuron activations over large datasets. Our findings show that 1-5\\% of neurons are universal, often with clear interpretations, and we taxonomize them into distinct neuron families.\r\n\r\nTo investigate spatial and temporal representations, we analyze LLM activations on carefully curated datasets of real-world entities in Chapter 4. We discover that models learn linear representations of space and time across multiple scales, which are robust to prompting variations and unified across different entity types. We identify individual \"space neurons\" and \"time neurons\" that reliably encode spatial and temporal coordinates. In Chapter 5, we use optimal sprase regression techniques to improve the sparse identification of nonlinear dynamics (SINDy) framework, demonstrating improved sample efficiency and support recovery in canonical differential systems. We then leverage this improvement to study the ability of LLMs to in-context learn dynamical systems and find internal representations which track the underlying system state.",
        "authors": [
            "Robert Wesley Gurnee"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158869",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Leveraging Structure for Efficient and Dexterous Contact-Rich Manipulation",
        "abstract": "Contact-rich manipulation has proved challenging due to the need to consider multiple combinatoric possibilities of making or breaking contact with the surrounding environment. As a result, existing methods have often resorted to combinatorial optimization that utilizes dynamics structure but considers all possibilities exhaustively, or compute-heavy and inefficient sampling methods that utilize blackbox optimization such as Reinforcement Learning (RL). In this thesis, I aim to show that by combining structured contact smoothing in conjunction with local gradient-based control and sampling-based motion planning, we can bypass the combinatorial explosion of contact modes while still leveraging structure and achieve highly efficient contact-rich manipulation. To achieve this capability, I first shed light on how RL abstracts contact modes and optimizes difficult landscapes by combining stochastic smoothing and zeroth-order optimization; yet, I show how following a similar stochastic strategy while using gradients suffers from several drawbacks such as empirical bias and high variance. To leverage structure in a more helpful manner, I propose a method for smoothing contact dynamics without relying on stochastic smoothing, bypassing these drawbacks. Using this smoothing scheme, I present a highly efficient and performant local control based on gradient-based trajectory optimization and model predictive control. Finally, I connect these local control capabilities with global sampling-based motion planners to achieve long-horizon global plans. The proposed method achieves contact-rich plans such as dexterous in-hand reorientation and whole-body manipulation much more efficiently than RL while being highly scalable compared to methods that explicitly reason about contact modes. These results achieve a reduction of contact-rich manipulation to kinodynamic motion planning, and exposes the true difficulty of contact-rich manipulation from combinatorial explosion in contact modes to combinatorial and highly non-local decisions over motion planning behaviors.",
        "authors": [
            "Hyung Ju Terry Suh"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158946",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Laboratory Astrophysics Studies of Magnetized Collisionless Shock Precursors and the ³He³He Proton Spectrum at the OMEGA Laser Facility",
        "abstract": "Laboratory astrophysics enables the study of astrophysical systems in the lab. There are broadly two types of laboratory astrophysics experiments: macrophysics and microphysics. Macrophysics experiments study a scaled down version of an astrophysical system while microphysics experiments create a small volume of matter with the same conditions as an astrophysical system. This thesis details work related to both macrophysics and microphysics laboratory astrophysics experiments. For the macrophysics contribution, collisionless shocks experiments were conducted at the OMEGA laser facility using the new gas jet platform. Collisionless shocks are shock waves formed through plasma processes when particle collisions are negligible. These shocks can form as bow shocks in the interaction between the solar wind and planetary ionospheres and can accelerate charge particles to high energies. In the experiment, a CH plasma flow collides with a hydrogen gas jet plasma to create a forming magnetized collisionless shock. Different diagnostics show a moving density jump, strong magnetic fields, and the acceleration of electrons. These observations coupled with magnetohydrodynamics and kinetic particle-in-cell simulations paint a complete physical picture of the forming shock in a configuration similar to the bow shock of Venus. Late time proton radiographs show a complicated structure which is studied for magnetic turbulence. Turbulence is important in several astrophysical systems, especially collisionless shocks where it dissipates shock kinetic energy and is essential for accelerating charged particles to cosmic ray energies. Magnetic power spectra extracted from proton radiography data show a break in the spectrum between the ion Larmor radius and the ion skin depth for high plasma β, a sign of kinetic turbulence. Large scale particle-in-cell simulations of high β turbulence also have this feature showing that the experimental data are consistent with high β kinetic turbulence. For the microphysics contribution, a new proton spectrometer is designed for measurements of the ³He³He proton spectrum. The ³He³He fusion reaction is the last step of the proton-proton I chain which produces the majority of the sun’s power. Previous experiments were not able to measure the ³He³He proton spectrum below 6 MeV. A new proton step range filter (SRF) spectrometer with a larger energy range is designed using a Monte Carlo tool. This tool uses Geant4 and is able to self-consistently apply the instrument response function. The new SRF design is validated and a method for analyzing experimental data using the Monte Carlo code is presented.",
        "authors": [
            "Timothy Mark Johnson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158838",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Enabling AI Copilots for Engineering Design With Parametric, Graph, And Component Inputs",
        "abstract": "Engineering design demands the synthesis of multimodal and often incomplete data—ranging from detailed parametric specifications, assembly graphs, visual references, and textual descriptions. Despite growing interest in generative models for design ideation and exploration, state-of-the-art approaches struggle with incomplete inputs, lack of support for modalities other than text and image, and limited controllability. This thesis addresses these gaps by unifying two complementary advances:\r\n\r\nFirst, we introduce a graph-guided diffusion approach for parametric data completion. By coupling Graph Attention Networks with a diffusion-based imputation mechanism, our method acts as a highly accurate and creative design auto-completion system for incomplete partial designs. On a dataset of 12,500 bicycles, this design imputation framework achieves a root mean square error (RMSE) of approximately 0.92 on numerical features and an error rate of around 0.18 for categorical attributes, outperforming both classical imputation methods such as MissForest, hotDeck, PPCA and advanced diffusion-based baselines such as TabCSDI. Moreover, it achives a Diversity Score of 3.10, surpassing all baselines, illustrating that the imputation process transforms incomplete data into multiple creative designs.\r\n\r\nSecond, we develop a multimodal control architecture that can extend foundation models to condition their generation processes with all or a subset of parametric inputs, assembly graphs, component images, and textual constraints. This model tremendously enhances both the controllability and precision of the generation process of foundational generative models, enabling controlling modalities that were not possible before. We first show that our model excels at tasks that state-of-the-art models struggle on. We further validate the performance of our model with surrogate models that investigate individual features. Our model achieves 95% or greater R^2 scores on different continuous parameters. Further, we show that our model is able to generate creative and novel designs while maintaining a high level of precision. This enables engineers to guide generative outputs along precise dimensional, aesthetic, and functional targets. Across numerous trials of different settings, we observe that our pipeline robustly fuses tabular parametric information, assembly graphs, and reference component images to produce results aligned with both specification precision and creativity. \r\n\r\nTogether, these contributions establish a coherent framework for AI-augmented design exploration. By viewing missing parameters as an opportunity for data-driven design autocompletion and by tightly integrating multimodal control over foundation models, this work elevates generative AI from a niche conceptual tool to a reliable design copilot. The implications of this thesis are profound: we show the possibilities and the pathways to AI copilot systems that can reduce data bottlenecks, broaden design spaces, and offer more thorough, constraint-adherent design candidates. As engineering problems grow in complexity and scale, the synergy of high-fidelity parametric imputation and multimodal control promises to accelerate innovation, cut development cycles, and guide human designers toward more inventive and manufacturable solutions.",
        "authors": [
            "Rui Zhou"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158805",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Radium and Mercury Dynamics in the Arctic: Investigating Terrestrial Inputs, Groundwater Discharge, and Chemical Cycling in a Changing Climate",
        "abstract": "The Arctic Ocean is distinctive due to its extreme seasonal variations in temperature and significant terrestrial inputs, including freshwater, carbon, nutrients, and toxins. Of particular concern is mercury (Hg) in its neurotoxic form, methylmercury (MeHg), which is already beginning to adversely affect Arctic human populations and wildlife. However, the region’s harsh conditions and remoteness have made conducting seasonal chemical and hydrological studies challenging. Tracers of boundary inputs, such as the radium (Ra) isotope quartet, offer potential for tracking and quantifying riverine and submarine groundwater discharge (SGD) of species like Hg into the Arctic Ocean. This thesis employs seasonal data and laboratory experiments to investigate the factors influencing terrestrial Ra inputs to the Arctic Ocean, quantifies SGD and associated Hg inputs to an Arctic coastal lagoon, and elucidates the chemical and geological factors influencing Hg cycling in Arctic groundwater.\r\n\r\nUsing historical and unpublished datasets combined with new laboratory investigations, differences in inputs of riverine Ra isotopes between the North American and Eurasian land masses were identified. The findings revealed higher Ra fluxes from the North American continent, attributed to greater sediment loads and lower organic matter in rivers compared to those on the Eurasian land mass. Subsequently, Ra data from five extensive field campaigns to Simpson Lagoon, Alaska, provided insights into Ra cycling on a more localized scale. These campaigns offered the first seasonal perspective on supra-permafrost SGD along an Arctic coastline, suggesting that SGD fluxes may rival those of rivers along the Beaufort Sea coast. Concurrently collected Hg groundwater concentrations allowed for the development of the first estimates of Hg fluxes from groundwater to the Arctic Ocean. If these estimates hold true along the rest of the Pan-Arctic coastline, they could significantly alter our understanding of microbial MeHg uptake in the Arctic Ocean. Finally, sediment cores from Simpson Lagoon and two other locations along the Beaufort Sea coast were used to examine how changing groundwater conditions, such as changing salinity, temperature, and redox conditions, influence Hg cycling. These experiments, alongside findings from Simpson Lagoon groundwater, indicate that Hg cycling in recently thawed permafrost sediments involves a complex interplay between organic material, metal oxides, and sulfide species, with groundwater conditions and soil carbon content playing crucial roles in Hg mobilization.",
        "authors": [
            "Emma Jacqueline Bullock"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158870",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Development of Dual Extruder Biomaterial 3D Printer",
        "abstract": "This research presents the design and fabrication of a novel dual-extruder biotic 3D printer for the precise deposition of natural biocomposites using organic materials such as pectin, chitosan, and cellulose. Unlike traditional FDM printers that rely on thermoplastic extrusion, this printer employs a syringe-based mechanical extruder capable of depositing viscous biomaterial hydrogels. The integration of a first-of-its-kind dual-extruder system enables the fabrication of multi-material prints and the exploration of biomaterial composites and complex geometric structures, thereby advancing sustainable, bio-inspired manufacturing.\r\nThis thesis emphasizes the machine engineering aspects of the printer's development, including project motivation, systematic design methodology, component design and fabrication, testing, and exploration of future work. Notable features of the system include user-friendly operation for non-experts, open-source accessibility, and compatibility with a wide range of biomaterials. By addressing existing limitations in biomaterial 3D printing technology, this work provides a robust platform to support future research in biomaterials, sustainable additive manufacturing, and bio-inspired design. Furthermore, the open-source nature of the printer fosters innovation and collaboration, accelerating the adoption of sustainable materials and manufacturing methods.",
        "authors": [
            "Jesse P. de Alva"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158914",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design of a cam and follower linear actuator for satellite optical systems",
        "abstract": "Optical systems for satellites are used to image and track the physical environment of earth from space. Where the optical system images can be controlled through the rotation and movement of the optical system. Optical alignment is achieved though linear actuators, which constrain different degrees of freedom of the optical system. Optical systems require precise alignment, meaning the linear actuators that align them must have precise resolutions. During satellite launch, the satellite experiences both high acceleration and large magnitude vibrations, \r\nwhich can damage equipment. Common precision actuation methods cannot meet the high stiffness required for these satellite linear actuators. A cam and follower linear actuator was \r\ndesigned to fulfill these stiffness and precision requirements. Through modeling the dynamic and kinematic interactions between the cam and follower, a cam shape was designed, and necessary materials were chosen. Next through analysis of process capabilities of available \r\nfabrication tools, manufacturing methods for different parts were selected. Finally, using components designed for testing, kinematic tests were conducted on the linear actuator. Testing \r\nof the actuator demonstrated it was capable of actuating with a precision of 9.15 microns. More testing is needed to understand the stiffness of the device.",
        "authors": [
            "Darrell Brown"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158847",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Uncovering the link between twin-twin interactions and damage nucleation in an (α+β) Ti alloy",
        "abstract": "Recently, a (α+β) Ti alloy was developed with an outstanding combination of both high strength and high ductility; however, the plasticity micromechanisms that lead to damage nucleation for this alloy had not yet been investigated in detail. In this work, post-mortem analysis and an in-situ SEM-EBSD tensile experiment were conducted to determine where damage was nucleating most frequently in the microstructure, and what deformation modes were associated with damage nucleation. Damage within primary α grains was found to be the most common, with most of these damage incidents occurring along {10̅12} twin-twin boundaries with a ~60° misorientation. The {10̅12} twinning mode is only activated in the localized neck, and twin activation is strongly dependent on initial crystallographic texture. The twinned domains are rotated such that prismatic slip is easier to activate, but prismatic slip transfer is unlikely across ~60° twin-twin boundaries due to geometric incompatibilities. The in-situ test revealed that a crack formed along a ~60° twin-twin boundary where slip was blocked. These findings provide new insights into how twin-twin interactions in Ti alloys can lead to damage nucleation and impact overall ductility.",
        "authors": [
            "Megan F. L. Cooper"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158903",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Forecasting the lift of a randomly maneuvering airfoil\r\nunder dynamic stall conditions, Re ∼ 10⁵",
        "abstract": "Dynamic stall is the abrupt flow separation from airfoils rapidly changing their orientation. This phenomenon, characterized by a delayed stall followed by a sharp drop in lift, has prompted efforts to prevent or delay it. This study aims to predict the lift of an airfoil randomly maneuvering under dynamic stall conditions by utilizing sparse surface pressure measurements, which we believe can maximize the effectiveness of various dynamic stall suppression techniques. Using data from large eddy simulations, we demonstrate that a long short-term memory network, fed with raw surface pressures, delivers accurate predictions. Also, a new method introduced here, IdDM, conclusively links the characteristic frequency range of pressure fluctuations that emerges during the dynamic stall to the chord-lengthscale vortex dynamics. However, further analysis suggests that the forecast predominantly relies on the lower frequency components tied to the airfoil motion, possibly because the vortex dynamics are dependent on and sensitive to the airfoil motion. Meanwhile, specific sensor locations are proven to be more informative than others in this random, unsteady flow, and we show that optimal sensor placement can be quickly determined using mutual information alone. It reveals that two pressure sensors positioned near the leading edge, one on each side of the airfoil, capture most of the information needed to predict lift. The lift can be predicted with sparse sensors because surface pressures are strongly correlated across the airfoil, with large-scale flow structures dominating the forces.",
        "authors": [
            "Donghyun Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158900",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Reasoning over Hierarchical Abstractions for Long-Horizon Planning in Robotics",
        "abstract": "We aim to enable robots to act intelligently in complex environments not explicitly designed around them. In order to do so, robots can simplify decision making by forming hierarchical abstractions of their world, and planning within those representations. However, in reality, the types of abstractions robots are able to build are often poorly aligned with the planning problems they must solve, which limits how useful those abstractions can be in efficient decision making. For example, autonomous agents struggle in many real world scenarios, particularly when their environments are large, cluttered with obstructions, or beset by uncertainty. These factors often imply that decisions made at higher levels of abstraction may not be easily refined to low level plans, leading to backtracking during either search or execution. In this thesis, we consider contributions which improve the efficiency and quality of long-horizon hierarchical planning in robotics. Specifically, we propose approaches which explicitly reason about the imperfections of the abstractions available to robots during planning, and show how those methods can improve performance on a variety of tasks and environments.\r\n\r\nThere are three primary settings for which we make contributions in this thesis. First, we will consider the problem of solving tasks in partially revealed environments, wherein our abstract plans cannot be known to be feasible until we attempt execution because the world is not fully known at planning time. To solve this problem, we first develop a high level planning representation which recognizes that actions that enter unknown space can either succeed or fail with some probability. The first contribution of this work is then to learn to predict the feasibility and cost of actions within that abstraction from visual input. We also describe a method for planning which uses these predictions, and we are able to show that our approach can generate plans that are significantly faster at completing tasks in unknown environments experimentally when compared with heuristic driven baselines. Next, we will discuss work in Task and Motion Planning (TAMP), where the world is fully known, but the problems require complex interaction with the environment to the point that we must intelligently guide search in order to find plans efficiently. We build upon our work in the first setting by once again learning to predict the outcome and cost of different sub-tasks within a TAMP abstraction. We further contribute a novel method to guide search in this setting for plans which minimize cost given our learned predictions, and demonstrate the ability to find faster plans than established TAMP approaches both in simulation, and on real world robots. In our final problem setting, we consider attempting to solve TAMP problems in real world, large-scale environments. To do this, we define an approach for constructing tractable planning abstractions from real perception using hierarchical scene graphs, ensuring that when we refine our abstract plans within these representations, the low-level trajectories still satisfy the given task’s constraints. A major contribution of this work is an approach for planning efficiently in these domains by pruning provably superfluous information from the world model. The unifying aim of the work in this thesis is to develop approaches which enable robots to solve complex tasks in large-scale, real world environments without human intervention. To that end, across all contributions, we demonstrate experimentally on real robots the importance of accounting for imperfections in hierarchical abstraction during planning.",
        "authors": [
            "Christopher P. Bradley"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158796",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Causal Inference with Survival Outcomes via Orthogonal Statistical Learning",
        "abstract": "The field of causal inference has recently made great strides in incorporating machine learning into confounding adjustment and estimation of heterogeneous treatment effects (HTE). However, there were some gaps regarding survival outcomes.\r\n\r\nFirst, overlap-weighted effect estimators based on machine learning nuisance models were not available for such outcomes. Thus, researchers wishing to mitigate bias and variance from poor overlap had to accept potential bias from nuisance model misspecification in its place. In Chapter 2, we fill this gap by proposing a class of one-step cross-fitted double/debiased machine learning estimators for cumulative weighted average treatment effects for both survival outcomes and competing risk outcomes. Our approach combines importance sampling, semiparametric theory, and Neyman orthogonality to resolve both model misspecification and lack of covariate overlap between treatment arms in observational studies with censored outcomes. We give regularity conditions for the consistency, asymptotic linearity, and semiparametric efficiency bounds of the proposed estimators. Through simulation, it is shown that the proposed estimators do not require oracle parametric nuisance models. We apply the proposed estimators to compare the effects of two first-line anti-diabetic drugs on cancer outcomes.\r\n\r\nSecond, a wide range of machine learning methods (or ”learners”) for estimating heterogeneous treatment effects were not applicable to estimating effects on survival outcomes, particularly in the presence of competing risks. In Chapter 3, we fill this gap by developing several once-for-all (orthogonal) censoring unbiased transformations which convert time-to-event data into continuous outcomes, such that all HTE learners and oracle rates for continuous outcomes can be borrowed. Our approach not only reduces the pressing need to develop various HTE learners for censored outcomes and especially competing risks, but also fully leverage the state of the art of existing schemes. Through direct application of HTE learners to these transformed continuous outcomes, we obtain consistent estimates of heterogeneous cumulative incidence effects, total effects, and separable direct effects. We provide generic model-free learner-specific oracle inequalities bounding the finite-sample excess risk. The oracle efficiency results depend on the oracle selector and estimated nuisance functions from all steps involved in the transformation. We demonstrate the empirical performance of the proposed methods in simulation studies.\r\n\r\nAn important application area for causal inference methods, and one which originally motivated my interest in the field, is drug repurposing. In Chapter 4, we apply the methods of Chapter 2 to investigate whether metformin, a diabetes medication, might also have unexpected beneficial effects on cancer. The analysis encountered three major challenges: poor overlap between treatment groups, model misspecification, and pre-cancer death as competing risks for cancer incidence. To resolve these issues simultaneously, we take balancingweighted total cause-specific effects, controlled direct effect, and separable effects as causal estimands and develop balancing-weighted double/debiased machine learning estimators for both cumulative incidence functions and restricted mean time lost, with all estimators satisfying Neyman orthogonality. Using the Clinical Practice Research Datalink (CPRD) data, we find that metformin revealed a preventive direct effect on cancer incidence over sulfonylureas. The results also demonstrate the advantage of choosing the average treatment effect for the overlap population as the target quantity.\r\n\r\nFinally, just as machine learning helps to automate nuisance model estimation for confounding adjustment and modeling effect heterogeneity, causally informed artificial intelligence (AI) and large language models (LLMs) might help to automate hypothesis generation for drug repurposing and surveillance opportunities. In Chapter 5, we explore this potential by developing a high-throughput screening approach to evaluate available drugs across multiple diseases. The screening methodology aims to identify drug-disease pairs with significant positive signals that could represent promising repurposing candidates, while also detecting pairs with negative signals that might indicate potential safety concerns–both being critical aspects for pharmacoepidemiology research. This systematic approach leverages the convergence of expanding healthcare data sources and modern data science advances to establish a data-driven framework for drug repurposing discovery and pharmacovigilance.\r\n\r\nTo conclude, we discuss the limitations of the proposed methods and provide possible future research directions.",
        "authors": [
            "Shenbo Xu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158798",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Examining the placenta’s role in neurodevelopment in the context of maternal obesity",
        "abstract": "The placenta is a key organ determining fetal development and likely contributes to programming of long-term offspring health, in particular neurodevelopment. Various maternal exposures, such as psychosocial stress, diabetes, infection, and high body mass index (BMI) are associated with higher risks of impaired neurodevelopment in the offspring. One third of women in the United States are affected by maternal obesity (MO) during pregnancy, making it one of the most common exposures.\r\nWe profiled the term placental transcriptome in humans using single-nucleus RNA-seq, comparing expression profiles in MO versus lean conditions, in each of the two faces of the placenta separately. On both sides of the placenta across several cell types, MO was associated with upregulation of hypoxia response genes. On only the maternal-facing side, hypoxia gene expression was associated with offspring neurodevelopment outcomes measured at multiple time-points, in the Genetics of Glucose regulation in Gestation and Growth (Gen3G) cohort, an independent pre-birth cohort with bulk RNA-seq from placental tissue. We leveraged Gen3G to determine genes that correlated with impaired neurodevelopment and found these genes to be most highly expressed in extravillous trophoblasts (EVTs). EVTs further showed the strongest correlation between neurodevelopment impairment gene scores (NDIGSs) and the hypoxia gene score. We validated these findings in EVTs in an independent single-cell RNA-seq cohort from second trimester placenta, and found that cultured EVTs have increased NDIGSs in response to exposure to hypoxia. These data suggest that hypoxia in EVTs may be a key process in the neurodevelopmental programming of fetal exposure to MO. Our work opens up new directions of research, such as exploring applications of antioxidants to potentially mitigate some of the offspring consequences associated with MO.",
        "authors": [
            "Fatima M. Gunter-Rahman"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158860",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Marketplace Multiculturalism",
        "abstract": "Picture Texas. No longer simply cowboys, footballs, and firearms, this land today is sustained by a daily choreography of cross-border commerce, managed by entertainment media turned handheld surveillance, and peppered with enclaves of immigrants from the world over. A contact zone where logistical and legislative apparati warp to serve consumer comfort, Texas today is the world tomorrow: forget the Alamo, it’s highways, tax-incentives, and backyard barbecue on the 21st century frontier. This thesis responds to a call for roadside service stations along a planned international tourist corridor in the Texas-Mexico borderlands with six interventions: a panoramic viewing tower disguised as a billboard, a sunken stadium for athletic agonism, a photovoltaic drive-in charging cinema, an international culinary incubator, a showroom for automated fulfilment, and a customs and border patrol welcome center. These structures are testing grounds for modes of relation and value exchange that edge beyond the outdated positivisms of globalization. They ask how architecture might produce new possibilities and publics by working within and taking advantage of contemporary systems of control. As tourist destinations, the stops suggest the nation’s true mythos lies not in static symbols but in choreographies of transaction and contact. Articulating in built form the dynamic processes that define a territory of sprawl, this proposal suggests that Texas’s most authentic monuments are the stops we make along the way.",
        "authors": [
            "Harris Chowdhary"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158823",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Predicting Flood Risks to City Infrastructure Systems\r\nUtilizing Scalable, Time Sensitive Modeling",
        "abstract": "Flooding is emerging as the most expensive and frequent natural hazard around the world. Floods are highly dynamic in nature and cause physical damage to our built environment, loss of life, economic damage, and major impacts to society. An example of this is the at-ground road system, which comprises 30-60% of a city’s area in the US, is highly susceptible to flood damage, while still needs to act as evacuation routes for local residents. Similarly, the underground built system is extremely vulnerable to flooding damage as well as life risk to anyone within it. With urban landscapes constantly evolving, accurately predicting flood propagation and extent is imperative to mitigate these risks, especially as floods worsen due to climate change.\r\n\r\nHistorically, the focus of flood risk assessment through industry and academia has been on the coastal urban environment, assessing the impact of fluvial flooding. This resulted in many risk assessment tools that mostly caters to the infinite amount of flood water identified from a riverine or coastal fluvial flooding. As for the rain-driven impact, the common practice simply changed the flood modeling to pluvial oriented, keeping the rest of the risk tool components identical for the different flood mechanisms. For pluvial flooding, existing urban flood modeling tools such as SWMM and PC-SWMM are limited by their catchment-based approach, neglecting surface runoff dynamics and spatial-temporal flood impacts. Consequently, these tools fail to capture the full extent of rain-driven floods, underestimating their severity and impact on urban environments.\r\n\r\nAddressing this gap requires sophisticated simulations that account for rain event characteristics and city morphology, yet such simulations are computationally demanding and require detailed urban data. Currently, flood impact analysis tools lack specificity for pluvial flood risks and do not address the risks to various city systems beyond building damage. As a result, the contribution of pluvial floods to overall flood risks is underestimated, compromising infrastructure resilience. As flood model results are a critical component in flood risk assessments, the accuracy of spatial temporal urban flood results will allow the pluvial flood impact assessment to be simplified and the flood damage to the different urban systems will be quantified.\r\n\r\nThis research aims to develop a scalable and streamlined method to accurately quantify the risks of rain-driven floods to urban infrastructure systems. It addresses three key questions: (1) To what extent does current practice underestimate pluvial flood impacts? (2) What are the impacts of pluvial flooding on pavement systems when incorporating spatial-temporal modeling? (3) What is the significance of modeling pluvial floods using urban underground spaces? Using advanced flood modeling and numerical soil-water infiltration techniques, this research will quantify damages and lifecycle impacts to pavement and underground spaces systems. The method will provide information on the spatial and temporal distribution of flood damage and will enable scaling up single-element assessments to system-wide impacts. This holistic approach will improve urban flood risk management, supporting informed decision-making and the development of resilient infrastructure systems.",
        "authors": [
            "Katerina Boukin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158861",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "(De)fluorination of Organic Substrates Mediated by Nontrigonal Phosphorus Triamide",
        "abstract": "Due to its high electronegativity and small size, fluorine atoms form the strongest single bond to carbon, and impart unique physical, chemical, and physiological properties to organic compounds. Therefore, the number of industrially synthesized products containing fluorine has seen a substantial increase in recent decades. The strategies to access organofluorine compounds include two opposite approaches: 1) (nucleophilic, electrophilic, or radical) fluorination, and 2) selective defluorination of polyfluorinated substrates. Both creating and breaking C−F bonds in selective manners are of great importance, and present challenges of their own. The work herein describes chemical transformations incorporating the cleavage or formation of the C−F bonds mediated by nontrigonal phosphorus triamide. Thanks to the enhanced biphilicity resulting from geometric deformation, the Cs-symmetric tricoordinate phosphorus compound can activate strong covalent bonds.\r\n\r\nAt the outset, Chapter 1 reviews the existing literature on (de)fluorinative chemical transformations focused on deoxyfluorination and hydrodefluorination, as well as examples of nontrigonal tricoordinate phosphorus compounds and their characteristic reactivity. Combining the two approaches, Chapters 2 and 3 introduce method development for accessing organofluorine compounds using a butterfly-shaped phosphorus triamide as a bond activator. In Chapter 2, the method for deoxyfluorination of aliphatic alcohol substrates via O−H activation by phosphorus, catalyzed by borane Lewis acids, is detailed. The scope of the method covers tertiary alkyl fluorides, which are generally challenging targets, selectively yielding stereoinversion products for chiral substrates. Chapter 3 reports a closed P(III)/P(V) synthetic cycle for the hydrodefluorination of polyfluoroarene substrates that consists of C−F oxidative addition, F-to-H ligand metathesis, and C−H reductive elimination. The overall sequence is analogous to transition metal-catalyzed aryl cross-coupling reactions. Taken together, the methods described in this dissertation highlight the potential of nontrigonal phosphorus compounds as a mediator for the manipulation of strong covalent bonds, useful in the development of synthetic methods that complement existing ones.",
        "authors": [
            "Soohyun Lim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158937",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design and Engineering of Protected Superconducting\r\nQubits",
        "abstract": "Building extensible quantum information processors becomes increasingly promising as the qubits exhibit longer coherence times. To this end, realizing protected qubits, whose Hamiltonians are inherently resilient to both relaxation and dephasing, has attracted strong interest. In this thesis, we primarily explore the soft 0 − π qubit, a leading candidate for implementing superconducting qubit protection with current fabrication techniques. To enhance protection, the soft 0 − π qubit requires its two major modes, the charge-mode (θ) and the flux-mode (ϕ), to satisfy an asymmetric condition: maximizing charge-mode capacitance while minimizing flux-mode capacitance. The main challenge is therefore reducing stray capacitance from the large charge-mode capacitor, which hinders the reduction of flux-mode capacitance. To address this challenge, we depart from the conventional coplanar interdigitated capacitor design and use parallel-plate capacitors (PPC) with small footprints, achieving the desired large charge-mode capacitance while reducing unwanted stray capacitances. By reducing the capacitor area by a factor of approximately 50, the PPC 0−π qubit has achieved an estimated Eᵠ_C /Eᶿ_C ratio of 30–50, placing it among the highest reported. Additionally, we propose enhanced mode-selective control of the soft 0−π qubit using these parallel-plate capacitors. Finally, we discuss the remaining challenges of the soft 0−π qubit and introduce alternative parameter regimes that can potentially improve Raman-based control and qubit readout.",
        "authors": [
            "Junghyun Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158919",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Freight Distribution During Disasters: Measuring and Improving Operational Performance of Critical Systems",
        "abstract": "The frequency and intensity of weather-related natural disasters have increased in the last őve decades. Moreover, the US faces more than a third of the disaster-related economic losses globally, majority of which are from storms. As the demand for distribution of essential freight increases during disasters, the physical and operational constraints decrease the capacity of the freight distribution systems. Accordingly, public and private-sector stakeholders seek disaster preparedness and response interventions to ensure timely and economic distribution of vital freight to the population in need. The goal of this thesis is to facilitate better strategic and tactical planning that results in higher operational performance of essential freight distribution systems during disasters. We study two critical freight distribution systems, namely, downstream fuel distribution and full truckload transportation of general freight. Truckload transportation plays a vital role in distributing relief supplies during emergencies, and fuel is required for humanitarian operations such as running generators, moving emergency response crews, and evacuation of the affected population. We collaborate with The US Federal Emergency Management Agency in response to multiple North-Atlantic storms and measure the operational performance of these systems under regular and disaster conditions, as well as identify public and private-sector interventions to make the performance better during future disasters. Our research contributes to the bodies of disaster modeling and management, fuel distribution, service procurement, and truckload procurement literature by, i) creating system level understanding of multi-server tandem cyclic queues with time-limited customers, ii) studying process improvement interventions for disasters, iii) quantifying the magnitude, geographical extent, timing, and duration of the causal effects of disaster conditions and consequent disaster relief activities on transportation procurement prices, iv) using datadriven analysis to design ŕexible truckload contracts that consider uncertainty in demand, and v) modeling dynamic-pricing where the buyer offers the price to service providers. In this thesis, we provide several actionable insights for public and private-sector stakeholders to manage freight distribution during future disasters. We identify which process improvement interventions are best suited for which type of downstream fuel distribution system, and which storage terminals should be prioritized under limited budget. We also measure how private-sector shippers should account for changes in truckload spot procurement prices during disaster episodes to manage their budgets and operational decisions. Moreover, we offer an alternate dynamic-priced truckload contract solution for public-sector shippers that deal with uncertain episodic demand in response to disasters. We demonstrate the impact of our research by implementing it to multiple real-life case studies in the US. Furthermore, our methodologies and results are generalizable to other geographical regions as well as other disaster conditions. Thus, we hope that they are used by public and private-sector actors to better manage essential freight distribution moving forward.",
        "authors": [
            "Shraddha Rana"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158803",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design of Lewis Acidic Pnictenium Ions Using Carbone and Capping Arene Ligands for Bond Functionalization",
        "abstract": "Interest in the chemistry of antimony and bismuth is rapidly growing due to isolation of low coordinate, subvalent or Lewis acidic compounds that can mediate reactivity traditionally reserved for their d block counterparts. Ligand strategies play a key role in the isolation of such species. Anionic ligands with large steric profiles, as well as carbenes, have been widely implemented to stabilize subvalent heavy group 15 element compounds. However, synthetic strategies to prepare Lewis acidic antimony and bismuth complexes remain underexplored. Cationization is one of the most common methods used to enhance the Lewis acidity of heavy group 15 elements by creating a vacant p orbital on the pnictogen atom. Lewis acids are also employed in frustrated Lewis pair (FLP) chemistry to enable intra- and intermolecular reactivity. Carbone ligands, which are neutral, 4 electron donor ligands, offer a unique ability to support highly electrophilic main-group elements. This dissertation investigates the stabilization of heavy pnictenium ions using neutral donor ligands, such as carbodicarbenes and capping arene ligands, and explores their potential in Lewis acid-mediated chemistry. In Chapter Two, the synthesis and characterization of a series carbodicarbene-pnictenium ions is described. The utilization of strongly donating carbodicarbene ligands enables the isolation of mono-, di- and tri-cationic antimony and bismuth cations. These ions have multiple bond character between carbon and antimony/bismuth, representing some of the first examples of stibaalkene and bismaalkene cationic compounds. The Lewis acidity of these ions was assessed using the Gutmann-Beckett method and computationally derived fluoride ion affinities, the latter of which indicates Lewis superacidity for the bis(pyridyl)carbodicarbene-pnictenium trications. In Chapter Three, the reactivity of the bis(pyridyl)-carbodicarbene stibenium trication toward C(sp³)–H and C(sp)–H bonds is demonstrated. The Lewis superacidic antimony cation mimics the chemistry of frustrated Lewis pairs in the presence of the sterically encumbered base 2,6-di-tert-butylpyridine to enable C–H bond breaking of acetonitrile and a set of terminal alkynes. Kinetic analyses, in conjunction with density functional theory, support a mechanism by which acetonitrile coordinates to antimony, acidifying the C–H bonds, which can be subsequently deprotonated by the base in solution. The resulting stiba-methylene nitrile and stiba-alkynyl adducts undergo reactivity with elemental iodine to generate iodoacetonitrile and 1-iodoalkynes while reforming a stibenium trication. In Chapter Four, capping arene ligands are coordinated to antimony and bismuth tribromide to afford a series of κ²-bound complexes. Bromide abstraction from these neutral adducts affords ionic compounds. Both the neutral and ionic species have distinctive Menschutkin interactions, whereby the lone pair on the pnictogen atom is oriented toward the π system of the pendant arene. Shortening of the distances between the pyridyl nitrogen atoms and pnictogen atom are observed upon cationization from the neutral adducts. The Lewis acidity of these complexes was assessed using the Gutmann-Beckett method. Notably, acceptor numbers as high as 111 are observed for these ions.",
        "authors": [
            "Levi Warring"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158947",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Underwater Semantic Simultaneous Localization and\r\nMapping",
        "abstract": "Building semantically meaningful object level maps of underwater environments is crucial for enabling higher-level autonomy, fostering human-robot collaboration, and providing compressed map representations for bandwidth-constrained underwater communications, while localizing against such maps can improve the positioning accuracy of underwater vehicles by correcting for odometric drift. However, underwater semantic simultaneous localization and mapping (SLAM) has lagged behind analogous terrestrial and aerial semantic SLAM techniques largely due to the lack of large labeled underwater datasets and the challenging sensor modalities specific to underwater environments. To address these shortcomings, this thesis develops a range of methodologies to advance underwater semantic SLAM capabilities. \r\n\r\nFirst, self-supervised learning and visual foundation models are leveraged to detect and segment underwater objects in an open-set manner, i.e., objects need not be present in the training dataset to be detected. The machinery of the open-set object detection technique breaks several assumptions made by existing closed-set semantic SLAM methods. Thus, new methods for object representation and data association are proposed and demonstrated. A method to localize underwater objects is then developed through an analysis of the geometry of underwater monocular cameras and multibeam sonars. \r\n\r\nFinally, a formulation of open-set object-level place recognition as a graph matching problem is introduced. The formulation includes a method for calculating and tracking semantic uncertainty for open-set object detections. Experimental results on both underwater and terrestrial datasets demonstrate that the proposed formulation can be used for real-time accurate open-set object-based place recognition. \r\n\r\nIn summary, techniques for underwater object detection, localization, and data association are introduced and integrated with probabilistic graphical models for open-set semantic SLAM. The proposed techniques are tested across a wide variety of scenarios, and are shown to generalize to terrestrial settings as well.",
        "authors": [
            "Kurran Singh"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158852",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "CO₂ Capture with Lithium Oxide in Molten Salt Media : A Case Study of CO₂ Capture via Electrochemically Produced Metal Oxide",
        "abstract": "As the unprecedented temperature rise originating from anthropogenic carbon dioxide (CO₂) emission intensifies, the development of post-combustion carbon capture technologies has been urged. Although its maturity, conventional thermal swing processes using aqueous amines, suffer from significant limitations, including high energy requirements and sorbent degradation. Electrochemical CO₂ capture technologies, which use electrical energy instead of thermal energy, have emerged as an energy efficient way to capture CO₂. This shift not only improves energy efficiency but also reduces reliance on fossil fuels, further contributing to reduction in CO₂ emissions. This work explored the potential of electrochemical metal oxide formation for CO₂ capture, a promising alternative to amine-based systems due to its exceptional sorbent (i.e., metal oxide) stability. Li₂O in eutectic mixture of potassium nitrate (KNO₃) and lithium nitrate (LiNO₃) was chosen as a case study due to the relatively well-understood chemistry of the system and the potential synergistic effects between metal oxide and the molten salt. Primarily, we investigated the synergistic effect of Li₂O in nitrate molten salt via thermal gravimetric analysis. Next, electrochemically produced Li₂O by reduction of oxygen gas was tested as a CO₂ sorbent while investigating parameters affecting its conversion to lithium carbonate (Li₂CO₃). Through this study, we suggested dissolution model as a crucial pathway for conversion. Lastly, we explored the effect of adding nitrite ion (NO₂⁻) to the molten salt. Irreversible side reaction between NO₂⁻ and CO₂ was confirmed with X-ray diffraction and NOₓ measurement. This thesis demonstrates the feasibility of electrochemical metal oxide-based CO₂ capture, highlighting some considerations in the capture step.",
        "authors": [
            "Gi Hyun Byun"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158875",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Learning-Based Complex Terrain Navigation Under Uncertainty",
        "abstract": "In complex off-road environments, accurately identifying traversable terrain is crucial for achieving fast and reliable navigation. Existing methods learn terrain properties directly from data via self-supervision to automatically penalize trajectories moving through undesirable terrain. However, challenges remain in properly quantifying and mitigating risk due to uncertainty in learned models and improving model generalization in novel environments. To address these challenges, this thesis presents a unified framework to learn uncertainty-aware, physics-informed traversability models and achieve risk-aware navigation in both indistribution and out-of-distribution terrain. First, the proposed method efficiently quantifies both aleatoric and epistemic uncertainty by learning discrete traversability distributions and probability densities of the traversability predictor’s latent features. Leveraging evidential deep learning, this work parameterizes Dirichlet distributions with network outputs and proposes a novel uncertainty-aware squared Earth Mover’s distance loss with a closed-form expression that enhances learning accuracy and navigation performance. Second, the proposed method achieves risk-aware navigation by simulating state trajectories with the worst-case expected traversability values to handle aleatoric uncertainty and by penalizing trajectories moving through novel terrain with high epistemic uncertainty. Third, the proposed method improves model generalization by embedding physics priors directly into the mathematical formulation of evidential neural networks and implicitly aligning learned models with physics models through a physics-informed training loss. Finally, through extensive simulation and real-world experiments on wheeled and quadruped robots, it is demonstrated that this work leads to faster navigation with higher success rates when compared to existing risk-aware approaches, even in environments with significant distribution shifts.",
        "authors": [
            "Xiaoyi Cai"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158876",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design-technology Co-optimization for Sub-2 nm Technology Node Based on 2D Materials",
        "abstract": "Emerging disruptive technologies such as Artificial Intelligence (AI) and 6G communications have driven stringent demands for hardware components that enable faster and more energy-efficient computation. With the diminishing returns of traditional silicon-based scaling and the escalating complexity of advanced semiconductor processes, two-dimensional (2D) materials offer promising opportunities when developed through Design-Technology Co-Optimization (DTCO). This thesis presents a comprehensive study of DTCO with a novel framework tailored for 2D material-based electronics that addresses critical challenges in material synthesis, device design, and circuit integration. In this framework, experimental material and device data are integrated into the design and optimization of MoS₂-based multichannel transistors (MCTs). With the help of DTCO, we have achieved record performance for double-gate, single-channel MoS₂ transistors as well as the first demonstration of high-performance, functional double channel MoS₂ transistors. Based on the results of MCTs, a Process Design Kit (PDK) is developed to facilitate circuit-level integration. These advancements constitute a promising foundation for the development of next-generation electronics beyond sub-2 nm technology node.",
        "authors": [
            "Aijia Yao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158931",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On Hing Travel Agency Fictional Archive of Disappearing Hong Kong",
        "abstract": "Hong Kong, shaped by rapid transformation and precarious land ownership, is a city where erasure defines its urban landscape. Amid this flux, a place I once called home was demolished, prompting the question: “How can one return to a place that no longer exists?” This thesis explores the transformative potential of disappearance, reframing it as a generative force that creates space for imagination, resistance, and continuity. Through On Hing Travel Agency (OHTA), demolished buildings \"travel\" into fictional worlds, becoming vessels of memory and imagination. Rooted in Hong Kong’s literary tradition—where fiction resists erasure and archives aspirations—the project employs fiction as both a tool of preservation and a site for belonging. Fictional destinations, inspired by Hong Kong novels, such as The Permanent City (1959), The Floating City (1986), and The Vanished Cities (2010), reflect pivotal historical moments while offering pathways to reconcile personal loss and master alternative spatial logics. The project culminates in the Lost Traveler’s Guide to Hong Kong, a publication curating maps, brochures, and layered narratives to immerse travelers in speculative thinking. By bridging the past and future, real and imagined, OHTA is a attempt to demonstrates how fiction can reclaim agency within the politics of disappearance, transforming loss into a catalyst for new narratives and creative engagement. Even in absence, Hong Kong’s disappearing spaces retain their resonance, generating new narratives and underscoring the creative potential of loss.",
        "authors": [
            "Ina Wu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158840",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Investigating the Atmospheric and Oceanic Drivers of Atlantic Multidecadal Variability and Predictability",
        "abstract": "Despite its numerous impacts across the Earth system, the relative importance of ocean and atmospheric dynamics in generating Atlantic Multidecadal Variability (AMV) remains an open question. This thesis presents three pathways to understanding how oceanic and atmospheric processes generate key spatio-temporal signatures of AMV through a combination of processed-based and data-driven approaches. Part 1 (Chapter 2) takes a \"bottom up\" approach, building a hierarchy of stochastic models to identify the contributions of vertical entrainment and seasonality in local upper-ocean processes to sea surface temperature (SST) variability. Through this hierarchy, I highlight unrealistic features present in slab ocean models widely used to isolate atmospheric contributions to AMV. On the opposite end of the spectrum, Part 2 (Chapter 3) utilizes a \"top-down\" data-driven approach where deep neural networks are trained to predict the North Atlantic SST Index in both the Community Earth System Model 1 Large Ensemble (CESM1) and observation-based datasets using atmospheric and oceanic predictors. I apply explainable artificial intelligence techniques to highlight a significant source of multidecadal predictability over the Transition Zone in oceanic predictors such as sea surface salinity (SSS) and sea surface height in the presence of external forcings. Part 3 (Chapter 4) returns to the process-based hierarchy, but applies this to understanding SSS variability. The stochastic salinity model is used to investigate the role of mixed-layer re-emergence, subsurface ocean damping and SST-evaporation feedback in shaping the pattern and amplitude of AMV.",
        "authors": [
            "Glenn Yu-zu Liu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158897",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Weak Shock Waves on a Chip: Generation and Applications",
        "abstract": "In conventional laser-shock experiments in solid media, shock waves are typically excited from the ablation of a photoacoustic transducer layer deposited onto the sample of interest. Unavoidably, the target materials are damaged. This leads to the necessity of changing targets after each exposure, likely lowering the shot-to-shot reproducibility and data quality, while lowering the throughput of the experiment. Motivated by the need to generate large-amplitude transient strain waves at a high repetition rate, this thesis introduces a novel platform for the non-destructive generation and amplification of acoustic waves with associated strain levels in the percent range — up to the formation of shock waves. The acoustic amplification scheme is first described. Then, owing to the capabilities of the technique to repeatedly load a material with finite-amplitude strain waves, a demonstration of the use of the platform for microscale fatigue testing is made. Finally, the strain localization of surface acoustic waves is leveraged by transiently modulating a monolayer of a transition metal dichalcogenide deposited on a substrate.",
        "authors": [
            "Jude Deschamps"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158942",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Planning Beyond Crisis: The Promise of Insurgent Planning in Post-Disaster Mocoa",
        "abstract": "During the evening of March 31, 2017, a catastrophic landslide engulfed the Colombian city of Mocoa, killing at least 335 people in roughly thirty minutes. Seventy others disappeared, and over one hundred people were reported injured across 48 neighborhoods, and roughly 1,500 housing units were destroyed. With a total of 22,000 people impacted, this catastrophe was the deadliest disaster affecting Colombia in recent decades. Yet, despite an alignment of major national political commitments, international cooperation, and a multi-million-dollar humanitarian budget, reconstruction plans have not been completed seven years later. Why? As the first comprehensive analysis of the landslide and its aftermath, this dissertation is a novel investigation into the competing forces that ultimately canceled the central reconstruction plan, demonstrating that the kind of disruption caused by the disaster mobilized new actors and new forms of agency. In contrast to the popular perception that this kind of lack of remediation suggests the failure of urban governance, the dissertation speaks to the success of activists who have neutralized the government’s reconstruction plan, which activists perceived as worsening the circumstances leading up to both the catastrophe and recovery. Distinguishing between the “landslide” and the “larger disaster,” the dissertation further explains the government’s proposed reconstruction plan within a history of violent extraction, dispossession and displacement. Framing an original case consisting of fifteen planning vignettes to trace actions, reactions and counteractions, I expose the reduction of the planning process as crisis urbanism. My research contributes to our understanding of variability among insurgent planning actors and their invented spaces for engagement in the context of disaster, by defining technocratic resistance as a valid form of dissent inside the government, and by proposing a new device for the study of insurgent planning called transformative spaces enabling local community’s right to plan. Drawing on contemporary debates on anti-crisis, risk and decolonial thought, the dissertation imagines an alternative paradigm for planning beyond crisis that enables radical community action through dissenting grassroots leadership. \r\n\r\nKeywords: crisis urbanism, technocratic resistance, insurgent planning, regenerative planning, anti-crisis, risk, decolonial thought",
        "authors": [
            "Juan Camilo Osorio Botero"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158822",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Dynamics of Diversity, Equity, and Inclusion Practice Adoption",
        "abstract": "Despite the widespread adoption of Diversity, Equity, and Inclusion (DEI) initiatives in corporate America, significant disparities persist in the representation, compensation, and treatment of women and racial minorities. This paper investigates why well-intentioned DEI efforts often fail to achieve their intended outcomes and identifies managerial barriers to progress. This research employs a qualitative dynamic modeling approach to analyze the complexities of DEI practice implementation within organizations. I conducted a scoping review, focusing on longitudinal and experimental designs to identify key mechanisms influencing the outcomes of DEI practices. The interplay between organizational processes and individual cognitive and behavioral responses can be illustrated via reinforcing and balancing feedback loops that I map onto a causal loop diagram, which reveals how DEI initiatives interact with existing organizational processes and cultural dynamics. This paper introduces a dynamic perspective on DEI practice implementation, highlighting the feedback mechanisms that can either hinder or facilitate progress toward diversity goals. The model reveals that certain DEI practices may inadvertently trigger reinforcing loops that perpetuate inequality. By mapping DEI practices and their effects, this study provides a framework for understanding how DEI outcomes can diverge significantly depending on different implementation strategies. It underscores the importance of considering the endogenous feedback effects of DEI initiatives and offers insights into strategic interventions that can disrupt undesirable reinforcing cycles and promote progress toward organizational diversity, equity, and inclusion.",
        "authors": [
            "Aishwarya Pandey Yadama"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158834",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Modeling and Analysis of Voltage Feasibility Problems for\r\nCost-Effective Microgrids",
        "abstract": "Global efforts to mitigate climate change have led to a significant increase in the integration of renewable energy resources into the electricity grid. This transition not only necessitates the adoption of renewable energy technologies but also requires rethinking and redesigning existing power grid infrastructures to accommodate the unique characteristics of these resources. This research focuses on modeling techniques which can assist in analyzing the feasibility of microgrid topologies. Microgrids have emerged as a flexible and efficient approach to implementing novel grid topologies that support higher levels of renewable energy penetration. They also support the integration of distributed energy resources (DERs), such as photovoltaic (PV) systems, thereby promoting a more sustainable and efficient energy grid design. This thesis utilized sanitized load and system topology data from a real world microgrid located in Illinois to test the feasibility of increasing the number of PV units the system can utilize for reactive power support. \r\n\r\nIn these systems, ensuring feasibility is a crucial concern due to power mismatches caused by the inherent variability of renewable resources. This work focuses of maintaining voltage within the constraints while increasing PV penetration on the system. We simulate the implementation of microgrids with PV generation using Alternating Current Optimal Power Flow (AC-OPF). The results of this thesis show the limits of feasible reactive power support from distributed PV units on a utility disconnected microgrid based on our voltage constraints. The study shows that there exists a limit to reactive power support provided by distributed PV units. Beyond this limit we see voltage collapse shown as infeasibility of power flow solutions. In order to avoid this problem we optimize the reactive power support from PV so that a solution exists within the constraints. The lesson learned for practical use of this result is that operators should use AC-OPF to compensate for reactive power using PV. Future research will explore the challenges and opportunities associated with the widespread adoption of microgrids, such as dynamic voltage instabilities that can occur with high levels of PV integration and complexities in inverter control strategies.",
        "authors": [
            "Aaron Jerome Jones"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158920",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Ending Well, Making the Harvest-Paths of Our Values",
        "abstract": "Any single story shrinks all others. In a place historically cultivated for the cocoa cash crop, this thesis proposes reorienting architectural practice towards a plural valuing of land and its constituent spirits. The journey begins in 2022 with my acquisition of a 99-year lease for a 5-acre land in Ghana. Prior to the conception of an academic proposal, this was to preserve and grow ecological and financial value through time.\r\nLocated on a hill-cluster in the Eastern Region, this place is crucial as the birthplace of Ghana’s cocoa industry, which became the world’s largest exporter by 1911. Spurred by economic and colonial incentives, farmer-settlers acquired and cultivated forest land including the one I presently steward. They forged communities that live on despite a subsequent decline of cocoa production in the region. Five centuries of colonial influence in West Africa reduced a plural landscape into singular extractive narratives, creating place-names like the Gold Coast, renamed Ghana after independence. The capitalist framework of monocultural extraction, one reliant on a colonial government and its land survey department, continues under contemporary African states. Architecture and planning—a practice historically tied to power and capital—remains instrumental in this system, often overlooking other ways of valuing land.\r\nThis thesis confronts the dispositions of an inherited profession by foregrounding the practices and materials of a socio-cultural paradigm. It is epitomized by the tree called Newbouldia laevis (African boundary tree) and its plural meanings in West Africa. It follows a cocoa harvest-path from a community named after a farmer-settler, Yaa-Aso, and ascends the hills, crossing the land limits of 7 farmers. It ends on the land I hold, with a lease ending in CE 2122.\r\nIn July 2024, I led a convocation of the farmers along the path in the defunct cocoa distribution building, toward framing futures based on other values apart from capital. 3 languages were spoken in that gathering - Twi, Anlo-Eʋe and English. It resulted in a 7-foot expansion of the path, and the pacification of a seasonal spirit-stream that crosses it. They set the context for imagining a series of 5 moments, herein recorded, that explore a value system of things spiritual and communal, offered by the transgressions of a widened path and the land I hold at its end.",
        "authors": [
            "Courage Dzidula Kwaku Kpodo"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158886",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Responsible Computational Text Generation: AI Content Classification and Policy Framework",
        "abstract": "Recent advances in generative AI, particularly in producing human-like text, have blurred the lines between human and AI authorship. Since these AI tools rely on stochastic generation rather than traditional scientific reasoning, concerns about misinformation and reliability have emerged, highlighting the need for AI detection tools and policy guidelines. In response, this study proposes a dual approach: (1) the application of adaptive thresholds to improve the use of AI text detectors and (2) an AI policy framework based on user patterns and opinions. To enhance detector performance, we present a threshold optimization algorithm that adapts to diverse subgroups, such as those based on text lengths and stylistic features, thereby reducing discrepancies in error rates. The commonly used method relies on a single universal threshold, which has led to inconsistent results across various text types because of different probability distributions. Our approach addresses these shortcomings by tailoring thresholds to the specific characteristics of each group. In parallel, the study examines the pressing need for comprehensive AI guidelines, given the rise of misinformation and academic integrity issues. While a few institutions have introduced comprehensive policies, many institutes lack approaches grounded in user patterns and opinions. To remedy this problem, we propose a policy framework based on a user study. The findings of this research will provide practical solutions for more effective AI text classification and a reliable framework for the necessity of AI writing policies.",
        "authors": [
            "Minseok Jung"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158904",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "High Order Immersed Finite Difference Methods for Complex Domains with Moving Boundaries and Interfaces",
        "abstract": "Moving domain boundaries and material interfaces are a hallmark of multiphysics systems such as fluid-structure interaction, alloy solidification, and multiphase flows. Simulating moving interfaces with traditional techniques requires a moving mesh that continuously adapts to the interface, which is costly and places restrictions on the interface motion. Immersed methods avoid these challenges by simulating moving geometries on a stationary Cartesian grid, locally altering the numerical method to account for boundaries and interfaces that are not grid-aligned. Most existing immersed methods have low-order spatial accuracy, requiring fine grids to generate accurate results. High order immersed methods can produce more accurate results at lower resolution, making them a promising tool for 3D simulations with tight error tolerances. However, the majority of available high order immersed methods have been numerical experiments developed for stationary 2D geometries and simple PDEs. In this thesis we demonstrate that high order immersed methods can be extended to complex nonlinear PDEs and moving 3D geometries, both of which are necessary to simulate practical engineering problems. We begin by introducing a boundary treatment that locally approximates PDE solutions with high order accuracy using a weighted least-squares fit, and show that the procedure remains valid for smooth 2D or 3D geometries satisfying a local curvature constraint. This boundary treatment is combined with a high order finite difference method to discretize the Poisson equation with up to sixth order accuracy. We then expand the scope of the method to include PDEs with immersed material interfaces, spatially-variable coefficients, vector-valued unknowns, cross-derivative terms, and nonlinearities. These techniques are applied to generate a sixth-order discretization of 2D nonlinear elasticity, demonstrating the applicability of high order immersed methods to complex PDE systems relevant in mechanical engineering. In the second half, we focus on large-scale 3D simulations with moving boundaries. We construct a third order immersed advection discretization with provable stability in one dimension, and show experimentally that the scheme remains stable in 2D and 3D domains. To treat moving boundaries, we introduce a general framework that allows high order immersed methods to maintain their accuracy in both space and time when paired with any explicit Runge-Kutta time integrator. We conclude by presenting results from massively-parallel high order simulations of the 3D advection-diffusion equation with moving boundaries on a multiresolution grid. Taken together, these results demonstrate that high order immersed methods can achieve the scale and complexity necessary to enable practical simulations that are difficult or impossible with traditional mesh-based techniques.",
        "authors": [
            "James Gabbard"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158843",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Persian Lessons: Islamic Art in America, circa 1876–1925",
        "abstract": "This dissertation investigates the prehistory of academic Islamic art history in the United States through the lens of American cultural history. It shows that between the US Centennial in 1876 and the inauguration of the Pahlavi dynasty in Iran in 1925, aesthetic theory and American citizenship were debated in the United States through objects identified, regardless of actual provenance, as “Persian.” This cultural phenomenon coincided with the acceleration of the transnational market for Islamic art, including architectural tiles, single-page paintings, and hand-knotted pile carpets. Examining instances of collecting, classifying, displaying, and otherwise handling and beholding Islamic art within different scales of home (family, nation, and international Christianity) and spaces of pedagogy (the living room, commercial gallery, advertisement, schoolroom, voluntary association, museum, and world’s fair), \"Persian Lessons\" reveals that notions of Persian art were instrumentalized in the service of competing American identities and ideologies in the late nineteenth and early twentieth centuries. \r\n\r\nThrough an analysis of published writings, museum archives, and government documents, the study shows how the art critic S. G. W. Benjamin, who also served as the first US diplomat to Iran in 1883–85, constructed an ideal of the Persian artist to champion liberal individualism and public art education. An investigation into the presence of Muslim prayer carpets in American Christian homes reveals that Sarkis Nahigian and other diasporic entrepreneurs from the Ottoman Empire became partners to middle-class women, who jointly turned the Oriental carpet into a symbol of obligation to the American nation. Lastly, an examination of visual and textual evidence recasts a collection of more than 20,000 objects—given to the Museum of Fine Arts, Boston, and William Hayes Fogg Art Museum of Harvard University by design pedagogue and museum patron-administrator Denman Waldo Ross between 1888 and 1935—as a tool of “training for citizenship.” Ross regarded Persian textiles and single-page paintings as value-neutral objects for the design education that he believed bolstered participatory democracy. \r\n\r\nThe fifty-year history that this dissertation covers concludes in the late 1920s and '30s with the establishment of the first official positions in Islamic art history at universities and museums in the United States. \"Persian Lessons\" thus shows that the founding of Islamic art history as an academic discipline was not simply imported from Europe. Professionalization stabilized a half century of domestic engagement with Persian art as a polysemic guiding light for American culture and society.",
        "authors": [
            "Roxanne Goldberg"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158801",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Synthesis and perception of sounds from physical interactions reveals auditory intuitive physics",
        "abstract": "Object interactions – collisions, scraping and rolling – create many of the sounds that we hear in the world around us. These sounds are generated via lawful physical dynamics. Anecdotally, humans possess some intuitive knowledge of the physical generative processes underlying sound production, but little is known about the extent and nature of this knowledge. This thesis characterizes the auditory perception of physical object interactions, making three main contributions. First, we develop realistic contact sound synthesis tools, in part via large-scale measurements of object acoustics. Second, we show that humans solve ill-posed problem of inferring of object mass and damping by using internalized knowledge of the distribution of object resonances. Third, we provide evidence for “auditory intuitive physics” in which human listeners derive physical information through sound, maintain it over time in object representations, and compare it across sensory modalities.",
        "authors": [
            "Vinayak Agarwal"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158825",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Archean origin of assimilatory sulfate metabolisms provides novel insight into redox conditions of early Earth environments",
        "abstract": "Dissimilatory sulfur metabolisms recording differing biological isotopic fractionation are well studied, important components of sulfur cycling (Mateos et al., 2023). Assimilatory sulfur metabolisms and genes across life provide a complementary window into sulfur biogeochemistry with individual pathways having specific isotopic fractionations acting on distinct redox states (e.g. sulfate, sulfide, sulfite) for anabolism (Liu et al., 2012). An assimilation pathway exists, which starts with sulfate adenylyltransferase (sat/ATP sulfurylase) catalyzing a reaction of adenosine triphosphate (ATP) and sulfate (SO42-) resulting in adenosine 5’-phosphosulfate (APS), and incorporation of more reduced sulfur into biomolecules. This sat/ATP sulfurylase enzyme represents the first step required by life to incorporate sulfate and informs our understanding of biological processes performing this fundamental chemical reaction. A phylogenetic and molecular clock analysis of the sat/ATP sulfurylase protein family (E.C. 2.7.7.4) was performed to determine the age of sulfate assimilation proteins. Extant diversity of sat proteins was estimated to have a last common ancestor ~3.24 Ga (95% CI 3.52–3.06 Ga) using relaxed molecular clocks calibrated with eukaryotic and cyanobacteria age ranges from previously published fossil calibrated investigations. These results suggest sulfate cycling in Paleoarchean environments, despite extensive evidence of low marine sulfate concentrations (Crowe & Canfield et al., 2014). Archean sulfate biogeochemical cycling could result from microbial sulfur oxidation and sources could include abiotic oxidation of volcanic sulfur, hydrothermal processes or pyrite (Canfield, 2001, Lyons et al., 2024). This phylogenomic evidence of sulfate during Archean times provides an independent complement to geochemical records and indicates that sulfur redox chemistry during the Archean was likely more complex than previously described.",
        "authors": [
            "Jack G. Payette"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158898",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Geographies of Selective Surveillance: Analyzing the Lived Experiences of Street-Level Trans Sex Workers and Muslims in India through the Matrix of Domination",
        "abstract": "In this paper, I present a study of public and private CCTV surveillance of urban public spaces in India, which I term as ‘geographies of selective surveillance’ — areas where state power is discretionarily exercised and abused, and the presence of the state is experienced principally through police pickets and everyday violence unleashed on marginal occupants, rather than by access to civic amenities and systems of justice. I analyze these experiences of surveillance from the standpoint (Harding, 1992) of minoritized communities of street-level trans sex workers in Kolkata and Muslims in Mumbai. I then situate these experiences within the Matrix of Domination (Collins, 1990), a theoretical framework that explains how systems of power are configured. Defining empowerment as the power to gain control of and/or benefit from a scenario by weakening the Matrix of Domination, I analyze the structural determinants that make surveillance empowering or disempowering for these communities. I find that on the one hand, surveillance can be an empowering tool for minoritized communities as evidence of harm and innocence in cases of false accusations or when police officials typically refuse to believe their experiences due to discriminatory attitudes. On the other hand, surveillance also offers new opportunities for the private exploitation of the instruments of state power through corruption as well as community-based moral policing to be done with greater success and efficiency. I argue that what ultimately determines how surveillance is experienced is not laws and policies, but rather how power is discretionarily exercised on the ground, refracted through the influence of cultural and political beliefs, and discourse.",
        "authors": [
            "Radhika Radhakrishnan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158809",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design and Manufacture of a Modular Continuous Unit Dose Pharmaceutical Lyophilizer",
        "abstract": "Pharmaceutical lyophilization (freeze-drying) enables long term storage and simplified transportation for aqueous vaccines and protein formulations. Modern industrial pharmaceutical freeze-driers rely on large batch and open loop formulation processing, limiting supply chains and resulting in variable quality products. This work describes the design and manufacture of a modular continuous lyophilization machine for pharmaceutical production. Additionally, the scaling and design methodology outlined in this work enables the development of both smaller systems for laboratory testing and larger machines to fit the needs and requirements of individual facilities. This machine introduces three new technologies to the pharmaceutical freeze-drying process. The first innovation is a continuous flow lyophilization topology which separates the lyophilization steps spatially rather than temporally. This layout allows product to travel through the system in smaller batches for increased product uniformity and quality control. The second innovation is a weight-based sensor for monitoring residual water content. This sensor enables in-situ monitoring of product during sublimation, and it resolves mass measurements as small as 5mg. The third innovation is the implementation of a thermal shock method of inducing controlled nucleation. The convective cooling and spatial non-uniformity within the machine allow vials to experience a 40°C temperature drop in less than 30 seconds. This nucleation front starts on the vial walls, rather than at the top surface of the solution in the vial, potentially increasing the water sublimation rate during drying compared to current nucleation methods. The machine designed and built for this work integrates into modern factory processes and can be scaled from the lab bench to a production line. The manufactured prototype demonstrates improvements on the production rate, flexibility, and quality of existing machines.",
        "authors": [
            "Steven Burcat"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158826",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Precision Pointing for the CubeSat Laser Infrared CrosslinK (CLICK) Mission",
        "abstract": "Advances in Free Space Optical Communications have led to numerous missions that have demonstrated optical space-to-ground links, however, fewer missions have demonstrated optical space-to-space links. NASA’s CubeSat Laser Infrared CrosslinK (CLICK) Mission aims to be the first to demonstrate optical space-to-space communication on a CubeSat scale using Commercial Off the Shelf (COTS) components that include a micro electromechanical system (MEMS) fine steering mirror for precision pointing. The first phase of the CLICK mission, CLICK-A, launched in September 2022 to demonstrate optical downlink. The second phase, CLICK-B/C, aims to demonstrate optical crosslink between two spacecraft: CLICK-B and CLICK-C. Optical crosslink communication requires precision pointing for both spacecraft to close the link. The development of the CLICK-B/C Fine Pointing, Acquisition, and Tracking (PAT) is presented in this thesis, as well as the analysis of disturbance rejection and evaluation of expected spacecraft disturbances. This thesis also asses the slewing required for differential drag control which is used to maintain the crosslink range between the two CubeSats. Preliminary results are presented from the CLICK-B/C flight hardware integration and testing phases, as well as findings from simulation of the lasercom payload’s performance.",
        "authors": [
            "Paige Forester"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158924",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Impact of Introducing Technical Design Elements in Makerspace\r\nTrainings",
        "abstract": "Makerspaces are used as a tool in higher education to support curricular, hands-on projects and encourage student extracurricular and personal projects. Because access to making is more self-driven, there is a gap between what makerspace trainings teach students and what students are expected to know by the time they reach capstone courses in engineering. To test the effects of introducing a technical makerspace training to students, several steps were taken. First, known barriers to making were explored and organized into categories. Second, Design Expertise was defined as a means to combat these barriers: it is a combination of (1) knowledge, (2) skill, (3) perspective, and (4) motivation. Third, a rigorous framework, the Design-Fabrication-Performance (DFP) matrix was created to break down design expertise into manageable chunks. Next, existing makerspace trainings at MIT were characterized using the DFP matrix. Afterwards, the DFP matrix was used to design a new, experimental training which would incorporate engineering design thinking and expertise with the typical makerspace machine training structure. Finally, 23 student participants were recruited, surveyed using a Likert scale (1 = strongly disagree, 5 = strongly agree), and interviewed to understand the impact of the training on participant perspectives, engineering identity, and maker motivation. Initial results suggest that student self-efficacy increases as a result of the training, This outcome is shown by the highest average differential of all survey responses (M = 0.78, SD = 0.85) for question 15: “I am confident in my ability to use GIR level knowledge to design and make things that perform as intended”. The maker training reinforced the motivation to make things for a majority of students, with the average score for the associated question being 4.48 (SD: 0.85). The training also positively impacted some traditionally marginalized groups in STEM. For the statement \"I feel comfortable in engineering at MIT\", women averaged 3.27 and men 3.90 before the training. The average differentials in the post- and pretraining scores to this question for these groups were 0.4 and 0.91 respectively. The training also appears to level playing field for students with less advanced backgrounds in engineering and science. For the question “I am confident in my ability to solve GIR level problems on my own”, students with parents with graduate degrees or higher averaged 4.44 before the training, while those with parents with undergraduate degrees or lower averaged 3.57. The average differentials are 0.22 and 0.64 respectively. Although students saw the value in modeling systems before design and fabrication, several questions demonstrated that students found modeling to be tedious and preferred to test and iterate on their designs in the makerspace; further work is needed to eliminate barriers to sustain student interest and participation in the long term. A longitudinal study following these students would also be needed to reveal long term outcomes such as STEM retention and long-term makerspace usage.",
        "authors": [
            "Layal A. Barakat"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158817",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Essays on Spatial Constraints and Gender Equality: the Impact of COVID-19 Lockdowns on Work-from-Anywhere Dynamics and Gender Equality in Job Searches",
        "abstract": "This dissertation explores the intersection of spatial constraints and gender equality by leveraging the COVID-19 lockdowns as a natural experiment to study the impact of work-from-anywhere (WFA) dynamics on job search behaviors. The introduction of mandatory lockdowns drastically shifted the labor market landscape, prompting an increase in the demand for flexible work formats. Utilizing data from over one million job seekers on an online employment platform, this research examines how the sudden wide availability of remote work options influenced job search activities differently across genders. Using unique data from a large online job platform, a comparison of pre- and post-COVID-19 lockdown data shows that women significantly increased their engagement with geographically flexible job postings, reacting more strongly than men to the rise in remote job opportunities at both the job viewing and application stages. This shift also resulted in a narrowing of the wage gap in positions viewed and applied for during the post-lockdown period compared to pre-lockdown benchmarks. Notably, the study identifies variations in job search behavior among those likely constrained by domestic responsibilities. While differences in job posting views suggest an initial differential impact, such differences vanish at the application stage. Collectively, these results indicate that the pandemic-induced shift towards remote work has contributed to a gender-equalizing effect in the job market, including those navigating domestic labor constraints. This research not only highlights the transformative potential of WFA arrangements in promoting gender equality but also provides insights into the mechanisms that drive these changes within the labor market. \r\n\r\nKeywords: organizational studies, gender inequality, flexible working arrangements, hiring, applications processes, decision making, digital platforms.",
        "authors": [
            "Tatiana Labuzova"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158794",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Cooling with less: Design and simulation of multifunctional building components for a material-efficient, heat-resilient architecture",
        "abstract": "As temperatures rise globally and the demand for housing intensifies, designing affordable buildings for heat resilience and with low carbon emissions becomes crucial. Conventional air conditioning (AC) systems, although often an effective and accessible cooling solution, are energy-intensive and typically fail to consider local climatic and urban contexts. This work alternatively focuses on the opportunity behind designing building components (such as slabs, blocks, roofs, or footings) for multifunctionality, integrating passive strategies and low-energy cooling systems within them in a material-efficient manner. Collapsing multiple functions into a single building component is typically regarded as a strategy that leads to better overall performance and reduced costs compared to implementing each function separately. However, the effectiveness of this strategy in cooling-dominated climates and in the context of the current climate crisis remains underexplored. \r\n\r\nThe dissertation proposes new designs and evaluation methods for three multifunctional building components: multi-hollowed blocks (ceramic blocks with interior air pockets), shaped chilled slabs (shaped concrete slabs with embedded radiant ceiling systems), and integrated heat sinks (thermally activated concrete footings and roofs). Each component is designed to optimize a specific cooling strategy based on its context within the building and intrinsic material properties - thermal mass, radiant cooling, and ground/radiative cooling. Chapter 2 demonstrates how shape-optimized ceramic blocks can double the heat capacity of existing commercial solutions without additional material or reduce their weight by 33% while increasing the heat capacity by 23%. Chapter 3 presents slab geometries that achieve embodied carbon reductions of up to 50% relative to conventional prismatic floors while reducing operational carbon by 12-14%. Chapter 4 finds that buildings in temperate climates with a Floor Area Ratio (FAR) of up to 4.5 can meet 100% of the cooling demand exclusively through heat dissipation systems integrated into the building’s foundations and roof.  Methodologically, this research puts together heat transfer theory and analytical models with state-of-the-art shape optimization methods; this effort results in a fast and accurate multi-objective simulation framework tailored for early design stages.\r\n\r\nThis thesis provides, for the first time, validated methods and quantitative results that support the viability of multifunctional building components in cooling-dominated climates, optimizing the shape of walls, blocks, foundations, and roofs to improve their structural and thermal performance simultaneously, reducing their weight and improving buildings’ resilience to heat.  From a climate adaptation perspective, this approach ensures that buildings are ready for extreme heat even when active systems are unavailable due to, for example, a power outage. From a carbon mitigation perspective, the presented results highlight the potential to reduce the whole-life carbon of buildings by shape-optimizing components for enhanced thermal performance and material efficiency.",
        "authors": [
            "Eduardo Gascón Alvarez"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158791",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Accelerating the Discovery of Novel Metal Organic Chalcogenolates: A Computational and Machine Learning-Driven Approach",
        "abstract": "Metal Organic Chalcogenolates (MOChas) are a class of robust, self-assembling, and hybrid materials featuring inorganic metalo-chalcogen frameworks that are scaffolded by organic ligands. These low-dimensional structures exhibit tunable optoelectronic properties, making them promising candidates for various applications, including optical sensors and nanotechnology. This tunable relationship between MOCha structural arrangements and targeted properties opens up a vast yet challenging search space for novel MOCha structures. Density Functional Theory (DFT) can predict properties of materials with good accuracy, making it a powerful choice for even hypothetical materials. However, the discovery of novel MOChas structures is constrained by poor scalability of DFT relaxation times for large systems and a lack of high-throughput design methods that can capture the complex geometries of MOChas. In this work, we employ DFT calculations to investigate the energetic and electronic properties of various MOChas, and provide insight into the optical behavior and kinetic favorability of such structures. To address the computational bottlenecks of high-throughput design and DFT workloads, we discuss the use of machine-learned interatomic potentials and various generative models that can enable rapid prototyping of novel MOCha structures.",
        "authors": [
            "Adriana J. Ladera"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158916",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Crafting Cannabinoid Capitalism: Health, Sustainability, and Regeneration in the United States",
        "abstract": "This dissertation offers a critical exploration of cannabis legalization through an ethnographic study of small-scale \"legacy\" cannabis farmers in Humboldt County, California, as they navigate a complex transition from prohibition to commodity capitalism. I focus on their collective efforts to envision and practice “regenerative agriculture\" as a response to both the historical injustices of prohibition and the compounding challenges of climate change. Drawing on history, STS, and the anthropology of food, agriculture, and medicine, I show how the logics of the war on drugs— rooted in carcerality, settler colonialism, and plantation agriculture—structurally and affectively persist in the socalled “post-prohibition” era, frustrating farmers’ efforts to resist monopolization and dispossession. Throughout, I attend to how the pervasive notions of “health,” “sustainability,” and “regeneration” are actively negotiated, modified, and put to use as material and symbolic tools in crafting medicinal, agricultural, and ecological futures. The Introduction weaves a tapestry of themes, histories, and theories that set the stage for the main ethnography. Through a blend of personal narrative, ethnographic vignette, and critical theory, it works to situate cannabis as a fluid and multifaceted object, highlighting people’s ambivalent hopes and cynicisms towards legalization. From alternative farming to molecularized biocapital, it articulates the intersecting influences of climate change, racial capitalism, and Indigenous sovereignties in ongoing projects to commercialize and legalize cannabis in a globally connected United States. Chapter One outlines my research methods and provides a social and narrative history of the study’s fieldsite, grappling with the anthropological complexities and complicities of studying working landscapes in a settler colonial “frontier ecology.” Chapter Two unpacks the shifting and embodied subjectivities of both farmers and workers as they reconfigure themselves in service of licensed production, highlighting sociocultural tensions and contradictions, the structural challenges of regenerative gardening, and the labor dynamics that shape these processes. Chapter Three analyzes how the inchoate and social nature of cannabis regulation both hinders and supports regenerative farming, emphasizing financial strain, and the ever-pervasive role that surveillance technologies are playing in cannabis governance. Chapter Four shifts to the harvest season, exploring farmers’ collective efforts to market their products through the concept of “drug terroir,” unpacking how their values and practices entangled with regional efforts to address wildfires and remediate leftover drug war infrastructures. Chapter Five moves off the farm and onto the topic of consumption as it historicizes the growing scientific literature about cannabis and pregnancy, demonstrating how carcerality continues to infiltrate maternal-fetal health science and conceptions of reproduction and health. The dissertation ultimately explores the ways in which American cannabis legalization often regenerates, rather than resolves, the legacies of prohibition and settler colonialism, while at the same time illuminating alternative and promising practices that might challenge these enduring forces.",
        "authors": [
            "Alexander Nicholas Rewegan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158789",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Missing Megawatts Problem: Improving Modelling Practices to Prepare for an Uncertain Future",
        "abstract": "Long-term energy system planning is one of the most pressing challenges for the power sector, which must maintain reliability while decarbonizing. Currently, no unified regulatory, modelling, or market framework exists in the United States to facilitate planning in pursuit of a clean and reliable grid. Variable renewable energy (VRE) generation can produce cheap power but they increase the grids exposure to interannual variability in demand and VRE generation. This raises questions about how grid planners will value VRE and clean firm power (such as nuclear power). This thesis evaluates the importance of considering interannual variability and clean firm power in long-term energy system planning. I use GenX, an open-source capacity expansion model, to model the U.S. New England region in 2050 assuming a high degree of electrification and various technology availability and emissions reduction pathways. I find that clean firm power will reduce the cost of decarbonizing the New England grid but that grid planners must consider decades of weather and demand data if they are to make appropriate investments. I also present a novel outputs-based timeseries clustering method which allows models like GenX to optimize grids using longer timeseries of weather and demand data. Based on my work, I recommend that policymakers, grid operators, and market designers establish rigorous standards around energy modelling for long-term planning that includes multiple scenarios and appropriately values technologies such as firm power.",
        "authors": [
            "Nirmal K. Bhatt"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158844",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis",
        "abstract": "We present VoxelPrompt, an agent-driven vision-language framework that tackles diverse radiological tasks through joint modeling of natural language, image volumes, and analytical metrics. VoxelPrompt is multi-modal and versatile, leveraging the flexibility of language interaction while providing quantitatively-grounded image analysis. Given a variable number of 3D medical volumes, such as MRI and CT scans, VoxelPrompt employs a language agent that iteratively predicts executable instructions to solve a task specified by a natural language input prompt. These instructions communicate with a vision network to encode image features and generate volumetric outputs (e.g., segmentations). VoxelPrompt interprets the results of intermediate instructions and plans further actions to compute discrete measures (e.g., tumor growth across a series of scans) and present relevant outputs to the user. We evaluate this framework on diverse neuroimaging tasks and show that the single VoxelPrompt model can delineate hundreds of anatomical and pathological features, measure many complex morphological properties, and perform open-language analysis of lesion characteristics. VoxelPrompt carries out these objectives with accuracy similar to that of fine-tuned, single-task models for segmentation and question-answering, while facilitating a large range of tasks.",
        "authors": [
            "Andrew Hoopes"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158933",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Polymer Deconstructability and Recyclability via Introduction of Cleavable Si−O Bonds",
        "abstract": "The synthesis of a new polysilylether via entropy-driven ring-opening metathesis polymerization (ED-ROMP) of cyclic bifunctional silyl ether-based monomers is reported. High molecular weight polymers (up to 100 k) with narrow dispersities were achieved at modest temperature. These polymers display excellent thermal stability and ultra-low T_g (–88 ºC). The polymers are both rapidly deconstructable via the cleavage of the labile silicon-oxygen linkages with either acid or fluoride triggers and partially depolymerizable by the addition of exogenous metathesis catalyst. Analysis of the deconstructed polymer products provided insight into the polymer microstructure, showing that the ED-ROMP process was regiorandom. Altogether, this work offers a new class of deconstructable polymers with a range of potential applications. Incorporation of these bifunctional silyl ether-based monomers into copolymers could aid in the triggered deconstruction of otherwise nondegradable hydrocarbon backbones.",
        "authors": [
            "Alayna Johnson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158921",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Sweating Details: Labor of “Los Constructores del Valle”",
        "abstract": "“You should always be grateful for the work you can find, so make sure you prove you deserve it.”- Commonly heard growing up amongst the Builders of the Valley in Orange, NJ. The necessary attitude that fuels the built environment.\r\n\r\nThis thesis proposes a dialogical method of tectonics through exploring the embodied experiences of those who physically build the city and its architecture, positioning architectural design as fundamentally tied to the labor that makes buildings possible. It centers on two primary questions: “Who builds this architecture?” and “How does this design impact a builder’s occupational livelihood?”\r\n\r\nTo challenge professional standards that perpetuate a disconnection between designers and builders, this thesis reconnects me, as a designer, with my educators from Orange, NJ. These individuals—professional construction workers—shaped my earliest understanding of the built environment and how to navigate it socially and professionally. Through this process, learning more about who they are, how they entered construction, and how the work has affected them over the years.\r\n\r\nThis education with ongoing dialogue pushes towards future opportunities of working together, focusing on designing better for the act of building by prioritizing the physical, mental, and financial longevity of my Educators. The culmination of this research and communication is materialized through four architectural details within a workspace, designed to showcase my Educator’s expertise and affinities as professionals. These details reimagine occupational choreography, opening up for future workflows that think through both lessening and healing the musculoskeletal disorders that many builders face after years of laboring across the tristate area.",
        "authors": [
            "Gabriel Andrade"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158839",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Thermally Hardened RF GaN HEMTs in Extreme Environments",
        "abstract": "Traditional, room temperature electronics based on silicon has truly changed the world around us over the past 70+ years. However, many more applications still exist that are limited by the temperature performance of silicon devices (<250◦C). This area of high temperature (HT) electronics is an increasingly growing field with critical future applications in geothermal energy, space exploration, hypersonic aircraft, and deep gas/oil drilling, among others. Gallium Nitride (GaN) high electron mobility transistors (HEMTs) are especially well suited for high temperature electronic applications due to their low intrinsic carrier concentration and excellent electrical properties. Despite great progress in HT GaN technology, most demonstrations target logic or mixed-signal applications, and the performance of radio-frequency (RF) GaN devices remains lacking at high temperatures despite the critical need for wireless communication systems and high-speed electronics for these high-temperature applications. In this thesis, we investigate the physics of GaN HEMT devices at high temperatures and design RF transistors that demonstrate record performance at these temperatures.",
        "authors": [
            "John Niroula"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158912",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Mining Multifaceted Customer Opinions from Online Reviews",
        "abstract": "Online reviews are a valuable source for studying customer needs and preferences. Previous studies focus on extracting a set of a priori defined constructs such as product attribute perception or explicit customer needs from reviews. Such a priori focus circumvents the limitations of certain natural language processing algorithms but discards valuable information in reviews that are not in the scope of the predefined construct. This study proposes a new method of extracting customer opinions and opinion targets from reviews with the Aspect Sentiment Triplet Extraction (ASTE) algorithm and then identifying theoretical constructs critical for product development with a posteriori interpretation method. We demonstrate the value of our proposed method by identifying granular opinion targets and expressions to find infrequent but important phenomena such as user innovations and delights.",
        "authors": [
            "Chengfeng Mao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158810",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Sub-Bottom Profiling Using an Autonomous Underwater Vehicle Equipped With a Sound Source and Towed Hydrophone Array",
        "abstract": "Sub-bottom profiling using an autonomous underwater vehicle equipped with a source and a towed array is an excellent method to finely survey large areas of the ocean bottom with minimal interference from the water column. This approach has the benefit of being able to determine the range dependence of the sub-bottom on a meter-by-meter scale rather than assuming constant sub-bottom properties over a large range. This thesis conducts theoretical and experimental studies to investigate the feasibility of using the arrival times of acoustic signals from an autonomous underwater vehicle source to a short, 16-element towed hydrophone array to determine the sound speed and layer thickness of the seabed through Bayesian geoacoustic inversion. This method provides range-dependent geoacoustic parameters with a resolution on the order of 10 meters. Numerical studies indicate that, for timing data with low variance, arrival times can be used to accurately estimate seabed properties. However, the performance of the Bayesian inversion model deteriorates as the variance of the timing data increases. Experimental data were collected during the Seabed Characterization Experiment at the New England Mud Patch and the New England Shelf Break. This thesis attempts to improve the arrival times through the use of sub-array focusing but concludes that this method is not feasible due to the experimental data exhibiting a high level of variance in the sub-bottom timing returns, likely due to the presence of scatterers in the sediment layer. Therefore, the mean and variance of the direct path, bottom, and sub-bottom timing returns were calculated using Gaussian process regression. Furthermore, the results show that layer thickness and sound speeds are highly coupled, making it challenging to uniquely determine seabed properties.",
        "authors": [
            "Paige Pfenninger"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158932",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Shining a Light on the Nucleus: Photonuclear Measurements from Correlations to Charmonium",
        "abstract": "The atomic nucleus is comprised of a collection of nucleons (protons and neutrons), which are bound together by the nucleon-nucleon (NN) interaction that originates from Quantum Chromodynamics (QCD). While most nucleons experience the force from the rest of the nucleus as a single net “mean-field” interaction that binds them relatively weakly, a small but impactful fraction are in configurations called “Short-Range Correlations” (SRCs), in which they pair with another nucleon at very short distance to experience strong interactions, significant binding, and high momentum. Hard, high-energy scattering reactions in which an SRC pair is broken apart, knocking both nucleons out of the nucleus, provide the ability to probe the details of these SRC configurations in the nucleus. Previous measurements have had limited statistics and kinematic reach, and the theoretical tools available were insufficient to draw quantitative conclusions regarding the ground-state properties of SRCs. The studies described in this thesis represent the first global analysis of SRC breakup measurements in order to present a unified picture of SRCs within light- to medium-size nuclei. This includes the use of a novel theoretical framework, the Generalized Contact Formalism, which connects scattering cross-section measurements and the ground-state properties of the SRC pair, to quantitatively interpret a variety of electron-scattering measurements. This is brought to culmination by a report on the first measurement of SRC pairs via the use of hard meson photoproduction reactions, which, despite differing significantly from the mechanics of electron-scattering, is well-described under a common framework, pointing to a consistent and universal picture of SRCs across reaction channels. I also report on the first measurement of J/ψ photoproduction in the near- and below-threshold kinematic region, giving the first insights to the gluonic structure of bound nucleons in the large-x “valence” region and providing constraints on a gluonic “EMC effect”. In addition to these studies, I provide details on the search for Primakoff production of axion-like particles using the photoproduction data taken for this experiment, and I conclude by describing studies of nucleon spin structure measurements that will be performed at the forthcoming U.S. Electron-Ion Collider.",
        "authors": [
            "Jackson R. Pybus"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158837",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Systems-Theoretic Framework For Safety-Driven Development of System Architectures",
        "abstract": "Modern complex systems are increasingly expected to exhibit emergent properties such as safety and security even as they become more complex, interconnected, and reliant on software than ever before. Because of this evolution in the characteristics of these systems, the methods available today for developing system architectures no longer provide systems engineers with adequate design support. As a result, it is becoming increasingly challenging for systems engineers to develop system architectures that exhibit emergent properties like safety. This thesis addresses this problem by developing a safety-driven architecture development framework that enables the design of emergent properties such as safety into a system architecture from the beginning. The key idea is that the results from a hazard analysis process known as Systems Theoretic Process Analysis (STPA) should drive design decisions. The framework therefore starts with an initial STPA analysis of the system to determine how unsafe or undesirable behavior could occur. Structured and systematic processes are then provided to help systems engineers use the STPA results to develop the required control behavior of the system and explore possible system architecture options to implement that control behavior. This framework therefore enables systems engineers to make more informed early architectural design decisions driven by safety considerations. This framework is applied to an Urban Air Mobility (UAM) case study to demonstrate that it provides the necessary design support to enable the development and refinement of an air traffic management (ATM) architecture for UAM. When creating a system architecture, assumptions may also need to be made to mitigate the inherent uncertainties and lack of detailed information about the system at that early stage of design. However, these assumptions are used as the basis for design decisions, and it is important that they remain valid to avoid flaws in the architecture arising when underlying assumptions become invalid. Thus, this thesis also develops and demonstrates a supporting framework to help identify these underlying assumptions and ensure they remain valid both during system development and after the system is placed into operation. Modern complex systems are increasingly expected to exhibit emergent properties such as safety and security even as they become more complex, interconnected, and reliant on software than ever before. Because of this evolution in the characteristics of these systems, the methods available today for developing system architectures no longer provide systems engineers with adequate design support. As a result, it is becoming increasingly challenging for systems engineers to develop system architectures that exhibit emergent properties like safety. This thesis addresses this problem by developing a safety-driven architecture development framework that enables the design of emergent properties such as safety into a system architecture from the beginning. The key idea is that the results from a hazard analysis process known as Systems Theoretic Process Analysis (STPA) should drive design decisions. The framework therefore starts with an initial STPA analysis of the system to determine how unsafe or undesirable behavior could occur. Structured and systematic processes are then provided to help systems engineers use the STPA results to develop the required control behavior of the system and explore possible system architecture options to implement that control behavior. This framework therefore enables systems engineers to make more informed early architectural design decisions driven by safety considerations. This framework is applied to an Urban Air Mobility (UAM) case study to demonstrate that it provides the necessary design support to enable the development and refinement of an air traffic management (ATM) architecture for UAM. When creating a system architecture, assumptions may also need to be made to mitigate the inherent uncertainties and lack of detailed information about the system at that early stage of design. However, these assumptions are used as the basis for design decisions, and it is \r\nimportant that they remain valid to avoid flaws in the architecture arising when underlying assumptions become invalid. Thus, this thesis also develops and demonstrates a supporting framework to help identify these underlying assumptions and ensure they remain valid both during system development and after the system is placed into operation.",
        "authors": [
            "Justin Wei Siang Poh"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158793",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Enhancing Robotic Manipulation of Liquid Using a Digitally Fabricated Intelligent Wearable Device",
        "abstract": "Despite recent exponential advances in computer vision and reinforcement learning, it remains challenging for robots to interact with liquids due to visual obstructions, transparent liquids, and fine-grained splashes. Yet, a substantial opportunity exists for robotics to excel in liquid identification and manipulation, given its potential role in chemical handling in laboratories and various manufacturing sectors such as pharmaceuticals or beverages. Recent advancements in electronic wearables, designed to replicate or surpass the functions and attributes of human skin, and their convergence with machine learning have provided opportunities to enhance the capabilities of robotic systems. Here, we present a novel approach for liquid class identification and position estimation with the robotic wearable device that can ‘see through’ the container, leveraging electrical impedance sensing. We design and mount a digitally embroidered electrode array to a commercial robotic gripper. Coupled with a customized impedance sensing board, we collect data on liquid manipulation with a swept frequency sensing mode and a frequency-specific impedance measuring mode. Our developed learning-based models achieve an accuracy of 93.33% in classifying 9 different types of liquids (8 liquids + air) and 97.65% in estimating the liquid position in the cup without any vision system present. We investigate the effectiveness of our system with a series of ablation studies. These findings highlight our work as a promising solution for enhancing robotic manipulation in liquid-related tasks.",
        "authors": [
            "Young Joong Lee"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158941",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Input Adaptive Allocation of Language Model Computation",
        "abstract": "Computationally intensive decoding procedures—including search, reranking, and self-critique— can improve the quality of language model (LM) outputs in problems spanning code generation, numerical reasoning, and dialog. Existing work typically applies the same decoding procedure for every input to an LM. But not all inputs require the same amount of computation to process. Can we allocate decoding computation adaptively, using more resources to answer questions whose answers will be harder to compute? We present an approach that predicts the distribution of rewards given an input and computation budget, then allocates additional computation to inputs for which it is predicted to be most useful. We apply this approach in two decoding procedures: first, an adaptive best-of-k procedure that dynamically selects the number of samples to generate as input to a reranker; second, a routing procedure that dynamically responds to a query using a decoding procedure that is expensive but accurate, or one that is cheaper but less capable. Across a suite of programming, mathematics, and dialog tasks, we show that accurate computation-allocation procedures can be learned, and reduce computation by up to 50% at no cost to response quality, or improve quality by up to 10% at a fixed computational budget.",
        "authors": [
            "Mehul Damani"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158949",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On Solving Larger Games: Designing New Algorithms Adaptable to Deep Reinforcement Learning",
        "abstract": "In this thesis, we explore the design of algorithms capable of handling large games where the state space is too large to store strategies in a tabular format from a theoretical perspective. Specifically, we focus on developing algorithms suitable for deep reinforcement learning in two-player zero-sum extensive-form games. There are three critical properties for effective deep multi-agent reinforcement learning: (last/best) iterate convergence, efficient utilization of stochastic trajectory feedback, and theoretically sound avoidance of importance sampling corrections. Chapter 3 introduces Regularized Optimistic Mirror Descent (Reg-OMD), which provably converges to the Nash equilibrium (NE) linearly in last-iterate. Chapter 4 shows that algorithms based on regret decomposition enjoy best-iterate convergence to the NE. Chapter 5 proposes Q-value based Regret Minimization (QFR), which achieves all three properties simultaneously.",
        "authors": [
            "Mingyang Liu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158922",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Systems for Usable Machine Learning",
        "abstract": "Many real-world decision problems are complex, with outcomes difficult to measure and evaluate. The impact of decisions made in these domains is nuanced and takes a long time to be fully realized. Individual mistakes can lead to significant costs, and computational tools such as ML models must be integrated alongside existing, well-established human workflows. These properties of such decision problems means that ML solutions must be usable in order to be effective — in other words, developed and deployed in such a way as to be used by humans in decision-making and improve outcomes. In order improve ML usability, developers create ML tools, or diverse kinds of interfaces that allow users to understand ML models and their predictions. In this thesis, we use real-world case studies to synthesize generalizable lessons for applying usable ML tools to complex, real-world decision problems. Based on experience developing ML tools for child welfare screening, we propose a formal taxonomy of feature properties related to usability and interpretability. We then discuss the design and development of a system to make generating ML explanations that use such interpretable features more effective. Pyreal is a framework and Python library implementation that uses updated data transformers to generate explanations of ML models and predictions using interpretable features. Motivated by the development and customization effort required to develop ML tools for new applications, we then discuss the development of Sibyl, a configurable and comprehensive system for generating usable ML interfaces for a wide range of applications. We then discuss our case study in applying Sibyl to the decision problem of wind turbine monitoring. We then discuss Explingo, our system for transforming traditional ML explanations into natural language narratives to further improve the usability of ML outputs. We finish by discussing the practical lessons this work demonstrates related to the need for usable ML, the challenges specific to these complex applications, ethical questions, and future directions.",
        "authors": [
            "Alexandra Zytek"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158953",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Building World Models with Neural Physics",
        "abstract": "World models learn the dynamics of environments in a data-driven manner, enhancing performance and efficiency in downstream tasks such as control, design, recognition, and generation, thanks to cost-effective simulation and differentiability. A pre-trained world model should ideally (1) accurately simulate ground-truth dynamics, (2) adapt easily to novel configurations, and (3) generalize across diverse physical effects. Previous attempts in this area have either utilized differentiable model-based physics with few parameters exposed or trained for specific scenarios with minimal physical priors integrated. These world models fall short of their objectives, limiting their applicability in real-world accuratecritic deployments and scalability to larger pre-trained world models. In this thesis, we aim to build world models with neural physics, a hybrid neural-physics framework that models the basic dynamics with differentiable physics while learning all additional modules through neural networks. By integrating neural physics, the world models adhere closely to physical principles while efficiently learning diverse effects. The modular structure of neural physics allows world models to generalize to novel configurations simply by installing different pretrained neural modules. We will demonstrate the effectiveness of this novel framework in applications such as reconstruction, robotic control, and scientific discovery.",
        "authors": [
            "Pingchuan Ma"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158927",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Development of Additively-Manufactured Quadrupole Mass Filters for Low-Cost and High-Performance Applications",
        "abstract": "With a growing need for more compact and affordable mass spectrometers, many efforts have been made to miniaturize quadrupole mass filters (QMFs). Unfortunately, these efforts have yielded devices with inadequate performance for practical applications in analytical chemistry. This study reports the successful creation of a low-cost, high-performance QMF by means of additive manufacturing. Vat photopolymerization of glass-ceramic feedstock was used to create a novel, monolithic structure, and selective electroless nickel-boron plating metallizes the structure, forming a completed QMF that is lightweight and inexpensive to produce (20 USD per device). Furthermore, additive manufacturing allows QMF dimensions to be rapidly scaled to the optimal sizes for a given application, which is larger than most prior affordable quadrupole designs. Despite the limited precision of additive manufacturing, optimization techniques can be leveraged to produce high-quality devices with smooth surfaces. As a result, our QMFs achieved mass resolutions up to 164 at 69 Da, with abundance sensitivities sufficient to detect carbon-13 isotopes at lower masses—a level of performance comparable to commercial devices. These results indicate that additive manufacturing, properly employed, can significantly advance the state of the art of QMFs and other mass spectrometry technologies.",
        "authors": [
            "Colin C. Eckhoff"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158944",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Structuring Representation Geometry in Self-Supervised Learning",
        "abstract": "The central promise of deep learning is to learn a map 𝑓 : 𝒳 → ℝ_𝑑 that transforms objects 𝒳—represented in their raw perceptual forms, such as images or molecular strings—into a representation space ℝ_𝑑 where everything that is hard to do with raw perceptual data becomes easy. For instance, measuring the similarity between two objects [scientific notation] expressed as tensors of pixel intensities is non-trivial in their raw form, but becomes straightforward if 𝑓 maps these objects to a space where simple Euclidean distances, ‖𝑓(𝑥₁) − 𝑓(𝑥₂)‖₂ are meaningful measures of similarity. While this simple recipe has shown standout success in a range of tasks, certain applications require representations that encode richer structural relationships beyond pairwise similarity. For instance, tasks that encode relational information— such as “𝑋 is a parent of 𝑌 ” or “𝐴 is a treatment for 𝐵”—require embedding spaces that capture richer structural relationships. In this thesis, we explore what 𝑓 should encode in order to be useful for a range of unknown downstream tasks, from the point of view of the geometric structure of representation space. We investigate this question in the context of self-supervised learning, a paradigm that extracts meaningful representations by leveraging the structure of the data itself without relying on explicit labels. Specifically, we propose adding additional geometric structure to the embedding space by enforcing transformations of input space to correspond to simple (i.e., linear) transformations in the embedding space. To this end, we introduce an equivariance objective and theoretically prove that its minima forces transformations on input space to correspond to rotations on the spherical embedding space. Our proposed method significantly improves performance on downstream tasks, and ensures sensitivity in embedding space to important variations in data (e.g., color, rotation) that existing contrastive methods do not achieve.",
        "authors": [
            "Sharut Gupta"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158966",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Cost Optimized Logistics for Commercial Operations in Low Earth Orbit and Cislunar Space",
        "abstract": "Designing profitable mission and logistics architectures is necessary to establish a profitable commercial market and support a robust space economy. It is the goal of the National Aeronautics and Space Administration (NASA) to establish such an economy in low Earth orbit (LEO) through the implementation of commercial LEO destinations and to commission self-sustaining lunar infrastructure through the Artemis missions. The ISS and the Apollo lunar landers demonstrated the ability to provide safe and reliable habitation, but the cost to support these missions has been on the order of billions of United States Dollars (USD). Minimizing the operational costs of commercial space systems will be required if commercial companies expect to generate a profit from their services. To address this, this thesis derives and demonstrates a manual cost optimization method for space system mission architectures, with respect to logistical and system design. In tandem, a computational tool called the Cost model for Space system Operations (COST-O) was developed. The demonstration included the iteration of a logistics and system design vector for two cases: a commercial LEO space station, and a commercial lunar in-situ resource utilization (ISRU) liquid oxygen generation system. These mission architectures were modelled and simulated in SpaceNet which first analyzed for feasibility and then were processed by COST-O. This data was used to make financial forecasts and were analyzed for cost sensitivity. The results suggest that for a commercial LEO space station, a closed loop ECLSS, large stockpile of resources, reduced resupply cadence, and a combination of tourists and visiting crew would be a profitable architecture at the crew capacity of at least three paying customers present on the station per day with an annual operational cost of 1,129,731,710 USD. Profits would be achieved by the end of ten years of steady state operations at the current market price of 3.12 million USD per crew member per day. Attempts to minimize this cost should first be made in the cadence of funded astronaut technician flights, as crew launches contribute most to the overall operational cost. Future work should address ways to minimize this, such as reducing the required amount of astronaut technicians that must be present at any given time. For a commercial lunar ISRU liquid oxygen generation system, an architecture supporting a closed loop system, using Starship as the launch and landing vehicle, a prepositioned stockpile of resources at the lunar surface, and a hydrogen reduction agent is most cost optimal, with an annual operating cost of 19,275,486,559 USD, and profitability achieved at the design rate of twenty metric tons of liquid oxygen produced and sold per year. At the current market price of 1.2 million USD per kilogram, the system would be profitable by the end of the first year of steady state operations. Attempts to minimize this operational cost further should improve the recyclability of the system. Future work should evaluate added robustness to the architecture by delivering multiple systems and should model deliberate cargo packing decisions.",
        "authors": [
            "Ireland Brown"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158866",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On the dynamics and interparticle forces of electrostatically stabilized colloidal suspensions",
        "abstract": "In a broad spectrum of industrial and biomedical applications, the equilibrium and dynamic properties of colloidal suspensions play a pivotal role, with systems ranging from simple gold nanoparticles in electrolyte solutions to complex assemblies like micelles, vesicles, nanocapsules, and dendritic polymers. Typically, these systems are approached through the Derjaguin–Landau–Verwey–Overbeek (DLVO) theory and Poisson-Boltzmann models, frameworks that approximate charged particles as point charges to predict interparticle interactions. While these frameworks have been instrumental for low-concentration, idealized systems, it falls short in capturing critical behaviors in more concentrated regimes. In such environments, overlooked phenomena—such as excluded volume effects and ion-ion correlations—become essential in shaping the colloidal system’s equilibrium and dynamics. By leveraging advanced computational techniques, we systematically interrogate these mesoscale interactions, offering insights that extend beyond the traditional paradigms of mean-field theory and enhance our understanding of colloidal behavior in complex environments. The first part of this work presents the development of efficient algorithms that significantly advance the computational speed of induced polarization calculations within Brownian Dynamics simulations of polarizable colloidal particles. By establishing a new benchmark in simulation methodologies, these algorithms lay the groundwork for exploring complex soft matter systems, enabling deeper insights into the dynamic and equilibrium properties of colloidal suspensions beyond the limitations of conventional theories. Together, these advancements provide a robust computational framework for examining mesoscale interactions in concentrated colloidal systems, where ion correlations, finite ion volumes, and thermal fluctuations critically influence behavior. The next part of this work focuses on the study of equilibrium properties of charged soft matter systems in crowded environments through the implementation of robust computational techniques. We meticulously examine charge-density correlations and clustering behaviors that arise due to the complex electrostatic interactions between colloidal particles. At high ion concentrations, the system undergoes distinct structural transitions that are modulated by the ionic strength and specific particle characteristics. These transitions are characterized by emergent patterns in the spatial distribution of charges, forming structured clusters that reflect the balance between electrostatic and entropic forces. We further our studies by computing the potential of mean force (PMF) between metallic nanoparticles, a measure of the effective interaction potential that inherently captures how particles interact across various separation distances in an electrolyte. The PMF analysis reveals oscillatory behavior in particle interactions at different concentrations. Our study delivers robust free energy profiles, enabling a more nuanced understanding of the electrostatic forces at play in dense colloidal suspensions. These insights shed light on the mechanisms of charge screening and packing within high-density systems. The final part of this thesis focuses on the study of the non-linear transport properties of concentrated macroions to external electric fields, revealing intricate dependencies on both ionic structure and external electric fields. Our studies reveal how conductivity is modulated by charge density correlations and field strength. A notable disruption of local ionic atmospheres was observed with increasing field strengths, which in turn accelerates ion mobility and significantly alters the transport properties. We further advance the investigation into the dynamic response of concentrated macroions and electrolytes by examining their behavior under time-varying electric fields. Through simulations involving frequency sweeps and chirp signals, we discerned that the dynamic response of these concentrated charged soft matter systems is best understood through the lens of two distinct transport regimes—characterized by short- and long-time responses. This bifurcation enables the introduction of a relaxation time scale that captures the intricate coupling between ionic correlations and the macroscopic system response, highlighting the pivotal role of excluded volume effects in densely populated environments. The study provides a detailed framework for manipulating ion transport in concentrated electrolytes and macroions, paving the way for innovations in fields reliant on precise control of electrostatic conditions and ionic mobility.",
        "authors": [
            "Emily Krucker-Velasquez"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158882",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Reproduction, settlement, and phenology of intertidal barnacles: Implications for larval dispersal",
        "abstract": "Knowledge of the consequences of ocean warming on marine populations and communities is urgent. Warming oceans are predicted to result in changes to the seasonal timing of reproduction and settlement (phenology); faster development rates and, for crustaceans, smaller larvae; reduced larval dispersal distances; and reduced connectivity between coastal populations. However, these predictions are largely based on laboratory and modelling studies, with little observational research to explore how these interactions unfold in natural ecosystems where temperature variability is pervasive. In this thesis, I investigate the links between reproduction and settlement timing of intertidal barnacles, and I explore the extent to which the timing of these events is explained by environmental and astronomical cycles and by water column conditions. In Chapter 2, I assess the cycles driving Chthamalus fissus reproduction and settlement in Southern California, and I offer a first order estimate of alongshore larval transport. I found that barnacles were reproductively active almost year-round, with clear lunar cyclicality and modest seasonality. Conversely, settlement exhibited little cyclicality on any timescale. Chapters 3, 4, and 5 focus on the effects of temperature on Semibalanus balanoides early life history along a steep temperature gradient in the northwest Atlantic over twenty years of warming. In Chapter 3, I investigate the effects of intertidal temperature on reproduction timing, analyzing separately the processes of fertilization, embryonic brooding, and larval release. In Chapter 4, I estimate larval duration in natural populations, and I measure the impact of temperature on larval duration in the laboratory and field. In Chapter 5, I investigate the effects of water temperature on larval size at settlement. I found that warmer nearshore temperatures significantly correlated with shorter brooding times of developing embryos, shorter field-estimated larval duration, and smaller larval settlers. Notably, the interplay between benthic reproduction, pelagic development, and temperature variability across space and time created counter-intuitive patterns in larval duration, size, and likely dispersal. Together, these findings point to the importance of reproductive timing in determining dispersal and population connectivity, and they highlight the need for extensive field measurements to quantify phenology and phenology shifts in benthic systems.",
        "authors": [
            "Jane B. Weinstock"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158871",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Computational Thermo-Chemo-Mechanics Framework for the Large-Scale Simulation of Material and Structural Failure in Hypersonic Environments",
        "abstract": "Materials and structures subjected to the extreme conditions of hypersonic flight undergo complex degradation and fracture processes. This thesis presents a theoretical formulation and a computational framework that enables the large-scale simulation of thermochemically fracturing solids exhibiting complex post-fracture interface response. The continuum theory is based on a general thermodynamically-consistent description of the coupled multiphysics problem, and the numerical formulation extends the scalable discontinuous Galerkin(DG)/Cohesive Zone Modeling paradigm to thermo-chemo-fracture mechanics. The approach is distinguished by its unified DG treatment of the coupled problems, which facilitates the analysis of fracture propagation, fracture-dependent heat and mass transfer as well as thermally-activated solid-phase chemical reactions. The framework is verified against two analytical solutions of boundary value problems drawn from thermo-poro-elasticity and thermally-driven delamination. Three-dimensional simulations of a benchmark thermochemically-driven fracture problem illustrate the parallel scalability of the fully-coupled computational framework. We utilize this framework to render models of passive oxidation-induced fracture in ultrahigh temperature ceramics computationally tractable. First, a rigorous constitutive theory is shown to capture the molecular diffusion of oxidant through the reaction product layer using only fundamental transport properties, i.e. without the need for calibration to reaction experiments. The physical processes observed on the diminutive scale of an oxide layer are explicitly resolved, but the approach is limited to microscale analyses by scale separation. We sidestep this limitation by specializing the general theory under specific phenomenological assumptions, thereby yielding a practical model that can reproduce oxidation experiments. We use this specialized model to analyze oxidation-induced swelling, fracture and delamination in SiC/coating systems, and unveil the coupled thermochemical response as well as fracture morphologies in the vicinity of critical flaws. Then, we conduct a parametric study of three-dimensional coatings that exposes the channeling mechanisms above penny-shaped delaminations of various sizes. The computational analyses identify a transition from decussating to circumferential channel cracking that explains the wide variety of surface channel cracks observed in experiment. The physical mechanisms and fracture morphology regimes are corroborated by a simple structural theory. Finally, cohesive fracture models, splitting methods and thermal solvers are developed specifically for applications to thermally shocked ceramics. Simple and rigorous calibration procedures are proposed which facilitate the direct analysis of fragmentation and comminution in brittle solids subjected to extreme advective heat transfer. The presented examples serve as evidence that the framework can successfully enable three-dimensional, thermochemically-coupled fracture analyses of unprecedented physical fidelity, which furnish new insights into complex hypersonic thermal protection system response.",
        "authors": [
            "Daniel N. Pickard"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158883",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Expanding options for the mechanical characterization of biological materials",
        "abstract": "The mechanical properties of biological tissues change over time and with disease progression, and they provide important information regarding the limits a tissue can sustain before injury. Therefore, quantifying these properties in biological materials and their synthetic simulants could be instrumental for accurate medical diagnoses, treatment of disease, and prediction of traumatic injury survivability. Conventional methods of mechanical testing, such as uniaxial tension, compression, and nanoindentation,  provide highly repeatable and reliable results for the stiff materials for which they were originally developed. However, the same cannot be said when these methods are applied to the characterization of soft and biological materials due to limitations of specimen size, fixturing capabilities, and sample preparation. Volume Controlled Cavity Expansion (VCCE) is a recently developed technique to measure local mechanical properties of soft materials in their natural environment. Through the highly controlled expansion of a fluid bubble at the tip of an injection needle, paired with simultaneous measurement of the resisting pressure, a local signature of a material's mechanical response can be obtained. \r\n\r\nThis thesis presents the first systematic application of VCCE to biological materials. It begins by presenting a cautionary example of the limitations of soft material testing, focusing on the synthetic silicone and tissue simulant polydimethylsiloxane (PDMS). We find that the wide range of mechanical properties  reported in literature are due to biases imparted by different testing methods. We then use VCCE to examine the elastic response of gelatin, whole blood clot and liver tissue, demonstrating with high repeatability that subtle mechanical changes occur within a matter of days as these tissues age. Finally, this work applies VCCE to investigate what happens to these materials after elastic expansion, and throughout a process of controlled damage. Biological materials are found to demonstrate toughening  that does not appear  in gelatin and PDMS. Because of these observed differences, we caution against using gelatin and PDMS for simulating the behavior of biological materials in extreme loading cases. Combining these findings, this thesis provides evidence that  more widespread adoption of VCCE in mechanical testing would provide a path to better understanding of the mechanics of soft and biological materials, with implications in fundamental mechanics research as well has in biological and healthcare applications.",
        "authors": [
            "Hannah Martin Varner"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158845",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "ΔB₀ Field Control in High Field MRI with Local Multcoil Shim Arrays",
        "abstract": "Local multicoil ΔB₀ shim arrays enable low-cost, simple to fabricate, and physically small, static magnetic field control in magnetic resonance imaging. The presented thesis will show frameworks for coil current calculation for homogeneity and novel selective excitation applications. As MRI RF coils trend towards repositionable and flexible systems for their ease of use and tight-to-the-patient fit, ΔB₀ shim arrays are left behind for lack of rapid, patient-on-the-table calibration. We show an inverse problem approach with physics-based regularization and adaptation to accelerate calibration by over 50 fold. The numerical tools developed for calibration also proved useful for design to enable novel upper bounds on ΔB₀ shim performance and new tools for automatic application and anatomy-specific local multicoil array design.",
        "authors": [
            "Nicolas Arango"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158963",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Scalable and Modular Manufacturing of Insect-Scale Aerial Robots Towards Swarm Flight Demonstrations",
        "abstract": "Insects demonstrate remarkable capabilities in navigating complex environments and executing tasks such as pollination and coordinated object transport. Inspired by these biological feats, insect-scale micro aerial vehicles (MAVs) have been developed with advanced flight functionalities, including collision resilience and aerial acrobatics. Despite these advancements, MAVs weighing less than a gram continue to face critical challenges in design, assembly, and repair. Additionally, limitations in sensing and control have prevented the realization of swarm-like behaviors, thereby constraining research on collective actions and potential applications such as distributed sensing. To overcome these obstacles, this work introduces a scalable and modular fabrication method for sub-gram MAVs. A parametric design algorithm automatically generates laser cutting templates from a minimal set of design parameters, while stereolithographic 3D printing is employed to fabricate static components such as airframes and connectors, significantly streamlining the production process. This modular approach improves assembly efficiency and repairability, reducing fabrication time by more than half. Using this methodology, two sub-gram MAVs successfully demonstrated controlled hovering and coordinated payload transport. These results represent a significant step toward enabling insect-inspired robotic swarms, providing a platform for future studies on collective flight behaviors and swarm robotics.",
        "authors": [
            "Yi-Hsuan Hsiao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158965",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Coarse Modality",
        "abstract": "One of the early successes of the application of possible worlds semantics to the analysis of natural language is Kratzer’s account of modality. A large part of the subsequent literature on modals has sought to expand the crosslinguistic coverage of that framework, and, in so doing, many new generalizations and constraints have been proposed and re-examined. The present dissertation situates itself within this tradition and makes both an empirical and theoretical contribution. Using the Italian adverb magari as the main empirical source, it will be argued that there exists a previously unnoticed type of modality which is referred to here as “coarse”. Its most evident manifestation is a special type of epistemic possibility, one that comes with an “antievidential” requirement. Antievidential possibility in assertions and questions is discussed in Chapters 1 and 3 respectively. Chapter 2 frames coarse modality as a more general phenomenon that comes about through modification of modal expressions. The theoretical argument of this dissertation is a novel corroboration of Kratzer’s premise semantics approach. It will be argued that the most natural and general account of coarse modality is possible by utilizing the premise set, a powerful resource of the system, in a novel way.",
        "authors": [
            "Enrico Flor"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158905",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Lagrangian perspective of mesoscale biophysical interactions in the subtropical ocean",
        "abstract": "The most kinetic energy in the ocean is at the mesoscale, which includes highly dynamic physical perturbations that persist for months, a biologically relevant timescale for phytoplankton growth and bloom development. Importantly, mesoscale currents and the associated biological responses (i.e., biophysical interactions) are not spatiotemporally static, so they are difficult to characterize. In this thesis, we interpret phytoplankton observations in an objective Lagrangian manner, or with a frame of reference that follows the motion of water parcels experienced by drifting organisms. We build a Lagrangian coherent eddy tracking algorithm that identifies the boundaries of water masses trapped for a month or longer. Using this tool, we assess the variability of the lateral advective properties of eddies across the North Pacific Subtropical Gyre, finding that only half of the remotely sensed eddies identified from the traditional, Eulerian sea level anomaly method trap waters for these timescales. We then statistically compare satellite-observed chlorophyll-a anomalies associated with eddies that trap versus mix across their boundaries. Lagrangian coherent vortices have more anomalous biological signatures in the gyre, so we argue that the role of leaky eddies in altering biogeochemistry may be underestimated due to lateral dilution. We also highlight substantial regional and seasonal variability in the dominant biophysical interactions within the oligotrophic regime, helping to explain inconsistencies of in situ eddy observations across this region. Lastly, we show how the Lagrangian water mass histories of in situ samples shape the phytoplankton community in the open ocean, quantified with amplicon sequencing and internal genomic standards. In non-eddy waters, we found that cyanobacteria are advantaged over eukaryotic phytoplankton when lateral mixing is minimized for several months. In or near mesoscale eddies, where vertical perturbations are a source of new nutrients, eukaryotic phytoplankton gene abundance has no dependence on the lateral mixing histories. The results suggest dispersal and niche generation drive phytoplankton variability but in different ways in and outside eddies. This thesis emphasizes how Lagrangian tools reveal mesoscale structures (otherwise invisible with Eulerian reference frames) that trap, transport, and transform ecosystems, generating phytoplankton patchiness and variability in the surface ocean.",
        "authors": [
            "Alexandra E. Jones-Kellett"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158812",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Floor Plan Design Collaborator: A Data-Driven Approach to Assist Human Architects in Design Exploration",
        "abstract": "After a long AI winter since the 1980s, artificial intelligence is now experiencing a renaissance due to enhanced computing power and access to vast amounts of data. Today, machines can talk, sing, and draw like human experts. Despite this progress, we are still far from the vision where human designers and AI collaboratively discuss and develop designs. This study argues that a data-driven approach holds great potential in the design process by quickly learning from existing examples and generating new alternatives for exploration. To support this claim, the study presents a generative framework that learns from existing examples and generates new designs. Specifically, the proposed framework employs Bayesian networks to encode site layout data and floor plan examples, generating new design examples through a Markov Chain Monte Carlo (MCMC) sampling procedure. Experiments on real-world examples demonstrate that the framework effectively summarizes the statistical information of given design examples and generates unseen examples based on the learned knowledge. The transparency of the data representation and the inner workings of the proposed framework facilitate an active feedback loop in the iterative learning and generation process between human designers and machines. Observations throughout the study reveal intrinsic limitations and potential improvements of contemporary optimization-based approaches from the perspective of both lateral and vertical design development.",
        "authors": [
            "Woongki Sung"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158842",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Hidden Roots of Neoliberal Success in Agrarian Transformation: State Engagement, Farmer Professionalization, and Technological Interdependence in the Senegal River Valley",
        "abstract": "Recent scholarship celebrates irrigated rice in the Senegal River Valley (SRV) as a success story. This is remarkable considering the SRV’s history of agrarian transformation, which critics characterize as incoherent, erratic, and self-destructive. How did this turnaround happen? How did good seeds emerge from bad soil? Conventional explanations point to enlightened market-based reforms and technological upgrading following state withdrawal from most agricultural activities. In other words, the SRV is portrayed as a triumph of neoliberalization. This dissertation offers an alternative, additive view. In Paper 1, I situate the SRV’s transformation in broad historical context, showing how notions of development, technological change, and poverty alleviation have evolved and the implications for what strategies are pursued. I illustrate how a popular contemporary development model— appropriate technology (AT) 2.0, an evolution of Schumacher’s 1970s AT 1.0—that valorizes smallscale technologies and market-led interventions is attractive in explaining successes like the SRV, even as it proves ultimately reductive. In Paper 2, I demonstrate how the state, despite policies curtailing its activities and a dominant narrative asserting its disengagement, continues to play an active role in the SRV. By imparting practical skills, such as pump operation, contract negotiation, and bookkeeping, state action helped farmers professionalize. A durable effect is a “we’re in this together” state-farmer mentality. When this relationship is tested, well-respected intermediaries, often religious leaders, intercede. In Paper 3, I show how farmers construct assemblages of resources, skills, and knowledge to achieve their goals. They rely on negotiation skills and social ties with local leaders, appealing to “public interest” couched in religious terms. In forsaking key aspects of the dominant assemblage to pursue alternatives, farmers exercise their agency and enhance market functioning by permitting flexibility, acknowledging technological interdependencies, and mitigating recurrent risks. This dissertation offers hope that successful agrarian development is possible in challenging, resource-constrained environments. Based on 11 months of fieldwork, I show how state and farmer actions bolstered market reforms, underpinning their success. In centering on-the-ground realities, I move beyond dominant explanations and neat theoretical classifications to reveal underreported but nonetheless fundamental processes and mechanisms through which development occurs.",
        "authors": [
            "Brian Jonars Besana Spielberg"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158867",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "An Instrument for the Measurement of Soft Material Nonlinear Mechanical Response",
        "abstract": "Soft material research has seen significant growth in recent years, with emerging applications in robotics, electronics, and healthcare diagnostics where understanding material mechanical response is crucial for precision design. Traditional methods for measuring nonlinear mechanical properties of soft materials require specially sized samples that are extracted from their natural environment to be mounted on the testing instrument. This has been shown to compromise data accuracy and precision in various soft and biological materials. To overcome this, the Volume Controlled Cavity Expansion (VCCE) method was developed. This technique tests soft materials by controlling the formation rate of a liquid cavity inside the materials at the tip of an injection needle, and simultaneously measuring the resisting pressure which describes the material response. Despite VCCE’s early successes, expansion of its application beyond academia has been hindered by cost, size, and expertise. In response to this, the first portable, bench-top instrument utilizing VCCE is presented here. This device, built with affordable, readily available components and open-source software, streamlines VCCE experimentation without sacrificing performance or precision. It is especially suitable for space-limited settings and designed for use by non-experts, promoting widespread adoption. The instrument’s efficacy was demonstrated through testing Polydimethylsiloxane (PDMS) samples of varying stiffness. This study not only validates instrument performance, but also sets the stage for further advancements and broader applications in soft material testing. All data, along with acquisition, control, and post-processing scripts, are made available on GitHub.",
        "authors": [
            "Brendan M. Unikewicz"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158836",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Algorithmic Advances in Range-Aided Navigation",
        "abstract": "This thesis contributes to the advancement of range-aided simultaneous localization and mapping (RA-SLAM) through algorithmic developments and real-world demonstrations. Broadly speaking, SLAM is the process by which an agent combines sensor measurements to simultaneously create a map of the world and localize itself within this map. SLAM has been called the ‘holy grail’ of field robotics, and in many instances it is a critical enabling capability for autonomous agents to operate in the real world. RA-SLAM is the specific case of SLAM which incorporates point-to-point distance measurements (e.g., distance measurements between an autonomous underwater vehicle and an acoustic buoy) into the inference process. The ability to leverage such measurements is desirable, as they can help in resolving ambiguities (e.g., am I in hallway A or B) and the relevant sensors are often low-cost and simple to integrate (and thus pose the potential to be widely deployed). However, there are theoretical challenges that have historically limited the reliability of RASLAM approaches. At the root of these challenges is the issue that a single range measurement does not uniquely determine the relative position between two points. In state-of-the-art RASLAM formulations, this ambiguity manifests as non-convexity in the maximum a posteriori inference problem. As a result of this non-convexity, standard local-search optimizers are highly dependent on quality initializations to obtain the correct state estimate. To address this issue of reliability, this thesis presents the first certifiably correct algorithm for RA-SLAM. This algorithm, Certifiably Correct RA-SLAM (CORA), is capable of (i) obtaining globally optimal solutions for many real-world RA-SLAM problem instances and (ii) providing certificates of correctness for these solutions. CORA leverages a novel semidefinite programming (SDP) relaxation of the RA-SLAM problem, which it solves efficiently using the Riemannian Staircase methodology. This methodology allows CORA to typically obtain globally optimal solutions faster than the existing state-of-the-art local solvers. These results expand our understanding of problems suited for efficient global solvers and highlight the key problem structures that appear necessary to develop and deploy such solvers, pointing towards exciting future directions in trustworthy model-based autonomy. We demonstrated the performance of CORA on a range of real-world RA-SLAM datasets, including a set of large-scale multi-agent experiments conducted as part of this work. In these experiments CORA reliably estimates agents’ trajectories in both single- and multi-robot settings. CORA gracefully scales to large problems consisting of multiple agents and tens of thousands of robot poses. These experiments not only validate CORA’s performance, but also fill an existing gap in open-source datasets available to the research community and provide practical insights to guide future deployments of autonomous navigation systems in large, complex environments.",
        "authors": [
            "Alan A. Papalia"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158863",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "thesis in the field of Mechanical Engineering: Relevance for Human-Robot Collaboration: Definitions, Systems, Algorithms, and Applications",
        "abstract": "Human-Robot Collaboration (HRC) combines the strengths of human and robotic capabilities to accomplish complex tasks, yielding significant impacts in various domains. To enable seamless interaction in dynamic and unpredictable environments, robots are required to efficiently and accurately perceive their surroundings, align reasoning with human cognition, anticipate key attributes, and generate safe, effective actions to support humans proactively. This thesis introduces relevance, a novel concept inspired by human cognition, to improve the efficiency, safety, and intelligence of HRC. Relevance enables robots to prioritize objects based on their importance to human goals, allowing them to concentrate computational resources on key elements. This focused approach reduces input space for essential algorithms, minimizes processing delays, and enhances safety and adaptability in dynamic environments, facilitating more natural and intuitive collaboration with humans. This thesis systematically explores the concept of relevance, introducing a hierarchical model for relevance quantification that combines scene understanding in cluttered environments with an event-based, multi-modality framework, enabling real-time relevance determination based on human objectives, preferences, spatial-temporal relationships, and constraints. A relevance-based perception strategy further directs models to prioritize key areas, reducing computational and inference times, while two new safety metrics—Critical Collision Probability (CCP) and Average Collision Probability (ACP)—quantify reduced collision risks in Human-Robot Collaboration (HRC). Additionally, a relevance-driven framework integrates relevance quantification with dynamic scene understanding and decision-making, achieving high human objective and relevance prediction accuracy. An advanced human intention prediction framework using head orientation, object affordance, and hand movement also enhances precision, accuracy, and F1 scores over baseline models. Results demonstrate that relevance quantification significantly reduces task planning time by 79.56% and inquiries by 80.84%, with a real-world coffee-serving demonstration highlighting its potential for proactive, autonomous assistance. Furthermore, the safe motion generation algorithm reduces collision incidents by 63.76% and collision frames by 44.74%, supporting accurate, safe robotic assistance in dynamic environments. The concept of relevance enhances the efficiency, safety, and intelligence of human-robot collaboration (HRC) within dynamic and unpredictable environments, supporting a deeper integration of robotics into diverse real-world applications. Its potential extends beyond HRC, with promising applicability in autonomous driving and other complex domains where adaptive, context-aware decision-making is essential.",
        "authors": [
            "Xiaotong Zhang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158873",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Co-Designing Efficient Systems and Algorithms for Sparse and\r\nQuantized Deep Learning Computing",
        "abstract": "Deep learning models are becoming increasingly complex, expanding from 1D text and 2D images to 3D point clouds, while their size continues to grow exponentially. This trend highlights the need for greater efficiency. This thesis systematically explores efficiency in two resource-intensive domains—autonomous driving and generative AI—by focusing on fundamental model compression techniques: sparsity and quantization, alongside the co-optimization of systems and algorithms. Sparsity is crucial for autonomous vehicle (AV) applications. LiDAR processing, which requires 3D sparse computation, is inefficiently handled by current GPU libraries, creating a performance bottleneck in AV perception. To address this, we propose TorchSparse++, a high-performance GPU system for 3D sparse convolution, achieving 1.7-3.3× speedups over state-of-the-art libraries. Additionally, we introduce BEVFusion, an efficient multi-sensor fusion framework that fuses information in bird’s-eye-view (BEV) space, reducing computation by 1.9× while enhancing accuracy compared to prior methods. Generative AI is constrained by the massive size of models, necessitating quantization for efficient deployment. This thesis presents two GPU systems for accelerating large language models (LLMs): TinyChat for edge LLM deployment and QServe for cloud-based LLM serving. TinyChat boosts edge LLM inference by 3× using activation-aware weight quantization (AWQ). QServe further improves performance with activation and KV cache quantization, enhancing the throughput of NVIDIA TensorRT-LLM by 1.2-2.4× on A100 GPUs. Finally, we introduce HART, an efficient autoregressive image generation method that achieves 4.5-7.7× higher throughput compared to diffusion models while maintaining visual quality. HART achieves this improvement by leveraging quantized, or discrete, visual tokens to capture the high-level structure of images, while a lightweight diffusion model is used for fast inference of finer details.",
        "authors": [
            "Haotian Tang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158928",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Learning from Past Market Outcomes: Evidence from the Music Industry",
        "abstract": "We leverage unique features of music albums to investigate how musicians learn from current products when developing new products. We find that songs on a musician’s next album tend to be more similar to the songs that are more successful on that musician’s current album. This effect is stronger when the musician has less experience, and when the song on the current album is more novel (for that musician). Our findings suggest that musicians learn from the success of previous songs when developing new songs, and that learning is stronger if the musician has more need to learn, and when the song contains more new information.",
        "authors": [
            "Jason Du"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158804",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Dynamic Markers",
        "abstract": "When I was a child, I was certain that all clouds came from New \tJersey. After passing through the Lincoln Tunnel, I-95 would gradually ascend, lifting our car to eye level with the billowing clouds emerging from beneath us. These clouds rose from the Meadowlands, a great marsh just two miles west of Manhattan, a landscape that has become defined by the infrastructure that occupies it. Nearly equal in land mass and opportunity to Manhattan, this landscape managed to resist holistic transformation due to our inability to control its water. Rather than becoming a prosperous site for agriculture in the 19th century, or the next metropolis in the early 20th, the Meadowlands fell out of focus and became a site to absorb the infrastructural networks needed to uphold rapid development at its edges.\r\n\r\nThe Meadowlands was sutured shut by the networks interlaced through it in an attempt to erase the failures of the past. Utilizing this landscape as an urban sponge neglected that the marsh hosted a series of ecological infrastructures of its own. The Meadowlands' soft, uncertain ground once managed variations in the water level, but the draining of the ground that came with development reduced its capacity, making pump stations essential for managing water in inhabited areas. Unlike the other forms of infrastructure in the Meadowlands, the presence of the pump station is subdued, its invisibility upholds the illusion that the developments within this landscape are not threatened by their surroundings. However, steady sea level rise and an increase in storm surges have caused these pumps to fail, pulling the veil on their existence and more importantly, the essential role they play in our continued occupation of this landscape. The urgent need to increase the capacity of the pump station provides an opportunity to reconsider their agenda.\r\n\r\nThis thesis proposes the Dynamic Marker, a new type of infrastructure that redefines the relationship between human systems and ecological flows. Grafted onto existing pump stations in the Meadowlands, it releases water as mist from 800 feet in the air, transforming the hidden mechanics of water management into a moment of wonder. The Dynamic Marker fosters microclimates and ecological connections, transforming infrastructure into a dynamic process that evolves with its surroundings. Over time, it becomes both a memorial to the marsh and a provocation for the future, inviting a rethinking of infrastructure as a participatory and adaptive force that responds to its surrounding ecology.",
        "authors": [
            "Evan Ortiz"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158828",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Genetic algorithm gradient ascent (GAGA) optimization\r\nof compact symmetry-breaking photonic crystals",
        "abstract": "Fundamental limits of thermal radiation are imposed by Kirchhoff’s law, which assumes the electromagnetic reciprocity of a material or material system. Thus, breaking reciprocity can enable breaking barriers in thermal efficiency engineering¹. This thesis presents 1D photonic crystals composed of Weyl/Dirac semimetal and dielectric layers, whose structures are optimized to maximize the nonreciprocity of infrared radiation absorptance/emittance in planar and compact designs. Two different mechanisms to enable nonreciprocal infrared absorbers/emitters are simulated and compared – anomalous Hall effect in Weyl semimetals 2 and electric-current-induced Fizeau drag in either Dirac or Weyl semimetals3 . To engineer an ultra-compact absorber structure that does not require gratings or prisms to couple light, a genetic algorithm (GA) was used to maximize nonreciprocity in the design globally, followed by the application of the numerical gradient ascent (GAGA) algorithm as a local optimization to further enhance the design. The first absorber design takes advantage of the intrinsic nonreciprocity of time-reversal symmetry (TRS) breaking Weyl semimetals due to their pseudomagnetic field in momentum space. GAGA methodology is then applied to design and optimize a flat absorber using inversion (IS) breaking Weyl/Dirac semimetals as active layers, in which tunable nonreciprocity is induced through an applied DC current bias. This momentum bias imparts plasmon Fizeau drag, the drag of an electrical current on propagating surface plasmon polaritons (SPPs). A semi-classical theory recently developed is used to model SPP transport along interfaces of 3D semimetals under Fizeau drag3 . Lastly, in both cases the optimization algorithm accounts for both s- and p-polarized absorptance spectra to create a final design suitable for thermal applications, which maximizes the nonreciprocal absorptance of p-polarized light and simultaneously minimizes the parasitic, reciprocal absorptance of s-polarized light.",
        "authors": [
            "Hannah T. Gold"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158958",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Navigating RAD Conversions: Suggestions for Public Housing Rehabilitation",
        "abstract": "Public housing in the United States, a critical resource for nearly 1.7 million residents, faces significant challenges due to aging infrastructure and chronic operating funding shortfalls. The Rental Assistance Demonstration (RAD) program, authorized by Congress in 2012, aims to address these issues by leveraging private financing to rehabilitate and modernize public housing properties. Although the RAD program has been around for more than a decade and leveraged over $18.5 billion of construction investments, close to 75% of the more than 2500 eligible local PHAs are yet to benefit from it. This thesis examines the evolution of RAD programs, including the two newer tools, RAD/Section 18 Blend and Faircloth-to-RAD, and their adoption by public housing authorities (PHAs).\r\nThe research incorporates a review of HUD program and policies, RAD implementation data, and interviews with industry practitioners, including PHAs, developers, and consultants, to understand the hurdles preventing the adoption of the program and the characteristics of successfully structured projects. This thesis offers insights into how specific strategies are used to overcome the hurdles and provides practical recommendations for PHAs seeking to leverage RAD for public housing preservation and development. Key findings highlight the importance of utilizing available funding sources to achieve financial feasibility and enhancing organizational skills and capacity.",
        "authors": [
            "Yu Yan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158788",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Accelerating Distributed Deep Neural Network Training\r\nand Fine-Tuning Through Resource Interleaving",
        "abstract": "The ever-growing increase in dataset and model sizes of deep learning has created a massive demand for efficient GPU clusters. As the number of GPUs increases, the communication overhead of distributed Machine Learning (ML) training and fine-tuning workloads quickly takes up a significant portion of iteration time. Yet state-of-the-art ML schedulers tend to ignore the communication pattern of ML jobs when placing workers on GPUs. This thesis advocates for communication-aware resource scheduling as a critical approach to optimizing network utilization in ML clusters. We introduce a key idea for accelerating Deep Neural Network (DNN) jobs that interleaves the communication demands of different jobs sharing a network link. To illustrate this concept of interleaving, we first demonstrate how intentionally creating unfairness in bandwidth share between different DNN jobs improves their iteration times. Building on this insight, we present two novel systems designed to minimize network congestion and accelerate DNN training and fine-tuning jobs. The first system, Cassini, achieves interleaving using a centralized approach. In contrast, the second system, MLTCP, achieves the same goal using a distributed approach. Both systems are practical and readily deployable, depending on the service provider’s preference on deploying centralized or distributed solutions. In particular, Cassini, is a centralized network-aware job scheduler for ML clusters. Cassini introduces a novel geometric abstraction to consider the communication pattern of different jobs while placing them on network links. To do so, Cassini uses an Affinity graph that finds a series of time-shift values to adjust the communication phases of a subset of jobs such that the communication patterns of jobs sharing the same network link are interleaved with each other. Second is MLTCP, a distributed technique to approximate an interleaved centralized flow schedule. At the heart of MLTCP lies a straight-forward principle based on a key conceptual insight: by scaling the congestion window size (or sending rate) based on the number of bytes sent at each iteration, MLTCP flows eventually converge into a schedule that reduces network contention. To evaluate these systems, we conduct experiments using real-world DNN models on a testbed with Nvidia A100 GPUS. Cassini and MLTCP improve the average iteration times by up to 1.6× and 1.9×, respectively, demonstrating their effectiveness in reducing network congestion and accelerating ML workloads.",
        "authors": [
            "Sudarsanan Rajasekaran"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158918",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Tailored Mechanical Response of 3D Microgranular Crystals with Hierarchical Architecture",
        "abstract": "Granular media exhibit extraordinary impact-mitigating properties due to their nonlinear grain-to-grain interactions, enabling efficient energy dissipation and wave perturbation under dynamic loading—behaviors unattainable in conventional monolithic materials. Recent efforts have sought to engineer granular systems with tunable mechanical responses, though few have begun to realize them as functional architected materials. Here, we introduce a two-level architected granular framework that programs spherical microgranular media across both grain-level (ellipsoidal microvoids) and bulk granular packing-level architectures, offering surprising control over static and dynamic properties. Using nanoindentation experiments, we reveal tunable quasi-static stiffness behavior, where hollow architected granular packings can exhibit superior mass-normalized energy dissipation compared to their fully dense counterparts. Finite element simulations uncover a structurally engineered Poisson effect, enabling nonlocal contact mechanisms that enhance load-bearing capacity across different packing structures. Future custom direct impact experiments demonstrate a potential route the effectiveness of our multi-scale design in dynamically programming energy dissipation. Our findings demonstrate that a hierarchical granular crystal exhibits enhanced specific energy absorption at a fraction of the weight of their fully dense counterparts and unique nonlocal stress redistribution, surpassing classical granular mechanics through architectural design. This work establishes a path toward lightweight, tunable, and impact-resistant metamaterials, with broad applications in nonlinear waveguiding, energy dissipation, and protective systems.",
        "authors": [
            "Samuel D. Figueroa"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158956",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "American (Ise): On the Lifecycle of Stadiums in the United States",
        "abstract": "When the Kingdome in Seattle was completed in 1976, it was celebrated as a marvel of modern engineering, expected to last for centuries. Yet, in an ironic twist, it was demolished by implosion in 2000, surviving only twenty-four years. The Kingdome epitomizes the issue of short lifespans that has plagued American stadiums since the post-war era. A broad survey of these structures reveals an average lifespan of just three decades—a startlingly brief tenure for buildings of their scale and significance. These stadiums also follow a distinctive model of renewal. Similar to the Shikinen Sengu ritual at the Ise Shrine, a new stadium is often constructed adjacent to its predecessor. However, unlike Ise, where materials from the old shrine are reused and disseminated throughout Japan’s network of shrines, old stadiums are almost always demolished and discarded. This thesis seeks to superimpose Ise as a model onto American stadiums, envisioning an architecture that embraces both impermanence and longevity through circularity. Investigations into the barriers to circularity specific to stadiums serve as the foundation for design proposals, spanning scales from the detail to the site. The project ultimately imagines a stadium in a constant process of disassembly and renewal, where its spatial and programmatic potential challenge paradigms of completeness. In the context of a climate crisis demanding waste reduction, and for a typology notorious for its excess, stadiums can learn to do more with less.",
        "authors": [
            "Mackinley Wang-Xu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158895",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Causal Foundations for Pragmatic Data Science",
        "abstract": "A key goal of scientific discovery is the acquisition of knowledge that is practically useful for societal endeavors, such as the development of medicine or the design of fruitful economic policies. In this thesis, I place front and center the role that scientific models play in the process of decision-making, emphasizing the importance of causal models in science, i.e., models which describe the possible effects of actions upon a system. The work contained explores central topics in this domain, including causal discovery (learning causal models from data), causal representation learning (learning how to coarse-grain observations into causally sensible “macro-variables”), and end-to-end causal inference (the interplay between causal discovery and downstream decision-making).",
        "authors": [
            "Chandler Squires"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158952",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Development of a computational tool and dynamometer for optimizing variable-speed centrifugal pump selection for a containerized, direct-drive photovoltaic electrodialysis desalination system",
        "abstract": "This thesis presents an optimized centrifugal pump selection methodology to improve the hydraulic efficiency of MIT’s Global Engineering and Research (GEAR) Center’s containerized, direct-drive photovoltaic electrodialysis desalination system capable of producing up to 300m3 of potable water per day. The novel flow-commanded current control scheme of this containerized desalination plant (CDP), which enables its minimal energy storage, also means that the centrifugal pumps used are operated at variable speeds to respond to the solar irradiance. Unfortunately, centrifugal pumps are typically designed for fixed operating conditions, and manufacturers often only report pump performance at their rated frequency. By estimating the hydraulic resistances of the CDP and testing potential pumps on a redesigned dynamometer, a MATLAB-based tool was developed to quickly and iteratively characterize pump performance at their expected operating points in the CDP. A\r\n\"Compatibility Factor\" metric, defined by the normalized area under a pump’s efficiency-flow curve at its operating points, was devised to quantify each pump’s efficiency across the entire operating range of flow rates achievable under the CDP’s system constraints. Using this methodology, two 7.5 kW pumps were selected per diluate and concentrate channels to the electrodialysis stacks for alternate operation use. Following testing pumps on a dynamometer, this work outlines a methodology for characterizing a pump’s variable-speed efficiency at its operating points in any modeled system. This approach facilitates informed pump selection for the CDP to increase its water production and reduce its specific energy consumption, with an estimated improvement in hydraulic efficiency from 10% in GEAR Center’s previous system to over 30%. Overall, this work is applicable to various photovoltaic pumping systems aiming to reduce carbon emissions through variable-speed operation.",
        "authors": [
            "Muriel A. McWhinnie"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158831",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Deep learning initialized compressed sensing (Deli-CS) in volumetric spatio-temporal subspace reconstruction",
        "abstract": "Object Spatio-temporal MRI methods offer rapid whole-brain multi-parametric mapping, yet they are often hindered by prolonged reconstruction times or prohibitively burdensome hardware requirements. The aim of this project is to reduce reconstruction time using deep learning. Materials and methods This study focuses on accelerating the reconstruction of volumetric multi-axis spiral projection MRF, aiming for whole-brain T1 and T2 mapping, while ensuring a streamlined approach compatible with clinical requirements. To optimize reconstruction time, the traditional method is first revamped with a memory-efficient GPU implementation. Deep Learning Initialized Compressed Sensing (Deli-CS) is then introduced, which initiates iterative reconstruction with a DL-generated seed point, reducing the number of iterations needed for convergence. Results The full reconstruction process for volumetric multi-axis spiral projection MRF is completed in just 20 min compared to over 2 h for the previously published implementation. Comparative analysis demonstrates Deli-CS’s efficiency in expediting iterative reconstruction while maintaining high-quality results. Discussion By offering a rapid warm start to the iterative reconstruction algorithm, this method substantially reduces processing time while preserving reconstruction quality. Its successful implementation paves the way for advanced spatio-temporal MRI techniques, addressing the challenge of extensive reconstruction times and ensuring efficient, high-quality imaging in a streamlined manner.",
        "authors": [
            "S. S. Schauman",
            "Siddharth S. Iyer",
            "Christopher M. Sandino",
            "Mahmut Yurt",
            "Xiaozhi Cao",
            "Congyu Liao",
            "Natthanan Ruengchaijatuporn",
            "Itthi Chatnuntawech",
            "Elizabeth Tong",
            "Kawin Setsompop"
        ],
        "journal_conference_name": "Magnetic Resonance Materials in Physics, Biology and Medicine",
        "publisher": "Springer International Publishing",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158263",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Sorption-based atmospheric water harvesting: from atoms to applications",
        "abstract": "Thirteen thousand trillion liters of water in the atmosphere is a natural resource found anywhere on the earth, and available to anyone. Sorption-based atmospheric water harvesting (SAWH) is the extraction of water vapor using sorbent materials across a broad spectrum of relative humidity, which opens new avenues to address water scarcity faced by two-thirds of the world’s population. SAWH technologies gained significant attention in 2017 with the development of a solar-powered system utilizing metal-organic framework (MOF) sorbents to extract water from the air. While groundbreaking, this proof-of-concept device produced only a few milliliters of water, far from sufficient to meet even a single person’s daily water needs. A large gap thus remains between laboratory discoveries and real-world applications. This thesis aims to advance the understanding of SAWH technologies from atoms to applications. It begins with a multiscale perspective on SAWH technologies towards real-world applications, addressing knowledge gaps across various length scales. Through this multiscale approach, we developed a framework that can bridge material innovations with device realization. At the molecular scale, the thesis seeks to address a fundamental challenge: the inability to directly observe water sorption processes. To overcome this long-standing challenge, we introduced the use of cryogenic transmission electron microscopy (cryo-TEM) to probe water sorption in nanoporous materials at the single-pore level. This approach allows us to image water sorption and material structures with atomic resolution. Owning to the high resolution and in situ capabilities of cryo-TEM, we resolved a partially water-filled state of MOF crystals and observed that water molecules tend to occupy the centers of pores and fill neighboring pores once adjacent ones are filled. This technique offers new insights into sorption mechanisms and holds significant potential for the development of new sorbent materials. Building on the material-device-bridging framework, we proposed a dual-stage device architecture inspired by multistage distillation in desalination, where condensation heat from one stage drives desorption in the next, increasing productivity and thermal efficiency. To guide materials selection based on operating conditions, a universal thermodynamic model is developed to predict the efficiency of sorbent materials given their sorption isotherms. Additionally, this analysis reveals practical strategies to improve devicelevel sorption kinetics and heat transfer performance, pushing the technology toward thermodynamic limits. At the global scale, the framework enables the optimization of material deployment tailored to diverse climatic conditions. The real-world impact is further demonstrated through a technoeconomic assessment, which illustrates SAWH technology’s competitiveness with bottled and tap water and pathways to further improve its cost-effectiveness. The thesis concludes with an outlook on future opportunities for SAWH technologies and a discussion of their societal and environmental impacts at scale, including their potential role in mitigating climate change.",
        "authors": [
            "Yang Zhong"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158799",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Falling isn't the End: Reimagining Demolition as a Creative Practice",
        "abstract": "This thesis investigates resilience not as an endpoint but as a condition of continuous transformation. It critiques the shortcomings of current architectural discourse in addressing climate disasters, waste, and carbon footprints. While these crises are widely acknowledged, architecture often operates within restrictive economic, legal, and cultural systems, relegating resilient design to the periphery or diminishing its potential impact.\r\nCollapse, traditionally perceived as failure, is reimagined here as a generative moment—an opportunity to rethink materials, systems, and the narratives that shape them. Central to this exploration is the concept of assembly, where materials are designed with deliberate life spans—some transient, others enduring. By anticipating the gaps and shifts that arise when permanence is no longer assumed, this thesis proposes new possibilities for adaptive design and architectural resilience within the evolving rhythms of life.\r\nTo articulate these ideas, the thesis employs speculative scenarios and temporal media. These tools position architecture as a system in flux, evolving in tandem with societal and environmental changes. Through narrative-driven methodologies, this work seeks to expand architectural discourse, prompting reflection on the discipline’s foundational assumptions while connecting it to broader cultural and systemic challenges.\r\nUltimately, this thesis redefines resilience—not as resistance or mere survival but as a dynamic and imaginative practice. It advocates for architecture’s leadership within the broader zeitgeist of sustainability, transforming pressing global challenges into opportunities for creative agency and systemic reinvention.",
        "authors": [
            "So Jung Lee"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158896",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Variational Lower Bound to Mitigate Batch Effect in\r\nMolecular Representations",
        "abstract": "High-throughput drug screening – using cell imaging or gene expression measurements as readouts of drug effect – is a critical tool in biotechnology to assess and understand the relationship between the chemical structure and biological activity of a drug. Since large-scale screens have to be divided into multiple experiments, a key difficulty is dealing with batch effects, which can introduce systematic errors and non-biological associations in the data. We propose InfoCORE, an Information maximization approach for COnfounder REmoval, to effectively deal with batch effects and obtain refined molecular representations. InfoCORE establishes a variational lower bound on the conditional mutual information of the latent representations given a batch identifier. Experiments on drug screening data reveal InfoCORE’s superior performance in a multitude of tasks including molecular property prediction and molecule-phenotype retrieval. Additionally, we show results for how InfoCORE offers a versatile framework and resolves general distribution shifts and issues of data fairness by minimizing correlation with spurious features or removing sensitive attributes.",
        "authors": [
            "Chenyu Wang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158954",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "From Data, to Models, and Back: Making Machine Learning Predictably Reliable",
        "abstract": "Machine learning systems exhibit impressive performance, but we currently lack scalable ways to anticipate their successes, failure modes, and biases. This position limits our ability to deploy these systems in the appropriate contexts, and to build systems which we can confidently deploy in high-risk settings. Motivated by this state of affairs, this thesis aims to develop design principles for predictably reliable machine learning. Our ultimate goal is to enable developers to know when their models will work, anticipate when they will fail, and understand “why” in both cases. In pursuit of this goal, this thesis combines large-scale experiments with theoretical analysis to form a precise understanding of the ML “pipeline,” from training data (and the way we collect it), to learning algorithms, to deployment. Fully realized, such an understanding would allow us to build ML systems the same way we build buildings or airplanes—safely, scalably, and with a robust grasp of the underlying principles. In this thesis, we focus on four design choices within this pipeline: model deployment (Part I), dataset creation (Part II), data collection (Part III), and algorithm selection (Part IV). For each of these design choices, we use targeted experiments to uncover the corresponding principles that actually underlie the behavior of ML systems. We distill these principles into concise conceptual models which allow us to both reason about existing systems and design improved ones. Along the way, we will revisit, challenge, and refine various aspects of conventional wisdom surrounding ML model development.",
        "authors": [
            "Andrew Ilyas"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158964",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Multi-Subject Image Generation",
        "abstract": "Diffusion models excel at text-to-image generation, especially in subject-driven generation for personalized images. However, existing methods are inefficient due to the subject-specific fine-tuning, which is computationally intensive and hampers efficient deployment. Moreover, existing methods struggle with multi-subject generation as they often blend identity among subjects. In this thesis, we present FastComposer which enables efficient, personalized, multi-subject text-to-image generation without fine-tuning. FastComposer uses subject embeddings extracted by an image encoder to augment the generic text conditioning in diffusion models, enabling personalized image generation based on subject images and textual instructions with only forward passes. To address the identity blending problem in the multi-subject generation, FastComposer proposes cross-attention localization supervision during training, enforcing the attention of reference subjects localized to the correct regions in the target images. Naively conditioning on subject embeddings results in subject overfitting. FastComposer proposes delayed subject conditioning in the denoising step to maintain both identity and editability in subject-driven image generation. FastComposer generates images of multiple unseen individuals with different styles, actions, and contexts. It achieves 300–2500 speedup compared to fine-tuning-based methods and requires zero extra storage for new subjects. FastComposer paves the way for efficient, personalized, and high-quality multi-subject image creation.",
        "authors": [
            "Tianwei Yin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158959",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "If These Hills Could Speak",
        "abstract": "If these hills could speak, what would they reveal, and how would they express it? This central question guides this thesis, which examines three hills in the heart of Ibadan, Southwest Nigeria— each occupied by the ruins of colonial monuments. Before the construction of these structures, the hills served as sanctuaries, providing water, food, and safety. However, under British colonial rule, architecture was utilized to disrupt this harmonious relationship. Over the course of 50 years, three monuments were erected that mark Britain’s colonial imprint on the city: a neoclassical courthouse (1925), built to assert control over the central market; a 60-foot tower (1936), which displaced the surrounding forests; and a theater (1977), built during a time of national struggle for unity and identity. Today, at the foot of these hills, a community has forged a way of life within a broken system. By repurposing and subverting structures in ways their creators never intended, this community embodies a praxis and poiesis of adaptive creativity within the built environment. This process represents a transformative act of pidginization—a collective tactic for repair, resistance, and reappropriation in response to an ongoing, imposed socio-political order. For these hills to speak again, the ruins must be transformed. This thesis begins that process by applying acts of pidginization learned from below to the three ruins. It proposes their conversion through deconstruction and de-monumentalization, with the aim of fostering economic development, ecological restoration, and cultural production in the city.",
        "authors": [
            "Tejumola Bayowa"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158841",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Do What You Say—Computing Personal Values Associated with Professions Based on the Words They Use",
        "abstract": "Members of a profession frequently show similar personality characteristics. In this research, we leverage recent advances in NLP to compute personal values using a moral values framework, distinguishing between four different personas that assist in categorizing different professions by personal values: “fatherlanders”—valuing tradition and authority, “nerds”—valuing scientific achievements, “spiritualists”—valuing compassion and non-monetary achievements, and “treehuggers”—valuing sustainability and the environment. We collected 200 YouTube videos and podcasts for each professional category of lawyers, academics, athletes, engineers, creatives, managers, and accountants, converting their audio to text. We also categorize these professions by team player personas into “bees”—collaborative creative team players, “ants”—competitive hard workers, and “leeches”—selfish egoists using pre-trained models. We find distinctive personal value profiles for each of our seven professions computed from the words that members of each profession use.",
        "authors": [
            "Aditya Jha",
            "Peter A. Gloor"
        ],
        "journal_conference_name": "Algorithms",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158295",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "It Is Time to Standardize Principles and Practices for Software Memory Safety",
        "abstract": "In this Inside Risks column, we explore memory-safety standardization, which we argue is an essential step to promoting universal strong memory safety in government and industry, and, in turn, to ensure access to more secure software for all. During the last two decades, a set of research technologies for strong memory safety—memory-safe languages, hardware and software protection, formal approaches, and software compartmentalization—have reached sufficient maturity to see early deployment in security-critical use cases. However, there remains no shared, technology-neutral terminology or framework with which to specify memory-safety requirements. This is needed to enable reliable specification, design, implementation, auditing, and procurement of strongly memory-safe systems. Failure to speak in a common language makes it difficult to understand the possibilities or communicate accurately with each other, limiting perceived benefits and hence actual demand. The lack of such a framework also acts as an impediment to potential future policy interventions, and as an impediment to stating requirements to address observed market failures preventing adoption of these technologies. Standardization would also play a critical role in improving industrial best practice, another key aspect of adoption.",
        "authors": [
            "Robert Watson",
            "John Baldwin",
            "Tony Chen",
            "David Chisnall",
            "Jessica Clarke",
            "Brooks Davis",
            "Nathaniel Filardo",
            "Brett Gutstein",
            "Graeme Jenkinson",
            "Ben Laurie",
            "Alfredo Mazzinghi",
            "Simon Moore",
            "Peter Neumann",
            "Hamed Okhravi",
            "Alex Rebert",
            "Alex Richardson",
            "Peter Sewell",
            "Laurence Tratt",
            "Muralidaran Vijayaraghavan",
            "Hugo Vincent",
            "Konrad Witaszczyk"
        ],
        "journal_conference_name": "Communications of the ACM",
        "publisher": "Association for Computing Machinery",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158237",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Natural Language Foundation Models in Medical Artificial Intelligence",
        "abstract": "Over the past decade, the transformative rise of deep learning, particularly large language models (LLMs), has inspired experts across diverse fields, including healthcare, to think deeply about how artificial intelligence (AI) can revolutionize their fields. In this time, general foundation models, rather than narrow and highly specialized task-specific systems, have begun to emerge as the dominant paradigm. In healthcare, AI systems are already seeing widespread implementation in a variety of real-world use cases, perhaps without adequate evaluation and validation. Indeed, their often impressive ability to process natural language, a crucial medium of knowledge and communication in medicine, suggests that many of these modern foundation models may hold immense promise in the healthcare space. However, there exists a need to better study and understand their strengths, limitations, and robustness, particularly in more realistic and clinically relevant settings.\r\n\r\nThis thesis focuses on two key classes of natural language-driven foundation models --- Contrastive Language Image Pretraining (CLIP) models, and Large Language Models (LLMs) --- and investigates how such models can encode and deliver useful clinical knowledge, for tasks like chest x-ray interpretation, differential diagnosis, history taking, and clinical management. As a whole, this thesis aims to further our collective understanding of the potential of natural language foundation models in medicine, while emphasizing the need for significant further research to address real-world challenges and understand the scopes in which such systems can be implemented safely and efficaciously.\r\n\r\nIn the first chapter, I provide an overview of some relevant background, including contrastive language-image pretrained models, large language models, and their evaluation in the medical domain. \r\n\r\nIn chapter 2, we improve the CLIP architecture for chest x-ray interpretation through a novel regularization technique applied during pre-training, and use this model for the zero-shot identification of chest x-ray findings.\r\n\r\nIn chapter 3, we examine the reliability of CLIP-style models.  First, we evaluate their robustness to shortcut learning to understand the potential protective effects of text self-supervision. Next, we explore how conformal prediction can be used to control zero-shot classification performance and preempt compatible inputs for these CLIP-style models.\r\n\r\nIn chapter 4, I describe the development of Articulate Medical Intelligence Explorer (AMIE), a conversational diagnostic AI fine-tuned with simulated medical dialogue. We evaluate the diagnostic capabilities of AMIE in two randomized studies with primary care physicians; first, in challenging clinicopathological conference (CPC) cases, and then in virtual text-based objective structured clinical examinations (OSCE).\r\n\r\nIn chapter 5, we explore AMIE's management reasoning capabilities in two subspecialty domains: genetic cardiovascular disease and breast oncology. In these studies, we design domain-specific assessments for case management and compare AMIE's performance to generalists under subspecialist evaluation, as well as studying its potential assistive effect.",
        "authors": [
            "Anil Palepu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158802",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Co-Living in Seoul: Addressing Housing Needs and Redefining Rental Market Trends",
        "abstract": "Co-living emerged as a novel asset class in the mid-2010s, addressing the housing needs of urban residents affected by rising housing costs, increasing urban migration, and the growing prevalence of single-person households. In South Korea, co-living has gained attention as a viable alternative to traditional housing, driven by unique local dynamics, including the decline of the dominant Jeonse system and a significant shortage of housing tailored to single-person households. With a growing preference for monthly rental systems over the Jeonse systems, both local conglomerates and start-ups have capitalized on the opportunity to offer company-operated co-living spaces. As the market grows, major international investors and global co-living providers have also entered, reflecting a unique market environment where institutionalized housing options are expanding alongside a notable shift in rental transaction systems. In this new era of urban housing, co-living is rapidly expanding and gaining popularity. This thesis seeks to answer the following question: What factors have driven the emergence and growth of the co-living market in Seoul, and what is its growth potential? To address this, it starts with an analysis of market drivers, provider strategies, and regulatory developments, followed by projections of market potential and an assessment of potential threats and mitigation strategies for long-term viability of co-living in Seoul. The goal is to offer insights for co-living providers to optimize their spaces and services. The findings suggest that while co-living addresses unmet housing demand, its long-term success depends on balancing operational efficiency with tenant satisfaction. While these strategies are applicable in other cities, they are particularly critical in Seoul, where the Jeonse system remains a strong and historically preferred alternative. In Seoul, co-living serves a dual mission: introducing an innovative housing model and reshaping the paradigm of the Wolse rental housing system. To succeed, co-living operators must clearly articulate their unique value proposition, addressing both the housing needs of urban residents and the broader evolution of the rental market.",
        "authors": [
            "Suhyeon Park"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158888",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Defining the Influence of Host Cell Proteostasis Networks and\r\nTemperature on Influenza Evolution",
        "abstract": "Viruses accumulate mutations and evolve more rapidly than any domain of life. Not only does the random acquisition of mutations drive this high evolutionary rate, but constant pressure from the host also contributes. As minimalistic pathogens, viruses rely on host machineries to synthesize, fold, and degrade their proteins. These proteostasis machineries can influence the accessible sequence landscape of viral proteins, and thus shape their evolution. Furthermore, the entire viral replication cycle takes place within the host cell. Therefore, the environment of the host, including factors such as temperature, can influence the evolutionary trajectory of viral proteins. The overarching goal of my thesis work is to better understand the influence of the host cell environment, with a particular focus on the proteostasis networks and the temperature of the cell.\r\nMy first project uses deep mutational scanning to elucidate the roles of the host proteostasis networks in defining influenza hemagglutinin’s evolutionary ability. My second project takes a similar approach to investigate how high or low temperature impact the accessible sequence space of HA. My third project combines both proteostasis network and temperature perturbations to investigate how the host cell environment can impact HA’s ability to escape neutralizing antibodies. My final project leverages the high mutation rate of influenza to study the phenomenon of error catastrophe, and the impact of altered proteostasis network environments on buffering the effect of mutations. Together, these studies clearly define a role for both the host proteostasis networks as well as temperature environment in determining influenza’s accessible sequence space, currently underappreciated factors in predicting how viruses evolve to evade selection pressures and a critical component to consider for successful vaccine and drug development as well as pandemic preparedness.",
        "authors": [
            "Jessica Patrick"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158935",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Fast Multi-query Planning in Graphs of Convex Sets",
        "abstract": "Planning in Graphs of Convex Sets (GCS) is a recently developed optimization framework that seamlessly integrates discrete and continuous decision making. It naturally models and effectively solves a wide range of challenging planning problems in robotics, including collision-free motion planning, skill chaining, and control of hybrid systems. In this thesis, we study the multi-query extension of planning through GCS, motivated by scenarios where robots must operate swiftly within static environments. Our objective is to precompute optimal plans between predefined sets of source and target conditions, in an effort to enable fast online planning and reduce GCS solve times. Our solution consists of two stages. Offline, we use semidefinite programming to compute a coarse lower bound on the problem’s cost-to-go function. Then, online, this lower bound is used to incrementally generate feasible plans by solving short-horizon convex programs. We demonstrate the effectiveness of our approach through a variety of experimental domains: collision-free motion planning for a warehouse robot arm, item sorting for a top-down suction gripper, and footstep planning for a bipedal walker. In particular, in a warehouse-like scenario involving a seven-joint robot arm, our method generates higher-quality paths up to 100 times faster than existing motion planners.",
        "authors": [
            "Savva Morozov"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158967",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Max-Stable Processes,  Measure Transport & Conditional Sampling",
        "abstract": "The modeling of extremes, known as extreme value theory (EVT), aims to understand events characterized by extreme deviations from the mean of a probability distribution. These events are significant in fields such as finance, environmental science, engineering, and insurance. EVT aims to predict the occurrence and impact of these events, which often have severe consequences. Applications of EVT include modeling extreme market movements in finance, natural disasters in environmental sciences, structural reliability in engineering, and catastrophic event risk management in insurance. Conditional sampling and simulation methods, such as normalizing flows and measure transport, are crucial for estimating extremes at un-monitored sites or under specific conditions, thereby improving our understanding and risk management strategies. The goal of this thesis is to make significant contributions to both extreme value theory and measure transport, as well as to establish a link between the two. First, we develop new Markov chain Monte Carlo algorithms for conditional sampling of max-stable processes. Next, we create models that incorporate physical laws, encoded by partial differential equations, to extend max-stable processes into regions without observations. Third, we design specialized transport map frameworks for distributions with bounded support, enabling accurate and efficient sampling and inference. Finally, we use transport maps parameterized by neural networks to learn and condition the distributions of shortest path statistics in polymer systems, accelerating the prediction of microstructural evolution under various conditions.",
        "authors": [
            "Dimitris C. Konomis"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158893",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design Exploration of a Miniaturized Stirling Engine",
        "abstract": "Increased interest in long-term space exploration has increased demand for small yet powerful energy sources, especially for remote and harsh environments where traditional power sources may be impractical. In such scenarios, space probes and high-reliability systems necessitate innovative solutions to meet their growing power and thermal management requirements while maintaining small form factors. Presently, micro power systems fall short of achieving the desired efficiencies for these applications, typically hovering around 2% [1]. Stirling engines, with their proven capability to attain high thermodynamic efficiency (30-40%), offer a promising solution if this efficiency can be maintained in a miniaturized form [2]. This study delves into the design space of a miniaturized Stirling engine with a target input of 2Wth, which could be tailored for small-scale (mesoscale ~cm3 ) high-efficiency power generation or micro-cooling. Previous research has laid the groundwork for understanding the thermodynamics of miniaturized Stirling engines, exposing substantial challenges, including overwhelming parasitic losses at this scale. The current study endeavors to mitigate these losses and explore the path to optimal efficiencies through Simulink modeling. Simulations have demonstrated design spaces capable of producing mechanical efficiencies as high as 14% with a 2Wth input, marking significant progress in addressing the limitations of current micro power systems. The research's innovative approach has significant implications for enabling the power generation required for small space probes, particularly for long durations and need self-sustaining power over extended periods [3], [4]. As the study advances, it holds the promise of developing a physical prototype using the findings from the design space study, which helps push the field forward for future power generation and micro-cooling in small-scale space technology. This thesis aims to map the design space of a miniaturized Stirling engine focusing on mitigating parasitic losses to achieve markedly greater efficiency compared to existing technologies.",
        "authors": [
            "Ryann Hee"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158848",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Systems-Theoretic Approach to Design of Early Concepts for Novel, Complex Systems in Aerospace",
        "abstract": "The complexity of engineered systems has grown exponentially over the last forty years. One of the main challenges in modern engineering is managing this complexity, particularly as the pace of technological change continues to accelerate across industries. Traditional approaches to generating early concepts for novel, aerospace control-oriented systems typically employ a design-first approach, ignoring critical steps required to truly understand the intent and context of a new system. This tendency also leads to a focus on low-level, highly granular design activities that seek to integrate advanced technologies together for technology’s sake. Unfortunately, today’s applied early concept generation methods do not facilitate the effective generation of early system concepts and an initial high-level design for aerospace control systems. To address these shortcomings, this thesis proposes a systematic and rigorous framework to generate early system concepts using Systems-Theoretic Accident Model and Processes (STAMP) principles and a new lens to examine system intent for a novel, complex system. This work also introduces a new level of abstraction for a portfolio-of-systems context and a method to propose an initial design artifact for new systems that is both architecture-agnostic and relevant during the earliest system engineering lifecycle activities. This method, Systems-Theoretic Concept Design, uses a top-down, three-phased approach to conduct mission analysis and determine the intent for a new system within a specific portfolio-level context. Upon building this intent model, the method enables the synchronization of stakeholder mental models through the use of transformation models built using the principles of Systems Theory. Finally, in its last phase, this early design concept generation framework delivers an initial design artifact that is technology- and requirements-agnostic in the language of Systems Theory using the semantics of STAMP. This initial design artifact is in the form of the Portfolio-of-Systems control structure, a control structure that frames a portfolio’s desired high-level capability as a control problem at a new level of abstraction while enabling analysis and examination of complex interactions across systems that may operate asynchronously or in geographically separated operating environments.",
        "authors": [
            "Alexander P. Hillman"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158885",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Green Herrings in a Yellow Room: A Counter Production of The Yellow Wallpaper",
        "abstract": "Charlotte Perkins Gilman’s The Yellow Wallpaper is a designer’s work of critical fabulation. Published in 1892, the short story follows an unnamed woman prescribed a “rest cure” by her husband, John. Confined to a room wrapped in gothic yellow wallpaper, the narrator becomes obsessed with its patterns. As her mind deteriorates, she sees a woman trapped behind the paper. This production reimagines Charlotte’s bedroom as not yellow, but green—a rich, vibrant green laced with the medium responsible for its provocative coloration: arsenic. The toxic pigment, invented in the late 18th century, induces bodily ailments, mental instability, and even death when used in textiles. Interiors threatened tenants with toxins as this green spread through 19th-century Europe before reaching New England and our narrator. Though known as an author and suffragette, Charlotte was first a designer. As a student in the inaugural class of the Rhode Island School of Design, she studied the arts just miles from the ports where the green pigment began its early residence. Her writing draws from arsenic publications, her scenes mimic medical case studies, and archives suggest she was aware of these toxic walls. This theatrical table reading positions the authoring of The Yellow Wallpaper within the simultaneous stories of the arsenic wallpaper. Why does the author mimic material traces of the green while redirecting her readers to the yellow? When does the color transition from literal to abstract? This work recontextualizes the foundational feminist text by unfabulating the story through design—questioning Charlotte’s literary misdirections and the public discourse surrounding the toxic color.",
        "authors": [
            "Leanah Sloan Aulgur"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158824",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Contact-aware and multi-modal robotic manipulation",
        "abstract": "Intelligent robotic manipulation has advanced significantly in recent years, driven by progress in foundational cognitive models, sensor-fusion techniques, and improvements in actuators and sensors. However, most contemporary robotic systems still lack the ability to effectively recognize and understand contact dynamics, which are critical for performing manipulation tasks beyond basic pick-and-place operations. This thesis argues and proves that contact awareness is essential for the successful deployment of robotic systems, not only in structured environments such as factories but also in unstructured settings like domestic households. Achieving contact awareness necessitates advancements in three key areas: the development of improved contact-sensing hardware, the creation of more expressive frameworks for representing and interpreting contact information, and the design of efficient modality-fusion algorithms to integrate these capabilities into robotic action planning. This work addresses these challenges by (1) proposing novel mechanical designs that enable touch sensors to adopt more compact and versatile forms while enhancing their durability and manufacturability, (2) introducing a foundational representation learning framework capable of learning a shared tactile latent representation, which can be transferred across different sensors and downstream tasks, and (3) developing a compositional diffusion-based approach for action prediction that integrates tactile sensing signals with other perception modalities, thereby enabling learning across diverse environments and promoting policy reuse. Along the way, this thesis demonstrates that tactile sensors can be both compact and versatile, challenging common perceptions to the contrary. It also establishes that tactile sensing is indispensable not only for high-precision tasks, such as electronics assembly, but also for everyday activities, including cooking and tool usage.",
        "authors": [
            "Jialiang Zhao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158785",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Organizational Forms and Practices: Essays on Implications for Frontline Workers and Performance",
        "abstract": "In three essays, this dissertation explores how organizational forms and workforce practices shape frontline work experiences and organizational performance. Using both quantitative and qualitative methods, I explore how frontline workers experience work and what factors shape their performance. In the first essay, I examine how workforce practices in nursing homes relate to organizational performance. Specifically, I evaluate performance on resident health outcomes for both pre-pandemic and COVID-19 conditions. Combining Federal and state administrative data sets with non-public data on early COVID-19 spread and mortality, I investigate the degree to which the organization of work for frontline workers predicted resident health as a measure of organizational performance for nursing homes. In a period of global stress on health and care systems, I seek to understand to what extent prepandemic predictors of performance remained important. When nurses spent more time with residents, residents experienced better care both before and during the pandemic. Yet contrary to expectation, the role of clinical outsourcing became more relevant during the pandemic, potentially reflecting greater workforce flexibility or targeted COVID-19 workforce support to facilities that outsourced nursing activities before the pandemic. These results depict how environmental changes and alternative performance measures call into question established relationships in the high-performance work systems literature. In the second essay, I use in-depth interviews and field observations to uncover the process of constructing ownership culture in an employee-owned firm. I demonstrate how workers co-create their own control system, supported by a high financial value of ownership, strategic managerial communication, peer pressure, and performance management. This critical case challenges the dominant view in the employee-ownership literature that success requires formal worker participation in decision-making. Further, it investigates the “black box” of culture-building in an employee-owned firm. The third essay builds on this understanding by evaluating the stated motives of individual worker-owners in a home care cooperative. The cooperative developed as a pilot initiative with non-profit partners to develop a home care organization that would provide quality jobs and quality care, while integrating immigrant workers. I traced the workers’ justifications for joining and participating in these cooperatives. Rather than aligning with expected motives from previous studies or with Worker Center motives, I find that these workers adapted motives to reflect their realities, such as multiple jobs and a lack of labor rights in practice. This analysis emphasizes the decoupling of workers’ experiences from stated organizational goals, emphasizing the importance of collecting workers’ perspectives. Taken together, these three essays contribute insights into how frontline workers shape organizational performance by interpreting organizational context, culture, and structure. Results indicate that organizational performance is not merely a function of workplace practices, but rather, directly influenced by frontline workers based on their individual motives and roles in workplace culture. These findings imply that by directly engaging with frontline workers’ motives, organizational leaders and policymakers can design organizations that improve work and performance.",
        "authors": [
            "Karen MacKenzie Scott"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158850",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Leveraging Information Sharing for Satellite Navigation and Coordination",
        "abstract": "As the number of objects in orbit grows, so does the risk of collisions. The sheer volume of collision warning messages far exceeds the capacity of human analysts, placing a significant burden on satellite operators and underscoring the need for autonomous, decentralized traffic management. Unlike centralized conjunction analysis, decentralized space traffic management distributes coordination across multiple independent nodes, allowing satellites to collaborate directly. This approach could enhance the resilience, speed, and international cooperation of space operations, helping to manage the space environment. For decentralized space traffic management to be viable, satellites must possess an accurate understanding of both the locations and intentions of other satellites. While satellites have precise knowledge of their own state, this accuracy diminishes when predicting the state of others. This gap is due to the limitations of onboard measurement systems and knowledge of each satellite’s structure, configuration, and maneuverability. Such differences motivate the exploration of information sharing between operators to improve coordination. Sharing information could benefit both individual operators and the broader space community by enabling more accurate trajectory predictions, facilitating formal maneuver negotiations, and enhancing overall orbital safety and efficiency. The main contribution of this thesis is to develop methods for autonomous satellite decision-making. By advancing the state of satellite autonomy, we can enhance high-level decision-making processes, enabling more adaptive and intelligent satellite coordination. This thesis begins by developing a multi-agent reinforcement learning environment to simulate satellite interactions in complex, high-dimensional settings. Then, we relax the assumption on synchronous communications and explore an alternate learning framework that relies on asynchronous communication between satellites. Our final contribution lies in a game-theoretic model of operator behavior in non-cooperative settings. Space is a competitive environment, and willingness to collaborate is mixed. As a result, we use game theory to obtain strategies to determine maneuvering and timing.",
        "authors": [
            "Sydney Dolan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158894",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Chemical Sensing of N-Nitrosodimethylamine and Methane",
        "abstract": "In Chapter 1, an introduction to chemical sensing is presented. Several modalities are introduced, including optical, gravimetric as well as chemiresistive together with brief in-troductory backgrounds. Subsequently, metrics to assess sensor performance are sum-marized. Finally, some strategies to combat interferants during chemical sensing are dis-cussed.\r\n\r\nIn Chapter 2, published work on a luminescent method to determine levels of N-nitrosamines is presented. This work involved the synthesis of five phosphines bearing N-heterocycles, followed by coordination with Cu(I) to give luminescent complexes. Emission spectra spanned the visible range, demonstrating the tuneability of these compounds. The complexes’ interactions with N-nitrosamines were also examined through spectroscopy and crystallography.\r\n\r\nIn Chapter 3, development of free-volume promoting monomers and catalysts for in-sertion polymerization is demonstrated. Insertion polymerized material was compared to that synthesized using Ring Opening Metathesis Polymerization (ROMP), showing that the former had superior properties for methane detection through higher surface areas and po-rosity.\r\n\r\nIn Chapter 4, the structure activity relationship of components within a previously pub-lished methane sensing assembly was thoroughly examined to identify how changes in humidity levels influenced sensing response. Poly-4-vinylpyridine modification was per-formed under flow conditions, while the chemical composition of the polyoxometalate (POM) component was also varied. Humidity was determined to most significantly affect the POM and influence the electrical contact between carbon nanotubes and gold.\r\n\r\nFinally, Chapter 5 presents several modifications of the parent porous framework out-lined reported in Chapter 3. A soluble monomer bearing adamantyl substituents was suc-cessfully synthesized by attachment of isopropyl units. Its propensity to participate in inser-tion polymerization was then examined. Sulfonation and nitration of the parent polymer I-AntN were also conducted and the product analyzed. Attempts at copolymerization of AntN with CO were also described.",
        "authors": [
            "Haosheng Feng"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158926",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Explicit formulas for weighted orbital integrals for the inhomogeneous and semi-Lie arithmetic fundamental lemmas conjectured for the full spherical Hecke algebra",
        "abstract": "As an analog to the Jacquet-Rallis fundamental lemma that appears in the relative trace formula approach to the Gan-Gross-Prasad conjectures, the arithmetic fundamental lemma was proposed by Wei Zhang and used in an approach to the arithmetic Gan-Gross-Prasad conjectures. The Jacquet-Rallis fundamental lemma was recently generalized by Spencer Leslie to a statement holding for the full spherical Hecke algebra. In the same spirit, there is a recent conjectural generalization of the arithmetic fundamental lemma to the full spherical Hecke algebra. This paper formulates another analogous conjecture for the semi-Lie version of the arithmetic fundamental lemma proposed by Yifeng Liu. Then this paper produces explicit formulas for particular cases of the weighted orbital integrals in the two conjectures mentioned above.",
        "authors": [
            "Evan Chen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158792",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Data Interpretation and Management for an Atmospheric Probe Mission to Venus",
        "abstract": "After nearly 40 years without a dedicated U.S. mission to Venus, the Rocket Lab Mission to Venus is planning to launch a small probe to analyze the composition of Venus’ cloud layers. As the probe descends through the atmosphere, it will spend around five minutes in the cloud deck, from 66 km to 48 km above the surface, and roughly 20 minutes total in the atmosphere [French et al., 2022]. The probe’s primary scientific instrument, the Autofluorescence Nephelometer (AFN), will gather data by measuring the light scattering off particles, providing insight into their chemical composition based on refractive index and particle size [Baumgardner et al., 2022]. Unfortunately, the natural phenomena described by Mie scattering [Mie, 1908], the physics theory underpinning the AFN, holds that light scattering for a small solid angle is fundamentally degenerate: different combinations of refractive index and particle size can lead to identical light scattering. This degeneracy limits scientists’ ability to uniquely determine physical parameters of interest, leading some previous authors to rely upon helpful, but perhaps limiting, assumptions that mitigate this degeneracy. Complicating matters still further, the probe’s communication with Earth is subject to a strict data budget, limiting the amount of AFN measurements that may be used for analysis to begin with. This thesis addresses two important problems associated with the Rocket Lab Mission to Venus: 1) how to mitigate the light scattering degeneracy with minimal assumptions and 2) how to transmit valuable information within the limited data budget. To address the first problem, I introduce a data retrieval algorithm, based upon Bayesian statistical inference [Lindley, 1965], which combines a physical model of the instrument and a prior probability distribution describing each physical property. In some cases, this method can estimate the correct particle size and refractive index of a particle as the maximum likelihood value, from a single measurement even as it relaxes certain assumptions that were previously standard in the field, such as a small refractive index range. Using my data retrieval algorithm, I reanalyze the data collected by the Pioneer MultiProbe Mission to Venus’ nephelometers without the need for supplementary data from a different instrument [Ragent and Blamont, 1980]. I also provide new insight into the particle size and refractive index distributions seen by the Pioneer Mission’s small probes, which had not been possible with previous techniques. To address the second problem, I propose a data strategy for limited data missions like the Rocket Lab Mission to Venus. The method developed in this work relies upon Gaussian Mixture Models, which can efficiently represent multiple measurements as",
        "authors": [
            "Maria Regina Apodaca Moreno"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158902",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Empowering Place: Unlocking Value for Investors by Integrating\r\nIndigenous Values in Luxury Hospitality",
        "abstract": "The luxury hospitality industry has long been attuned to shifting consumer preferences, particularly as travelers increasingly seek unique, meaningful experiences. In today’s global market, trends centered on personalization, wellness, authenticity, and regeneration—further accelerated in the post-pandemic travel era—present both challenges and opportunities for real estate investors. This shift raises a critical question: How and where can value be unlocked in this evolving landscape?\r\n\r\nThis thesis explores how real estate investors can maximize value creation in the luxury hospitality sector by leveraging traditional performance metrics alongside a complementary\r\nframework designed to uncover underexplored opportunities and enhance collaboration among stakeholder groups. Through the analysis of two case studies—Salterra Resort & Spa in South\r\nCaicos, Turks & Caicos Islands, British West Indies, and Puntacana Resort and Club in the Dominican Republic—the study demonstrates the practical application of this framework in\r\ntropical, coastal, and island regions, where the interaction between tourism, local communities, and fragile ecosystems is particularly pronounced. By showcasing its success, this research provides adaptable stakeholder rubrics and qualitative system dynamics causal loop diagrams as templates, while broadening the scope for innovation and inspiring further exploration of sustainable, value-driven approaches in luxury hospitality.",
        "authors": [
            "Nadra Alia Peragallo"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158878",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Two's More Fun than One: How the Presence of Multiple Nutrients Changes Microbial Competition and Foraging in Unexpected Ways",
        "abstract": "Microbes exist in incredibly diverse environments with many possible resources (i.e. nutrients) to compete and forage for. To make this complex system tractable, ecologists often study microbes in the presence of a single resource in order to predict and explain what happens with multiple resources. But what gets lost when we do this? Are there phenomena that only emerge in the presence of multiple resources? Here, I explore the ecological implications of three phenomena that each require the presence of at least two resources. First, I show that the diauxic lags that occur when a microbe needs to switch between resources after one is depleted can allow ‘fast-switcher’ microbes to coexist with competitors that exclude them in single-resource environments. Then, I derive a rich temporal niche structure that arises from variations in the order in which resources are depleted in ecosystems with a pulsed resource supply and show that these temporal niches reshape community structure, vastly increasing the expected diversity of microbial ecosystems. Finally, I present a novel differential strategy in which a microbe attempting to intercept a moving source of multiple resources can treat one resource as an attractant and the other as a repellent to significantly increase its chances of successfully intercepting the source as compared to just being attracted to the resources released by the source. Each of these phenomena fundamentally requires the presence of at least two resources and reshapes microbial behavior and ecology. Thus, they collectively highlight the need to carefully consider how characterizations from single-resource environments actually combine to determine what happens in multi-resource environments and what new dynamics must be accounting for in such a bottom-up approach. I conclude with an argument that the case of two resources may be particularly relevant to study due to how much complexity can emerge at just the first step up from one resource to two.",
        "authors": [
            "Blox Willow Bloxham"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158877",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Robot Fleet Learning From Heterogeneous Data",
        "abstract": "One of the key roadblocks for training generalist robotic models today is heterogeneity. Previous robot learning methods often collect data to train with one specific embodiment for one task, which is expensive and prone to overfitting. Similar to humans, robots and embodied agents inherently have to deal with heterogeneous inputs and outputs due to the nature of the perception-action loops across diverse environments. The data format and distributions collected from these systems and used for training them are varied in different modalities such as color, depth, tactile, and proprioceptive information, and/or collected in different domains such as simulation, real robots, and human videos. Moreover, fleets of robots and machines ingest massive amounts of streaming data generated by interacting with their environments in a distributed fashion, and teams of robots shall co-acquire diverse skills through their experiences in varied settings. The core idea behind my research, fleet learning, is to embrace the heterogeneous nature of robot learning to develop efficient and general algorithms. In this thesis, I will present a few angles toward tackling such challenging problems and application domains, ranging from tokenizing data, aligning representations, and merging policies, to composing skills. We develop insights and theories, often from linear settings, for how fleet learning can lead to more principled and effective use of robotic data and propose algorithmic progress, often through alignments, toward building generalist robotic foundation models. Empirically, we show advanced robotic manipulation capabilities by leveraging data from multimodal sensory inputs and multiple domains. In addition to outperforming several previous state-of-the-art across simulation and real-world benchmarks, we develop intelligent systems for robotic applications such as package handling in warehouses as well as dexterous tool-use tasks that have applications such as manufacturing, logistics, and household robots.",
        "authors": [
            "Lirui Wang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158917",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Decadal to centennial-scale climate interactions across the Indo-Pacific region",
        "abstract": "An improved understanding of decadal to centennial-scale climate variability is critical for properly attributing recently observed low-frequency changes to internal climate oscillations and/or anthropogenic forcings as well as improving predictability of decadal variability. This thesis investigates ocean and atmospheric circulation changes and associated impacts within the tropical Indo-Pacific, where low-frequency changes in heat and freshwater impact the livelihoods of billions of people. Because the instrumental record is too short to investigate centennial variability, this thesis leverages numerical simulations and records from paleoclimate archives to provide insights into low-frequency tropical dynamics. In Chapter 2, we explore the dynamics that drive Indonesian Throughflow surface transport variability using a series of forced global high-resolution ocean simulations. We show that surface wind changes associated with Pacific decadal variability drive changes in the western boundary currents that modulate the Indonesian throughflow, consistent with mechanisms identified on interannual timescales. This work identifies a relationship between atmospheric circulation and transport through a key low-latitude passageway. Motivated by paleoclimate evidence of multi-year droughts in Southeast Asia, we investigate their potential drivers in Chapter 3 using an ensemble of coupled climate model simulations. These simulations illustrate that Indo-Pacific internal variability dominated Southeast Asian rainfall extremes during the last millennium, although the influence of volcanic eruptions was detectable. We found that multi-year pluvials were contributed by both Pacific and Indian Ocean modes, while droughts were largely only driven by Pacific Ocean impacts. Our analysis not only quantifies the role of internal and external drivers to Southeast Asia rainfall but also presents a probabilistic analysis framework that may be useful for water resources prediction. Lastly, in Chapter 4 we reconstruct the Indian and Pacific Walker circulations and the Indian Ocean Basin Mode by synthesizing tropical records (corals, tree-rings, and speleothems) of past ocean and atmospheric conditions to investigate basin interactions over the past four centuries. Our results demonstrate that IndoPacific climate was generally coupled on decadal-centennial timescales throughout the past four centuries but was notably decoupled in the early 19th century. Using climate models, we attribute this decoupling to a series of strong volcanic eruptions. Dynamically, we link this inter-basin decoupling to volcanically induced changes in hemispheric temperature gradients, which modulate the teleconnections across the Indo-Pacific. These past disruptions in basin interactions provide context for ongoing and simulated future decoupling under a high emission scenario, as global warming also alters interhemispheric temperature gradients. This thesis sheds light on the complex dynamics that drive ocean-atmosphere variability across the Indo-Pacific on decadal to centennial timescales.",
        "authors": [
            "Shouyi Wang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158787",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Examining the Economic Impact of Anti-Warehouse Development Policies in California: A Case Study of the San Diego Market",
        "abstract": "This thesis conducts a detailed examination of the implications of anti-warehouse development policies in San Diego, focusing on their impacts on key economic indicators from 2024 to 2034. The research provides an overview of the U.S. industrial market, addressing crucial topics such as logistics market size, job creation, and the growth of e-commerce, while also exploring the NIMBY phenomenon and its influence on community opposition to developments, including a discussion of Bill 98 and its legislative implications. A specific focus on the industrial market in Southern California reveals important insights into job growth, rental rates, and market dynamics in San Diego. Through a comprehensive analytical approach, the study addresses the effects of development policies by presenting ten distinct scenarios that project delivery volumes, uncovering potential reductions ranging from 10% to 90% compared to a baseline scenario without restrictions. The analysis anticipates vacancy rates and job losses across various years, utilizing the LINEST function for forecasting key market indicators, including asking rents and asset valuations. Additionally, the research highlights the critical importance of logistics categories and decarbonization strategies to meet net-zero goals, as well as contemporary warehouse design trends and transportation innovations. The conclusions drawn from this research emphasize the complexities of balancing community interests with economic growth and sustainability in the region, as well as the broader economic implications of restrictive development policies on San Diego's warehouse industry, which could adversely affect the economic vitality of the warehouse sector.",
        "authors": [
            "Peggy Ghasemlou"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158829",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Computational-Experimental Process Development for Laser Powder Bed Fusion Additive Manufacturing",
        "abstract": "Laser powder bed fusion (LPBF) additive manufacturing (AM) is instrumental for advances in high-value industries such as aerospace and medical devices. However, widespread adoption is still held back, in part due to challenges with powder handling, identification of process parameters, part qualification and quality control, and low build rates that lead to high part costs. This thesis presents workflows, tools, and understanding for practitioners and researchers seeking to address these challenges, in particular (i) powder spreading, (ii) parameter selection, and (iii) build rate improvement. Cohesive powders (D50 ≤ 20 𝜇𝑚) are challenging to spread and therefore not commonly used in LPBF, but promise more stable melting conditions during laser melting and potentially allow for finer geometrical resolution. Various spreading strategies are explored using an integrated discrete element-finite element (DEM-FEM) framework and a schematic process window for counter-rotational roller spreading is proposed. A new strategy of spreading with a transversely oscillating tool is chosen for experimental implementation and validated using a custom-built mechanized powder spreading testbed. Powder layers are analyzed using X-ray transmission imaging and layer quality is statistically correlated to kinematic spreading parameters. A methodology for performing melt track experiments using high-precision metal templates as well as a machine learning-based automated image analysis tool is presented and applied to melt track scaling studies. Based on single track parameter studies with layer thicknesses and laser spot sizes of up to 600 𝜇𝑚, a dimensionless LPBF process window using the normalized enthalpy Δ𝐻 / ℎₛ as well as the Fourier number is developed. A workflow for rapid LPBF build parameter selection is proposed, that is shown to fabricate near-full dense parts (up to 99.99 %) on the first attempt. Build rate scaling analysis reveals the trade-off between laser spot size and laser scan speed given laser power limitations. Further, LPBF with a standard powder (15 − 45 𝜇𝑚) is compared to a fine powder (0 − 25 𝜇𝑚) under similar processing conditions. The fine powder exhibits superior melt track stability and continuity, as well as significantly increased melt track cross-sectional area, allowing build rate to be increased by almost 20 %. Finally, to enable better understanding of the underlying thermo-fluid dynamics of the melt pool, an approach for computational model parameter estimation using Bayesian inference is presented and applied to the important model parameter of laser absorptivity. This is within the context of a Smooth Particle Hydrodynamics (SPH) computational melt pool model, developed collaboratively by researchers at the Technical University of Munich. The diffuse interface approach employed in SPH is validated using a discretization refinement study, showing the sensitivity of physical phenomena characteristic for LPBF, such as the vapor-induced recoil pressure, to computational hyper-parameters. These combined contributions enhance both practical implementation and theoretical understanding of LPBF, ultimately advancing the field of additive manufacturing towards more cost-effective and higher quality LPBF processes.",
        "authors": [
            "Reimar Weißbach"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158884",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Remembering Energic Connectivities: Appropriate Technology and Domestic Infrastructure in the Energy Crisis",
        "abstract": "The electric grid is a large, complex machine. And yet, it represents but one, narrow framework for energic relations. Visions for just and sustainable futures – for social and ecological repair – should wander further afield. One place they could go is home. In this essay, the Appropriate Technology Small Grants Program, an oft-forgotten chapter of U.S. energy history, shows us how small-scale, place-based inventors transformed homes and neighborhoods into converters and conductors of nearby flows and potentials. At the height of the energy crisis of the 1970s, these inventors pursued a distributed solution to shortage. Along the way, they re-wired the material and conceptual strictures of the modern dwelling and broke into a vast reserve of lowcost, renewable power. Home, they showed, was a workshop to understand and design energic connectivities. But tracing the effects of home-based appropriate technology leads us somewhere else – to the frontiers of energy extraction, where social justice activists proved that small-scale, place-based energy systems could replace unjust mines and dams. What emerged, then, through renewed attention to the possibilities for home and energy, was a powerful counter to the logics of sacrifice at both ends of the energy continuum. Today, as we chart our own response to crisis, it helps to remember how others tried to create solidarities and resist tradeoffs with small-scale, place-based infrastructures. We can, I think, do more with energy.",
        "authors": [
            "Turner Day Adornetto"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158811",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Congestion Control for DNN training clusters",
        "abstract": "The modern DNN workloads generate network traffic having striking differences with the conventional data-center traffic. DNN training jobs generate periodic traffic pattern where all subsequent flows depend on the completion of the currently running flow. Although this periodic behavior calls for a new non-conventional congestion control protocol for DNN training clusters, it also creates an unprecedented opportunity to approximate optimal schedule for DNN jobs in a distributed manner without requiring priority queues, centralized information, or switch hardware support. Prior work on MLTCP proposed updates to existing congestion control algorithms to make them capable of minimizing network congestion when DNN jobs compete for the network. In this thesis, we propose several techniques to expand the scope of prior work to support DNN jobs with more complex communication patterns or parallelization strategies, and further improve the performance speedup over TCP. With two straightforward ideas of updating the congestion control parameters, we expand the performance benefits of MLTCP to a wider set of periodic DNN jobs. Augmenting existing congestion control algorithms with MLTCP provides an effective guiding mechanism to a random search to find the optimal interleaved schedule for competing DNN jobs. Our contributions boost this guided search to improve performance further. We provide detailed theoretical analysis and extensive flow-level simulations to take a deep dive into the convergence, performance speedup, and fairness of MLTCP with the proposed changes.",
        "authors": [
            "Sanjoli Narang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158950",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Acoustic scattering of spherical directional waves by smooth and statistically rough solid elastic cylinders",
        "abstract": "Realistic sonars radiate spherically spreading waves and have directivity. Therefore, they insonify a target over a finite number of Fresnel zones and span a continuum of oblique incident angles, even when the center of the beam is at normal incidence. These effects strongly influence both the overall scattered pressure levels and resonances. For example, because of the spreading of the beam and associated oblique insonification within the beam, normal modes associated with axially propagating guided waves are excited that would not have otherwise existed for an idealized incident plane wave. This thesis analyzes acoustic scattering by solid elastic cylinders insonified by realistic sonars both theoretically and experimentally. A theoretical model to predict scattering by arbitrary-length cylinders is derived based on the apparent volume flow accounting for the above-mentioned practical sonar properties, namely, spherical spreading and directionality. The formulation is first bench-marked against the formally exact T-matrix solution and tested against previously published laboratory data for finite cylinders. It is found that the formulation outperforms the T-matrix solution in predicting laboratory observations at near-normal incidence. Laboratory experiments are then conducted on arbitrary length smooth cylinders insonified by a directional sonar, with a small number of Fresnel zone excited, to evaluate the theory for monostatic as well as bistatic geometries. The formulation is found to outperform the classical scattering models in predicting the new measurements. For example, resonances associated with axially propagating guided waves excited at broadside incidence observed in the experiments are predicted by the proposed formulation but not by the classical models. The measurements are found to agree well with predictions in terms of overall scattering levels and resonance locations. In addition to testing the predictions, the bistatic laboratory observations presented herein substantiate the significant effects on scattering due to the properties of the incident field from practical sonars. The comparison between theoretical and experimental results is then extended for the more complex case involving statistically rough elastic cylinders with one-dimensional Gaussian roughness. The roughness is found to have a considerable impact on all aspects of scattering—overall levels as well as locations and shapes of resonances. General agreement is found between the theoretically predicted and measured ensemble averaged scattered pressure. Both the theory and data reveal two main observations in the ensemble-averaged scattered field: overall scattered pressure levels are seen to decrease, and resonance effects are diminished compared to the corresponding case of smooth cylinders. Effect of various statistical properties of the rough cylinder, namely, different root mean square (RMS) roughness for fixed correlation length and different correlation lengths for fixed RMS roughness on the scattered field are investigated. Finally, the fluctuations of the scattered field are analyzed using the derived formulation.",
        "authors": [
            "Miad Al Mursaline"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158835",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Commodifying and Consuming Endocrine Drugs in Republican China (1920s–1940s)",
        "abstract": "Since the introduction of hormone pharmaceuticals into China during the early twentieth century, these substances became objects of fascination for a growing urban elite class. Drawing from newspapers, medical journals, and advertisements, this article examines the unique trajectories of hormone medicine in China. In conversation with previous scholarship on the dynamics of advertising and consuming hormones in China, this article examines specifically the discourses around the production and science of hormones. The circulation of hormones was informed by ideas of traditional Chinese medical cosmologies and enrolled in a nationalist movement encouraging the consumption of hormones produced by emerging Chinese medical entrepreneurs. This article provides a case study in a postcolonial context that problematizes historiographies depicting a linear transition of global hormone science from backwards to scientific, from traditional to modern.",
        "authors": [
            "Thelma Yuanzhi Wang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158808",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Strange Attitudes on Top",
        "abstract": "This dissertation investigates how attitude verbs of belief and desire engage with embedded material of a similar nature. Chapter 1 looks at the (cross-linguistically unusual) Slovenian existential doxastic attitude verb dopuščati (‘allow for the possibility’) and the embedding of epistemic modal verbs under it. Chapter 2 looks at the (overall puzzling) want and its Slovenian counterpart hoteti, and at their behaviour with respect to embedded doxastic attitudes, epistemic adverbs, and epistemic adjectives. Chapter 3 looks at the (cross-linguistically unusual) Koryak variable-force variable-flavour attitude verb ivək (‘think’, ‘allow for the possibility’, ‘say’, ‘suggest’) and at how its apparent bouletic flavour (‘wish’, ‘hope’, ‘fear’) is derived with the help of covert desiderative components inside the embedded clause. Attitude verbs have the standard role as quantifiers over possible worlds (Hintikka 1962), parameters of evaluation are assumed to contain a set of worlds called the information state (Yalcin 2007; a.o.), which the attitude verb modifies and passes to the embedded clause, while the epistemic modal base is taken to be ‘local’, forming a subset of the information state (Mandelkern 2017, 2019a). Some of the overarching theoretical contributions are the introduction of a new parameter of evaluation (‘selected state’), which is crucial in modelling embedding under non-universal attitude verbs, and a refined view of epistemic modality. Subjective epistemic modality is proposed to involve a second constraint on the shape of the modal base, whose effect is to strengthen embedded necessity claims and help derive the infelicities observed in chapters 1 and 2. We also address the connection between beliefs and desires in the context of various desire interpretations (wants in chapter 2, hopes and wishes in chapter 3).",
        "authors": [
            "Maša Močnik"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158857",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Tackling Algorithmic Problems on Massive Graphs",
        "abstract": "As datasets grow increasingly larger, traditional computational models, which require reading the entire input, become impractical due to constraints on time, memory, and randomness. This thesis explores alternative algorithmic approaches for processing massive graphs under these constraints. Specifically, we focus on algorithms for the following graph problems. Motif Counting and Sampling: This involves developing efficient algorithms for counting and sampling small motifs (constant sized subgraphs) like stars and triangles, which are crucial for applications in biology, chemistry, and social networks. The thesis presents improved methods for both approximate and exact counting and sampling of general motifs. Graph Sparsification and Spanners: The problem of sparsifying graphs involves removing (usually most) edges of the input graph in a way that preserves essential properties such as connectivity and approximate distances. This thesis introduces algorithms for constructing sparse spanning graphs, as well spanners – sparse subgraphs that approximate distances up to a multiplicative factor. We obtain faster algorithms in parallel settings, and also initiate the study of average case graph inputs in the sublinear setting, and obtain results beyond the worst case lower bounds We investigate both of these problems in different models, including sublinear query access, local computation algorithms (LCAs), and the MPC model, and also discuss implications of these in distributed and parallel models of computation.",
        "authors": [
            "Amartya Shankha Biswas"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158948",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "An Identity-Oriented Systems Engineering Framework for Complex Sociotechnical Systems: A case study of Zero Robotics",
        "abstract": "Historical and ongoing discrimination of certain identity groups such as by racial, gender, social class, and other differences leads to persistent inequalities in various fields of society including socioeconomic, health system, political powers, education opportunities, etc. Technology however often entrenches or sustains the hierarchies and further strengthens these social inequalities. While there are many frameworks for studying complex systems, a framework with a focus on advancing social justice and an integration of technological and social considerations is missing. This work introduces the Intersectional Antiracist Technology Framework as a new tool and applies it to an existing complex system of Zero Robotics in STEM education. STEM education, with increasing importance in the modern world’s competitions, is one of the most popular methods to cultivate students’ interests and capabilities in solving complex problems. However, the disparities in access to quality STEM learning opportunities and inclusion in STEM activities remain significant challenges in promoting social equality. This work builds upon the systems engineering tools and uses the innovative Intersectional Antiracist Technology Framework to describe, explain, and evaluate an existing complex system of Zero Robotics. Zero Robotics is an education outreach program that is designed as an early intervention to enroll students in aerospace and related fields. The program aims to serve students across the pipeline and provide them with learning opportunities through interactions with a space robot. It is a perfect example of a complex sociotechnical system that has technological and social factors. Through the case study of Zero Robotics, data are collected through interviews, surveys, participant observation, and available documents. Qualitative program outcomes are assessed from student surveys before and after the Zero Robotics competition. This work is the first attempt to apply the Intersectional Antiracist Technology Framework to an existing complex system that is being managed by the author. The findings from this study demonstrate insights that can be gained about complex, sociotechnical systems by viewing them from multiple Stakeholder perspectives and blending the information about the technical and social design aspects.",
        "authors": [
            "Yiyun Zhang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158821",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Observations of Surfzone Vorticity Using Optical Remote Sensing",
        "abstract": "The surfzone is the dynamic interface between the land and ocean, where waves shoal and break as they reach shallow water near the shore. Currents and circulation patterns in the surfzone transport sediment, nutrients, pollutants, and other materials along and across the coast, and can create hazardous conditions for swimmers (rip currents). However, understanding of the strength and structure of eddies and vortices in the flow field primarily remains limited to numerical models and theory. Here, novel observations of surfzone vorticity at small [O(10m)] and large [O(100m)] spatial scales are presented and related to incident wave conditions and the measured underlying bathymetry. Field experiments were conducted at a sandy beach on the Atlantic Ocean, and nearshore flows were observed using optical remote sensing (coastal imaging) and in situ sensors. Remote sensing algorithms are expanded from previous applications to estimate high spatial resolution two-dimensional surface flows by tracking the motion of naturally occurring foam throughout the surfzone. Estimated currents are correlated with in situ flow measurements, and errors increase as the sea-surface viewing angle becomes more oblique and image quality decreases. Large spatial-scale vorticity estimated using remotely sensed flows increases with alongshore bathymetric inhomogeneity, and complex circulation patterns corresponding to holes and channels in the seafloor persist for days at a time. Small spatial-scale vorticity estimated from a 5-m diameter ring of 14 current meters increases with the directional spread of the incident wave field, consistent with increased vorticity injection from the crest-ends of breaking waves. Small spatial-scale vorticity estimated using remotely sensed flows is spatially variable and correlated with the amount of wave breaking observed at a given location. Enhanced vorticity at large and small spatial scales occurs in the inner surfzone, and virtual drifters released into the remotely sensed flow fields demonstrate cross-shore variability in dispersion and mixing. This thesis expands the understanding of vorticity dynamics in the surfzone through unique field observations and provides new tools for coastal research and monitoring through development of remote sensing techniques.",
        "authors": [
            "Ciara Jaya Dooley"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158879",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Characterizing Engineered Skeletal Muscle Rings as Actuators Using Strain Sensing Methods",
        "abstract": "A novel instrument was designed to characterize a force exertion model of engineered skeletal muscle rings. The instrument uses strain gauges to transduce a muscle ring contraction and has a verified resolution of 5 μN and 1.4 μm over the ranges of 5 μN and 1400 μm respectively. Experiments were carried out with four muscle ring specimens at six different structural stiffnesses. Each ring was excited at 1 Hz for 30 seconds while force and displacement was monitored. It was determined that the relationship between muscle contractile distance and force is related by a negative power function.",
        "authors": [
            "Laura M. Rosado"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158858",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Impact of Environmental Regulation on Data Center Valuation",
        "abstract": "Artificial intelligence has become one of the defining trends of modern society, with applications spanning virtually every industry. This societal shift has also influenced the real estate landscape. While data centers have existed for decades, it is only in recent years that they have garnered significant attention, demonstrated by their strong rent growth and compressed cap rates.1 Along with the attention over data centers, there also has been extensive research on how data centers impact the environment, such as \"Quantifying the Sustainability Impact of Data Center Availability\" by Manish Marwah et al. which present how data center power architecture may impact the environment and \"The Environmental Footprint of Data Centers in the United States\" by Md Abu Bakar Siddik, Arman Shehabi, and Landon Marsto. This research delves into quantifying the environmental impacts of data centers, specifically focusing on carbon and water footprints. However, what remains unexplored is how environmental regulations influence the valuation of data centers as a distinct real estate property type. This thesis examines how data center valuations could be impacted if existing environmental regulations were applied to regions where data centers are concentrated. The findings reveal a complex dynamic: while penalties under these regulations would reduce net operating income (NOI), potentially devaluing these assets, the same regulations would discourage new development, exacerbate the already constrained supply, and ultimately drive-up market rents for these properties. As a result, these opposing forces create ambiguity regarding the net impact of such regulations on data center valuations, with the outcome depending on which force prevails. What is clear, however, is that tenants would bear the brunt of these regulations, as landlords are likely to pass on increased costs through higher rents. On the other hand, while the environmental impacts of data centers and AI applications is critical to achieving sustainability goals, the societal benefits of AI solutions—ranging from advancements in healthcare to increased operational efficiencies—must also be considered. Balancing these competing priorities presents a unique challenge for policymakers and investors, with significant implications for the future of real estate and the digital economy.",
        "authors": [
            "Donghyun Lee"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158830",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Data-Driven General Purpose Foundation Models for\r\nComputational Pathology",
        "abstract": "The field of computational pathology has undergone a remarkable transformation in recent years. Researchers have leveraged supervised and weakly-supervised deep learning with varying degrees of success to address a wide range of tasks, including cancer subtyping and grading, metastasis detection, survival and treatment response prediction, tumor site-of-origin identification, mutation prediction, biomarker screening, and more. However, traditional task-specific models often require extensive labeled data and struggle to generalize across diverse pathology tasks. This limitation motivates the exploration of foundation models, which promise a more scalable, versatile solution by learning broad representations that can be adapted to various downstream applications. In this thesis, I will investigate the capabilities and limitations of data-driven foundation models in computational pathology. Specifically, I will explore two frameworks for developing general-purpose encoder models for pathology images: one using paired image-text data, and another leveraging self-supervised learning on large-scale unlabeled images. Additionally, I will examine downstream applications of these foundation models, including zero-shot transfer to gigapixel whole slide images and the development of an interactive multimodal AI assistant for pathologists.",
        "authors": [
            "Ming Yang (Max) Lu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158957",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Numerical investigations of vortex dynamics: bursting, twist waves, and sensitivity analysis",
        "abstract": "Vortical structures are ubiquitous in real-world fluid flows, from the vortices generated by swimming fish to the wakes of aircraft and propellers. They form the backbone of high Reynolds number turbulent flows. Their dynamics are governed by non-linear processes, leading to a range of vortical instabilities that significantly influence engineering applications. Despite decades of research, many questions remain about core mechanisms responsible for the dynamic evolution of vortical structures due to the nonlinearity and complexity of flows at high Reynolds numbers. A particular scenario that lacks systematic investigation is vortices with initial core-size variations, which leads to the phenomena of twist wave propagation and vortex bursting. In this thesis, we first examine straight vortex tubes with initial core-size perturbations at high Reynolds numbers by performing high-fidelity numerical simulations. The differential rotation along the vortex tubes generates twist wave packets that propagate and collide, resulting in a sudden increase in the local core size – the phenomenon of bursting. We analyze the effects of perturbation amplitudes on the detailed evolution at each stage, including the underlying mechanisms for the growth and decay of the bursting structure. The bursting process is associated with significant energy dissipation, which is quantified and compared to that of unperturbed vortex tubes. Meanwhile, vortices in real fluid flows are often nonrectilinear and experience strain from environmental or self-induced effects. We extend our study to curved vortex tubes and investigate the impact of centerline non-rectilinearity on twist wave propagation and the stability of the bursting structure. Additionally, we adopt a relatively recent geometric perspective on vortical flows and analyze the helicity dynamics during the flow evolution. To systematically initialize vortex dynamics simulations based on a late-time or time-averaged flow metric, we explore different methods for sensitivity analysis of two-dimensional vortical flows. The sensitivity values obtained are then used in gradient-based optimizations, which shows promising pathways for control and optimization of vortical flow applications. Additionally, we present a numerical study of the locomotion of a rotating cylinder pair with periodic gaits in a low Reynolds number flow. We characterize the motion pattern and efficiency of the cylinder pair through a combination of theoretical arguments and numerical simulations, which provides a foundation for potential engineering applications at the microscale. Overall, our findings provide understanding of fundamental mechanisms for vortex bursting and associated twist wave dynamics at high Reynolds numbers, explorations of sensitivity analysis for vortical flow applications, along with insights into locomotion at low Reynolds numbers.",
        "authors": [
            "Lingbo Ji"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158892",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Precisely Loose: Unraveling the Potential of Particles",
        "abstract": "Random, irregular, erratic, arbitrary, unspecifiable, and unpredictable—particles. In a post-extractive future, our reliance on standardized materials, continuously sourced through the exploitation of raw resources, will no longer be sustainable. Instead, architecture will increasingly contend with materials that defy standardization. This thesis focuses on these non-normative materials—particles, encompassing construction demolition debris, manufacturing defects, naturally occurring gravels, and locally sourced mineral waste. Ubiquitous yet underutilized, these materials hold potential not only for use, but also for reuse. However, they are often dismissed as rigid and unpredictable ingredients that require precise manipulation and cumbersome processing in order to achieve predictable results. What kind of architecture could emerge if we embraced the inherent nature of these particles, not as rigid materials to be controlled, but as dynamic, fluid entities? By embracing their uncertainty as a generative design agent, how would design approaches and construction processes transform? This thesis presents a catalogue of precisely loose methods for engaging with particles. These methods offer an alternative design approach that moves beyond the obsession with refinement and control over material behavior. By pouring, pushing, reconfiguring, and containing—in lieu of identifying, cutting, placing, and stacking—this series of interactions explores the potential of plurality, investigating how loosely controlled particles can adapt to collaborative construction processes. In doing so, this thesis redefines architectural material culture rooted in rubble, offering a framework to reimagine our relationship with the irregular, the unpredictable, and the overlooked.",
        "authors": [
            "Jeonghyun Yoon"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158833",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Quality-Centric Single-Image Procedural Material Generation",
        "abstract": "Procedural materials, represented as functional node graphs, are ubiquitous in computer graphics for photorealistic material appearance design. They allow users to perform intuitive and precise editing to achieve desired visual appearances. However, even for experienced artists, creating a procedural material given an input image requires professional knowledge and significant effort. Current inverse procedural material modeling approaches enable the automatic generation of procedural materials from input images. However, the visual quality of the generated materials is fundamentally limited by insufficient high-quality training data from industry-standard procedural materials, reliance on token-space supervision without visual feedback, and the absence of approximation-free node parameter post-optimization. My thesis presents advanced dataset augmentation, model training, and parameter post-optimization algorithms to address these challenges, significantly improving the perceptual match between the generated procedural material and the input image. Furthermore, the methodologies can be applied to other inverse procedural graphics problems to expedite similar artistic creation processes.",
        "authors": [
            "Beichen Li"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158945",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Building Insurance",
        "abstract": "Over the past 350 years, the building insurance industry has been shaped by a series of major urban fires, each incrementally standardizing risk assessment and property valuation as financial products of risk management. In recent years, however, climate change has introduced unprecedented weather events that challenge the fine tuned models of insurance; in particular, the rise of wildfires in California and the Pacific Northwest have led to local withdrawal of insurance altogether. Within these contexts, the spatial conditions inherited by a highly insured past continually sustain separation, individual prosperity, and standard assemblies as inheritances of expansionist agendas. At this juncture of system failure, this thesis asks: how can architecture rethink more cooperative forms of building and living together that localize risk sharing, responsibility, and stewardship? While wildfire defense strategies put forth by insurance companies and building code armor stick-frame American single family home and its aesthetic traditions, this thesis proposes a new building typology entirely: a neighborly cooperative of adjoined homes. Under a single roof, property lines are transformed into sites of mutual stewardship, manifesting insurance no longer as an abstract response to risk, but as a series of social and spatial relationships between neighbors.",
        "authors": [
            "Charles Perot Janson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158891",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Application of foundation models for molecular representation in cancer drug discovery and precision oncology",
        "abstract": "Drug discovery is a resource-intensive and time-consuming process, often requiring decades of effort and substantial financial investment, with a high risk of failure. Despite advances in high-throughput screening technologies, the size of chemical space presents a significant challenge: it is not feasible to experimentally screen all potential drug-like molecules. Most commercially available chemical libraries consist of molecules that are synthesized on demand from pre-existing building blocks, further limiting the exploration of novel chemotypes. This thesis aims to explore whether drug discovery could be accelerated by leveraging advances in deep learning (DL) models to identify promising hit candidates and improve the prediction of drug response in cancer. Development of cancer drugs that will be effective on a predictable set of targets remains a major challenge. We are developing a DL model capable of identifying potentially novel cancer drug chemotypes and reliably predicting drug response on cancer cell line targets. Leveraging recent progress in transformer-based architectures and graph neural networks, we use molecular language models, graph models and cell foundation models to embed both molecular and genomic data into low-dimensional subspaces and then use standard machine learning (ML) tools in these low-dimensional spaces to predict the efficacy of the molecules in particular cell lines. We utilize the large-scale drug repurposing and oncology datasets from the PRISM project at the Broad Institute, which provide a wealth of drug repurposing and oncology data, enabling robust training of ML models. We show that these vector embeddings are superior to existing methods, as they enable more accurate drug response predictions. The first part of this thesis is dedicated to development of a deep learning cancer drug discovery model, focused on in silico screening of chemical space to search for cancer drug candidates. The second part is focused on development of a precision oncology model, based on a multichannel neural network architecture. Our pipeline involves training single-target models on drug molecular structures, followed by integrating genomic data to enhance biological context and train a hybrid model capable of predicting drug response for novel drug:target pairs. Our results demonstrate that vector embeddings produced by the proposed framework outperform existing approaches, offering a more accurate and efficient means of exploring chemical space. This work highlights the transformative potential of ML/DL methods in drug discovery, enabling targeted, cost-effective exploration of chemical libraries, and advancing the development of precision oncology treatments.",
        "authors": [
            "Khrystofor Khokhlov"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158915",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "OPTASAT: An Open-Source, Flexible Software Framework for Small Satellite Operations",
        "abstract": "The unprecedented growth in access to space has created a corresponding growth in the number of spacecraft and the number of people operating spacecraft. This has meant that many of these operators are operating spacecraft for the first time. Gone are the days when the only operators of spacecraft were national governments, militaries, and massive corporations. The operators of small spacecraft today include many early-career individuals who need the tools to enable them to make strong decisions in the behavior of their spacecraft. The tools for operating spacecraft are often overlooked by teams focusing on the spacecraft themselves, but these operating tools are critical for mission success. Spacecraft operations tools have not developed in a similarly low-cost, widespread fashion as the spacecraft have. The best tools for modeling and understanding the situation of a satellite in space remain locked behind high barriers to entry including high cost, long training, and complex interfaces. In the same way that satellites have gone from the size of automobiles to the size of toasters, the software for operating them needs to go from expensive, complicated, high-performing suites to simple, flexible, approachable options that are accessible to the democratized space operators. New spacecraft operations staff need straightforward, direct interfaces which give them the knowledge of where their spacecraft is, where it will be, and what it will be able to do, and they need to know when all the options at their disposal are viable. Operators also need to be given the capability to adjust their software in whatever ways are necessary to tailor it to the particular parameters of their missions, to reflect the incredible variety of spacecraft and missions that exist today. A gap exists in spaceflight software. Users need software that can perform their mission planning tasks in the short term and to inform them of the upcoming parameters of their spacecraft which concern them, whether this is the spacecraft’s location, solar illumination, orientation, or any other property which is relevant to their particular mission. This software must also allow the users to be aware of the expected output of their sensors, especially imaging sensors, such that they may have an understanding of what they are imaging and what it ought to look like. Finally, this software must be open-source, enabling the user to audit the software and make changes to the software to customize it to their preferences, which may differ from anything the original software developer could have imagined. Such spaceflight software does not yet exist. This dissertation develops and presents OPTASAT, the Open-source Python Tool for Awareness of Spacecraft and Analysis of Telemetry, which provides an extensible, modular interface for incorporation of multiple tools which contextualize spacecraft data in a manner which maximizes usefulness for the operators. A priority is visualization of data to facilitate rapid understanding and distillation of the complexity of a spaceflight operation. This software has been released as a fully-featured, open-source software toolkit which performs the mission analysis components deemed most crucial to those who stand to benefit from it. This software is intended to fulfill the needs of small spacecraft missions. Several particular application cases are studied, including that of an Earth Sensing mission, and Astronomy mission, and modeling communications for a real laser crosslink mission. These case studies are evaluated for their ability to present the relevant information to the operator. For Earth Sensing, this involves displaying information regarding the spacecraft’s location with respect to the Earth, and enabling the selection of ground targets for imaging. For astronomy, the relevant information concerns the stars visible in the sky, and the spacecraft’s relationship to sources of interference like the Sun and Moon. For the laser crosslink example, we study the operator’s understanding of the spacecraft as they pass over a ground station and determine the operational configurations available for this communication. OPTASAT fills gaps in the field. OPTASAT presents users with a tool which is flexible and intuitive to use for understanding data from spacecraft in a way that is not currently available in the offerings on the market. Additionally, it takes functionality that is currently available in proprietary paid software and makes it available for free, in an open source offering that is accessible to everyone. OPTASAT will allow spacecraft operators (especially those operating spacecraft for the first time) to confidently know the state of their spacecraft, enabling them to make the best decisions for their satellites. This will reduce barriers to entry and smooth the learning curve, reducing the amount of overhead to new spacecraft operators. OPTASAT will be yet another step in the ongoing process of making space more accessible to a larger pool of users.",
        "authors": [
            "Thomas Joseph Murphy III"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158868",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Assessing Impacts of Digital Sketching on Concept Generation in Early Stage Design",
        "abstract": "Digital design tools have become increasingly popular for facilitating designers with different steps of the design process because they can simplify or automate certain components of these steps. Computer Aided Design (CAD) tools have assisted designers with tasks such as modeling and visualizing products prior to production and easily creating engineering drawings for manufacturing. Artificial Intelligence (AI) tools are being explored as collaborators who assist designers with interpreting sketches, assessing user needs, and generating ideas. Digital sketching tools such as tablets are a popular way for designers to easily create drawings that include different colors and styles and create multiple drafts of a concept by copying and pasting elements from previous sketches. However, the introduction of new tools into the design process always has broader implications for the design process. For instance, using CAD tools too early in the process can lead to design fixation and result in designers thinking a concept is more refined than it actually is due to the high quality and polish of the visualization created. Many researchers are now investigating when and how the best way to use AI tools in the design process is, but all struggle with the associated ethical implications of using the right training data and ensuring that the results are validated due to the serious risks related to misuse of AI. This dissertation focuses on one such digital design tool: tablets that are used for sketching. In an effort to expand the discipline’s understanding of how tablet use for sketching may enhance or detract from the design process, this thesis describes a series of studies investigating differences in ideation sketch attributes between tablets and paper/pen. Several of these sketch attributes have been linked with success in design- for instance, creating more sketches during ideation is linked with having better eventual design outcomes. This work investigates how sketch quality and quantity is impacted by the tools used for a short high level brainstorming session as well as a more detailed engineering concept generation task. Subsequently, it explores differences in content or novelty of ideas generated using each medium. Finally, it examines ways in which designers’ ideas evolve throughout the ideation process on both tablets and pen and paper. These aspects of the ideation process are important to understand, especially if the use of tablets leads to different results. The first area of investigation is related to exploring differences in sketch metrics including quantity, quality, and understandability between different sketching tools. These metrics have been found to be related to longer-term design outcomes and perceived creativity of concepts, so understanding the effect of the tablet on these sketch metrics can provide an understanding of how using a tablet for sketching could enhance or detract from overall design performance. The first study in this section investigates differences between pencil, pen, and tablet sketches during a short concept generation exercise and finds that sketch quality was highest for pencil drawings and lower for pen drawings but that tablet drawings do not significantly differ in quality from either pencil or pen drawings. Subsequently, a longer engineering design specific concept generation exercise was conducted to compare tablet sketching to pen and paper sketching. Here, there were no differences found in sketch quantity or understandability between paper and tablet. However, sketch quality, smoothness, and proportion/accuracy were all found to be higher on pen and paper than tablet. The second area of investigation explores whether or not using a tablet influenced designers’ ideation patterns. For instance, does the ability to copy and paste result in designers creating more interrelated ideas during brainstorming instead of exploring a variety of different design directions? There were no major differences found in the overall quantity of concept evolution present between tablet sketching and pen and paper sketching. However, tablet sketches across an ideation session had statistically significantly more concept chaining (related ideas appearing in a row) than paper and pen sketches despite having a similar number of related ideas overall. Additionally, concept chaining patterns were different for design prompts that had more than one functional requirement since not all ideas addressed all parts of the design prompt. However, for these prompts, the results from the primary functional requirement exhibited the same concept chaining patterns with more chaining present for tablet sketching than paper and pen sketching. The final area of investigation explores how designers’ ideas themselves are influenced by the sketching tool used through explorations of concept novelty and concept evolution. One study investigated novelty differences in concepts generated on tablet vs paper and found no correlation between the sketching tool used and the novelty of concepts generated. A second study was conducted to specifically compare designers’ own understanding of the interrelatedness of their ideas with the interrelatedness that could be assessed from the functional similarity of their sketches. Here, designers’ and reviewers’ assessments were found to not be aligned. In other words, sketches as standalone design artifacts did not communicate the extent of interrelatedness of concepts that was clear to the designer. Furthermore, the sketching tool used (tablet vs paper and pen) does not influence the level of agreement between designer and reviewer assessments. As such, using a tablet for sketching does not enhance or detract from the level of interrelatedness represented in sketches. These results suggest that assessing visual or functional similarity from sketches alone, regardless of the sketching tool used, may be insufficient in understanding all the relationship between a series of concepts as understood by the designer. Overall, these results indicate that using tablets as sketching tools does not have a clear significant benefit or burden on designers during ideation. It does not appear to enhance designers’ creative skills when it comes to sketch quantity or novelty though it did result in lower quality sketches, which has implications for the perceived creativity of concepts. Tablets were found to exhibit more instances of concept chaining than paper and pen sketches, though this trend did not persist when designers assessed their own concepts. Finally, this dissertation demonstrates that it is critical to seek designer input in identifying similarities across sketches as functional similarity may not be aligned with designers’ own understanding of which of their ideas are related.",
        "authors": [
            "Madhurima Das"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158800",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exciton Dynamics and Optical Properties of Lead Halide\r\nPerovskite Nanocrystals: From Nanorods to Nanocubes",
        "abstract": "Lead halide perovskites, particularly CsPbBr3, have emerged as leading light emitters for their spectral purity, brightness, and facile synthesis. Their soft, ionic lattice makes them unusually defect tolerant but introduces problems with stability. Additionally, dephasing mechanisms and coupling to phonons are not yet well understood in these semiconductors. \r\nIn the first part of the thesis, I investigate highly confined, anisotropic CsPbBr3 nanorods, elucidating the photophysics governing their broad single-particle linewidths. I utilize ensemble and single particle photoluminescence techniques across a wide temperature range in order to pinpoint exciton-phonon coupling mechanisms, structural and surface effects, and spin mixing in these novel materials.\r\nIn the second part of the thesis, I focus on the opposite size regime, where collective behaviour dominates the optical properties. I develop a novel spectroscopy to pinpoint dephasing mechanisms that could reduce superradiant and coherent emission in order to promote rational design and future integration of these nanocrystals into quantum information devices.",
        "authors": [
            "Tara Šverko"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158955",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention",
        "abstract": "Key-value (KV) caching plays an essential role in accelerating decoding for transformer-based autoregressive large language models (LLMs). However, the amount of memory required to store the KV cache can become prohibitive at long sequence lengths and large batch sizes. Since the invention of the transformer, two of the most effective interventions discovered for reducing the size of the KV cache have been Multi-Query Attention (MQA) and its generalization, Grouped-Query Attention (GQA). MQA and GQA both modify the design of the attention block so that multiple query heads can share a single key/value head, reducing the number of distinct key/value heads by a large factor while only minimally degrading accuracy. In this work, we show that it is possible to take Multi-Query Attention a step further by also sharing key and value heads between adjacent layers, yielding a new attention design we call Cross-Layer Attention (CLA). With CLA, we find that it is possible to reduce the size of the KV cache by another while maintaining nearly the same accuracy as unmodified MQA. In experiments training 1B- and 3B-parameter models from scratch, we demonstrate that CLA provides a Pareto improvement over the memory/accuracy tradeoffs which are possible with traditional MQA, potentially enabling future models to operate at longer sequence lengths and larger batch sizes than would otherwise be possible.",
        "authors": [
            "William Brandon"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158929",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Novel Structures for Scalable Vertical Gallium Nitride Power Devices",
        "abstract": "Solid state electronic devices have been the backbone of modern power systems for decades. However, as we enter an era fuelled by renewable energy and defined by pervasive electrification, novel power devices must be developed to address the increasingly stringent demands for high power density and efficiency. In this thesis, the theory and fabrication of several new gallium nitride (GaN) power devices will be developed to push beyond current device limitations.\r\n\r\nA key advancement surrounds the acknowledgment that vertical GaN power devices are fundamentally three-dimensional. Fabrication of these devices does not readily benefit from the decades of expertise gained in planar processing within the silicon industry. Instead, we will present how a new approach to creating vertical fin-based devices will enable self-aligned fabrication of vertical GaN finFETs and related devices. \r\n\r\nWithin this work, we also explore the scalability of vertical GaN finFETs. Working with 8-inch GaN substrates, we demonstrate that vertical finFETs can be fabricated using a fully CMOS compatible process flow. This enables a scalable pathway to the widespread adoption of GaN by leveraging existing manufacturing capabilities.\r\n\r\nAs a final look towards the future of GaN devices, we explore methods to surpassing the one-dimensional, unipolar limit of GaN through devices known as superjunction. The theory that has been highly successful for Si devices is applied to GaN, and a new framework for designing devices is presented. Using our approach to creating vertical fin-based devices, we are able to fabricate record high aspect-ratio demonstrations of a new class of fin diodes that reveal a promising path towards the next generation of GaN power devices.",
        "authors": [
            "Joshua Andrew Perozek"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158939",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Ponderomotive Forces in Pilot-Wave Hydrodynamics",
        "abstract": "Droplets bouncing on a vibrating bath may self-propel (or ‘walk’) via a resonant interaction with their self-induced pilot wave. In pilot-wave hydrodynamics (PWH), the spontaneous emergence of coherent, wave-like statistics from chaotic trajectories has been reported in several settings. Owing to the similarity of PWH to Louis de Broglie’s realist picture of quantum mechanics, the question of how such statistics emerge has received considerable recent attention.\r\n\r\nA compelling setting where coherent statistics emerge in PWH is the hydrodynamic analog of the quantum corral. When walking droplets are confined to a circular cavity or ‘corral’, a coherent statistical pattern emerges, marked by peaks in the positional histogram coincident with extrema of the cavity eigenmode. Stroboscopic models that idealize the drop’s bouncing dynamics as being perfectly resonant with their Faraday wave field have proven incapable of capturing the emergent statistics.\r\n\r\nIn this thesis, we present new experimental and theoretical findings in a variety of pilotwave hydrodynamical settings where non-resonant bouncing plays a key role in the droplet dynamics and emergent statistics. First, we find that modulations to resonant bouncing influence the stability threshold of a Bravais lattice. Second, we demonstrate that resonant bouncing can be disrupted by the imposition of suboctave driving, which may be used to induce a rearrangement of bound states of bouncing droplets.\r\n\r\nWe then proceed to an integrated experimental and theoretical study of the hydrodynamic corral, highlighting the role of non-resonant bouncing in the emergent statistics. We first introduce a new experimental method for simultaneously measuring the drop position and pilot wave height. We then report new measurements of the pilot wave and vertical bouncing dynamics. We demonstrate that the complex pilot wave arising in corrals may play the same role as suboctave driving in disrupting resonant walking. Our experimental findings motivate a new theoretical framework that predicts that modulations in the histogram emerge as a consequence of ponderomotive effects induced by non-resonant bouncing. We then connect the ponderomotive drift observed in hydrodynamic corrals to extant theories of quantum mechanics.",
        "authors": [
            "Davis J. Evans"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158854",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Shut Up and Dribble? Exploring the Real Estate Strategies and Trends of NBA Teams",
        "abstract": "NBA teams have always had to think about real estate through one certain lenses: the arena they play their 41 home games in (plus any subsequent playoff games). But now, NBA teams have evolved past only just thinking about the arena. Teams have increasingly gotten involved in real estate development. This thesis seeks to explore the impact of real estate as a revenue driver for NBA teams, trends observed, and strategic decisions that teams must consider. This thesis will explore current real estate activities of all 30 NBA teams and will examine the choices that teams must make regarding arenas, real estate development, and practice facilities. The findings will help teams and municipalities understand best practices for team-driven real estate, and how strategies can vary team by team based on their situations.",
        "authors": [
            "Viet Nguyen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158797",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Microfluidic Platform for Vascularized Tissue Models",
        "abstract": "This thesis presents a microfluidic platform designed to support 3D vascularized tis­sue models for microphysiological systems. The platform delivers pneumatic pressure and vacuum signals to drive fluid flow and pressure on tissue culture devices with integrated pumps and back-pressure regulators. The mechanical performance of the pumps and back-pressure regulators is characterized. Tissue compartments in each device contain endothelial and stromal cells suspended in a hydrogel during culture. An oxygenating reservoir stores and replenishes oxygen in circulating cell culture me­dia. During assembly, screws are used to compress an elastomeric membrane, forming a seal and transmitting pneumatic pressure signals from the connection manifold to acutate the fluidic control elements. After a biological experiment the tissue culture devices can be disassembled, cleaned, and re-used, thus enabling cost-effective experi­mentation and prototyping. Each of the 4 layers of the tissue culture devices arc ma.de of thermoplastic polymers, and their design is translatable to injection molding for future production at scale. The design and manufacturing methods for the platform and individual device features are discussed. Two major biological experiments are presented to demonstrate the platform's ability to support emergent vascularization in the tissue culture device over 7 days. Microscope images show development of perfusable microvessel networks.",
        "authors": [
            "Matthew Johnson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158859",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Fabrication and Characterization of Horizontally Aligned\r\nCarbon Nanotube Thermoplastic Bulk Nanocomposite\r\nLaminates",
        "abstract": "Carbon nanotubes (CNTs) have advantaged mass-specific mechanical properties and excellent thermal and electrical conductivity, making them an attractive reinforcement for composite systems. Due to an increasing need for more sustainable materials, incorporation of CNTs into thermoplastic matrices presents a promising solution for recyclable and repairable polymer nanocomposites (PNCs). This thesis presents an approach to fabricating and characterizing thermoplastic PNCs that incorporate ultra-high volume fractions of horizontally-aligned carbon nanotubes (HA-CNTs). An MIT-developed bulk nanocomposite laminating (BNL) process was adapted to fabricate multi-ply, unidirectional composites with poly(methyl methacrylate) (PMMA) and acrylonitrile butadiene styrene (ABS) matrices. For the HA-CNT/PMMA system, the BNL process was tailored to fabricate 4-ply and 8-ply laminates with fiber volume fraction v_f > 45 vol.%, using a 9 wt.% PMMA in anisole solution. Through characterization via X-ray microcomputed tomography (µCT), scanning electron micrography (SEM), thermogravimetric analysis (TGA), Fourier transform infrared (FTIR) spectroscopy, and polarized Raman spectroscopy, HA-CNT/PMMA laminates were shown to be free of micro-scale voids with weak or non-existent process-structure interactions, i.e., the CNTs had negligible effect on the polymer structure. TGA and IR helped demonstrate that the BNL process did not lead to decomposition or chemical changes to neat PMMA, and FTIR also revealed that the fabrication process did not induce covalent bonding between CNTs and PMMA. The crystalline behavior of PMMA was studied via dynamic scanning calorimetry (DSC) as well as X-ray diffraction (XRD), which demonstrated that BNL processing temporarily lowers neat PMMA glass transition temperature T_g by 4 ◦C with no permanent change after removal of thermal history. However, CNT inclusion leads to higher laminate T_g by 11 ◦C as shown through both DSC and dynamic mechanical analysis (DMA), which can be explained by CNT constraints on polymer chain movement as opposed to any crystallinity changes in the PMMA. Storage modulus of 8-ply HA-CNT/PMMA laminates was shown to be more than 600% of neat PMMA via DMA, while a decrease in tan(δ) of the laminate compared to neat PMMA indicates an increase in elastic behavior due to CNT inclusion. 4-ply laminates were subjected to a minimum radius of curvature test showing a ∼ 50% increase in yield strain compared to neat PMMA. Electrical properties of 4-ply HA-CNT/PMMA laminates were measured via 4-point probe testing, which demonstrated good Ohmic contact between CNTs, with conductivity of ∼ 2 × 10⁴ S m⁻¹ and anisotropy ratio of 1.2. A preliminary investigation was completed to evaluate the feasibility of using the BNL process for the HA-CNT/ABS system. Uniform suspensions of ABS in anisole were developed to use the BNL polymer infiltration method of spin-coating and vacuum-assisted infusion. It was shown that the nature of the ABS suspension led to uneven polymer distribution over the HA-CNTs. This work has demonstrated the successful incorporation of high volume fractions of aligned CNTs into PMMA thermoplastic matrices as well as the electrical conductivity of such composites, opening an avenue to the development of other high v_f thermoplastic PNCs and exploration into additional multifunctional capabilities.",
        "authors": [
            "Yuying Lin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158815",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Leveraging Right and Left Ventricular Coupling for Optimization of Mechanical Circulatory Support",
        "abstract": "Mechanical circulatory support devices have the potential for profound impact on cardiogenic shock patients. They enable volume propulsion and pressure gradient generation first by unloading and later by decoupling native cardio-vascular interactions, which reduces cardiac load and energy consumption while increasing organ perfusion in the face of disease.  However, there is a potential price in that coupling evolved to optimize blood flow dynamics and the complex interplay between individual cardiovascular components and interposing organs like the lung. Disrupting native coupling with mechanical support risks decompensation if the heart and lung cannot tolerate these changes.\r\n\r\nOne particularly concerning consequence of altered coupling is that upwards of 40% of patients with left-sided mechanical support face ensuing right heart failure, which requires urgent action and often is associated with even higher mortality rates. We hypothesized that better understanding of right heart function and the mechanisms of right heart (in)tolerance to left-sided support will improve device utility by aiding device selection as well as titration throughout a patient’s clinical course. In particular, we focused on right and left ventricular coupling, which consists of serial coupling across the closed-loop cardiovascular circuit, and parallel coupling that enables intracardiac interdependence and force transmission between the ventricles. Each interaction plays a critical role in a patient’s tolerance to mechanical support and optimal setpoint.\r\n\r\nWe used a series of controlled porcine experiments to evaluate right and left heart coupling during mechanical support. In each set of experiments, we induced graded models of disease that range from health to progressive impairment, enabling evaluation of  mechanical support across a spectrum of right and left heart states. Through these studies, we improved mechanistic understanding of the differences between right and left heart function, and how those differences dictate the response to left-sided support. Specifically, we found that pulmonary vascular compliance enabled a unique right heart adaptability to varied flow, but limitations in compliance due to disease yield right heart intolerance to support. We leveraged the indwelling pump to dynamically alter load in the system, creating a method to rapidly evaluate pulmonary vascular compliance adaptability and therefore predict the need for right-sided support. Finally, we created a metric using device-organ interactions for tracking right-left coupling over time, which can aid optimization of device speed based on relative right and left ventricular volume setpoints. Translation of these findings to the clinic could better inform use of mechanical circulatory support technologies with the goal of improving outcomes for cardiogenic shock patients.",
        "authors": [
            "Kimberly Kate Lamberti"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158818",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Designing Sustainable Recommender Systems",
        "abstract": "Recommender systems are widely deployed to serve users with content they like. However, content must be created and insufficient demand dampens a creator’s production incentive. We argue that the canonical recommender system may not be sustainable if, by promoting the content each user likes the most, it suppresses the creation incentive of the less popular but still valuable content. We propose a “sustainable recommender system” solution – subsidize creators with demand according to their “sensitivity,” which measures how easily a creator can be incentivized by demand, and their “contribution,” which measures how important a creator is to users overall. Theoretically, we prove that this algorithm maximizes long-term user utility by internalizing the externality of user choice on other users. Computationally, our main innovation is to estimate creator contribution using computer vision, where we train a deep-learning model to compute how creator distribution affects system-wide user utility. Analyzing data from a large content platform, we show that our algorithm incentivizes valuable creators and sustains long-term user experience.",
        "authors": [
            "Lei Huang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158881",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Business and Redevelopment Outline for the Re-Use of a Prime Site in South Boston",
        "abstract": "This development and business plan considers the neighborhood context and current market conditions characterizing the subject site’s redevelopment potential. The subject site, further defined in this thesis, is a prime parcel of land in the South Boston neighborhood of Boston, MA currently improved and used for quick-serve restaurant operations. Proximate to the Seaport, Fort Point, and Dorchester, South Boston is surrounded by demand drivers resulting in explosive growth that make it one of the most desirable and expensive housing submarkets in the entire City of Boston. Development considerations are fully defined in the report including zoning, equity, financial projections, ground lease, and market-level factors. A conclusion is made on the feasibility of the proposed project with recommendations for next steps resulting from the modeled base-case scenario. Market assumptions and any unresolved development issues are clearly identified and discussed.",
        "authors": [
            "Zachary D. Proman"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158849",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Computational Modeling of Biological Function",
        "abstract": "How biological function emerges from complex molecular patterns is a fundamental question in biology. Addressing this question requires a deep exploration of the concepts of genotype and phenotype, which serve as the foundation of this inquiry. This dissertation focuses on providing a quantitative approach through the lens of computation to dissect the dynamic relationship between genotype and phenotype. In particular, recent advancements in high-content genotyping methods, such as genome-wide association studies (GWAS) and single-cell RNA sequencing, have provided powerful tools for mapping the molecular basis of biological function, but also have introduced challenges due to the high dimensionality, vast combinatorial possibilities, and multimodal characteristics of the data. The overarching goal of this dissertation is first to provide a critical discussion on the theories of genotype and phenotype as they relate to biological function and propose new methods to map their relationship. Specifically, we present the integrated genetics framework designed to analyze and interpret the manifold of genotypes and their associated phenotypes simultaneously. We applied this approach to develop a multimodal foundation model for human transcriptomics at the cellular level. To further test the capabilities of this method, we apply it to dissect the aging process. The results of this study provide novel concepts and methods for analyzing the genetic data along with phenotypic information with higher resolution. Moreover, the results shed light on uncovered potential cross-tissue biomarkers that are undetectable through conventional gene expression analysis alone. Overall, this study aims to advance our understanding of the dynamic interplay between gene patterns and phenotypic manifestation and demonstrates the potential of computational modeling in uncovering new dimensions of cellular function and complexity.",
        "authors": [
            "Farhan Khodaee"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158814",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Goal Inference from Open-Ended Dialog",
        "abstract": "Embodied AI Agents are quickly becoming important and common tools in society. These embodied agents should be able to learn about and accomplish a wide range of user goals and preferences efficiently and robustly. Large Language Models (LLMs) are often used as they allow for opportunities for rich and open-ended dialog type interaction between the human and agent to accomplish tasks according to human preferences.\r\n\r\nIn this thesis, we argue that for embodied agents that deal with open-ended dialog during task assistance:\r\n\r\n1. AI Agents should extract goals from conversations in the form of Natural Language (NL) to be better at capturing human preferences as it is intuitive for humans to communicate their preferences on tasks to agents through natural language.\r\n\r\n2. AI Agents should quantify/maintain uncertainty about these goals to ensure that actions are being taken according to goals that the agent is extremely certain about.\r\n\r\nWe present an online method for embodied agents to learn and accomplish diverse user goals. While offline methods like RLHF can represent various goals but require large datasets, our approach achieves similar flexibility with online efficiency. We extract natural language goal representations from conversations with Large Language Models (LLMs). We prompt an LLM to role play as a human with different goals and use the corresponding likelihoods to run Bayesian inference over potential goals. As a result, our method can represent uncertainty over complex goals based on unrestricted dialog. We evaluate in a text-based grocery shopping domain and an AI2Thor robot simulation. We compare our method to ablation baselines that lack either explicit goal representation or probabilistic inference.",
        "authors": [
            "Rachel Ma"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158960",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Data-Rich Personalized Causal Inference",
        "abstract": "There is a growing interest in individual-level causal questions to enable personalized decision-making. For example, what happens to a particular patient’s health if we prescribe a drug to them, or what happens to a particular consumer’s behavior if we recommend a product to them? Conducting large-scale randomized experiments to answer such questions is impractical—if not infeasible—due to cost, the level of personalization, or ethical concerns. Observational data offer a valuable alternative, but their lack of explicit randomization makes statistical analysis particularly challenging. In this thesis, we exploit the richness of modern observational data to develop methods for personalized causal inference. In the first part, we introduce a framework for causal inference using exponential family modeling. In particular, we reduce answering causal questions to learning exponential family from one sample. En route, we introduce a computationally tractable alternative to maximum likelihood estimation for learning exponential family. In the second part, we leverage ideas from doubly robust estimation to enable causal inference with black-box matrix completion under a latent factor model.",
        "authors": [
            "Abhin  . Shah"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158911",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Multi-Agent Hybrid Prediction in Autonomous Driving",
        "abstract": "In autonomous driving, the hybrid task of predicting both high-level actions and lowlevel trajectories of human behaviour is fundamental to safe downstream decision-making. Much of the existing work in behaviour prediction tackle this problem without sufficiently modelling agent-agent interactions, limiting their ability to capture the full range of possible joint outcomes. Another key challenge in multi-agent prediction is the intractable prediction space that grows exponentially in the number of agents and duration of the prediction horizon. As a result, scalability is a major challenge. This thesis presents two approaches to address these challenges in multi-agent hybrid prediction. In our first approach, we model interactions and address scalability by learning to factor the joint prediction distribution. We observe that agents do not interact with all other agents in the scene, but rather, there are groups that strongly interact. Therefore, we group agents and represent the high-level interaction outcomes of groups with discrete variables. We additionally assume that inter-group interactions are sparse and can be sufficiently represented with a directed acyclic graph. These assumptions enable us to factor the distribution into a product of factors, effectively reducing the prediction space, and providing an order in which to easily sample discrete values. We evaluate the performance of this method on a large-scale autonomous driving dataset and show that it exceeds prior methods in coverage of possible interaction outcomes by 24% to 48% on various multi-agent validation data splits, while maintaining state-of-the-art prediction error. Our second approach represents agents in a traffic scene as a set of concurrent hybrid models and assumes a collision avoidance model of interactions, rather than learning the model from data like the first approach. Our method begins enumeration based on a simpler collision-agnostic prior distribution. Based on our factored representation, we determine the next best assignment to the prior. We extract bounding conflicts to correct the prior and increasingly reduce the error between the distribution used by enumeration and our collision-aware posterior distribution. Our experiments show that enumeration using A* with bounding conflicts (A*BC) is faster than A* and is therefore better at addressing scalability. In terms of prediction metrics, we find that our collision-aware posterior performs worse than the collision-agnostic prior and suggest future directions for improvement.",
        "authors": [
            "Tiffany Yee Kay Yau"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158832",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Engineering Scalable Quantum Systems From First-Principles to Large-Scale Control",
        "abstract": "Color centers in solids are promising platforms for quantum communication, sensing, and computing, featuring highly coherent optical transitions, as well as native electron and nuclear spins that can be used as quantum memories. Existing state-of-the-art demonstrations have shown that multi-qubit control, spin-photon entanglement, and heralded entanglement are possible with devices consisting of a few color centers. However, the path to scaling the number of color centers integrated in these devices to the thousands or millions needed for advanced quantum networking and computing applications remains unclear. In particular, the requirement for highly coherent quantum operations both necessitates operation at cryogenic temperatures, and precise classical control signals delivered to each color center. Precise qubit control greatly increases the system complexity, while the cryogenic operation limits the amount of power that the system can dissipate. Both factors severely limit the number of color centers that can realistically be included in a single device using existing methods. This work will tackle the scaling problem from a system-level perspective from two directions. Firstly, I will quantify performance trade-offs between coherence, temperature, and optical properties of the group-IV color centers. A novel color center system, the ¹¹⁷SnV⁻ hyperfine color center, will be presented and its advantages compared to traditional group-IV color centers will be explored. Secondly, a method to integrate color centers with application specific integrated circuit (ASICs) will be demonstrated. The ASICs provides multiplexed control signals and increased control field efficiency, thus decreasing both the wiring complexity and thermal load per qubit. This work will thus pave the way to color center-based devices in which the number of qubits is not limited by the complexity or power dissipation of the control system.",
        "authors": [
            "Isaac B. W. Harris"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158925",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Learning Generalizable Systems by Learning Composable Energy Landscapes",
        "abstract": "How can we construct intelligent embodied agents in the physical world? Such agents should be able to autonomously solve tasks that have not been seen before, subject to external disturbances in the environment, as well as new combinations of factors such as lighting, varying sensor inputs, and unexpected interactions with agents and other objects. An important subgoal towards constructing such intelligent agents is to construct models that can robustly generalize, not only to distributions of tasks similar to ones seen at training time but also to new unseen distributions. This departs from standard machine learning techniques which usually assume identical training and test distributions. Towards this goal, in this dissertation, we’ll illustrate how we can achieve certain forms of generalization by estimating energy landscapes over possible predictions for each task, with accurate predictions assigned lower energy. This modeling choice formulates prediction as a search process on the energy landscape, enabling zero-shot generalization to new constraints by adapting the energy landscape. In addition, this allows us to generalize to entirely new distributions of tasks in a zero-shot manner by composing multiple learned energy landscapes together. In this dissertation, we first introduce a set of techniques to train energy landscapes and an algebra in which we can compose and discover composable energy landscapes. Next, we illustrate how energy landscapes can be composed in a diverse set of ways, ranging from logical operators, probability distributions, graphical models, constraints, and hierarchical compositions, enabling effective generalization across vision, decision-making, multimodal, and scientific settings.",
        "authors": [
            "Yilun Du"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158938",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Cooling Innovation and Circularity: Addressing Water Stress in the Age of AI-Driven Data Centers",
        "abstract": "This thesis examines the growing demand for data centers and the critical challenges posed by their water and energy consumption. As artificial intelligence (AI) technologies expand, the infrastructure supporting these systems has become essential. The study highlights the projected increase in data center capacity driven by AI workloads and focuses on the impact in water-stressed regions across the United States. Given the resource-intensive nature of data centers, the research explores cooling technologies aimed at reducing environmental impact. Traditional air cooling is compared with innovative liquid and evaporative cooling techniques. Additionally, the thesis promotes circular economy principles, emphasizing resource efficiency, reuse, and regeneration as a pathway to sustainable operations.",
        "authors": [
            "Reem Kseibati"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158889",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Addressing Challenges in Object-Based Robot Navigation and Mapping",
        "abstract": "Developing fully autonomous systems that can safely traverse and interact with the environment has been a long-term objective in robotics. Many relevant tasks, such as planning and mobile manipulation, require the robot to possess an object-level understanding of the ambient world. In particular, it would be crucial to maintain a globally consistent objectbased map of the environment for these operations. Without external assistance – such as a prior map or a motion capture system – the robot needs to navigate and map the environment using an object-based SLAM system. This thesis is dedicated to addressing several key challenges in developing object SLAM systems. The first challenge arises from the ambiguity of object poses in single-view observations. When an object is observed from a single vantage point, it can often have multiple probable poses due to symmetry, occlusion, or perceptual failures. It would be difficult for an object SLAM system to incorporate such ambiguous measurements. To address this issue, we introduce an ambiguity-aware object SLAM method. We use Gaussian max-mixture models to represent and efficiently track the multiple object pose hypotheses, and gradually disambiguate the poses to construct a globally consistent object-level map. The second challenge is the performance degradation of neural networks when deployed in novel robot operating environments, commonly known as the domain gap problem. Specifically, when a pre-trained 6DoF object pose estimator is used in a novel environment, its pose predictions are often corrupted by outliers, and quantifying their uncertainties becomes difficult. Using these noisy predictions with unmodeled uncertainties as measurements in an object SLAM system can lead to significant estimation errors. To mitigate the problem, we propose a SLAM-supported self-training pipeline for domain adaptation of 6DoF object pose estimators. We exploit robust pose graph optimization (PGO) results to pseudo-label robot-collected images and fine-tune 6D object pose estimators. In particular, we develop an Automatic Covariance Tuning (ACT) method to model pose prediction uncertainties automatically during the PGO process. The third challenge is environmental changes. As changes occur in the scene, such as object insertion, removal, or rearrangement, the robot needs to efficiently detect these changes and update the map accordingly. While detecting and reflecting scene changes is relatively straightforward with handcrafted map representations like point clouds or voxels, it becomes significantly more difficult with learned radiance-field-based scene representations, such as Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) models. In this thesis, we develop a radiance-field-based 3D change detection method to identify 3D object-level scene changes. Our approach can rapidly detect object changes in cluttered environments represented with radiance field models from as few as a single post-change image observation. We also develop efficient update methods for NeRF and 3DGS models to reflect physical object rearrangements, guided by sparse post-change images. By addressing these challenges, this thesis advances the robustness and adaptability of object SLAM systems in real-world environments, paving the way for more reliable and autonomous robotic systems capable of complex interactions with the environment.",
        "authors": [
            "Ziqi Lu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158807",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Complementary Cost‐Effective Electrochemical Platforms for Point‐Of‐Use Biosensing",
        "abstract": "The COVID‐19 pandemic has illustrated the urgent need for rapid and affordable point‐of‐use diagnostics. Electrochemical biosensors are useful for such applications because they enable quantitative readout and show drastically improved sensitivity compared to prevalent lateral flow technologies. However, to‐date, the poor quality of commercially‐available, mass‐produced electrodes has prohibited the scaled production and commercialization of such biosensors beyond glucose sensing. Low‐cost gold leaf electrodes have previously been developed that can be fabricated with no specialized equipment at the point‐of‐use. These electrodes are more effective for biosensing than prevalent commercially‐available systems. Yet, their manual fabrication can be tedious and is not scalable in its current form. Here, performance of mass‐produced gold electrodes generated using roll‐to‐roll manufacturing is evaluated, offering the potential to scale production. Upon comparison of these electrodes with the gold leaf, it is found that these electrodes are high quality, equivalent to the gold leaf electrodes, and support biosensing applications through the detection of both DNase I and BtsI‐v2 activity with comparable performance. These results demonstrate the role of complementary technologies to achieve point‐of‐use sensing by enabling flexibility between mass‐produced manufacture and on‐site production.",
        "authors": [
            "Mason Monaco",
            "Marjon Zamani",
            "Ava Sarram",
            "Chao‐Chi Kuo",
            "Chathurika Abeyrathne",
            "Miaosi Li",
            "Ariel L Furst"
        ],
        "journal_conference_name": "Advanced Sensor Research",
        "publisher": "Wiley",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158292",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Essays on Sustainability in Agriculture and Food Systems",
        "abstract": "Agriculture and food systems face severe challenges from climate change, population growth, and food insecurity. These unprecedented issues leave millions vulnerable to hunger and malnutrition, underscoring the urgent need for a transition toward sustainable agriculture and food systems. The first research stream in this thesis focuses on promoting sustainability in agriculture, particularly through contract farming. In Chapter 2, we model contract farming as a bi-level optimization problem for a farmer and a company. We analytically demonstrate that different contract structures offer varying incentives for farmers to invest in quality-improving efforts, resulting in different levels of quality for agricultural products. Empirical analysis of production-level data supports these model predictions.\r\n\r\nThe second research stream examines sustainability in food systems, specifically addressing the issue of food waste. In Chapter 3, we explore the impact of online grocery shopping on household food waste. Using large-scale Nielsen Consumer Panel data and instrumental variable analysis, we establish a statistically significant causal relationship, showing that households with higher frequency of online grocery shopping experience lower waste per capita, a proxy of household food waste. These findings emphasize the role of digital platforms in fostering sustainable consumption and call for continued support for online grocery shopping to mitigate consumer-level food waste. In Chapter 4, we turn to retail-level food waste. We design and implement behavioral interventions aimed at reducing food waste in restaurant kitchens in Ghana. As a Sub-Saharan African country, Ghana faces both food waste and food insecurity. Through a six-week field experiment and a difference-in-differences analysis, we demonstrate that interventions focused on public- and private-interest lead to 9% and 19% reductions in food waste in kitchens, respectively. Follow-up surveys and further analyses reveal that this result may be related to the demographic/socioeconomic characteristics of workers (e.g., age and income), their perception of power distance within the management hierarchy, and their satisfaction with restaurant management.",
        "authors": [
            "Xinming Liu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158806",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Tracking carbon fluxes across ocean interfaces using dissolved gas observations",
        "abstract": "The cycling and exchange of carbon between Earth’s systems play a pivotal role in regulating climate, yet two major carbon fluxes remain poorly constrained: the biological carbon pump (BCP) and carbon release from Arctic permafrost. This thesis focuses on dissolved gases as tracers and drivers of these processes through both autonomous and field-based observations. It encompasses (i) improvements to sensor-based measurements of O₂, (ii) the use of these measurements to assess the strength of the BCP in two distinct export regimes, and (iii) isotopic approaches to carbon dioxide (CO₂) and methane (CH₄) dynamics at a coastal permafrost site. The first part of the thesis is centered around the NASA EXPORTS campaign and studies the BCP at two contrasting field sites. Using autonomous platforms, carbon export was evaluated at both sites and demonstrated that at the lower productivity site, a greater proportion of fixed carbon was routed to sinking particulate organic carbon (POC), while the higher productivity site resulted in near equal proportions of dissolved organic carbon production and sinking POC. These findings underscore the value of autonomous sensors in capturing spatial and temporal variability in oceanic carbon cycling. The second part of this thesis shifts focus to the Arctic, where rapid warming threatens to mobilize vast (~1,500 Pg) amounts of carbon currently stored in permafrost. This study presents observations from the spring thaw at a coastal Arctic site and demonstrated that even sites with high CH₄ and CO₂ concentrations drew less than 10% of their carbon source from ancient permafrost sources. The variability in CH₄ and CO₂ emissions reflects the complex interplay between hydrological changes, primary productivity, and microbial processes. The research highlights the need for regular monitoring of Arctic rivers, which integrate changes in the terrestrial system, as a potential early warning system for abrupt permafrost thaw. This thesis leverages the fundamentals of dissolved gas geochemistry to examine key climate-relevant biogeochemical cycles across diverse environments that are sensitive to global change. These insights contribute to refining Earth system models and emphasize the need for expanded monitoring to predict future shifts in global carbon cycling and climate dynamics.",
        "authors": [
            "Shawnee Nicole Traylor"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158813",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design and Optimization of Tunneling Nanoelectromechanical Switches",
        "abstract": "As silicon complementary metal-oxide-semiconductor (CMOS) technology nears its scaling limits, nanoelectromechanical (NEM) switch relays have emerged as promising candidates for complementing CMOS technology due to their superior characteristics, including zero leakage, steep subthreshold swings, high on-of current ratios, and robustness in harsh environments. However, the practical integration of NEM switches still faces challenges such as high actuation voltages, stiction, and slower switching speeds compared to CMOS. One promising strategy to mitigate these issues is the integration of a self-assembled monolayer (SAM) to create tunneling NEM switches. Such switches could achieve nanometer-scale mechanical modulation of gaps between electrodes, showing the potential to overcome the limitations of a conventional NEM switch by exhibiting low actuation voltages, high switching speeds, and minimizing stiction. Nevertheless, the tunneling NEM switches reported to date still show limited performance and require intricate fabrication processes. Additionally, functional tunneling NEM switches demonstrated are limited to two-terminal architectures. This thesis explores innovative designs, fabrication techniques, and material choices to address these limitations and to develop tunneling NEM switches with enhanced performance and reliability for next-generation NEM logic applications. To this end, switches with various structures have been fabricated and investigated, and their respective characteristics are analyzed. In a three-terminal lateral structure fabricated using entirely conventional nanofabrication techniques, switching is demonstrated in both contact and tunneling modes. While operation in direct contact mode shows a high on-of ratio, the integration of the SAM leads to a significantly reduced actuation voltage of 2 V and a lower hysteresis. Further, two-terminal vertical structured devices are studied in tunneling mode, and they consistently demonstrate operation cycles exceeding 100, with a maximum of over 7000, which manifests the reliability prospects of SAM. The trends in IV characteristics indicate that the SAM might have experienced physical deformation due to compression, highlighting a potential area for future research in the molecular engineering of the self-assembly monolayer.",
        "authors": [
            "Tong Dang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158940",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Techniques for Foundational End-to-End Verification of Systems Stacks",
        "abstract": "Today's software is full of bugs and vulnerabilities. Formal verification provides a promising remedy through mathematical specifications and machine-checked proofs that the implementations conform to the specifications. However, there could still be bugs in the specifications or in the verification tools, which could lead to missed bugs in the software being verified. Therefore, this dissertation advocates for foundational end-to-end verification, a proof-based software development method that can mitigate both of these concerns:\r\n\r\nIt is end-to-end in the sense that the correctness proofs of individual components are used to discharge the assumptions of adjacent components throughout the whole stack, resulting in end-to-end theorems that only mention the top-most and bottom-most specifications, so that bugs in intermediate specifications cannot invalidate the soundness of the end-to-end statement anymore.\r\n\r\nThe method is foundational in the sense that the soundness of the proofs relies only on the foundations of mathematics and on the correctness of a small proof-checking kernel, but not on the correctness of other, domain-specific verification tools, because these tools are either proven correct once-and-for-all, or they output proofs that are checked by the kernel.\r\n\r\nEnsuring that all the reasoning can be checked by the same small foundational kernel requires considerable effort, and the first part of this dissertation presents techniques to reduce this effort:\r\n\r\nOmnisemantics, a new style of semantics that can be used instead of traditional small-step or big-step operational semantics, offer a smooth way of combining undefined behavior and nondeterminism, and enable forward-simulation compiler correctness proofs with nondeterministic languages, whereas previous approaches need to fall back to the much less convenient backward simulations if support for nondeterminism is needed.\r\n\r\nLive Verification is proposed, a technique to turn an interactive proof assistant into a programming assistant that displays the symbolic state of the program as the user writes it and allows the user to tweak the symbolic state as long as the tweaks are provably sound. An additional convenience-improving feature is that instead of stating lengthy loop invariants, the user only needs to give the diff between the symbolic state before the loop and the desired loop invariant, resulting in shorter and more maintainable annotations. Finally, in order to make Live Verification practical, a number of additional proof techniques is presented.\r\n\r\nThe second part of the dissertation shows how these techniques were useful in three collaborative case studies: An embedded system running on a verified processor with an end-to-end proof where the software-hardware interface specification cancels out, a cryptographic server with an end-to-end proof going from high-level elliptic-curve math all the way down to machine code, and a trap handler to catch unsupported-instruction exceptions whose correctness proof combines program-logic proofs about C-level functions, a compiler correctness proof, and proofs about hand-written assembly.",
        "authors": [
            "Samuel Gruetter"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158951",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Using AI to Improve Price Transparency in Real Estate Valuation",
        "abstract": "This thesis explores the integration of artificial intelligence (AI) into real estate valuation, focusing on visual property attributes to enhance traditional Hedonic models. By incorporating Vision Language Models (VLMs) and generative AI, the research evaluates the potential of these technologies to assess non-standard variables like aesthetic appeal, condition and cohesiveness of interior and exterior property photos. The study contrasts traditional hedonic regression models, which rely on quantifiable factors such as square footage and location, with a new approach that includes AI-generated scores derived from property photos. The study employs three distinct models: the No_Rubric Model, the Composite Model, and the Verbose Model with the Hedonic model serving as the baseline for evaluating their performance. The results demonstrate that incorporating visual data significantly improves model\r\naccuracy, aligning valuations more closely with buyer preferences and sold prices. This shift addresses the industry's need for price transparency and highlights how developers can design properties that better meet market demands.",
        "authors": [
            "Cunjia Xu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158862",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Investigation of Long-timescale Behavior of Positive DC\r\nStreamer Coronas",
        "abstract": "Positive DC streamers are filamentary low-temperature discharges that are relevant to many applications, including sterilization, ionic wind generation, agriculture and atmospheric electricity. Even when excited by a DC voltage, streamers in atmospheric-pressure air typically self-pulsate with a frequency of several kilohertz. The generally-accepted explanation for DC streamer self-pulsation is that it is driven by recovery of the electric field near the tipped anode, due to electrostatic removal of ionic space charge from the inter-electrode gap over inter-pulse timescales. However, this theory has not been validated, either experimentally or numerically. Most prior works investigating DC streamers have focused on the streamer propagation phase (a few tens of nanoseconds) - few have investigated longer timescales, including the bridging of the electrode gap by the streamer and the subsequent current pulse (hundreds of nanoseconds) and the period in-between streamer pulses, leading up to initiation of the next streamer discharge (hundreds of microseconds). The work presented in this thesis focuses on investigation of the longer timescales of positive DC streamer development in a tip-to-plane geometry, in particular beyond the streamer propagation phase, through the current flow and inter-pulse phases. This begins with an experimental study to measure the long-timescale development of the electric field inside a streamer corona using the E-FISH laser diagnostic technique. This shows some surprising results, which do not seem to be consistent with the theory of DC streamer selfpulsation being driven by electric field recovery at the anode. The near-anode electric field is not observed to recover during the inter-pulse period - instead, the near anode behavior seems to be dominated by a persistent glow discharge and a curious wave-like feature is observed in the electric field, traveling towards the anode on ionic timescales. This is followed by the development of a 1.5D reduced-order numerical model of a DC streamer, which is optimized for solving over long timescales via a ‘triple-stack’ of transient solvers. The model is able to fully resolve the boundary sheath layers of the plasma and is able to capture detailed behavior of the cathode sheath development during bridging via the use of a kinetic flux boundary condition for the charged species. This model is firstly applied to modeling the bridging and current flow phases of streamer development, and its prediction shows a good qualitative match to the behavior of the experimental current pulse. Parameter sweeps show that the streamer current pulse is sensitive to the assumed radial behavior and the rate of electron-ion recombination, but insensitive to the applied boundary conditions or secondary emission. The final section describes an extension of the 1.5D streamer model to simulate the streamer inter-pulse phase and initiation of a second streamer. It is shown that initiation of a second streamer can be predicted by a fluid model and that radial expansion of positive ions plays an important role; however, it has proven difficult to integrate that effect into the 1.5D model. The model results are consistent with streamer self-pulsation being due to electric field recovery; however, comparison with the results of the E-FISH experiment suggest there may be different mechanisms driving positive DC streamer self-pulsation, depending on the presence or not of a glow discharge on the anode.",
        "authors": [
            "Lee R. Strobel"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158816",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Computational Methods to Improve Satellite Attitude Determination and Control with a Focus on Autonomy, Generalizability, and Underactuation",
        "abstract": "The attitude determination and control system (ADCS) onboard a satellite uses sensors to measure orientation and angular velocity, enabling the satellite to manage angular momentum, counteract disturbances, and point in the desired directions. Many historical ADCS approaches are designed for constant pointing goals, high accuracy sensors, powerful actuators, or larger, high-inertia satellites. Many modern satellites are small satellites (tens of kilograms or less), with lower-cost actuators and sensors, and may have more complicated attitude goals. This dissertation presents a variety of computational approaches to improve ADCS performance by leveraging detailed satellite dynamics modeling and estimation, disturbance inclusion, and trajectory planning–all optimized for efficient onboard computation suitable for small satellites. The proposed framework generalizes ADCS operations, allowing it to adapt automatically to different satellite types, mission requirements, and operational goals, reducing reliance on predefined ground-based commands. This framework can be used in place of standard control laws to make ADCS more autonomous and “hands-off,” calculating its own slews and desaturation while meeting pointing goals, even in cases of underactuation or large disturbances. This generalized and autonomous framework is a contribution of this work, alongside each of its components, which can be individually used in their own right. One key component of this work is a generalized state estimator that integrates a dynamic model of the spacecraft. This estimator demonstrates high accuracy across various satellite configurations, achieving angular error as low as 0.01◦ in low Earth orbit (LEO) with highquality sensors (but no star trackers), compared to the typical 1◦ error of conventional methods. The estimator can account for biases, sensor errors, and external disturbances, ensuring robust performance (e.g., 0.1◦ error in LEO) even with lower-quality sensors (MEMS gyroscopes, plus magnetometers and sun sensors). This adaptability highlights the increased autonomy of the system, as it requires minimal human intervention to maintain high accuracy across diverse mission scenarios. Another major contribution is the integration of disturbance modeling into control laws. By accounting for disturbances directly (either individually or as an all-in-one value tracked by the estimator), rather than through reactive measures like integral control, the proposed methods improve stability and performance, particularly for underactuated systems–improving pointing accuracy by up to 20 degrees. The developed control laws are adaptable to various actuator configurations, disturbance environments, and pointing objectives. This flexibility extends to modifying pointing goals, such as aligning specific vectors rather than requiring a fully specified orientation, enhancing mission adaptability. This work also implements a novel trajectory planning method that generates efficient pointing trajectories for both constant and time-varying goals. The method, based on the Augmented Lagrangian iterated-LQR (ALTRO) approach, creates sequential mission trajectories that optimize performance even under underactuation or disturbance conditions. The planned trajectories are followed by two types of robust closed-loop controllers, applicable across satellite architectures ranging from large weather satellites to 3U CubeSats. By enabling onboard trajectory planning and adaptive control adjustments, this method significantly reduces the need for ground-based planning and interventions, further advancing autonomous operation. The combined framework of estimation, disturbance-aware control, and trajectory planning achieves significantly higher accuracy than traditional ADCS approaches. This enables the use of commercial off-the-shelf components in high-performance missions, overcoming the limitations of low-cost sensors and actuators. The proposed methods allow satellites to operate with weaker or fewer actuators, such as magnetic-only control, while still achieving precise pointing, thereby expanding the feasibility of more autonomous, robust, and cost-effective satellite operations.",
        "authors": [
            "Patrick McKeen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158874",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Non-orthogonal multiple access using guessing random additive noise decoding aided macrosymbols",
        "abstract": "We propose guessing random additive noise decoding-aided macrosymbols (GRANDAM) as a nonorthogonal multiple access (NOMA) method that can detect, error correct, and decode multiple users in multiple input multiple output (MIMO) systems that involve imperfect channel estimation, symbol-wise asynchronous transmission, and interference. GRAND-AM is a NOMA method that uses both joint multiuser detection and joint error correction decoding to handle multiple access interference (MAI) from the users of interest. Our method avoids codebook design and iterative decoding techniques, which are associated with other commonly researched NOMA techniques. We introduce the concept of a macrosymbol, which is constructed from the combination of all user symbols, for the joint detection component of GRANDAM. For the error correction decoding component, we introduce multiple access channel (MAC) codes, which are codes that are used to split the channel rate between users and correct errors due to the MAI. Each user has their information bits encoded with independent MAC codes, which can be short, low rate linear codes such as cyclic redundancy check (CRC) codes or space time codes such as the Alamouti code. We use a soft detection variant of GRAND, a near maximum likelihood (ML) universal decoding algorithm that inverts noise effect sequences from a sequence of symbols to arrive at a codeword, to correct the received sequence of macrosymbols, and ensure that all user codebooks are simultaneously satisfied in the joint decoding process. We show that the methodology of using joint detection and joint decoding at the receiver leads to lower error rates compared to an individual detection and decoding technique, and has comparable performance to an orthogonal multiple access (OMA) system with a similar code rate and length.",
        "authors": [
            "Kathleen Yang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158962",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design and Testing of a Hovercraft with Electroaerodynamic Propulsion",
        "abstract": "Electroaerodynamic (EAD) multistaged ducted (MSD) thrusters are a novel solid-state thruster architecture that has been shown to provide order-of-magnitude improvements in thrust density compared to single-stage EAD thrusters. This makes MSD thrusters well-suited for use in EAD hovercraft, where generating sufficient pressure is crucial for hovering. This study explored the feasibility of a wire-to-airfoil corona discharge MSD thruster powered hovercraft through a scaled-down prototype and final design. The hovercraft was tethered to a ground-based power supply and carried a payload mass to simulate having on-board power electronics to limit the scope of the project. The design of an EAD hovercraft involved applying the principles of hovercraft lift to a design optimization that implements the recently developed EAD MSD thruster model. A hovercraft prototype was designed and constructed to validate the models applied during the design phase and to test hovering capabilities without a payload. Using the manufacturing lessons and insights gathered in the prototype testing, a full-scale model was designed and built to hover while having an additional payload capacity that would be representative of a set of power electronics.",
        "authors": [
            "Matthew Quiram"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158851",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Using Systems Architecture and the EVDT Framework\r\nfor Monitoring Methane Emissions in Rio de Janeiro",
        "abstract": "Methane is a powerful greenhouse gas that has important implications for climate change. Over the past decade, satellites have rapidly improved their ability to detect this gas from above the atmosphere. This Thesis uses two Systems Engineering frameworks, Systems Architecture and EVDT, to examine a case study of methane monitoring in Rio de Janeiro, Brazil. Data from one of these novel satellite systems, GHGSat, is taken over the Seropédica landfill near the city, and compared to Rio’s own IPCC- and GPC-derived greenhouse gas inventory. This is followed by a participant observation in the summer of 2024 involving interviews, discussions, and site visits. A near-doubling of methane was observed over Seropédica, raising questions about the cause of this increase. The direct engagement with Stakeholders provided by this study contributes to a literature gap in satellite monitoring of urban landfills in southeastern Brazil.",
        "authors": [
            "Frederick Henry Oladimeji Ajisafe Jr."
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158786",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Evaluating Chongqing Tiandi Project: An Asset Management Perspective",
        "abstract": "This thesis uses the Chongqing Tiandi project as a case study to analyze the entire process of development and asset management for large-scale urban renewal projects in China's second-tier cities. It focuses on the motivations and outcomes of Shui On Land's transition from an asset-heavy to an asset-light model. Based on theoretical analysis (Chapter 2), corporate-level financial analysis (Chapter 3), and project-level in-depth studies and interviews (Chapter 4), the thesis explores the logic and impact of this strategic transformation from multiple perspectives. The theoretical analysis summarizes real estate lifecycle management theory, portfolio theory, and corporate strategic transformation theory, providing a framework to examine Shui On Land's strategic decisions. The financial analysis reveals that, from 2015 to 2017, Shui On Land faced significant financial pressure with high debt ratios and cash flow constraints, necessitating systematic asset disposals. While the company disposed of multiple assets during this period, Chongqing Tiandi's 79.2% equity disposal was particularly strategic due to its position as a high-risk, low-return asset within the company's portfolio. The project-level analysis and interviews demonstrate that replicating successful development models from first-tier cities in second-tier markets faces unique challenges. In Chongqing Tiandi's case, these challenges manifested in multiple ways: limited residential price premiums due to local land supply policies, substantial investment requirements for super high-rise developments exceeding $1 billion, and persistently low office rental rates in the local market. These factors compromised the project's financial self-sustainability and made it particularly vulnerable in Shui On's portfolio, especially when compared to projects in other second-tier cities like Wuhan. The development and subsequent equity sale of Chongqing Tiandi not only provided essential financial support for Shui On Land but also reflected a strategic decision to divest from a project where market conditions created both immediate challenges and future uncertainties. This research provides valuable references for the development of large-scale projects in China's second-tier cities, emphasizing the need for developers to utilize funds efficiently, adapt flexibly to market changes, and focus on achieving long-term value. These insights hold significant implications for sustainable development in complex market environments.",
        "authors": [
            "Junsi Yang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158890",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "For and Beyond the Plaques: Sustainable Certification Adoption\r\n and Its Impact on Real Estate Decision-Making in the Boston-Cambridge Market",
        "abstract": "As demand for green and healthy buildings grows, real estate developers face complex decisions regarding building certification adoptions, which have become influential in real estate market dynamics. This thesis investigates how developers in the competitive Boston-Cambridge area navigate the sophisticated certification landscape—focusing on LEED, ENERGY STAR, WELL, Fitwel, and WiredScore/SmartScore—to gain competitive advantages, attract and retain tenants, maximize financial performance, and align with regulatory requirements and ESG goals.\r\nUsing a mixed-methods approach, including quantitative analysis of certification overlaps and trends, along with qualitative insights from industry interviews, the study provides a comprehensive understanding of how real estate developers strategically use certifications to influence asset value while meeting tenant and investor expectations. Findings offer potentially actionable insights into how certifications shape market positioning and inform the decision-making process in real estate development.",
        "authors": [
            "Shenglin Huang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158865",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On the nature and measurement of variational bias: a developmental perspective",
        "abstract": "Natural selection cannot work with imaginary phenotypes, only those realized by developmental systems. The observed diversity of life on Earth occupies only a subset of conceivable forms in the absence of selection. This is because of the non-linear and discrete nature of genotype-to-phenotype maps as an outcome of the developmental system. Despite that, it is widely accepted in population and quantitative genetic modelings that the phenotypic production from random mutations is isotropic and uniform. Conventional methods linking genetic variants and phenotypic variation often assume that the origin of phenotypic variation is purely due to genetic and environmental factors. Here, in this thesis, I adopt a developmental causation view which proposes that patterns of variation may emerge as an inherent consequence guided by physico-chemical principles and that part of the nature can not be fully reducible to genetic factors. The distribution of phenotypic variants that arise from genetic and environmental variation is influenced by the developmental processes that transform the embryonic phenotype into the adult form. This developmental process is subject to constraints that stem from the structure, character, composition, or dynamics of development. We term such a constraint as developmental bias. Despite the prevalence of developmental bias, detecting and testing its role remains a challenge. To address this gap, in the thesis, I propose frameworks and showcase examples aimed at identifying developmental bias and testing its implications in shaping phenotypic evolution. Specifically, I answer three questions: (1) How does the central conponent of nonlinear genotype-to-phentype map --- transcriptional regulation --- bias the analyses of gene-gene interactions? (2) How to disentangle the contribution of developmental bias in trait-trait interdependencies? (3) How expression variability affects gene retention and gene expression evolution following gene and genome duplication.",
        "authors": [
            "Haoran Cai"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158887",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Near-Optimal Learning and Planning in Separated Latent MDPs",
        "abstract": "We study computational and statistical aspects of learning Latent Markov Decision Processes (LMDPs). In this model, the learner interacts with an MDP drawn at the beginning of each epoch from an unknown mixture of MDPs. To sidestep known impossibility results, we consider several notions of δ-separation of the constituent MDPs. The main thrust of this paper is in establishing a nearly-sharp statistical threshold for the horizon length necessary for efficient learning. On the computational side, we show that under a weaker assumption of separability under the optimal policy, there is a quasi-polynomial algorithm with time complexity scaling in terms of the statistical threshold. We further show a near-matching time complexity lower bound under the exponential time hypothesis.",
        "authors": [
            "Fan Chen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158934",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "thesis in the field of Chemical Oceanography: Marine iodine biogeochemistry: inorganic speciation, redox dynamics and organic complexation",
        "abstract": "Iodine holds significant importance across various disciplines, including medicine, industrial processes, organic synthesis, paleoclimatology, atmospheric chemistry and modern climate science. The ocean, as a major surficial iodine reservoir and the primary source of this element to the atmosphere, plays a central role in global iodine cycling. Despite significant progress, key aspects of iodine cycling in the marine environment remain poorly understood. This thesis leverages recent advances in high-precision techniques, including liquid chromatography and mass spectrometry, to enhance our understanding of marine iodine biogeochemistry. Detailed analyses of the major inorganic iodine species in seawater, iodide and iodate, were conducted in the oligotrophic waters of the North Pacific and the oxygen minimum zones of the Eastern Tropical Pacific. The observed distributions reflect the impact of both in situ and ex situ processes on dissolved iodine concentrations, offering valuable insights into the prevalence and extent of anoxic conditions within oxygen minimum zones. Iodate formation rates were investigated through surface seawater incubations using iodide-129, a long-lived radioisotope, as a tracer. The experimental results underscore the pivotal role of particles in mediating redox transformations between iodide and iodate, while also emphasizing the significance of iodine species with intermediate oxidation states in these processes. Building on this observation, a significant focus of this thesis is the characterization of dissolved organic iodine in the ocean. Two innovative methodologies for identifying dissolved organic iodine compounds are presented. The first approach focuses on labelling cultures of the cyanobacterium Synechococcus with iodide-129 to generate a diagnostic isotopic pattern in resultant dissolved organic iodine complexes. The second approach employs sequential purification and isolation of a target compound from a large-volume seawater sample collected in the North Pacific. Collectively, the findings presented in this thesis significantly enhance our understanding of iodine cycling in the marine environment, offering novel insights into the distribution and composition of both inorganic and organic iodine, as well as the rates and dependencies governing iodine cycling processes. Furthermore, the methodologies introduced here pave the way for future research to elucidate the mechanisms driving iodine redox transformations in seawater, refine the marine distribution of inorganic iodine, and advance the molecular characterization of dissolved organic iodine.",
        "authors": [
            "Iulia-Mădălina Ștreangă"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158819",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Mechanisms of terrestrial organic carbon export and preservation in the marine environment",
        "abstract": "Export of terrestrial carbon from land to sea is a globally important carbon flux that is poorly constrained and has implication for atmospheric carbon levels over modern and geologic timescales. Many factors control the fate of exported carbon and the subsequent impact on carbon budgets, including the timescales of export, the composition of organic matter, and degradation processes. This thesis uses biomarkers, bulk geochemical tools, and incubation studies to interrogate the factors controlling terrestrial carbon export and preservation in the marine environment. The thesis focuses on two globally important river systems that collectively deliver 25% of the total terrestrial carbon flux to the ocean, the Ganges-Brahmaputra (G-B) Rivers and the Amazon River. The first two chapters focus on the G-B Rivers, utilizing compound specific biomarker analysis within a high sedimentation rate (30 cm/yr) terrestrial archive in the Bay of Bengal, we interrogate (i) timescales of organic carbon export from land to sea, and (ii) basin-scale geochemical responses to rice agriculture expansion. These analyses utilize the radiocarbon ages and stable carbon-13 isotopic composition of lipids produced by Archaea and Bacteria. We identify that ca. 75% of these biomarkers experience millennial scale storage in the G-B basin, in agreement with previously assessed plant-derived compounds, highlighting that an overarching soil stabilization mechanism controls the age of exported terrestrial organic matter. Individual biomarkers and bulk geochemical analysis chronicle the change in methane-derived soil carbon within the basin due to rice paddy expansion, highlighting that 49% of Bangladesh’s methane emissions from 1990-2008 have been abated by soil storage. The last two chapters focus on the Amazon River, to examine the fate of terrestrial organic carbon in the marine environment, (iii) utilizing geochemical analysis of historical sediments and sediments from a field campaign in 2023, and (iv) utilizing terrestrial and marine endmembers in incubation experiments simulating the dynamic coastal environment. Sediment geochemical and biomarker analyses highlight the preservation of an isotopically distinct terrestrial endmember in the coastal sediments, which has led to at least 50% underestimation of the burial efficiency. Quantitative stable isotope probing incubations using 13C-lignin indicate the dual role of microbially-mediated and photo-degradation, and highlight that the microbial communities primarily responsible for lignin degradation in the marine environment are of terrestrial origin, and identify a new ecological role for Bathyarchaeota. This thesis integrates diverse biogeochemical techniques across the terrestrial-marine interface to examine important open questions in globally important carbon budgets, merging isotope geochemistry, microbiology and earth science. The findings contribute to our understanding of the modern carbon cycle and the impact of anthropogenic perturbations of the last decades and into the future.",
        "authors": [
            "Brenna L. Boehman"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158872",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design Concepts for High-Acceleration Linear Actuators\r\nfor Precision Motion",
        "abstract": "Advances in semiconductor photolithography scanners have made it possible to produce smaller, more affordable chips with higher throughput. Some of the key lithographic scanner components supporting these advancements are electromagnetic actuators responsible for positioning the long-stroke (LS) and short-stroke (SS) stages of the reticle stage in its scan direction. Such actuators need to provide the highest thrust at the deceleration and reacceleration phases when the stages turn around at the ends of the scanning trajectory. Thus, enhancing their acceleration capability and force output is essential for boosting chip throughput. However, the improved performance may demand large current densities that are unsustainable in terms of the associated power dissipation generated by ohmic losses in the copper coils. In this thesis, we continued a previous study conducted in our lab that explored the use of mechanical contact forces managed by a piezoelectric stack actuator (PEA). In this configuration, intermittent contact by the PEA can be used to apply forces to decelerate and reaccelerate the SS stage with respect to the LS stage during turnaround events. With such force assist, the non-contact precision actuators responsible for positioning the SS stage with respect to the LS stage no longer need to generate large thrusts for the deceleration and reacceleration. As a result, we can in principle decrease the weight and power loss of the SS-stage precision actuators, which thus lowers the thrust requirements for the LS-stageAdvances in semiconductor photolithography scanners have made it possible to produce smaller, more affordable chips with higher throughput. Some of the key lithographic scanner components supporting these advancements are electromagnetic actuators responsible for positioning the long-stroke (LS) and short-stroke (SS) stages of the reticle stage in its scan direction. Such actuators need to provide the highest thrust at the deceleration and reacceleration phases when the stages turn around at the ends of the scanning trajectory. Thus, enhancing their acceleration capability and force output is essential for boosting chip throughput. However, the improved performance may demand large current densities that are unsustainable in terms of the associated power dissipation generated by ohmic losses in the copper coils. In this thesis, we continued a previous study conducted in our lab that explored the use of mechanical contact forces managed by a piezoelectric stack actuator (PEA). In this configuration, intermittent contact by the PEA can be used to apply forces to decelerate and reaccelerate the SS stage with respect to the LS stage during turnaround events. With such force assist, the non-contact precision actuators responsible for positioning the SS stage with respect to the LS stage no longer need to generate large thrusts for the deceleration and reacceleration. As a result, we can in principle decrease the weight and power loss of the SS-stage precision actuators, which thus lowers the thrust requirements for the LS-stage actuators responsible for accelerating both the LS and SS stages, resulting in lowered power consumption. Using the single degree-of-freedom experimental setup previously built in our lab, we conducted several characterization experiments to develop a PEA position feedback controller augmented by a hysteresis-compensated feedforward trajectory to shape the contact compression and forces. We find that introducing a viscoelastic contact interface is essential for stabilizing the PEA controller and slowing the contact dynamics to remain within the controller bandwidth. Our feedforward trajectory successfully brings a 0.84 kg mass moving towards the PEA with an initial speed of 60 mm/s to zero velocity in approximately 1.5 ms using 36 µm of PEA stroke length. These results demonstrate the feasibility of using PEAs as mechanical assist devices for high-acceleration turnaround events in lithography tools.",
        "authors": [
            "Adam K. Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158901",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Developing Telecom Band-Compatible Molecular Color Centers for Quantum Networking",
        "abstract": "Quantum networking is a new modality of information transmission that will revolutionize the future of telecommunications. However, the realization and widespread use of quantum networking demands low signal loss and distortion over long distances. To achieve this, prospective materials for quantum networking must emit in fiber optics’ optical communications band defined as 1260 to 1625 nm, commonly known as the “telecom band.” Vanadium dopants in silicon carbide have demonstrated near-infrared emission combined with a spin-photon interface, but these systems lack tunability over emission wavelength, preventing emission in the telecom band. This thesis combines the promising electronic structure of these dopants and the inherent tunability of molecular systems to create a family of luminescent paramagnetic vanadium complexes that can achieve both telecom band emission and generalized finetuned control over emission wavelength. Chapters 2 and 3 will outline approaches to target telecom band emission in a series of V_III complexes through a gradual and controlled increase of metal-ligand bonding covalency. This strategy culminates in a series of V_III complexes which tune emission wavelength from 1237 nm to 1424 nm, achieving emission into the telecom band. Chapter 4 will discuss the impact of these strategies on the magnetic properties and spin dynamics of these systems through an analysis of their behavior under high-frequency high-field EPR spectroscopy. This work provides a blueprint for the next generation of molecular spins with optical addressability in the near-infrared regime for applications in quantum networking.",
        "authors": [
            "Rianna Bliss Greer"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158936",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Encoder-Agnostic Learned Temporal Matching for Video Classification",
        "abstract": "In recent years, large transformer-based video encoder models have greatly advanced stateof-the-art performance on video classification tasks. However, these large models typically process videos by averaging embedding outputs from multiple clips over time to produce fixed-length representations. This approach fails to account for a variety of time-related features, such as variable video durations, chronological order of events, and temporal variance in feature significance. While methods for temporal modeling do exist, they often require significant architectural changes and expensive retraining, making them impractical for offthe-shelf, fine-tuned large encoders. To overcome these limitations, we propose DejaVid, an encoder-agnostic method that enhances model performance without the need for retraining or altering the architecture. Our framework converts a video into a variable-length temporal sequence of embeddings, which we call a multivariate time series (MTS). An MTS naturally preserves temporal order and accommodates variable video durations. We then learn pertimestep, per-feature weights over the encoded MTS frames, allowing us to account for variations in feature importance over time. We introduce a new neural network architecture inspired by traditional time series alignment algorithms for this learning task. Our evaluation demonstrates that DejaVid substantially improves the performance of a state-of-the-art large encoder, achieving leading Top-1 accuracy of 77.2% on Something-Something V2, 89.1% on Kinetics-400, and 88.6% on HMDB51, while adding fewer than 1.8% additional learnable parameters and requiring less than 3 hours of training time.",
        "authors": [
            "Darryl Ho"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158930",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Annealing Techniques for Color Center Formation",
        "abstract": "Color centers in diamond have emerged as leading atom-like quantum systems for applications spanning from quantum repeaters to sensors. However, the optical and spin properties of engineered diamond color centers are limited by crystal damage produced during ion implantation, crystal irradiation, and annealing. In this thesis, we develop advanced material processing methods and characterization techniques to address critical challenges in the formation of high-performance diamond color centers to advance towards the efficient creation of desired dopant-vacancy centers with minimal formation of deleterious multi-vacancy clusters.",
        "authors": [
            "Ian Christen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158913",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Singlet exciton fission-enhanced silicon photovoltaics: Interfacial engineering, device design and spectroscopic technique development",
        "abstract": "The growing global energy demand combined with resource and space limitations necessitate enhancements in crystalline silicon solar cells, which are the current dominant solar technology. However, their efficiencies have only increased incrementally over the recent 20 years, as they are starting to approach the theoretical efficiency limit. The main source of loss is thermalization, where energy in excess of the bandgap absorbed by silicon is lost as heat. Singlet exciton fission in organic molecules has been proposed to reduce these losses. By having the organic layer absorb the high energy light and transferring the triplet excitons generated from the singlet fission process to silicon, the photocurrent in this spectral region can be doubled, with the potential of raising the efficiency from the traditional limit of 29.4 % to up to 42 %.\r\n\r\nThe greatest challenge with these devices has been to demonstrate an increase in the silicon photocurrent, a necessary condition to show that the technology is viable. Scientifically, there are three main components to this problem. The first is to successfully couple the triplet excitons to silicon. The second is that not much is understood regarding the exciton and charge carrier dynamics at this interface. Finally, the silicon solar cell architecture should also be considered to extract transferred carriers effectively.\r\n\r\nThis thesis tackles these three parts from an interfacial materials, device architecture and spectroscopy approach. Using tetracene as the singlet fission layer and n-doped silicon, we show that defect-induced states in a thin interlayer of hafnium oxynitride that lie near the band edge of silicon are beneficial for triplet exciton transfer. We also identify that triplet-induced electric field-effect passivation is beneficial for the triplet sensitization process of silicon, and design a new bilayer interface consisting of a zinc phthalocyanine donor layer that introduces preferential near- silicon band edge states, and an ultrathin oxide chemical passivation layer. We then study various device architectures, confirming the importance of using a device designed to extract surface charge carriers efficiently, demonstrating the first enhancements in single-junction silicon solar cell external quantum efficiencies and photocurrent from singlet fission. Finally, we build and use advanced spectroscopy techniques and numerical frameworks to study exciton and charge carrier dynamics in singlet fission-sensitized solar cell materials, confirming that the triplet excitons are contributing to all the positive effects observed in the devices.\r\n\r\nThese results have shown that singlet fission-sensitized silicon solar cells are a viable technology for enhancing silicon solar cell efficiencies beyond the conventional single-junction limit. This interface remains a rich area for fundamental scientific studies, involving coupling between molecular dark states to bulk silicon. We hope that the key findings can help direct research efforts towards scalable implementation of this technology, and stress that the fundamental understanding of the interface also has broad implications to other silicon technologies that can benefit from enhanced quantum yields, including photodetectors.",
        "authors": [
            "Narumi Nagaya"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158864",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Use of System Theoretic Process Analysis (STPA) onNovel Tiltrotor Aircraft to Prevent Mode Confusion",
        "abstract": "Initiatives are underway to develop tiltrotor and vertical take-off and lift (VTOL) aircraft that enhance commercial and military aviation’s autonomy, capability, and survivability. These designs integrate rotary and fixed-wing elements, introducing distinct safety considerations. These safety concerns are largely due to the differing mental models of operators trained in either rotary or fixed-wing aviation, alongside the rising reliance on autonomy. The traditional hazard analysis techniques (e.g., Fault Tree Analysis and Failure Models and Effects Criticality Analysis) do not adequately account for system component interactions or human factors in complex new aircraft designs. System Theoretic Process Analysis (STPA) is a powerful new hazard analysis technique for novel tiltrotor aircraft that includes their unique safety requirements. It is a top-down system hazard analysis technique that identifies loss scenarios (N. G. Leveson and J. Thomas Mar2018). It satisfies the tasks described in MIL-STD-882E (Department of Defense 2023). This research demonstrates the use of STPA to identify and mitigate potential instances of mode confusion between the operator’s mental model and the autonomy’s decision logic in the uniquely dynamic tilt-rotorcraft environment. Two previous tiltrotor aircraft accidents are analyzed utilizing Causal Analysis based on System Theory (CAST) to help set a framework for the importance of human and machine collaboration in systems. These accidents show a trend in the dangers of aircraft system mismanagement between various controllers. The CAST results for these accidents help provide information about how to prevent these types of incidents in the future, setting the stage for the use of STPA on novel tiltrotor aircraft, as demonstrated in this thesis. STPA can be used before design, implementation, and fielding, allowing for better early design of systems and reducing the cost of later redesign or modification.",
        "authors": [
            "Natalie Ann Basnight"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158856",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "More than the sum of parts: deconstructing tissues in their spatial, temporal, and environmental contexts",
        "abstract": "The human body is composed of ~37,000,000,000,000 cells, exquisitely organized into tissues delivering emergent functions beyond individual cells’ capabilities (e.g., the brain’s seemingly-effortless computations, the liver’s wide-ranging chemical processing). In my PhD, I studied how healthy tissues arise from properties and interactions of constituent cells, and how disease outcomes stem from dysregulation of underlying cellular parts. 1) To study how cells’ spatial organization shapes tissue function, I created photochemistry tools to discover gradients in how immune cells combat cancer across a tumor’s core vs. periphery. 2) To then explore spatially-structured tissues, I turned to tuberculosis (TB) granulomas: just centimeters apart, the immune system can kill bacteria in one granuloma or permit years-long bacterial survival in another. Reconciling this paradox, I discovered that bacterial killing needs coordinated signaling across immune cells, but TB-permissive granulomas structurally remodel to inhibit TB spread at the expense of “walling out” immune cells. 3) Connecting disease to lifestyle exposures, I determined tobacco smoking increases TB risk via blood-to-lung migration of TB-permissive cells. 4) Intrigued by past stresses seeding future dysfunction, I studied similar themes in adaptations to high-fat diets, discovering tradeoffs where individual liver cells promote their own survival at the expense of reduced tissue function and increased cancer risk. Through these studies, I dissected tissues and diseases with unprecedented resolution via single-cell multi-omics and mechanistic perturbations, defining the parts, interactions, and causal regulators that underlie tissue (dys)function.",
        "authors": [
            "Constantine Tzouanas"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158827",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Designing Visual Intelligence from Photons to Action",
        "abstract": "For embodied agents to perceive and effectively act within their environment, they must sense the world around them and translate this information into meaningful and safe actions; a process fundamental to both biological and human-engineered systems. Nature has evolved highly attuned visual systems, resulting in diverse and efficient eyes capable of facilitating complex behaviors. Conversely, roboticists have engineered sophisticated cameras and sensors, enabling robots to perform tasks beyond the capabilities of natural systems. This thesis explores the design of visual intelligence by integrating insights from both biology and engineering in two complementary parts. In Part I, we computationally recreate the evolution of vision within simulated embodied agents. By evolving the physical and neural aspects of vision in simulation - and training these visually-capable agents with deep reinforcement learning - we demonstrate that task-specific environmental pressures lead to distinct eye morphologies and behaviors, mirroring observations in biological evolution. This in silico approach enables us to investigate the fundamental principles underlying the emergence of animal eyes and provides a framework for exploring novel sensor designs subject to both biological (e.g., survival) and engineering constraints (e.g., manufacturability). In Part II, we leverage visual cues not typically used in nature (i.e., active illumination and multi-bounce light) to demonstrate enhanced robotic navigation via non-line-of-sight imaging. Using single-photon LiDARs, we capture the temporal propagation of individual photons, enabling the detection of objects around corners. This sensing capability allows us to develop robots that effectively anticipate and avoid hidden obstacles, reducing navigation time by 50% and overall trajectory length by 33%. Together, these works demonstrate how the synthesis of biologically-inspired design principles with advanced sensing modalities can enhance embodied agents' capabilities, while providing insights into both natural vision evolution and robotic perception.",
        "authors": [
            "Aaron Young"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158899",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Verification of Go Channels",
        "abstract": "Goose is a tool for translating a subset of the Go programming language into Perennial/Iris, which is an extension of Coq. However, Goose did not support channels, which are an important synchronization tool that Go is well known for.\r\n\r\nThis thesis presents an extension to Goose to support channels, including a model to represent Go channels and operations in GooseLang, the language defined in Perennial/Iris that Goose translates into, an extension to the Goose translator to support channels, and a library of separation logic specifications that define the expected behavior of channel operations on open channels. Finally, this thesis evaluates how effective this model and library is for verifying Go code containing channels, and discuss some limitations and potential future work.",
        "authors": [
            "Jessica Zhang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159079",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "All Pass Readout With Ring Resonators for Qubit Measurement",
        "abstract": "Quantum computers may advance computing by solving some NP complexity problems, such as factoring and simulating quantum systems. Superconducting qubits, configurable artificial atoms comprised of circuit elements, are a leading platform to create quantum computers. Many schemes for superconducting qubit readout include a weakly coupled port as a capacitor in the feedline, which allows for directionality in the readout signal. However, this impedance mismatch creates problems with resonator linewidth variation, standing waves, and voltage nodes in the feedline, leading to challenges in scaling to larger frequency multiplexed systems. This thesis proposes an all-pass readout scheme that utilizes ring resonators that do not require a weakly coupled port, allowing for more modular qubit readout architectures.",
        "authors": [
            "Alicia Zang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159080",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Multimodal Graphical User Interface for 3D Model\r\nFabrication Through Generative AI",
        "abstract": "In recent years, three-dimensional model generation and manipulation through generative AI has seen significant developments. Current projects enable the generation of threedimensional assets from natural language prompts and input images, as well as functionalityaware model manipulation through mesh segmentation and categorization. However, all these workflows lack a coherent, unified platform that caters to users’ needs and each method’s technologies. Programs that rely on terminal-based commands lack the graphics needed for model interactions, and plugin extensions for 3D modeling applications are unintuitive and hard to extend for new functionalities. Additionally, both approaches require users to have prior computer engineering and/or 3D graphics knowledge. For this thesis, I propose the creation of a web-based, multimodal graphical user interface that consolidates all these different technologies in a single platform. By supporting model stylization and model generation (both from text prompts and input images), users can utilize combined workflows and expand the range of output possibilities for 3D asset creation. Other features in our interface include model uploading, saving, and downloading to enable a continuous stream of work on a single 3D asset. Apart from all this, we expand the current capabilities of existing image-to-3D generation programs by enabling users to combine up to six images together and create a merged 3D object. Each of these images corresponds to a view angle from which the outputted mesh will be built.",
        "authors": [
            "Isabel Báez Alicea"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159092",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Beyond Lifetime Value: A Customer Journey Analysis to\r\nFan Engagement and Spending in Professional Sports",
        "abstract": "This research examines engagement and spending behaviors in a professional sports ecosystem, introducing Customer Journey Analysis (CJA) as a dynamic alternative to traditional customer lifetime value (LTV) models. Through an analysis of over 930,000 net new fans acquired from July 1, 2021, to June 30, 2024, this study identifies critical patterns in acquisition channels, spending behaviors, and engagement metrics over multiple seasons. Notably, the findings highlight the significant influence of early touchpoints, such as ticket purchases and email interactions, on fan progression. Metrics like email open rates and multi-channel engagement emerge as strong predictors of future spending, revealing nuanced insights into fan behavior. This research emphasizes the importance of integrating behavioral and financial metrics to sustain fan involvement. By transitioning from static LTV models to a multi-dimensional CJA framework, actionable strategies are proposed for optimizing engagement channels, improving retention, and driving long-term revenue growth. Key findings reveal that predictive modeling and customer segmentation analysis are instrumental in identifying high-potential fans and distinct audience profiles. Tailored retention strategies, including personalized follow-ups and exclusive engagement incentives, address churn risks while fostering ramp-up and loyalty across diverse fan groups. Future work should explore tenured fan behaviors and incorporate diverse data sources, such as in-venue spending and team performance metrics, to deepen understanding of fan evolution across different lifecycle stages.",
        "authors": [
            "Christina Elizabeth Antonakakis"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159093",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Battery Pack Design and Transient Performance Modeling\r\nfor High-Power Legged Robots",
        "abstract": "Legged robotics has recently shifted toward advanced optimization-based control methods, such as Model Predictive Control (MPC), to generate agile and energy-efficient locomotion. By casting the control problem as an optimization task, robotic systems can account for complex robot dynamics and operational constraints, including joint limits and actuator capabilities. However, high-performance maneuvers also demand rigorous consideration of onboard battery constraints. This work presents an empirically derived lithium-ion battery model that captures transient voltage sag and time-dependent internal battery state, enabling more accurate prediction of feasible power delivery. Additionally, a custom high-power battery pack was designed to meet the power demands of the MIT Humanoid, emphasizing power density, safety, and maintainability. Although the work presented in this thesis does not integrate the battery model into a trajectory optimization framework, it establishes the foundation for future research that aims to couple battery and robot dynamics in robot control. Ultimately, this approach will facilitate safer and more capable legged robots by ensuring that planned trajectories respect both physical and electrochemical constraints.",
        "authors": [
            "Christopher K. Evagora"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159094",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On the Inductive Biases of Conditional Diffusion Models",
        "abstract": "Diffusion models have achieved remarkable progress in recent years across various domains and applications, but how diffusion models generalize is still not well understood. While prior work predominantly focuses on unconditional diffusion models, in this thesis we focus on understanding generalization for conditional diffusion models, which is especially relevant for modern text- or observation- conditioned applications. In particular, we are interested in the inductive biases of conditional diffusion models which predispose them to certain forms of interpolation in regions outside the support of the training data. We observe that neural networks are capable of learning qualitatively different forms of interpolation, which may be influenced by the architecture and capacity of the network and other aspects of neural network training. We develop a potential framework to model the interpolation behavior of neural networks via nonparametric estimation, which happens to have the property of being schedule consistent, or truly denoising at every time step. We find that, assuming a neural network with sufficient capacity, conditional diffusion models are biased towards smoothing, which can lead to non-schedule consistent behavior away from the training data and has a number of interesting consequences.",
        "authors": [
            "Christina Yu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159081",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Using Predictive Models to Identify Trends Among Successful Dual-Use Startups",
        "abstract": "This study examines predictive models for assessing the success of dual-use startups in the United States. Utilizing data from the Small Business Innovation Research (SBIR) and Small Business Technology Transfer (STTR) programs, this research focused on startups founded post-2000 to reflect contemporary technological advancements. A key objective of this study was to create a rich and comprehensive dataset, addressing gaps in the dualuse startup literature and providing a foundation for future research. Machine learning approaches, including Logistic Regression, Random Forest, and Gradient Boosting Machines, were applied to evaluate critical success factors, with XGBoost identified as the most effective model. Despite the challenges of class imbalance, the study highlights the potential of data-driven methodologies to uncover trends and inform strategies for supporting dual-use startups. By integrating predictive modeling with the construction of a robust dataset, this research contributes both to the academic understanding of dual-use innovation ecosystems and to practical frameworks for fostering their growth.",
        "authors": [
            "Samantha Ying"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159082",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "ALFA-Chains: An Artificial Intelligence Approach to Exploit Chain Discovery in Networks",
        "abstract": "Exploit chains play a crucial role in advanced persistent threats (APTs) and other malicious cyber campaigns. Sophisticated attackers can navigate across a network, escalate their privileges, and compromise valuable targets by executing the right exploits in the right order. However, finding these exploits chains is a challenging task requiring a broad knowledge of the vulnerabilities present in computer systems and the exploits that take advantage of them. Networks can be complex, with many hosts and intricate software stacks. Moreover, the range of known exploits and vulnerabilities is constantly growing, complicating the process of determining how they can be linked. This thesis introduces a solution, ALFA-Chains, that automates the discovery of exploit chains by leveraging classical AI planning, Large Language Models (LLMs), and existing exploit/vulnerability databases. ALFA-Chains describes networks and exploits using the Planning Domain Description Language (PDDL), a formal language to represent planning problems. This allows us to use optimized off-the-shelf planners that have been developed by the AI planning community over many years. Our system takes natural language descriptions of exploits and classifies them into categories based on their preconditions and effects. From this intermediary representation, we can programmatically generate PDDL that captures the requirements needed to run the exploit and the access gained by the attacker. Due to this automated approach, ALFA-Chains is able to consider a vast set of exploits when determining if a network is susceptible to exploit chaining. We show how ALFA-Chains can process 1,880 Metasploit exploits and their corresponding 2,002 CVEs to detect exploit chains in a variety of realistic network configurations. We proceed to discuss potential applications of ALFA-Chains, including automated penetration testing and vulnerability prioritization.",
        "authors": [
            "Miguel A. Tulla Lizardi"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159083",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Simulating Weather For A Mixed Reality Platform",
        "abstract": "Complex systems are inherently difficult to teach in a traditional classroom setting. The We’re In This Together (WIT) project aims to provide a different teaching strategy by using AR/VR headsets to situate the students directly inside the system. WIT’s first game attempts to tackle common weather concepts including precipitation and fronts; however, the most recent version fails to demonstrate and model the concepts in an accurate and comprehensible way. This project focuses on developing a brand-new simulation layer for the game that better captures the causes behind common weather phenomena. The new simulation uses a particle-based approach to model the movement of air in the atmosphere and creates a more thorough and interactive experience to help students explore the various aspects of weather.",
        "authors": [
            "Hao Ni"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159089",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "GIM: Guidance as Initialization Method",
        "abstract": "This work makes two contributions: the evaluation of early stop guidance for deep Fully Connected Networks (FCNs) and the introduction of guidance as an initialization method (GIM). Network initialization has been a meaningful and challenging topic in the field of machine learning (ML) for a long time. Many initialization methods exist, ranging from data-independent to data-dependent approaches. Initializations allow for a better understanding of model behavior and improvements in model performance. The novel guidance tool enabled us to propose GIM, a new technique that initializes a model by leveraging representational similarity with respect to models of different architectures. A model with an architecture that performs poorly in a specific task can be initialized with guidance from a model with an architecture that performs well in the respective task. We focus on the case of FCNs in the task of image classification and provide experimental results to validate our approach.",
        "authors": [
            "Juan Sebastian Duitama Cortes"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159090",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Convergence of the Arnoldi Iteration for Estimating Extreme Eigenvalues",
        "abstract": "Krylov subspace methods, like the Arnoldi iteration, are a powerful tool for efficiently solving high-dimensional linear algebra problems. In this work, we analyze the convergence of Krylov methods for estimating the numerical range of a matrix. Prior bounds on approximation error often depend on eigenvalue gaps of the matrix, which lead to weaker bounds than observed in practice, specifically in applications where these gaps are small. Instead, we extend a line of work proving gap-independent bounds for the Lanczos method, which depend only on the matrix dimensions and number of iterations, to the more general Arnoldi case.",
        "authors": [
            "Cecilia Chen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159091",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Realistic Tactile Stylization for Digital Fabrication using Enhanced UV Unwrapping Method",
        "abstract": "While recent advances in Generative AI enable visual stylization of 3D models using image prompts, they typically neglect tactile properties. TactStyle addresses this limitation by enabling creators to enhance 3D models with both visual and tactile properties derived from texture images. Using a fine-tuned image-generation model, TactStyle generates highly accurate heightfields that faithfully replicate the tactile properties of input visual textures and applies them to 3D models. However, applying textures to 3D models presents challenges, such as ensuring even texture resolution, avoiding texture warping, and minimizing visible seams. TactStyle’s current implementation often struggles with significant texture stretching and distortion caused by poor UV mapping, compromising the accurate heightfields and diminishing the tactile fidelity of printed models. Our research systematically evaluates various UV unwrapping methods, including alternative UV projections and an optimization-based neural UV mapping, to improve the realism and accuracy of texture application on 3D models in digital fabrication. Building on these findings, we will release a Blender plugin that integrates the optimal UV unwrapping methods with TactStyle, enabling creators to easily customize their 3D models with accurate tactile properties using only reference texture images. This work enhances the practicality and accessibility of tactile 3D model customization, bridging the gap between visual and tactile design elements.",
        "authors": [
            "Zoe Wong"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159088",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Lifting 2D Vision Models into Structured Scene Representations",
        "abstract": "Intelligent agents can leverage structured scene representations capable of capturing object compositionality, affordances, and semantics as a world emulator. However, 3D scene data is limited, rendering supervised and self-supervised methods ineffective. Recent advances in 2D foundation models exhibit remarkable performance and generalization. Concurrently, several works have demonstrated lifting feature maps produced by these models into a 3D feature representation. This thesis further explores how lifting can be effectively employed to construct pixel-level fidelity structured scene representations.\r\n\r\nLearned scene representations such as NeRF and Gaussian Splatting do not support additional functionality besides novel view rendering. The world is compositional: a scene can be described in terms of objects. Correspondingly, we present a lifting solution for efficient open-set 3D instance segmentation of learned scene representations. Compared to previous approaches, our solution is more than an order of magnitude faster and can handle scenes with orders of magnitude more instances.\r\n\r\nToward identifying affordances, we tackle the problem of zero-shot mesh part segmentation. Learning-based mesh segmentation does not generalize due to a lack of diverse mesh segmentation datasets, while traditional shape analysis methods are overfitted to previous benchmarks. We present a lifting solution for mesh part segmentation that overcomes these limitations, showing comparable performance to top-performing shape-analysis methods on traditional benchmarks while exhibiting much better generalization on a novel mesh dataset curated from an image-to-3D model.\r\n\r\nBeyond feature fields, lifting can be used for a variety of applications, including scene understanding and editing. However, current lifting formulations are inefficient and often exhibit additional unintended modifications. To address these deficiencies, we generalize lifting to semantic lifting, which incorporates per-view masks indicating relevant areas. These masks are determined by querying corresponding per-view feature maps derived from feature fields. However, it is impractical to store per-view feature maps, and the scene representations can be expensive to store and query. To enable lightweight, on-demand retrieval of pixel-aligned relevance masks, we introduce a Vector Quantized Feature Field. We demonstrate the effectiveness of semantic lifting with our method on complex indoor and outdoor scenes from the LERF dataset.",
        "authors": [
            "George Tang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159118",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Transformers as Empirical Bayes Estimators The Poisson Model",
        "abstract": "We study the ability of transformers to perform In Context Learning (ICL) in the setting of Empirical Bayes for the Poison Model. On the theoretical side, we demonstrate the expressibility of transformers by formulating a way to approximate the Robbins estimator, the first empirical Bayes estimator for the Poisson model. On the empirical side, we show that transformers pre-trained on synthetic data can generalize to unseen prior and sequence lengths, outperforming existing methods like Robbins, NPMLE, and ERM monotone in efficiency and accuracy. By studying the internal behavior of the representations of the intermediate layers of these transformers, we found that the representation converges quickly and smoothly over the layers. We also demonstrate that it’s unlikely transformers are implementing Robbin’s or NPMLE estimators in context.",
        "authors": [
            "Mark Jabbour"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159119",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Enabling Semantically Grounded, Long Horizon Planning\r\nand Execution for Autonomous Agents",
        "abstract": "Robots have been playing an ever increasing role in complex environments, often in coordination with teams of systems or humans. Autonomous systems of the future will need to be tightly grounded in the real world, drawing information directly from their environment to develop an understanding of the world. They will need to maintain a semantic understanding of their environment, including the kinds of objects they observe and their relationships to each other. At the same time, they must be able to reason over diverse constraints related to their tasks, such as time limits and resource usage. While there are existing approaches which enable robots to execute tasks with semantic goals, such as finding a certain type of object in a room, they often fail to consider the multitude fo task specific constraints which are vital to robust performance. On the other hand, planners which consider task specific constraints require a human to provide all information about the environment manually. These systems are too cumbersome to model complex tasks, requiring hours of manual effort which is prone to errors. This thesis presents an architecture for semantically grounded planning which leverages the strengths of constraint based planners while automating the environmental modeling step with an advanced semantic perception engine. By automating environmental modeling, we are able to create a system which executes complex semantically grounded tasks such as navigating to certain objects within a certain room, without major user input which is typically required of these systems.",
        "authors": [
            "Lucian Covarrubias"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159120",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Strategizing against online learners in normal form repeated\r\ngames",
        "abstract": "With the advent of machine learning and AI, learning algorithms are becoming more and more prevalent in online learning settings, where sequential decision-making is required. In such settings, the decisions of each agent can affect the utilities (or losses) of the other agents, as well as influence the decisions made by other agents later on in the interaction. Therefore, if an agent is good at anticipating the behavior of the other agents, in particular how they will make decisions in each round as a function of their experience thus far, he could try to judiciously make his own decisions over the rounds of the interaction so as to influence the other agents to behave in a way that ultimately benefits his own utility. In this thesis, we study repeated two-player games involving two agents: a learner, which employs an online learning algorithm to choose his strategy in each round; and an optimizer, which knows the learner’s utility function, parameters and the learner’s online learning algorithm. The optimizer wants to plan ahead to maximize his own utility while taking into account the learner’s behavior. We study this setting in zero-sum and general-sum games. In zero-sum games, we provide algorithms for the optimizer that can efficiently exploit a learner that employs a specific online learning algorithm in discrete and continuous-time dynamics. Specifically, the learner employs the Multiplicative Weights Update (MWU) algorithm for the discrete-time games, and the Replicator Dynamics in the continuous-time games. In general-sum games, we provide a negative result. Our negative result shows that, unless P=NP, there is no Fully Polynomial Time Approximation Scheme (FPTAS) for maximizing the utility of an optimizer against a learner that best responds to the history in each round. We additionally provide exponential-time algorithms that efficiently strategize against a learner that uses MWU, as well as a new way of thinking about strategizing against online learners via calculus of variations.",
        "authors": [
            "Angelos Assos"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159121",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design and Implementation of a Nonblocking Randomized Work Stealing Scheduler",
        "abstract": "This thesis presents FLCN (Free of Locks, Cilk is Now), a nonblocking work-stealing runtime scheduler that supports Cilk multithreaded programming. The existing OpenCilk runtime system uses lock-based synchronization and thus suffers from lock contention, does not provide progress guarantees, and can experience performance degradation with high worker counts and in multiprogrammed scenarios. FLCN leverages the existing runtime system’s provably efficient scheduling algorithm and introduces several new data structures and concurrency protocols to form a correct and performant lock-free system. In addition to enabling fork-join task parallelism, FLCN supports other Cilk features such as reducer hyperobjects. Through analyzing the performance of FLCN on various canonical benchmark programs, I find that for programs with low amounts of work, FLCN performs worse than the existing runtime. However, for most programs, I find that FLCN is either competitive with or marginally outperforms the existing runtime. Additionally, FLCN consistently exhibits higher scalability than the existing runtime, performing especially better when using hyperthreads and in multiprogrammed environments. I also outline future work that could make FLCN a more comprehensive and performant system, including ideas for improving FLCN’s work efficiency that would in turn better its performance on programs with low amounts of work.",
        "authors": [
            "Sabiyyah Ali"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159144",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Productivity in the Workplace for Product Development Teams",
        "abstract": "Productivity is a measure of the value generated for every hour worked. In a product development team, productivity can be affected by endogenous and exogenous factors, such as biological rhythms, work style, availability, work interruptions, team size, location, and the management strategies taken in a project. These factors will have an effect on the amount of effective work value generated in a workweek.\r\n\r\nA mathematical model and a Monte Carlo simulation were used to quantitatively assess the impact of these factors on the estimated cost and duration of a product development project. Based on the model results, we determined that workweek capacity and interruptions in the workplace are central to productivity. In addition, we demonstrated that combining different management strategies could be used to bring the project back on schedule and within budget to reduce the effects of these inefficiencies due to diverse endogenous and exogenous factors.\r\n\r\nFor these reasons, this case study on a product development project will provide insight to engineering managers and project leaders about the effects of these inefficiencies in the workplace. The findings will help pave the way toward a more accurate project estimation and better modeling of project dynamics to reduce the amount of uncertainty in product development teams.",
        "authors": [
            "Jorge Farfan Perdomo"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159145",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Digital Thread Maturity in Manufacturing: A Cross-Industry Study Using the Model-Based Enterprise Capability Assessment Framework",
        "abstract": "Modern-day manufacturing organizations find themselves in volatile and competitive markets with increasing pressure to deliver products faster, at lower cost, and with increased quality. In response to this pressure, many organizations are considering how technological advancements may improve the efficiency of their product development operations. Leading organizations have digitally transformed their businesses by shifting away from manual processes, static documents, and siloed operations toward automation, model-based data, and interconnectivity enabled by a digital thread. Accordingly, organizations pursuing the competitive edge offered through the digitalization of their business operations have often used different assessment tools to benchmark their current capabilities and define their vision for the future of their organizational operations.\r\n\r\nThis thesis proposes a set of model-based and digital thread capabilities that are central to the long-term success of product development operations, along with a corresponding maturity model that may be used to identify gaps between current- and future-state capability implementation. Using the proposed capability maturity model, known as the Model-based Enterprise Capability Assessment Framework (MECAF), this study evaluated and compared capability maturity across various organizations in the Aerospace and Defense, Automotive, and Heavy Machinery industries. Through interviews with each participating organization, this thesis also explores the expected benefits, common challenges, and anticipated value of implementing model-based capabilities. Additionally, this thesis proposes an approach to bridging the gap from strategy to implementation based on the lessons learned and best practices of the organizations studied.",
        "authors": [
            "Michael Scott Peters"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159146",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Detecting Expertise Influence on Teamwork in Sustainable Urban Design Workshops through a System Model",
        "abstract": "The design of sustainable urban communities near transportation hubs, such as train stations, may play a vital role in enhancing neighborhoods by fostering new jobs, encouraging mixed-use developments, and promoting a cleaner environment. The engagement of experts and non-experts is often promoted as part of the urban planning process, yet workshops, while motivating, do not necessarily affect the systems design and long-term sustainability of the neighborhood in a substantive way.\r\n \r\nPrior studies present methods for detecting teamwork during the design of complex systems, including model-based co-creation and urban design workshops. While interactive model-based workshops promote increased engagement of non-experts, the traditional role of experts in framing the design options and the workshop dialogue remain. This thesis research seeks to examine how expertise shapes decision-making in urban sustainability contexts using enhanced system models. \r\n \r\nThe research approach focuses on sustainable urban design workshops for compact city development, following three key steps.  First, a neighborhood system model incorporating a commute flow simulator is developed to support collaborative exploration and design decision-making processes. Second, during a pilot experimental workshop, participants are divided into control and treatment groups, challenged to design a vibrant community with economic, social, and environmental benefits. The treatment group receives an expert-proposed, advocated solution to assess its impact on exploration and decision-making. Finally, results are analyzed using Large Language Models (LLMs) and statistical methods to assess how expert-driven solutions impact teamwork collaboration, decision-making speed, and final design alignment with the advocated solution.\r\n\r\nWhile the pilot workshop primarily serves to validate the approach and test the methodology, conclusive results cannot be drawn due to its exploratory nature. Nevertheless, this research successfully developed a robust urban design system model, enabling stakeholders to generate innovative solutions that foster a thriving community. Additionally, it established a methodology to advance the understanding of expertise in teamwork dynamics, laying a strong foundation for future studies in teamwork analysis and urban design challenges.",
        "authors": [
            "Chen Li"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159136",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Scalable and Sustainable Microwave Power Beaming to\r\nMobile Lunar Surface Assets",
        "abstract": "Lunar missions are hindered by the challenges of maintaining continuous operation, especially during the 14-day lunar night, when solar power sources may be unavailable, causing significant mission delays and limiting efficiency. Frequent returns to charging stations supplied by fixed lunar surface power plants further disrupt workflows and restrict the operational range of lunar vehicles. To address these issues and enhance lunar mission performance, a continuous, secure, and shareable power source is essential. While nuclear power and larger battery systems are viable options for continuous lunar energy supply, they pose challenges such as safety risks, complex deployment, and limited scalability. This thesis focuses on exploring microwave-beamed power systems as a flexible and scalable solution for sustained lunar operations. Ideally, the power source would enable 24/7 operations without requiring vehicles to return to base stations, allowing for unrestricted navigation across the lunar surface, including in permanently shadowed regions (PSR). In addition, it would support the construction of critical infrastructure, accelerating the development of the lunar economy. This thesis aims to support sustained lunar exploration and infrastructure development by exploring the design space for microwave-beamed power systems under three different demand use cases of increasing scale, loosely corresponding to the three phases of the Artemis program: Local (Shackleton Crater), Regional (navigation between equatorial regions and South Pole), and Global (entire lunar surface). A case study focused on the YUTU-2 lunar rover investigates alternative architectures for each use case, comparing power beaming from tall towers vs. satellites. Evaluation reveals that the most effective solution for the Local use case is a tower-based approach featuring a single 100m tower, >10,000 solar modules, and using 1 GHz operating frequency, at a cost of $3.4M/W. For the Regional use case, a satellite-based solution is preferred, utilizing 6-7 satellites per plane, 210,000 solar modules, and a frequency range of 1.0 GHz, at a cost of $1.7M/W - $1.8M/W. The Global use case also favors a satellite-based approach, employing 6 satellites per plane across 5 polar planes, with varying numbers of solar modules and utilizing a frequency of 1 GHz, at a cost of $0.8M/W. The trade studies showed that larger receiver antenna areas and lower frequencies improve performance and cost-effectiveness. Furthermore, larger microwave-beamed power systems leverage economies of scale, lowering the cost per watt by an average of $1M/W when scaling from the Regional to the Global power system, with potential for further reductions through future expansions.",
        "authors": [
            "Chu Pang Alex Ng"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159137",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Study of High Harmonic Fast Waves Interactions in the Scrape off Layer of NSTX-U",
        "abstract": "High-harmonic fast wave (HHFW) heating experiments in the National Spherical Torus Experiment (NSTX) at Princeton Plasma Physics Laboratory (PPPL) have shown that up to 60% of the injected power can be lost in the Scrape-Off Layer (SOL) when the fast wave is able to propagate in front of the antenna [Hosea, Phys. Plasmas 15, 056104 (2008))]. This work discusses progress in modeling HHFW propagation and losses in the divertor region using more realistic SOL plasmas in the NSTX-U SOL 2D geometry. Previous RF studies assume density is a function only of magnetic flux, decaying exponentially, which may be insufficient to accurately determine the wavefield, especially in the divertor and high-field side plasma regions. In this work, the temperature profile is first evaluated by solving the non-linear heat conduction equation using a finite element approach in the Petra-M workbench assuming axisymmetry. A 2D density profile is then obtained from a prescribed outer midplane radial profile assuming pressure is uniform on a flux surface. This approach results in density and temperature profiles in which the strong asymmetric nature of diffusion is successfully captured. In particular, it is shown that for a parallel to perpendicular heat conduction anisotropy ratio of up to 10⁸ the expected exponentially decaying temperature profile is obtained using a non-linear iterative solver with proper mesh refinement conditions. Furthermore, this work focuses on investigating the effect of the SOL plasma density profile on the fast-wave propagation at different antenna phasing. The simulation results show that the gradient of the midplane density profile affects the wavefield pattern. As the density profile broadens, the wavefield intensity is reduced in the SOL and increased in the core. Finally, HHFW power in the plasma was studied by adding electron-ion collision power dissipation as a proxy for HHFW power deposition. The simulation results show that increasing the density gap width between the antenna and the core results in more power deposited in the SOL relative to the core.",
        "authors": [
            "Ricardo Antonio De Levante Rodriguez"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159138",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Quantum Economic Advantage Calculator: An Extension of the Quantum Tortoise and Classical Hare Framework",
        "abstract": "For some algorithmic problems, quantum computation has the potential to provide enormous speedups over classical computers. However, the drastic slowdowns associated with running error-free quantum hardware make achieving these theoretical advantages challenging. Researchers and industry leaders planning for the future would benefit from understanding when it will be both feasible and advantageous to switch to quantum computing platforms. This thesis builds on the framework by Choi, Moses, and Thompson (2023) to evaluate the feasibility and timeline for achieving Quantum Economic Advantage (QEA)—the point at which quantum hardware can outperform comparably-priced classical machines for specific computational tasks. This thesis substantially extends and deepens this framework and introduces a calculator to make these analyses accessible. The model incorporates parameters from quantum hardware vendors, such as physical-logical qubit ratios and overall connectivity, alongside the computational complexities of specific problems, to estimate the year of QEA. Most of the parameters in the tool are freely adjustable, allowing users to explore how varying assumptions about quantum improvement and technological advancement influence the projected timeline for QEA.",
        "authors": [
            "Frederick Mejia"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159127",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Causal Representation Learning for Predicting Genetic Perturbation Effects on Single Cells",
        "abstract": "Advances in sequencing technologies have significantly deepened our understanding of gene regulation in cells. Among these, Perturb-seq has emerged as a powerful technique, enabling high-resolution profiling of transcriptomic responses to genetic perturbations at the single-cell level. Such insights have profound implications for functional genomics and the identification of therapeutic targets. This thesis investigates the efficacy of mechanistic computational models for predicting the effects of previously unseen genetic perturbations on cellular expression profiles. While existing deep learning approaches excel at interpolating within observational data, they often struggle to extrapolate to novel perturbations. To address this limitation, this study introduces a hybrid framework that integrates a linear causal model, grounded in the gene regulatory network, with variational deep learning techniques.\r\n\r\nThe proposed mechanistic model utilizes a learned gene regulatory network to represent perturbational effects as shift interventions that propagate through the network. This approach operates within a low-dimensional gene space, effectively capturing the essential information needed to reconstruct full transcriptomic profiles. By incorporating this mechanistic causal model into a variational autoencoder (VAE), the framework generates detailed and comprehensive transcriptomic responses while maintaining the capacity to handle noisy, large-scale single-cell data.\r\n\r\nTwo deep variational architectures are explored within this framework, corresponding to different output distributions. The single cell variational inference (SCVI) architecture, employing a zero-inflated negative binomial output distribution, demonstrates challenges in learning perturbational data distributions. In contrast, a standard VAE architecture with a Gaussian output distribution on normalized gene expressions, when paired with the structural causal model, achieves superior performance compared to current state-of-the-art methods. This hybrid approach, termed the Single-Cell Causal Variational Autoencoder (SCCVAE), demonstrates robust capabilities in both interpolation and extrapolation.\r\n\r\nFor observed perturbations, the SCCVAE framework reveals latent representations that identify functional perturbation modules and simulate single-gene knock-down experiments across varying penetrance levels. These findings highlight SCCVAE as a powerful tool for interpreting and predicting perturbational responses at the single-cell level, advancing the integration of causal and variational approaches in computational biology.",
        "authors": [
            "Emily Liu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159128",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Leveraging Blockchain Technology for Enhancing Genomic Data Management: A Multidisciplinary Framework for Privacy, Trust, Identity Protection, and Equity",
        "abstract": "The effective adoption of blockchain technology in genomic data management is influenced not only by its technical advantages but also by external factors such as regulatory conditions, and the demands of consumers and patients. This thesis explores the critical factors required for blockchain platforms to thrive in managing genomic data, focusing on how these systems can be structured to address the high-priority needs of various stakeholders, including patients, healthcare providers, regulators, and researchers. Through a comprehensive examination of privacy, security, regulatory compliance, and equity concerns, the research develops a multidisciplinary framework that balances technological innovation with real-world stakeholder expectations. By conducting an in-depth stakeholder analysis and analyzing existing blockchain platforms used for genomics, the thesis presents a roadmap for creating blockchain solutions that are both technologically viable and aligned with the complex social, legal, and ethical landscape of genomic data management. This framework aims to maximize value for all stakeholders while mitigating associated risks, positioning blockchain as a viable tool in the future of personalized medicine.",
        "authors": [
            "Yuner A. Niu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159129",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Healthcare Agents: Large Language Models in Health Prediction and Decision-Making",
        "abstract": "Large Language Models (LLMs) are transforming healthcare, yet utilizing them for clinical applications presents significant challenges. In this thesis, we explore two critical aspects in healthcare AI: (1) leveraging LLMs for multimodal health prediction from wearable sensor data and (2) developing collaborative AI framework for medical decision-making. We first introduce a Health-LLM framework that performs multimodal fusion of temporal physiological signals from wearable devices with contextual metadata to predict health outcomes. By implementing novel context enhancement strategies, our framework demonstrates significant improvements in prediction accuracy across multiple health domains compared to existing benchmarks. Furthermore, we present MDAgents, an adaptive framework that optimizes multi-agent LLM collaboration for complex medical reasoning tasks. MDAgents dynamically configures agent roles and interaction patterns based on task complexity, implementing a hierarchical consensus mechanism that emulates clinical team dynamics. Through comprehensive evaluation on medical diagnosis and reasoning tasks, MDAgents exhibits superior performance in\r\nmultimodal medical reasoning compared to single-agent approaches. Our findings demonstrate that LLMs, when architected for multimodal integration and strategic collaboration, can serve as robust agents in healthcare systems, advancing both preventive medicine through continuous health monitoring and clinical decision support through distributed AI reasoning.",
        "authors": [
            "Yubin Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159124",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Three Essays on the Economics of Land Use, Environmental Value, and Public Spending",
        "abstract": "Across the world, public spending on government programs profoundly alters land use, preservation of environmental value, and the wellbeing of rural populations. These essays explore three such programs and derive lessons for improving their targeting. Chapter 1 tests the effect of conservation easement tax incentives on land conservation in Virginia, using a difference-in-difference design around a 2002 tax reform. This finds that the environmental quality distribution of easements is wide and matches the statewide quality distribution of all undeveloped land, suggesting the program has considerable room to improve targeting. Increasing tax incentives attract donations of similar or lower quality, but targeting tax incentives only at high-quality land would substantially increase high-quality acres at a cost of 1.18 low-quality acres per high-quality acre. Chapter 2 investigates the targeting of short-term incentives for long-term behavior change, focusing on the case of the EQIP agricultural incentives program. The model connects the short-term and long-term effects of incentives as products of the immediate adoption costs and long-term repeated costs and benefits of a practice. If populations vary primarily by adoption cost, targeting groups with the greatest short-term effect will also maximize the long-term effect. If populations vary primarily by long-term costs and benefits, the groups with the greatest short-term impact are those for whom the practice is highly unprofitable in the long run, and a program can improve long-term impacts by instead targeting those for whom the practice is slightly profitable in the long run. A discontinuity analysis comparing successful and unsuccessful EQIP applicants shows that EQIP induces significant short-term change. Chapter 3 investigates the behavior of Mongolian livestock markets after severe weather shocks, and the role that a livestock insurance program may play in smoothing shocks. During severe Mongolian winters, livestock sales increase and prices fall as credit-constrained nomadic herders look to make necessary investments to protect their remaining herd. National integration in livestock markets absorbs a significant share of the weather-related shocks, as 40-60% of district price risk is due to national market fluctuations and 20-40% is due to province effects. This paper finds that national mortality strongly drives price variations, and livestock insurance reduces sales during high-mortality periods.",
        "authors": [
            "Kelsey R. Larson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159125",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Structure, Function, and Interaction in Protein Language Models",
        "abstract": "In recent years, transformer architectures have shown remarkable capabilities in learning meaningful representations from text and images. This approach has been extended to the realm of protein sequences through pretrained protein language models, which have excelled in various protein engineering tasks. In this thesis, we investigate a pre-trained protein language model’s ability to predict protein structure and the effects of mutations. For many advanced protein understanding tasks, such as predicting protein function and protein-protein interactions, fine-tuning of the model is essential. We explore methods to fine-tune the Evolutionary Scale Modeling (ESM2) model, a pretrained protein language model, for predicting protein functions structured as Gene Ontology terms and predicting protein-protein interactions. Notably, we develop a novel method of modeling the hierarchy constraint in GO term prediction that improves training convergence and test performance while making the model hierarchically consistent with GO. This research aims to enhance our understanding of protein language models in decoding complex biological information, thereby contributing to advancements in computational biology.",
        "authors": [
            "Jared Zheng"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159126",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Bridging the Gap: Generative Machines and Inventive Minds",
        "abstract": "Recording technologies, from the phonograph to digital media, have profoundly reshaped the human experience by enabling the capture and reproduction of our sensory world. These technologies allow us to relive experiences through artifacts of remarkable fidelity like photographs and videos, extending the reach of our perception and memory. Of course, we didn’t stop at the phonograph; we have built a rich ecosystem of tools for creating, sharing, and exploring recorded media that have had transformative effects on cognition and culture. Recently, a new and powerful class of tools has emerged: generative models. Unlike recorded media, which reproduces external experiences, generative models can translate our ideas directly into artifacts. Here, ideas refer to abstract mental constructs that seed media creation, externally expressed in text prompts, sketches, vocalizations, or other intuitive representations. Just as recorded media augmented our ability to perceive and remember, generative media promises to expand our ability to imagine and invent by offering a more immediate path from cognition to high fidelity creation. Creative work often has us operating at our limits, negotiating boundaries between knowledge and novelty, skill and aspiration, from individual exploration to collective understanding. Generative models, in principle, have the potential to scaffold and accelerate how we transcend these limits by increasing the efficiency with which we discover and pursue new ideas. In this thesis, I suggest that realizing this potential presents a complex set of challenges that span computation and design. I argue that it requires us to develop a rich stack of precision tools for human-AI co-creation, as we have done and continue to do for recorded media. Specifically, I present contributions across two key dimensions of this:\r\n1. Computational machinery that supports creative work. I present research on topics including visually-driven acoustic simulation, interpretable and controllable sound generation from descriptions, and audiovisual content understanding. Focusing on sound as a case study, I describe systems that effectively represent and manipulate creative knowledge across modalities and levels of abstraction. \r\n2. Interactive systems and studies that investigate the integration of human and machine effort in content creation. This includes work on conceptual integration in AI-assisted story writing, author-in-the-loop description authoring for accessibility of complex scientific figures, and generative constraints for human ideation. In all, this work seeks insights for designing systems that support human creators through exploration, collaboration, and feedback, rather than aiming to replace or constrain human agency and expertise. \r\nTo conclude this thesis, I present a discussion on bridging AI and HCI to gain insights into human creative work and develop stable, generalizable design knowledge for augmenting it. I argue for the design of flexible, parametric tools that enable systematic study of creative behavior under different augmentation designs. Based on this, I propose a conceptual framework to seed the development of a more robust science of human-AI co-creation.",
        "authors": [
            "Nikhil Singh"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159134",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Generative Discovery via Reinforcement Learning",
        "abstract": "Discovering new knowledge is crucial for technological advancement and mirrors how humans and animals learn new skills—often through trial and error. Ancient humans, for example, discovered fire by experimenting with different methods, and children learned to walk and use tools through repeated attempts and failures. In chemistry, scientists find new catalysts by testing various compositions. But how exactly do humans use trial-and-error to improve existing solutions (like learning more efficient ways to walk or synthesizing novel compounds)? Can we design computational models that mimic or exceed human discovery? Such computational models could greatly accelerate progress in science and engineering since they can automate or assist human scientists’ and engineers’ works and discover new knowledge more efficiently (e.g., new compounds, streamlining the robot controller design, etc.). Reinforcement learning (RL) is well-suited for discovery tasks because it enables machines to learn through trial and error. My work overcomes the following major limitation of today’s RL algorithms and thereby advances their discovery potential: Mitigate the bias of reward shaping. RL relies on reward signals from trial-anderror experience, but these signals can be sparse, meaning they are only provided once a desired solution is found and otherwise zero. Most trials, therefore, offer little to no feedback. A common strategy to improve performance under sparse rewards is to provide additional hints (i.e., reward shaping) to guide RL algorithms. However, if these hints are inaccurate, they can steer the algorithm toward worse solutions than those without them. I propose a new RL framework that can be combined with any standard RL algorithm, ensuring that training with hints finds better solutions instead of harming performance. Learning with sub-optimal data. RL can learn not only from online interaction with the world but also from datasets of logged experiences. For expensive or time-consuming tasks like material discovery or robot learning, offline RL could be preferred because it leverages existing data rather than requires new interaction with the world. However, such datasets could contain mostly low-reward solutions, which limits the offline RL algorithm’s performance in finding solutions better than what’s in the dataset (as we show later in this thesis). I introduce sample reweighting strategies that reweight the dataset in a way that current offline RL algorithms trained with the weighted samples are able to discover solutions far better than what’s in the dataset, even if low-reward solutions predominated the dataset. Safety via Diversity. Standard RL algorithms aim to find a single “best” solution. Yet, in many discovery problems—such as drug development—it is more valuable to generate multiple high-rewards solutions with distinct properties (i.e., diversity) than to focus on only one. I study this problem in an emerging discovery task-red-teaming large language models (LLMs). In red-teaming, we desire diverse prompts that trigger undesired outputs from target language models. Current approaches leveraging RL to train an LLM to red-team another one, but they fall short of the diversity of generated prompts and often converge to a few prompts that consistently trigger undesired outputs. I propose to reward the agent to maximize the diversity of generated prompts, which also improves the the success of prompts at triggering undesired outputs from the target LLM.",
        "authors": [
            "Zhang-Wei Hong"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159135",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Learning Diffusion Models to Enable Efficient Sampling for Task and Motion Planning on a Panda Robot",
        "abstract": "A search then sample approach to bilevel planning in the context of task and motion planning is one method of effectively solving multi-step robotics problems. In this planning framework, high-level plans of abstract actions are refined into low-level continuous transitions by sampling controller parameters associated with each action. Efficiently sampling these parameters remains a significant challenge, as exhaustive searches often become computational bottlenecks, especially for tasks requiring complex or multimodal parameter distributions. Moreover, relying on samplers hand-designed by humans is both impractical and limiting. To address these challenges, we propose using diffusion models to learn efficient sampling distributions from demonstrations. By avoiding the limitations of hand-specified and naïve sampling methods, our approach enhances planning efficiency and achieves superior performance across diverse tasks that require learning multimodal parameter distributions to solve successfully.",
        "authors": [
            "Quincy Johnson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159141",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Prompt Injection Generation Using Small Language\r\nModels with Reinforcement Learning with Artificial\r\nIntelligence Feedback",
        "abstract": "Large language models (LLMs) have become an integral part of many fields from customer support automation to research assistants. However, despite their growing adoption, they face significant challenges, particularly when it comes to safety in sensitive contexts. Existing methods like Reinforcement Learning with Human Feedback (RLHF) and keyword filtering have contributed to improving the robustness of these models, but these approaches are very resource-intensive and the models can still be vulnerable to malicious attacks like prompt injections and jailbreaking. One notable limitation in testing defenses against such attacks is the scarcity of appropriate datasets. This thesis investigates the use of small language models (SLMs) to generate goal hijacking messages, a subset of prompt injection messages. Techniques such as LoRA fine-tuning and full fine-tuning of even smaller models are employed in this short form text generation model. We also introduce a fine-tuned SLM enhanced with Reinforcement Learning with Artificial Intelligence Feedback (RLAIF), which removes reliance on slow human feedback by using faster AI-generated feedback instead. By optimizing the reference model and reward functions, we improve alignment with ground truth prompt injection messages while addressing issues such as mode collapse and overfitting. These findings show promise, and further research is necessary to determine how well the approach can generalize to other domains and perform in real-world scenarios. Future work is likely to focus on multilingual datasets and distributed computation to further extend the applicability and efficiency of the method.",
        "authors": [
            "Aneesh Gupta"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159142",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploring the Role of Foundation Models for Training Generalist Robot Learning Policies",
        "abstract": "Numerous methodologies to solving goal-conditioned short-horizon tasks require hundreds of expert demonstrations, but these demonstrations are effort-intensive to collect, reducing the scalability of these approaches. Even with approaches that do work, they may have difficulty generalizing to slightly different settings. In this work, we explore two approaches to training generalist robot learning policies using large-scale foundation models. \r\n\r\nThe first approach aims to use a video foundation model to generate task-conditioned synthetic demonstrations at scale from a single expert demonstration. The objective is to leverage these synthetic demonstrations as proxy for expert demonstrations to train models that learn rewards from expert videos for solving complex visual RL problems. \r\n\r\nThe second approach seeks to improve upon the generalization ability of behavior cloning policies. Moving away from the use of videos for training, we explore using privileged representations such as keypoints or object-poses learned using open-set foundation models. By tracking pose or keypoint correspondences, the aim is to minimize the required number of demonstrations to achieve task completion and improve generalization within classes of objects.",
        "authors": [
            "Eugenia Y. Feng"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159143",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Inferencing Techniques for Enhanced Monitoring of Thermal-Fluid Systems",
        "abstract": "Sensor data augmentation for accurate system monitoring is relevant to many engineering applications, as there is often a gap between available instrumentation and measurement needs. Installing sensors can be limited due to factors such as harsh environmental conditions, the need to avoid operational distortions, and limited space. While continued efforts to develop novel sensor technologies to improve measurement density and quality are important, it is equally crucial to maximize the use of data from existing sensors and measurements. In this work, we employed physics-based methods to solve inverse heat transfer (IHT) problems. Because accurate and well-understood physics models provide strong prior knowledge, physics-based IHT can provide clear solution with use of small amount of temperature measurements. However, existing work in IHT relies on 'perfect' physics models and has been used to solve relatively simple problems such as conduction heat transfer problems. This thesis extends the IHT problem scope to thermal fluid systems, including the efficient use of sensor data and uncertainty quantification (UQ).\r\n\r\nWe leveraged high-resolution thermal-fluid experiments to demonstrate the solution of two types of IHT problems. The first problem estimates the operating conditions of the experiment based on the minimal use of sensors from high-resolution temperature data. The estimated solution is used to reconstruct the entire temperature distribution on a heating surface, while the rest of the data is used to validate the inverse problem methodology. The estimation result is supported by UQ considering measurement errors and modeling errors that adds value to the estimation. The second IHT problem consists of identifying sharp-featured 2D heat source distributions with an array of temperature sensors from a subset of experiment data. Solving IHT involved regularization prior with strong sparsity-promoting capability. The designed iterative solution optimization process finds the unknown heat source distribution as well as regularization hyperparameter. In addition, Bayesian inference enhanced the solution quality by providing UQ of the heat source magnitude.\r\n\r\nExpanding the scope of IHT problems, we also addressed online state estimation in dynamic systems. This work focuses on a hypothetical inverse conduction problem of a transient heat source in a composite materials system. The physics modeling of system is assumed to include uncertainty arising from gap thermal resistance at material interfaces, which complicates the estimation of an internal heat source from external sensor data. To address this challenge, the IHT approach leverages future time-step measurements to correct estimates at the current time step, enabling more efficient use of limited sensor information. The approach is sampling-based and its statistics provides UQ on the quantity of interest.\r\n\r\nWhile this work addresses inverse problems within specific thermal-fluid systems, the methodology is designed for broad applicability beyond these cases. It lays the groundwork for advanced sparse sensing and inverse problem-solving in thermal systems, offering a more efficient, tractable, and reliable tool for engineers and researchers addressing system monitoring with modeling uncertainty. Looking forward, these methodologies could be valuable for digital twin applications, where live sensor measurements are integrated to provide robust, real-time estimation of the state of physical systems.",
        "authors": [
            "Haeseong Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159130",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "1863 Virginia: A short story",
        "abstract": "The question that motivates “1863 Virginia: A short story” is rooted in interracial solidarity and whether it exists outside of a common enemy. During this time in U.S. history, free and enslaved black people; slave-owning and poor white people; and assimilated and resistant native people co-existed. The story follows Indi, a Pamunkey woman, and Abram – a self-liberated and formerly enslaved African man from White House plantation. Due to her tribe's Black Laws, Indi is exiled for giving birth to a child of a Black man. Abram loses the love of his life to his murderous master Mr. Lee and runs away from White House plantation where he stumbles across Indi, Baby Joseph, and another person Indi took in during her time in exile named Sophia. Slave catchers come to Indi’s home looking for Abram and she must decide whether she will give him up or defend him. The text seeks to understand the interior character of people surviving impossible realities while also staying true to the connection of human beings and nature. There is a character Mae, a horse, who expresses herself and the river Pamunkey, who speaks.",
        "authors": [
            "Kelvin Green II"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159139",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Modeling, Design, and Assembly of Spring Tires",
        "abstract": "With a renewed interest in the Moon and the need for autonomous lunar rovers that drive longer distances and operate over extended durations, designing efficient and robust mobility systems is paramount. Created by NASA Glenn Research Center, the spring tire is a compliant airless tire engineered for planetary rover missions in lunar and Martian environments. It consists of hundreds of coiled springs woven together to create a toroidal-shaped mesh wheel that can deform to uneven terrain, providing additional durability and traction. This work aims to apply this technology to two robotic testbeds: ERNEST, an autonomous lunar traversal rover built at NASA Jet Propulsion Laboratory, and IPEx, a lunar regolith mining robot built at Kennedy Space Center. This thesis discusses the modeling of these spring tires with numerical methods along with the design of two spring tire prototypes for use on the aforementioned rover platforms. A streamlined assembly process for these compliant wheels is also outlined as well as the results of compression testing, rough terrain driving, and drawbar pull testing to assess their performance.",
        "authors": [
            "Michael Lu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159140",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Artificial Intelligence Tools, Curricula, and Agents for Creative Learning",
        "abstract": "Children's early development of creativity contributes to their learning outcomes and personal growth. However, as children enter formal schooling systems, their creativity declines. Although Artificial Intelligence (AI)-powered tools for K-12 learning hold immense potential for reducing barriers to creative expression, access to these AI tools and AI knowledge among K-12 students and educators remains inequitable to children from groups underrepresented in STEM. In this thesis, I explore how AI, as an emerging creative medium, can be made more accessible to all young creators. I explore two mechanisms of making a mode of creation more accessible: Creative AI literacy materials for diverse classrooms and AI agentic interactions for scaffolding creative expression for diverse learners. \r\n\r\nUtilizing literacy as a mode of making Creative AI tools accessible, I outline the design and evaluation of various Creative AI curricula that I have developed for diverse groups of K-12 students and teachers. To adapt AI learning to art classrooms, I co-developed the AI and Art curriculum with creative educators, designed specifically for use in creative classrooms with creative educators and learners. I implemented the curriculum with 94 middle and high school students across six week-long sessions. I report findings from teacher co-design sessions and students’ learning experiences. Teachers designed learning objectives and AI tools for their classrooms. Students gained knowledge and skills in art concepts, AI concepts, and the application of art in AI. Students also demonstrated significant shifts in their attitudes towards using AI in the creative process, and their sense of belonging in both AI and art communities was heightened. I discuss how AI curricula can be adapted to diverse disciplines and how art can serve as a meaningful avenue for students to engage with AI concepts. \r\n\r\nUtilizing social interaction from AI agents as a mode of fostering creative expression in children with neurodevelopmental disorders, I designed and applied inclusive child-robot interactions for collaborative creativity, where 32 elementary school children and a social robot collaboratively created picture stories. The robot provided creativity scaffolding during different parts of the creative storytelling process through social interactions such as feedback, question-asking, divergent thinking, and positive reinforcement, while personalizing the scaffolding to meet the unique needs of neurodivergent children. I investigated the impact of the social robot on children’s exhibited creativity and their emergent creative collaborative interactions in storytelling over multiple sessions. Inclusive design practices eliminated creative barriers for children with neurodevelopmental disorders, and the robot's creativity scaffolding interactions positively influenced children’s creative product and creative process in storytelling. I propose Inclusive Co-creative Child-robot Interaction (ICCRI) guidelines for fostering creativity in children with neurodevelopmental disorders, and accommodating diverse creator styles in complex, open-ended creative tasks.\r\n\r\nIn this thesis, I contribute curricula, learning tools, child-robot interactions, and findings from examining long-term child-AI co-creative interactions. I discuss design implications for integrating AI tools, curricula and agents in creative learning environments. This thesis is a step towards empowering all children with powerful modes of creation, while helping them be responsible creators, thinkers and citizens in an AI-driven future.",
        "authors": [
            "Safinah Arshad Ali"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159100",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Systems-Theoretic Approach to Organizational Design and Analysis",
        "abstract": "A significant challenge for large organizations lies in organizational design, particularly for public sector bureaucracies and the largest of industry’s private firms. Organizations tend to turn to organizational design improvements when facing effectiveness and efficiency issues. Unfortunately, these large organizations struggle with organizational design because of the sheer size and complexity of their organization which results in a fragmented and often times faulty approach to improving their organization. Organizations, at their core, are a special type of system or a set of components that operate or work together to achieve some common purpose. Organizations are purely social systems in that their elements are not technical or engineered. \r\n\r\nSystems Theory provides a lens through which these types of social systems can be studied. Just like in engineered systems, an organization's emergent behavior is determined by its internal elements' complex interactions. Traditional organizational design and analysis methods focus on optimizing these internal elements in the hopes of re-integrating optimized elements in pursuit of organizational-level optimal behavior. Just like in traditional systems engineering, component-level optimization does not yield system-level optimal behavior. \r\n\r\nThis thesis codifies a systems-theoretic approach to organizational design and analysis using the language of Systems Theory and the semantics of Systems-Theoretic Accident Model and Processes. By extending traditional Systems-Theoretic Process Analysis (STPA), a tool for hazard analysis used primarily for engineered systems, this work refines STPA’s concepts and terminology to be more accessible for analyzing social systems. Building off this extension, this thesis leverages a contemporary Department of Defense reorganization effort as a case study, illustrating Systems-Theoretic Organizational Design and Analysis (STAODA) as a tool to assess organizational design options.",
        "authors": [
            "Lauren E. Gutierrez"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159101",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Enhancing Coast Guard Infrastructure Management: A Multi-Criteria Framework for Prioritizing Maintenance Projects",
        "abstract": "The United States Coast Guard is currently transforming its decision-making process for prioritizing shore infrastructure maintenance and repair projects. Current decision-making subjectivity appears to be generating inadequate project prioritizations. Stakes are high for an aging infrastructure portfolio in harsh coastal conditions, with increased national reliance on the Coast Guard in a fiscally constrained budgetary environment. Data availability, quality, and fidelity continue to increase, supporting the rationale for more robust and data-informed decision-making frameworks. \r\n\r\nThe research begins with examining Coastal and Shore Operations (CSO) funding history, along with a thorough description of the current Centralized Planned Obligation Prioritization (C-POP) process. The complex, sociotechnical nature of the problem is highlighted by identifying all involved stakeholders and categorizing them through the leading view of stakeholder theory and salience. A detailed review of the governing asset management literature is conducted, gradually narrowing from a broad, international, and asset-type neutral perspective to more tailored infrastructure cross-asset prioritization material. Requisite framework data substance, collection, and analyses are described, and recommendations for data processing improvements are made. \r\n\r\nTwo leading prioritization models are examined: the Importance and Urgency Quadrant Model and the Value Focused Multi-Criteria Decision Model. Their respective data visualizations are generated and analyzed. Using the multi-criteria analysis rooted in multi-attribute utility theory, four portfolios of measurably increasing value are constructed, compared with a baseline portfolio reflecting actual project selections in December 2023. These portfolio iterations include a linear programming solution to the Knapsack Problem of selecting projects that maximize overall portfolio utility within a budget limit while incorporating some of the more social and qualitative system properties. \r\n\r\nA traceable, adaptable, defendable, and objective data-informed multi-criteria framework is proposed, which aims to facilitate the effectiveness of the overall Coast Guard shore infrastructure portfolio in the long term.",
        "authors": [
            "Zachary N. Ballard"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159102",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "CIRCUIT: A Benchmark for Circuit Interpretation and Reasoning Capabilities of LLMs",
        "abstract": "The role of Large Language Models (LLMs) has not been extensively explored in analog circuit design, which could benefit from a reasoning-based approach that transcends traditional optimization techniques. In particular, despite their growing relevance, there are no benchmarks to assess LLMs’ reasoning capability about circuits. Therefore, we created the CIRCUIT dataset consisting of 510 question-answer pairs spanning various levels of analog-circuit-related subjects. The best-performing model on our dataset, GPT-4o, achieves 48.04% accuracy when evaluated on the final numerical answer. To evaluate the robustness of LLMs on our dataset, we introduced a unique dataset design and evaluation metric that enable unit-test-like evaluation by grouping questions into unit tests. In this case, GPT-4o can only pass 27.45% of the unit tests, highlighting that the most advanced LLMs still struggle with understanding circuits, which requires multi-level reasoning, particularly when involving circuit topologies. This circuit-specific benchmark introduces a scalable and reliable automatic evaluation method, transferable to other reasoning domains, and highlights LLMs' limitations, offering valuable insights for advancing their application in analog integrated circuit design.",
        "authors": [
            "Lejla Skelić"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159084",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Toward An Explainable Electric Power Grid Operation Assistant Using Large Language Models",
        "abstract": "This thesis explores potential applications of LLMs for assisting the analyses and decisionmaking of complex electric power grid operators. The power grid is a critical piece of infrastructure currently challenged by increased electrification, integration of renewable energy sources, and distributed energy resources (DERs). Human operators struggle to process the massive amounts of data produced by modern smart grids and need innovative solutions to handle the increased complexity of operational decisions. This thesis investigates the potential role of Large Language Models (LLMs) in grid operation tasks, focusing on interpretability and generalizability while exploring how LLMs can assist operators by providing actionable insights and recommendations. Multiple versions of LLM agents were developed, including naive and tool-assisted designs, and were evaluated on the Learn to Run a Power Network (L2RPN) benchmark for steady-state and cascading failure scenarios. While the LLM agents performed better in scenarios requiring exploratory decision-making, they struggled in steady-state operation and were constrained by their integration with tools and the testing environment. This work was limited by compute constraints, which affected the choice of model and the length of evaluation scenarios, and future work is needed toward seamless interaction of LLMs and power systems simulators, however LLMs have the potential to transform future grid operation, paving the way for more resilient and sustainable energy sector of the 21st century.",
        "authors": [
            "Anish Ravichandran"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159085",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Investigating Model Editing for Unlearning in Large Language Models",
        "abstract": "Data regulations on the Right to be Forgotten such as that in the General Data Protection Regulation (GDPR) of the European Union protect the right of users to remove private information from organizations. With the increasing usage and influence of large language models (LLMs) that are trained on personal data, a question of how to implement the removal of information within these models arises. In addition, large language models (LLMs) are trained on a large corpus of data that is usually scraped from the Web. A current challenge with ensuring reliable and safe outputs from LLMs is false, toxic, harmful or biased information from Web data that is captured in the knowledge of the model. Machine unlearning aims to remove unwanted information from a model, but many methods are inefficient for models with large numbers of parameters or fail to remove the entire scope of information without harming performance in the knowledge that is to be retained. Model editing algorithms solve a similar problem of changing information in LLMs, but they focus on redirecting inputs to a new target rather than removing that information altogether. Despite the parallels between model editing and unlearning, there has yet to be a thorough investigation of the potential of model editing approaches within this setting. In this work, we explore ROME, IKE, and WISE editing algorithms and design new editing targets for an unlearning setting. For evaluating the potential of the model editing algorithms, we focus on unlearning fictitious information using the Task of Fictitious Unlearning (TOFU) benchmark. Through this investigation, we show that model editing approaches can exceed the performance of current unlearning methods at removing information depending on the setting. They share the limitation of traditional unlearning of being unable to encapsulate the scope of what is to be unlearned without damage to overall model performance. We hope to leverage this information to improve methods for unlearning model knowledge and therefore improve the reliability of LLMs.",
        "authors": [
            "Shariqah Hossain"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159086",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Identifying the Role of Transcription Factor RFX3 in 9PDeletion Syndrome",
        "abstract": "9p deletion (9p-) syndrome is primarily characterized by intellectual disability, developmental delays, and autism. This project investigated how much of the neuronal phenotypes of 9p- syndrome could be attributed to RFX3, a transcription factor and autism risk gene. Bulk RNA-seq data of iPSC-derived neurons from patients with 9p- syndrome and CRISPRengineered cell lines was analyzed using Principal Component Analysis, Differential Gene Expression analysis, and Functional Enrichment analysis. The findings indicate that RFX3 plays a significant role but is not the sole driver of the neuronal phenotypes. SMARCA2, a gene linked to intellectual disability and part of the SWI/SNF complex, was identified as a direct target of RFX3 in the commonly deleted region of chromosome 9p. Notably, the combined deletion of RFX3 and SMARCA2 led to greater dysregulation of SMARCA2 expression and SWI/SNF complex components than the deletion of either gene alone. These findings highlight the potential synergistic effects of RFX3 and SMARCA2 in 9p- syndrome and suggest their combined disruption may underlie the neuronal phenotypes observed.",
        "authors": [
            "Lilly Edwards"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159087",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Higher-Order Interactions in Social Systems",
        "abstract": "The de facto representation of a social network is a graph— individuals are represented as nodes, and relationships between pairs of individuals are represented as edges. This results in a powerful abstraction by which social relationships can be systematically studied to understand emergent population-scale behavior. However, many social interactions occur in groups: three individuals may co-author a paper, a team of employees may collaborate on a task, a single tweet may mention four users. Breaking such interactions into a collection of pairwise relationships can oversimplify the rich social contexts in which these individuals know one another. This thesis explores a different paradigm of social network analysis, namely, using \"higher-order\" network models such as hypergraphs and simplicial complexes which can explicitly encode co-present contexts between three or more individuals. The first two projects describe how higher-order interactions can differ from pairwise interactions in terms of micro-level content and macro-level structure, respectively. The latter two projects then develop an applied mathematical toolkit for the algebraic topological analysis of higher-order interactions in social networks.",
        "authors": [
            "Arnab Kumar Sarker"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159095",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Uncanny Valley: An Empirical Study on Human Perceptions of AI-Generated Text and Images",
        "abstract": "This thesis explores how the uncanny valley phenomenon—historically tied to near-human robots—applies to text-based AI interactions and AI-generated images. While the concept has been predominantly studied in the context of robotics, the advent of generative AI reveals that text and visuals that are 'almost, but not quite' human can also provoke unease. \r\n\r\nTwo experiments structure the study. The first examines GPT4-Turbo (GPT4o) text conversations. Sixty participants engaged with one of three “chatbots”: an “Uncanny-Valley Bot” (prompt engineered to fall in the uncanny valley), a “Human-Like Bot” (prompt engineered to converse like humans), or a human control. Godspeed Questionnaire results indicate that the uncanny valley effect surfaces in text-only form: participants consistently rated the “Uncanny-Valley Bot” lowest in anthropomorphism, animacy, likeability, and perceived intelligence. Furthermore, the experiment revealed that the distinction between GPT and humans is becoming increasingly blurred, with 60% of participants mistaking a human for GPT and 40% mistaking GPT for a human. Lastly, results highlighted a strong user preference for naturalness, human imperfections, and vulnerability. While human flaws enhance relatability, deviations that disrupt perceived humanity trigger the uncanny valley.\r\n\r\nThe second experiment investigates AI-generated images produced by Stable Diffusion XL at varying degrees of realism. Fifty-six participants ranked each image’s “strangeness,” revealing that highly realistic or clearly stylized outputs raise fewer concerns. By contrast, images that inhabit the uncanny valley elicited discomfort. To quantify these findings, recognized metrics like Frechet Inception Distance (FID) and Kernel Inception Distance (KID) were used to compare real and AI-generated images. Both metrics strongly correlated with human perceptions, suggesting that distance metrics can be used to determine realism. The study also shows that image generation models can detect visual features associated with the uncanny valley. However, performance drops when the prompt calls for subtle, “mid-range” realism, indicating the model’s difficulty in maintaining comfort and believability at intermediate levels.\r\n\r\nCollectively, the two experiments confirm that uncanny valley responses are not confined to physical robots but persist in text-based dialogue and AI-synthesized images. Yet challenges remain. Short interaction windows, small participant samples, and reliance on selected AI models call for studies on the generalizability of these findings. Future work should adopt longitudinal designs, larger samples, and multiple AI systems. Addressing the uncanny valley in both textual and visual content is essential for advancing user trust, and comfort in AI.",
        "authors": [
            "Deepali Kishnani"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159096",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A framework for determining remote sensing capabilities for ecosystem services valuation",
        "abstract": "Nature provides vital services—clean water, air purification, and climate regulation—to human societies thanks to the \"natural capital\" like forests and lakes on our planet. Accurately measuring and valuing these ecosystem services is crucial for informed economic and development decisions. Remote sensing (RS) technology offers a powerful way to monitor natural capital (e.g., mapping forest cover, assessing water quality). However, current data lack the accuracy and precision needed for robustly monitoring the value of these services. This deficiency has impeded the use of natural capital assessment data in economic decision-making. This research partly addresses this challenge by developing a new framework to investigate the necessary sensor characteristics (spectral, radiometric, temporal, spatial) for effectively monitoring natural capital and quantifying ecosystem services. The framework first identifies the different types of services provided by an ecosystem, then uses a physics-based approach to identify crucial physical parameters and determines the necessary measurements that need to be made from a sensor for their quantification. The sources of uncertainty impacting quantification and value estimation are also analyzed in detail. The approach is integrated to formulate a system utility function that is used to compare performance of existing and proposed RS systems, and the overall results are subsequently used in proposing required capabilities for future remote sensing systems for natural capital monitoring. The framework is demonstrated on a case study focused on the flood attenuation function (service) provided by wetlands. Water budget models are utilized to identify essential parameters for monitoring water storage by wetlands. Using a study area encompassing the Fall Lake Creek reservoir (Oregon, USA), water storage capacity is measured and monitored by integrating USGS digital elevation models with Sentinel-1 synthetic aperture radar, Sentinel-2 optical data, and Planet Scope optical data. Results are validated against USGS published ground truth measurements. A strong correlation (r² of 0.95) was observed with all three datasets. An uncertainty analysis was conducted, using the random fields method, in which synthetic spatially autocorrelated errors were added to the RS datasets. Radiometric uncertainties were studied through addition of gaussian noise as a percentage of reflectance values, and results showed effects of < 2.5% on estimated water volume. Elevation data uncertainties (which were approximated to simulate uncertainties in globally available DEMs) showed higher effects, and errors in estimated storage volumes increased proportionally. A study of inundation (for a case study over Miami, FL) revealed that as the root mean square error of the DEMs increased from 2m to 7 m, the risk of flooding (defined as water depth accumulation of greater than 90 cm) increased more than 3 times. A utility function was developed to evaluate sensors based on their ability to estimate wetland water volumes. This function considers sensor characteristics like spatial, radiometric, and temporal resolution. Notably, the function estimates that a future optical system with 2x improved spatial and 4x improved temporal resolution (compared to Sentinel-2) can increase utility 7-fold.",
        "authors": [
            "Aparajithan Sampath"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159097",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Assessing Blood-Based Laboratory Diagnostics for Alzheimers’s Disease: A Systems Approach",
        "abstract": "This thesis adopts a systems approach to analyze the complex network of stakeholders involved in adopting blood-based laboratory screening tests for Alzheimer’s disease (AD). Traditional diagnostic methods, including cerebrospinal fluid (CSF) testing and positron electron tomography (PET) brain imaging, are invasive, costly, and inaccessible to many. Blood-based tests offer a less invasive and more cost-effective alternative, yet they remain underutilized in clinical practice. By conducting a literature review, stakeholder interviews, and a Kano analysis, the thesis identifies and evaluates the key stakeholder needs to support the widespread adoption of these tests, such as the need for demonstrated clinical performance of these tests, reimbursement, broader education of patients and health care professionals, and safe, effective medicines to treat AD. The research highlights two emerging tests that have published studies demonstrating clinical validation, a key parameter of clinical performance. A stakeholder tension analysis is included with proposed tension resolutions using stakeholder saliency to guide prioritization. Addressing these stakeholder needs could facilitate broader implementation, improve early diagnosis, and support emerging therapeutic interventions for AD, thus reshaping the diagnostic landscape for this increasingly prevalent disease.",
        "authors": [
            "Stephanie Christine Peralta Walker"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159103",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Creating Links: Building an Educational Platform to Ask Relevant Questions in Education",
        "abstract": "In this thesis, I document the findings and process through which we built an educational platform (JANN) to do research while having a positive impact on a community. Through JANN we have coordinated more than 100k hours of tutoring sessions and built (to our knowledge) one of the largest databases of educational recordings in the world. Broadly the contributions here are twofold: first, we demonstrate the research potential building a platform can offer. Second, using our educational platform, we pursue novel questions in the field of education with granular information that is traditionally inaccessible for research.\r\n\r\nAfter introducing the work and describing the construction of the platform, the first chapter details an RCT where we show the effect of receiving tutoring on Math performance. Second, we document how we built an estimator of emotions using audio. The estimator was further validated on our dataset and then used to show that activating emotions are related to better class quality. Third, we document an RCT where Math tutors were asked to dedicate some time per week to teach Socioemotional learning skills. We show that this had a positive effect on learning. Moreover, it also caused tutors to teach longer Math classes. Students showed more trust in their tutors, and ultimately the classes had a higher prevalence of positive emotions. Finally, we also study doing causal inference on observational data on another platform. Using Facebook data we study digital groups and through a regression discontinuity design we find that joining a group has a positive effect on making new friends and can diversify a person's connections in terms of income. \r\n\r\nOverall, we find that building a platform, can broaden the granularity of the data one has access to, make research more scalable, and ultimately also have a positive effect on a community.",
        "authors": [
            "Bernardo García Bulle Bueno"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159104",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Economic Engineering of Personalized Experiences",
        "abstract": "Consumer applications employ algorithms to deliver personalized experiences to users, among others, in search, e-commerce, online streaming, and social media, impacting how users spend their time and money. This dissertation studies the design of such personalization algorithms and the economic consequences of their deployment.\r\n\r\nThe first chapter focuses on the impacts of reward signal precision on online learning algorithms frequently used for personalization. Reward signals are precise when individual measurement is accurate and heterogeneity is low. While some algorithms, which we call \"risk-averse\", favor experiences that yield more precise reward signals and hence favor measurability and homogeneity, others, in the limit, choose experiences independently of the precision of their associated reward signals.\r\n\r\nThe third chapter analyzes how preference measurement error differentially affects user groups in optimal personalization. If such measurement error is symmetric, welfare maximization requires delivering majority-preferred experiences at a rate beyond their proportion in the user population and hence increasing concentration. However, asymmetric preference measurement errors may arise due to users' actions to reduce measurement error. Participants in a survey of TikTok state that they engage in such costly actions.\r\n\r\nThe fifth chapter studies, through the introduction of a new desideratum for market design, how to achieve personalization without infringing on user privacy. Contextual privacy demands that all (preference) information elicited by an algorithm is necessary for computing an outcome of interest in all possible configurations of users’ information. This property is demanding, as it requires that no two pieces of information can jointly but not unilaterally influence the outcome. Algorithms can protect the privacy of users who are queried late and whose information is not used to compute public statistics of the user population, hence achieving the relaxed notion of maximal contextual privacy.\r\n\r\nTwo brief chapters introduce new models of human-machine interaction. The first examines the design of generative models, while the second proposes stated regret of past consumption as a new data modality and presents a corresponding data collection tool.",
        "authors": [
            "Andreas A. Haupt"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159105",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Diagnosing Supply Chain Threats to Defense Innovation",
        "abstract": "As the U.S. Department of Defense (DoD) shifts focus to an era of global power competition, the demand for rapid innovation and disruptive technologies has grown significantly. Prototyping remains a vital tool for advancing technological innovation, enabling early learning and risk reduction in developing complex systems. However, persistent supply chain challenges threaten the success of defense prototyping projects, causing schedule delays, and diminished effectiveness. \r\nThis research identifies the underlying causes of supply chain disruptions specific to Federal Acquisition Regulations (FAR) governed prototyping efforts, offering a socio-technical systems analysis that accounts for stakeholder relationships, market dynamics, and regulatory frameworks. Through extensive data collection, including stakeholder interviews across agencies, organizations, and supply chain roles, 181 issues were identified and analyzed, revealing over 500 contributing factors. The disciplined analysis of these factors identified three systemic root causes: (1) the misapplication of production management strategies that focus on efficiencies at scale and low tolerance for risk; (2) pooled supply chain management functions, which marginalizes prototyping’s unique demands and creates inefficiencies; and (3) regulatory and organizational barriers to entry that deter non-traditional suppliers, hindering innovation.\r\nTo address these systemic challenges, the thesis recommends restructuring organizations to better align with the unique demands and risks of prototyping while simultaneously creating pathways to reduce barriers for new suppliers. Resolving these issues will require a coordinated effort across the prototyping ecosystem. By addressing these root causes, the DoD can improve the efficiency and effectiveness of prototyping programs, ultimately sustaining U.S. technological superiority in an increasingly competitive global environment.",
        "authors": [
            "Donald E. Schneider"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159098",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Efficient Deep Learning Systems for Visual Perception on\r\nthe Edge",
        "abstract": "Deep learning for visual perception on edge devices has become increasingly critical, driven by emerging applications in autonomous driving and AR/VR. Typically, sparse convolution on 3D point clouds and Visual Language Models (VLMs) for image processing are two important methods for visual understanding and reasoning. However, the limited compute resources and memory on edge devices pose significant challenges, necessitating specialized system support for deep learning models. Specifically, the efficiency challenges for edge visual perception are twofold: First, the sparsity and inherent irregularity of point cloud data introduce substantial complexity for parallel processing. Second, the colossal model sizes and amount of computation of LLMs and VLMs render edge deployment particularly challenging. In this thesis, we aim to address the efficiency issues of on-device deep learning via system-algorithm co-design. We first introduce TorchSparse++, a high-performance inference engine for sparse convolution on GPUs. Unlike existing sparse convolution systems, TorchSparse++ well balances the efficiency and implementation simplicity, achieving the best performance across different application scenarios. Specifically, we first create a highly efficient Sparse Kernel Generator that generates performant sparse convolution kernels at less than one-tenth of the engineering cost of the current state-of-the-art system. On top of this, we design the Sparse Autotuner, which extends the design space of existing sparse convolution libraries and searches for the best dataflow configurations for training and inference workloads. Consequently, TorchSparse++ achieves 2.9×, 3.3×, 2.2× and 1.7× measured end-to-end speedup on an NVIDIA A100 GPU over state-of-the-art MinkowskiEngine, SpConv 1.2, TorchSparse and SpConv v2 in inference; and is 1.2-1.3× faster than SpConv v2 in mixed precision training across seven representative autonomous driving benchmarks. It also seamlessly supports graph convolutions, achieving 2.6-7.6× faster inference speed compared with state-of-the-art graph deep learning libraries. Furthermore, to democratize the power of large foundation models in edge AI, we propose AWQ and TinyChat, a hardware-friendly full-stack solution for efficient on-device LLM and VLM deployment. AWQ is a novel quantization method based on the insight that not all weights in an LLM are equally important. Protecting only 1% salient weights can greatly reduce quantization error. Specifically, AWQ employs an equivalent transformation and scales up the salient weight channels to reduce the weight quantization error, during which the scale is determined by collecting the activation statistics offline. Alongside AWQ, we further introduce TinyChat, an efficient and flexible inference framework tailored for 4-bit on-device LLM/VLMs. With on-the-fly dequantization, extensive kernel fusion and platform-aware weight packing, TinyChat offers 2.7-3.7× speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also enables the deployment of the 70B Llama-2 model on mobile GPUs. Together, these techniques significantly reduce the computational and memory costs for deploying deep learning models on edge devices, increasing the accessibility of deep learning for practical application. We hope that this thesis can inspire future research on efficient edge AI across diverse modalities.",
        "authors": [
            "Shang Yang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159099",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Unsupervised Time Series Anomaly Detection Using Time Series Foundational Models",
        "abstract": "The rapid generation of time series data across a wide array of domains—such as finance, healthcare, and industrial systems—has made anomaly detection a critical task for identifying irregular patterns that could signal significant events like fraud, system failures, or health crises. Traditional approaches to time series anomaly detection, including statistical models like ARIMA and deep learning methods, have proven effective but often require an extensive training phase, which can be both data and time-consuming. In recent years, the emergence of foundational models, including large language models (LLMs) and specialized time series models, has opened up new possibilities for anomaly detection. These models, pre-trained on vast and diverse datasets, offer the potential to perform tasks with minimal task-specific training. This thesis investigates the feasibility of leveraging these foundational models for time series anomaly detection, with the aim of determining their effectiveness in detecting anomalies without the traditional training requirements. We also aim to investigate whether foundational models pretrained specifically on time series data yield better results compared to large language models (LLMs) that were not pretrained for time series tasks.",
        "authors": [
            "Linh K. Nguyen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159109",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Leveraging Single-Cell ATAC-Seq for Genomic Language\r\nModels and Multimodal Foundation Models",
        "abstract": "Single-cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq) has emerged as a powerful tool for profiling chromatin accessibility at single-cell resolution. By capturing epigenomic landscapes, scATAC-seq provides critical insights into the regulatory elements that govern gene expression. However, the sparsity of scATAC-seq data, resulting from its low sequencing depth relative to the genome’s potential complexity, poses significant challenges for effective and accurate modeling. To advance the utility of scATAC-seq in modern biology, we explore its integration into deep learning frameworks through two innovative applications. First, we demonstrate how incorporating scATAC data enhances the performance of existing genomic language models by providing complementary context about chromatin accessibility. Specifically, we introduce scATAC to improve SegmentNT, a DNA segmentation model that leverages the Nucleotide Transformer (NT) to predict 14 types of genomic and regulatory elements from DNA sequences up to 30kb at single-nucleotide resolution. Second, we introduce a novel multimodal foundation model that extends existing scRNA-seq foundation models by integrating scATAC-seq data. This model captures crossmodal relationships between gene expression and chromatin accessibility, establishing a unified framework that can be fine-tuned for diverse downstream tasks, including cell type classification and cross-modal imputation. Our work highlights the potential of incorporating scATAC-seq data into existing genomics deep learning strategies, providing a framework for integrating regulatory DNA analysis more seamlessly into genomic modeling.",
        "authors": [
            "Dong Young Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159110",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Impact of Occupational Flexibility on Labor Market Outcomes of Women Following Childbirth",
        "abstract": "The purpose of this study is twofold: (1) determine how occupational flexibility of a couple influence the effects of childbirth on women’s labor market outcomes and (2) measure the parental gender gap and decompose the gap for high and low flexibility occupations. Using data from the Panel Study of Income Dynamics and occupational flexibility characteristics from the O*NET database, we utilize an event study specification to determine the impact of a first child on various labor market outcomes for women within high vs low flexibility occupations and women whose spouses are within high vs low flexibility occupations. Our findings indicate that occupational flexibility impacts labor market outcomes following childbirth. We find an increase in income for women within high flexibility occupations following their first childbirth. Their low flexibility counterparts face an initial income decline that represents 6.1% of the average income. This income decline persists beyond the observed time frame. Occupational flexibility also has positive impacts on employment at the extensive and intensive margin. In the long term, the difference in weeks worked and weekly hours adjustments between women in high and low flexibility occupations is 3.4 weeks and 3.6 hours, respectively. Husbands’ occupational flexibility also exerts a positive, although smaller, positive influence on income and employment at the extensive and intensive margin. Furthermore, we estimate a parental gender gap that is twice as large in low flexibility occupations compared to high flexibility occupations. For instance, the parental gender gap among 30 to 34 year olds is 20.1 percent in the low flexibility cohort but only 12.3 percent in the high flexibility cohort. We also observe the absence of a fatherhood premium among the high flexibility cohort. Given the correlation between occupational flexibility and the gender gap, workplace flexibility shows promise as a policy tool to address gender disparities in labor market outcomes.",
        "authors": [
            "Jia-en Jane Hu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159111",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Instructify: Demystifying Metadata to Visual Instruction Tuning Data Conversion Supplementary Materials",
        "abstract": "Visual Instruction Tuning (VisIT) data, commonly available as human-assistant conversations with images interleaved in the human turns, are currently the most widespread vehicle for aligning strong LLMs to understand visual inputs, converting them to strong LMMs. While many such VisIT datasets are available, most of them are constructed via ad hoc techniques, separately proposed by different groups, commonly poorly documented, without available (reproducible) code, and employing paid closed-source model APIs like GPT-4, Gemini, or Claud to convert image metadata (labels) to VisIT instructions. This incurs significant cost and difficulty to scale, improve quality, or produce VisIT data for new datasets. In this work, we address these challenges and propose an open and unified recipe and approach, Instructify, for converting available metadata to VisIT instructions using open LLMs. Our multi-stage Instructify features an efficient framework for metadata grouping, quality control, data and prompt organization, and conversation sampling. We show that our approach can reproduce or improve the data quality of the available VisIT datasets when applied to the same image data and metadata sources, improving GPT-4 generated VisIT instructions by ∼3% on average and up to 21% on individual benchmarks using open models, such as Gemma 2 27B and LLaMa 3.1 70B. We further show that our approach enables effective performance scaling (in terms of resulting LMM performance on a large variety of benchmarks) of the produced VisIT data both in terms of quantity and quality. In addition, we explore the impact of multiple factors, including conversation format, base model selection, and resampling strategies.",
        "authors": [
            "Jacob A. Hansen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159112",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Computational Tsirelson's Theorem for All Compiled Nonlocal Games",
        "abstract": "Nonlocal games, defined as cooperative tasks between spatially separated players, have been a foundational tool in the study of quantum advantage and have been useful in classically verifying quantum computations. To address the challenge posed by the spatial separation assumption, Kalai et al. (STOC' 23) introduced a compilation procedure that compiles any nonlocal game into an interactive game between a classical verifier and a computationally bounded quantum prover. This compilation preserves classical soundness and quantum completeness, though quantum soundness has been established only in the asymptotic limit of the security parameter or for specific classes of games. In this work, we advance towards a concrete framework to bound the quantum value of compiled nonlocal games. Building on the notion of nice sum-of-squares certificates, introduced by Natarajan and Zhang (FOCS' 23) to bound the value of the compiled CHSH game, we extend the niceness framework and construct a hierarchy of semidefinite programs that searches exclusively over nice certificates. We show that this hierarchy converges to the optimal quantum value of the game. Additionally, we present a transformation to make any degree-1 sum-of-squares certificate nice. This approach provides a systematic method to reproduce known bounds for special classes of games and showcases the general applicability of the framework to low-degree certificates. Source code: https://github.com/chiragfalor/\r\nNice-SoS-SDP",
        "authors": [
            "Chirag Falor"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159113",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Applied Plankton Image Classification for Imaging FlowCytobot Data",
        "abstract": "As the ability to gather vast quantities of data from oceanographic bioimaging sensors increases, so too does the need to process, analyze, and store that data in a consistent, standard way that enables replicability and accessibility for future studies. The Imaging FlowCytobot (IFCB), an automated submersible flow cytometer, produces high resolution images of plankton at rates up to 10 Hz for months or years, resulting in billions of images. This project compares various methods to categorize incoming images of plankton gathered by the IFCB - Convolutional Neural Nets (CNNs), Vision Transformers (ViT), and self-supervised learning (MAE). The benefits and downsides of each model are analyzed and discussed for future IFCB operators to process their data using the methods that best align with their research questions, along with step-by-step explanations about the pros and cons of each method depending on the use case.",
        "authors": [
            "Barbara R. Duckworth"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159114",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The SpaseCroissant Oven: Automatic Metadata Generation For Open-Source Space Weather Datasets",
        "abstract": "The rise of machine learning (ML) algorithms has led to a parallel rise in ML-ready datasets. A novel metadata schema released by OpenAI and MLCommons called Croissant, which is specifically designed for ML-ready datasets, aims to increase data accessibility, user understanding of data, and accuracy of claims based on data. However, current methods to automatically generate Croissant metadata present difficulties, such as involving manual entries. This can be especially difficult when attempting to preserve information about large ML-ready datasets, which are often derived from large scientific repositories belonging to organizations such as National Aeronautics and Space Administration (NASA). These major scientific repositories provide their own metadata standards, such as NASA’s Space Physics Archive Search and Extract (SPASE) schema but context from this metadata can often be lost during data processing. This thesis presents a novel, improved approach to Croissant metadata generation which involves a hybrid parsing logic and Large Language Model (LLM) inference approach, as well as recommendations for future Croissant standards and SPASE to Croissant schema metadata conversion, that aims to retain this lost context.",
        "authors": [
            "Edenna H. Chen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159115",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploring Fine-Tuning Techniques for Removing\r\nTamper-Resistant Safeguards for Open-Weight LLMs",
        "abstract": "Open-source models present significant opportunities and risks, especially in dual-use scenarios where they can be repurposed for malicious tasks via adversarial fine-tuning. In this paper, we evaluate the effectiveness of Tampering Attack Resistance (TAR), a safeguard designed to protect against such adversarial attacks, by exploring its resilience to full-parameter and parameter-efficient fine-tuning. Our experiments reveal that while TAR enhances tamper resistance compared to models without safeguards, it remains susceptible to variability. Specifically, we observe inconsistencies where the same adversarial attack can succeed under some initializations and fail under others. This is a critical security risk as even a single instance of failure can lead to models being exploited for harmful purposes. These findings highlight the limitations of current tamper-resistant safeguards and emphasize the need for more robust safeguards to ensure the safe and ethical deployment of open-source models.",
        "authors": [
            "Sarah Zhang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159116",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Toward Affordance-Based Generation for 3D Generative AI",
        "abstract": "Recent advances in 3D content creation with generative AI have made it easier to generate 3D models using text and images as input. However, translating these digital designs into usable objects in the physical world is still an open challenge. Since these 3D models are generated to be aesthetically similar to their inputs, the resulting models tend to have the visual features the user desires but often lack the functionality required for their use cases. This thesis proposes a novel approach to generative AI in 3D modeling, shifting the focus from replicating specific objects to generating affordances. We trained models that allow users to create point clouds that satisfy physical properties called affordances, which are properties that describe how an object should behave in the real world. By ensuring that the generated objects have the expected affordances, we explore how existing tools can be augmented to generate 3D objects whose functionality is consistent with their appearances.",
        "authors": [
            "Sean Wang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159117",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Investigation of the energy transfer network in upconverting nanoparticles",
        "abstract": "Upconverting nanoparticles (UCNPs) have emerged as promising luminescent materials for a wide range of applications, including bioimaging, drug delivery, and photovoltaics. The intricate network of energy transfer processes within UCNPs enables their unique ability to convert low-energy infrared (IR) radiation into higher-energy visible light through photon upconversion, presenting significant challenges for accurate modeling. Despite their broad applications, theoretical models of UCNPs remain incomplete, and current models fail to accurately reproduce all experimental results. This thesis presents a comprehensive comparison of prevalent modeling approaches with the aim of developing improved models that more faithfully reproduce experimental observations. Using the Judd-Ofelt theory, we calculated essential transition rate parameters, including electric dipole (ED), magnetic dipole (MD), multiphonon relaxation (MPR), and energy transfer (ET), using constants sourced from the literature. We implemented both Monte Carlo models and Ordinary Differential Equation (ODE) models. Using the calculated rate parameters, we simulate the energy transfer pathways in Yb³⁺-Er³⁺ and Yb³⁺-Tm³⁺ UCNPs. Simulation results from all models were compared with experimental data to evaluate their effectiveness in capturing key luminescent properties such as population evolution, lifetime, saturation curves, and spectral purity.",
        "authors": [
            "Yuxuan Zheng"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159106",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Scalable Embedded Tiny Machine Learning (SETML): A General\r\nFramework for Embedded Distributed Inference",
        "abstract": "The growth of machine learning applications has increased the necessity of lightweight, energyefficient solutions for resource-constrained devices such as the STM32C011F6 microcontroller. However, such devices struggle with supporting larger models even after miniaturization techniques such as quantization and pruning. To facilitate machine learning inference on such devices, this work introduces Scalable Embedded Tiny Machine Learning (SETML), a general framework for distributed machine learning inference on microcontrollers. Furthermore, the framework is designed to be compatible with sensor-based applications that can take advantage of small hardware, such as gesture recognition, by testing binary size constraints with an accelerometer and its supporting library. This work evaluates the latency, power consumption, and cost trade-offs of using multiple small and efficient devices versus a larger device. The STM32C011F6 microcontroller is used as the primary hardware in the tested device network, while evaluation of the system is done in comparison with a device using a similar core processing element, the Seeeeduino XIAO SAMD21.",
        "authors": [
            "Justice Vidal"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159107",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "First-Person Teleoperation of a Bimanual Robotic System",
        "abstract": "First-person teleoperation of robots is a large field of research that could serve many benefits for automation. Teleoperation is a popular method to collect demonstrations for imitation learning that are easily learned by the robot, and thus it’s important to create teleoperation systems that are intuitive and enable human-like perception of a scene. Adding a first-person component to basic teleoperation systems is key to improving operators’ visual perception and making teleoperation possible for extended periods of time. Existing teleoperation systems do not integrate elements that provide the operator with a good perception of the task space, such as a first-person VR view and the ability to leverage the neck to search around the space. They rely on techniques such as third-person view of the space, or provide a first-person view but without the ability to move the neck to look around. This thesis proposes a VR-based teleoperation system with an actuated 5-DoF neck for enabling human-like perception and improving the ability to perform high quality demonstrations for use in imitation learning.",
        "authors": [
            "Nandini Thakur"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159108",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Mapping the Spatial Transcriptome Across Whole Organisms",
        "abstract": "This study utilizes Expansion Sequencing (ExSeq) to thoroughly investigate the spatial transcriptome of the Caenorhabditis elegans (C. elegans) body. Beyond mapping gene distribution within individual specimens, this research sequences multiple C. elegans to identify both shared and distinct transcriptomic features. The findings lay crucial groundwork for future integration of transcriptomic data with in situ connectomics and in vivo neural activity recordings. Understanding the spatial transcriptome in C. elegans is vital for insights into neural circuit coordination, disease mechanisms, and developmental biology.",
        "authors": [
            "Ruihan Zhang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159122",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Engineering Disease Resistance in a Reservoir Species for the Mice Against Ticks Project",
        "abstract": "This thesis explores the application of genome editing technologies to combat zoonotic infectious diseases through the development of a novel heritable immunization strategy targeting reservoir species. Focusing on Lyme disease, where white-footed mice (Peromyscus leucopus) serve as the primary reservoir, we propose embedding immunity into the germline of these animals to disrupt the disease transmission cycle and reduce the prevalence of the disease in the environment. By establishing genome engineering protocols for Peromyscus and demonstrating heritable protection against Lyme disease in genetically engineered Mus musculus, we show the feasibility of heritable immunization for long-term disease prevention. This work highlights the potential of genetic engineering for ecological interventions, offering a novel approach to public health challenges while fostering responsible community engagement in ecosystem engineering.",
        "authors": [
            "Joanna Buchthal"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159123",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Data futures: Transforming digital traces into public goods in the age of commercial surveillance",
        "abstract": "For decades, government agencies have collected surveys to produce datasets and statistics that serve as public goods, enabling research and empowering communities from whom data are collected. These data sources are costly to collect and are in decline as survey response rates drop. In contrast, increasing quantities of data are collected from the public by companies -- data we unavoidably generate by making purchases, using the Internet, or simply operating a mobile phone.  This data collection might be considered a form of surveying the public, but where privatized datasets empower corporations rather than communities, and the ensuing potential harms cannot be empirically assessed without access to these data. \r\n\r\nThis thesis considers a future where corporations can more accurately track populations and estimate statistics than the government agencies traditionally tasked with such efforts. This thesis illustrates how this future may be nearby and explores resulting questions through case studies. Namely, are there more privacy-preserving or equitable or cooperative ways to manage these data, to benefit the public from whom they are sourced?\r\n\r\nThe first set of case studies use location data from mobile phones, first developing a more privacy-preserving approach by leveraging recurrent neural networks to generate realistic synthetic data, and second developing aggregated mobility metrics to improve country level population estimates and COVID-19 epidemic models. The next set of case studies use web browser data to evaluate risks of cross-site user tracking that are present despite privacy-enhancing browser developments. The first web study repurposes data collected by a data broker; the second uses a dataset we crowdsourced and openly published to benefit this research and future research. For the next set of case studies, we crowdsourced and published a first-of-its-kind open dataset of purchase histories from thousands of Amazon.com users, along with their sociodemographics. We use this dataset to demonstrate how corporate data can provide insights into societal changes and also evaluate privacy risks due to inferring sensitive consumer information from purchases.\r\n\r\nThe data used in this thesis (mobile device locations, web browsing data, purchase histories) are examples of digital traces collected continuously from people throughout everyday activities, without explicit consent. This work points towards cooperative data sharing as a paradigm to empower research that benefits the public while prioritizing consent. Could such a paradigm exist with public support and participation? In order to study this and inform future crowdsourcing efforts, we embedded behavior experiments and surveys into our crowdsourcing tools, shedding light on what impacts users' likelihood to share their data, how users believe their data should be used, and how results differ across demographics.\r\n\r\nThroughout these studies, this thesis asks a broader question: Can we envision, and build towards, a future with alternative data economies that shift the power dynamics of data collection, along with the control and benefits of these data? To begin to address this question, this thesis proposes speculative, privacy-enhancing, and cooperative commerce networks. Such system changes may incur new costs for consumers. The final case study measures consumers' willingness to pay for privacy in new package delivery networks.",
        "authors": [
            "Alex Berke"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159147",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Evaluating the Effects of Pharmaceutical Interventions, Social Policies, and Exogeneous Shocks on People's Health and Behavior",
        "abstract": "Aging individuals tend to suffer from chronic conditions, some of which manifest in midlife (e.g., type 2 diabetes and hypertension) and some later (e.g., neurodegenerative disorders). As the global population increases and as people are living longer, finding strategies to prevent or delay these diseases has become a key priority. Concurrent advances in public health and biomedicine offer an array of pharmaceutical (e.g., oral drugs, vaccines) and non-pharmaceutical solutions (e.g., preventative and behavioral health measures). Meanwhile, exogenous shocks such as pandemics also affect the health and well-being of aging and other vulnerable individuals or populations (e.g., immunocompromised individuals, multigenerational households). In such circumstances, pharmaceutical interventions may not be readily available, forcing governments to implement socio-behavioral policies such as lockdowns and mask-wearing mandates and companies to adopt remote and hybrid work practices. Natural experiments, such as the social isolation induced by the COVID-19 pandemic or incentive-based vaccine distribution programs aimed to bolster vaccine uptake during this time, provide an opportunity to assess retrospectively the effect of federal, state, or local government policies. Another example consists of leveraging new drug approvals and changes in clinical guidelines to learn from electronic health records (EHR) which existing treatments could be repurposed to delay neurodegeneration and/or increase longevity, and if so, for whom they would work best. However, unlike randomized controlled trials, natural experiments suffer from multiple sources of confounding. The use of appropriate causal inference methods can help mitigate confounding bias, including via weighting and regression discontinuity designs. This thesis illustrates the use of existing causal inference approaches in population health and proposes new methods to evaluate the effects of pharmaceutical interventions (Chapters 1 and 2), exogenous shocks (Chapters 3, 4, and 5), and socio-behavioral policies (Chapters 3 and 5) on the health and well-being of aging and other vulnerable individuals or populations. Specifically, Chapters 1 and 2 leverage the target trial emulation framework to study the comparative effectiveness of antidiabetic and antihypertensive drugs towards preventing dementia or delaying its onset, using EHR data from Mass General Brigham healthcare system. Our target trial emulations suggest the diabetes drug metformin and the antihypertensive drug class of angiotensin receptor blockers as potential repurposing candidates for dementia, especially if initiated before age 70. Chapter 3 uses regression discontinuity designs to quantify the benefits of a local vaccine companion program in Massachusetts during the COVID-19 pandemic. We estimate that this initiative may have bolstered vaccine uptake among older adults aged 75+ by up to 22 percentage points. Chapter 4 implements counterfactual time series modeling to estimate pandemic-period excess mortality associated with overdoses in the US, by substance and geography. We find ∼25,650 excess deaths nationally (March 2020-August 2021), disproportionately affecting Southern and Western regions of the country and attributable mainly to synthetic opioids, methamphetamines, and alcohol as well as polysubstance use. Chapter 5 characterizes changes in team coordination among knowledge workers at a large global tech company to better understand the rise of hybrid work practices and their potential implications for well-being. Using two-way fixed effect regression models, we find evidence of voluntary alignment of work schedules with managers and greater co-attendance among employees who were recently hired or work in shared office spaces. Collectively, these five studies demonstrate how we can effectively learn from data about past events, medical records, and office attendance logs, to provide insights that inform the design of future public health strategies.",
        "authors": [
            "Marie-Laure Charpignon"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159148",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Intuitive Audio Interaction and Control in Multi-Source Environments",
        "abstract": "In an increasingly noisy world, managing auditory focus is a persistent challenge. This thesis explores how embodied interactions—primarily head tracking, alongside experiments with gaze tracking, speech commands, and audio-visual segmentation—can enhance user control over complex auditory environments. By linking head orientation to volume adjustments, we investigated whether natural, instinctive movements could serve as intuitive, hands-free mechanisms for isolating and amplifying relevant sounds. User studies revealed that head tracking is effective in structured audio contexts, such as music, where distinct sources are easily separable. However, its utility diminishes in dense, overlapping conversations, highlighting the need for finer control mechanisms. While gaze and segmentation offer promising refinements, cognitive load and system responsiveness remain key challenges. These findings underscore that embodied audio interaction must be adaptive, content-aware, and seamlessly integrated with user intent.This research contributes to human-computer interaction by demonstrating both the potential and limitations of movement-based audio control. Future work should refine multimodal fusion, improve segmentation accuracy, and enhance accessibility to create systems that dynamically respond to users’ natural behaviors—reducing cognitive strain and enabling more fluid, user-centric auditory experiences.",
        "authors": [
            "Erick O. Oduniyi"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159149",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Urban Mining & Regenerative E-Waste Ecosystems: Visions towards Sustainable Entrepreneurial Futures for Informal Settlements and Recycling Communities",
        "abstract": "In the face of the growing challenge of urban waste, especially within rapidly expanding informal settlements projected to house over 45% of the global population by 2050 (United Nations Department of Economic and Social Affairs, 2022), innovative solutions are imperative. The thesis proposes a paradigm shift towards urban mining, emphasizing the significant value embedded in discarded electronics—where a tonne of circuit boards can hold ten times more precious metals than traditional ore (Minnesota Center for Environmental Advocacy, 2022). The global distribution of off-shored e-waste has led to the emergence of informal settlements that depend on e-waste recovery to support livelihoods and income generation. These communities have become prime examples for urban mining, embracing circular economic strategies to find adaptive ways to repurpose e-waste. Accra, Ghana’s Old Fadama, home to one of the largest e-waste sites in the world, has become a vital economic hub for informal e-waste processing.  With a population of over 100,000 dwellers, local and migrant workers have built resilient communities through innovative recycling practices, tech repairs, and DIY digital fabrication methods. However, they face imminent environmental risks, health hazards, and displacement threats.\r\n\r\nFocusing on Old Fadama, the thesis will address the narratives of urban mining communities and look toward a systematic sympoiesis between economic, environmental, and social realities. By doing so, the thesis seeks to answer how we can foster nurturing and circular relationships for informal settlements and develop regenerative ecosystems for urban mining in the city environment. As an integrated field research, case study, and implementation, the thesis will: conduct key urban analysis for understanding e-waste sites and urban mining communities; identify technology interventions and policy recommendations that can improve local conditions; and utilize data-driven communication to advocate for new opportunities for urban systems tied to e-waste extraction through immersive multimedia as part of a public exhibition.\r\n\r\nUsing a novel methodology, the thesis adopts the learnings from the economic, physical, and community-based interventions observed in informal e-waste recovery processes. The thesis combines quantitative data from satellite imagery and remote sensing with qualitative insights gathered through crowdsourced GIS mapping, films, interviews, and creative capacity-building workshops. These combined insights aim to enhance urban models, nurturing the innovation potential already present within urban mining communities. The thesis research will contribute to the previous work of MIT City Science Group’s “Power of Without” initiative, a comprehensive roadmap for understanding and collaborating with informal settlements and proposing non-Western decentralized infrastructure solutions. The thesis aims to provide practical insights for implementing innovations in urban mining communities by developing sustainable e-waste recovery strategies and supporting micro-industries in cities, which could serve as a model for similar contexts globally.",
        "authors": [
            "Georine Pierre"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159131",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Causal Inference Under Privacy Constraints",
        "abstract": "Causal inference is an important tool for learning the effects of interventions in observational or experimental settings. It is widely used in many fields such as epidemiology, economics, and political science to find answers like the average treatment effect of a medical procedure or the individual treatment effect of a personalized ad campaign. In commercial applications, the era of big data allows companies to increase their experiment volume, incentivizing them, in turn, to collect more user data. On one hand, large volumes of data are necessary to train generative models like ChatGPT. At the same time, companies’ increasing use of user data has drawn heavy criticism and consumer backlash, incurring legitimate concerns about privacy and consent. As concerns over user data safety and privacy grow, rules and regulations like GDPR change what kinds of data companies and researchers can acquire and how they can analyze the data. The necessity of now performing causal inference under a range of privacy constrants has carved new spaces for research at the intersection of causal inference and privacy. In my thesis, I will be exploring three paradigms for protecting user data — data minimization, differential privacy and synthetic data — and how to perform causal inference techniques under these new privacy regimes.",
        "authors": [
            "Leon Yao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159132",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Investigation of Multi-Z Impurity Transport in Tokamaks using Neural Networks",
        "abstract": "Achieving clean, sustainable energy at scale is a pressing global challenge. Fusion of light elements holds significant potential to address this critical need. While only experimental fusion reactors are currently operational, significant progress is being made in the research and design of near-future tokamak fusion power plants. Reactor success will depend on a comprehensive understanding of heat and particle transport, including the role of impurities. This thesis focuses on the development of machine-agnostic neural network surrogates for TGLF, designed to predict impurity transport coefficients alongside heat and electron particle fluxes in DD plasmas. Training data are derived from synthetic fluxes generated for L, H, and I confinement modes in Alcator C-Mod, DIII-D, and ASDEX-Upgrade. To reduce training complexity, shot data are discretized by radius, and networks are developed at six ρ coordinates: 0.2, 0.4, 0.6, 0.7, 0.8, and 0.9. Fifteen plasma parameters are selected as inputs to the neural networks after examining TGLF flux sensitivities across all five output channels. Predicted impurity fluxes for arbitrary charge states and masses, ranging from 4He to 184W, are used to derive diffusive and convective transport coefficients. Three types of synthetic TGLF data are created and applied to network training to produce accurate models. The primary synthetic data type approximates experimental data by sampling within a perturbation range of ±10% around a given shot. Supporting data types enhance network performance by improving trends in single-parameter (1D) scans and addressing areas of highest network uncertainty. Hyperparameter optimization and testing resulted in highly accurate networks. Testing set relative errors averaged over ρ = 0.4–0.7 and 0.9 show approximate deviations of 0.12 ± 0.029 for heat flux and 0.42 ± 0.095 for particle flux channels. However, error metrics at ρ = 0.2 and 0.8 require location-specific tuning and potentially more data to match the accuracy achieved at other radii. The networks are used to analyze boron and carbon impurity peaking within machinespecific H-modes. Their predictions are then compared to published results. Qualitative results for boron peaking correlations in ASDEX-Upgrade are clearly reproduced, while carbon peaking trends in DIII-D are weaker. Sparse DIII-D data, which also includes atypical advanced modes, is believed to have contributed to reduced accuracy in these cases. Using H-mode shots spanning low to high local collisionality, impurity diffusion trends with charge state (Z) in ITG and TEM dominated plasmas were examined, showing good agreement with published studies. Additionally, analysis of network-derived convective transport shows that Z-sensitivity increases with collisionality. Network scans of the ion and electron heat flux responses to temperature gradients also reveal the clear presence of a critical gradient at all radii. These results demonstrate that the neural networks developed in this work can reliably reproduce TGLF results and deliver fast predictions of heat, electron particle, and impurity transport in tokamaks.",
        "authors": [
            "Jamal Johnson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159133",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Deep Learning-Based Classification of Phonotraumatic Vocal Hyperfunction Severity from Stroboscopic Images",
        "abstract": "Phonotraumatic vocal hyperfunction (PVH) is a vocal disorder characterized by damaged vocal folds from excessive or abusive voice use. Clinical assessment of PVH relies on timeconsuming videostroboscopy examination, which poses challenges for large-scale clinical studies. We address the need for more efficient clinical assessment tools by proposing deep learning approaches for automatically detecting PVH severity from stroboscopic images. One of the main challenges in building deep learning models for this task is a lack of labeled stroboscopy data. Motivated by this challenge, we explore two approaches: direct classification and segmentation-then-classification. In the segmentation-then-classification approach, we first train a model to segment the glottis, a clinically relevant part of the vocal fold anatomy. Then, we use the predicted segmentation along with the stroboscopic image as inputs into a classification model. This approach helps to guide the model towards key anatomical features. We achieve up to 0.53 accuracy in four-class PVH severity prediction with the direct classification approach. Incorporating glottal segmentations improves the accuracy to 0.64, underscoring the value of providing anatomically-informed segmentations when assessing PVH severity. By creating an automated PVH severity tool, our work has the potential to help clinicians more efficiently monitor disease progression and to facilitate large-scale screening, thereby contributing to improved patient care.",
        "authors": [
            "Purvaja Balaji"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159150",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Systems Analysis of Plant Responses to Drought",
        "abstract": "Understanding how plants respond to environmental stress is critical for ensuring stable crop performance and predicting how natural populations may adapt to a changing climate. While plant biology has traditionally focused on plant physiology and molecular biology of model plants to elucidate plant responses, there is immense diversity in how plants respond to environmental conditions, arising from complex genotype-by-environment interactions (GxE).  \r\nThis dissertation investigates these themes, aiming to advance our understanding of the mechanisms driving plant responses to environmental stress and providing insights for improving agricultural resilience and sustainability, as well as contributing to evolutionary biology. This thesis focuses on three projects: \r\n(1) While GxE is widely observed in traits and gene expression patterns, the mechanisms driving these interactions remain unclear. This thesis will present a framework using casual inference to study GxE interactions in gene regulatory networks to uncover the molecular mechanisms driving diverse environmental responses. We study two genotypes of the model grass species Brachypodium distachyon, leveraging natural variation and RNA-sequencing to study their responses to drought stress. \r\n(2) Natural perturbations can be used to understand complex traits. In wild species, limited resources drive allocation strategies that balance trade-offs between survival risks and fitness benefits, which is central to their ecology. This thesis particularly focuses on understanding a whole plant trait – carbon allocation – using divergent responses of annual and perennial species of Brachypodium to drought stress. \r\n(3) Does domestication trade-off stress tolerance for rapid growth? Plant domestication is thought to create trade-offs between high yield and stress tolerance, raising concerns about yield stability in future climates. This thesis will present a high-throughput phenotyping approach to study this question, focusing on leaf growth environmental response and its cellular regulatory mechanisms.",
        "authors": [
            "Jie Yun"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159151",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Brewing Resilience: A Case Study in Adapting Small Business Strategy with Systems Thinking",
        "abstract": "This thesis explores how systems thinking—a methodology often reserved for large organizations—can be effectively applied to small businesses facing complex challenges. Using Lamplighter Brewing Co., an independent microbrewery in Cambridge, Massachusetts, as a case study, the research examines how the brewery adapted to the disruptions of the COVID-19 pandemic and the evolving economic landscape that followed. It documents the iterative application of systems thinking principles to identify root causes, leverage points, and actionable solutions to address issues such as declining revenue, rising costs, and misaligned organizational structures.\r\nLamplighter's interventions ranged from restructuring its management and marketing teams to pivoting its sales and production strategies. By leveraging tools such as causal loop diagrams and stock-and-flow models, the brewery uncovered systemic dynamics driving its performance. The research highlights the importance of iterative learning, targeted interventions, and holistic analysis in fostering resilience and sustainability in resource-constrained environments.\r\nWhile focused on the craft brewing industry, the findings offer transferable insights for small businesses in similarly dynamic sectors, demonstrating that systems thinking can empower smaller organizations to navigate complexity, adapt strategically, and thrive amidst uncertainty.",
        "authors": [
            "Andrew C. Jones"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159152",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "First and last as superlatives of before and after",
        "abstract": "First and last have been variously described as ordinals, superlatives, or both. These descriptions are generally not accompanied by extensive argumentation, and those who label first and last as superlatives do not present and argue for a particular decomposition. Thus, first and last’s status as ordinals vs. superlatives and their internal composition remain open issues. In this paper, I argue that first and last are superlatives, in particular the superlative forms of before and after. To argue that first and last are superlatives, I show that they pattern like superlatives and unlike ordinals (second, third, etc.) with respect to plurality, modifier choice, “modal superlatives” with possible, and the ordinal superlative construction. I next argue that the relations between before and first and between after and last show themselves overtly in many languages and in English paraphrases; furthermore, first and last semantically differ in ways that before and after have also been noted to differ. While I acknowledge one observation that prima facie counterexemplifies these claims, I argue that it constitutes a genuine counterexample only if one formalizes my decomposition of first/last using a standard Heimian (Heim in Notes on superlatives. Manuscript, MIT (1999)) entry for -est. The counterexample, which concerns the “upstairs de dicto” reading of superlatives, ceases to be an issue if one treats before and after as simplex and formalizes my decomposition using a Containment Hypothesis-inspired semantics (Bobaljik in Universals in comparative morphology: Suppletion, superlatives, and the structure of words, MIT Press, Cambridge, 2012) for -est.",
        "authors": [
            "Johanna Alstott"
        ],
        "journal_conference_name": "Natural Language Semantics",
        "publisher": "Springer Netherlands",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158273",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Iron-sulfur clusters: the road to room temperature",
        "abstract": "Iron-sulfur proteins perform a wide variety of reactions central to the metabolisms of all living organisms. Foundational to their reaction chemistry are the rich electronic structures of their constituent Fe-S clusters, which differ in important ways from the active sites of mononuclear Fe enzymes. In this perspective, we summarize the essential electronic structure features that make Fe-S clusters unique, and point to the need for studies aimed at understanding the electronic basis for their reactivity under physiological conditions. Specifically, at ambient temperature, both the ground state and a large number of excited states are thermally populated, and thus a complete understanding of Fe-S cluster reactivity must take into account the properties, energies, and reactivity patterns of these excited states. We highlight prior research toward characterizing the low-energy excited states of Fe-S clusters that has established what is now a consensus model of these excited state manifolds and the bonding interactions that give rise to them. In particular, we discuss the low-energy alternate spin states and valence electron configurations that occur in Fe-S clusters of varying nuclearities, and finally suggest that there may be unrecognized functional roles for these states. Graphical abstract",
        "authors": [
            "Brighton A. Skeel",
            "Daniel L. M. Suess"
        ],
        "journal_conference_name": "Journal of Biological Inorganic Chemistry",
        "publisher": "Springer International Publishing",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158250",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "First demonstration of a TES based cryogenic Li2MoO4 detector for neutrinoless double beta decay search",
        "abstract": "Cryogenic calorimetric experiments to search for neutrinoless double-beta decay ( 0 ν β β ) are highly competitive, scalable and versatile in isotope. The largest planned detector array, CUPID, is comprised of about 1500 individual Li 2 100 MoO 4 detector modules with a further scale up envisioned for a follow up experiment (CUPID-1T). In this article, we present a novel detector concept targeting this second stage with a low impedance TES based readout for the Li 2 MoO 4 absorber that is easily mass-produced and lends itself to a multiplexed readout. We present the detector design and results from a first prototype detector operated at the NEXUS shallow underground facility at Fermilab. The detector is a 2-cm-side cube with 21 g mass that is strongly thermally coupled to its readout chip to allow rise-times of ∼ 0.5 ms. This design is more than one order of magnitude faster than present NTD based detectors and is hence expected to effectively mitigate backgrounds generated through the pile-up of two independent two neutrino decay events coinciding close in time. Together with a baseline resolution of 1.95 keV (FWHM) these performance parameters extrapolate to a background index from pile-up as low as 5 · 10 - 6  counts/keV/kg/yr in CUPID size crystals. The detector was calibrated up to the MeV region showing sufficient dynamic range for 0 ν β β searches. In combination with a SuperCDMS HVeV detector this setup also allowed us to perform a precision measurement of the scintillation time constants of Li 2 MoO 4 , which showed a primary component with a fast O(20  μ s) time scale.",
        "authors": [
            "G. Bratrud",
            "C. L. Chang",
            "R. Chen",
            "E. Cudmore",
            "E. Figueroa-Feliciano",
            "Z. Hong",
            "K. T. Kennard",
            "S. Lewis",
            "M. Lisovenko",
            "L. O. Mateo",
            "V. Novati",
            "V. Novosad",
            "E. Oliveri",
            "R. Ren",
            "J. A. Scarpaci",
            "B. Schmidt",
            "G. Wang",
            "L. Winslow",
            "V. G. Yefremenko",
            "J. Zhang"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158261",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Identifying Novel Emotions and Wellbeing of Horses from Videos Through Unsupervised Learning",
        "abstract": "first_pageDownload PDFsettingsOrder Article Reprints\r\nOpen AccessArticle\r\nIdentifying Novel Emotions and Wellbeing of Horses from Videos Through Unsupervised Learning\r\nby Aarya Bhave 1ORCID,Emily Kieson 2ORCID,Alina Hafner 3 andPeter A. Gloor 1,*ORCID\r\n1\r\nMassachusetts Institute of Technology, System Design & Management, Cambridge, MA 02142, USA\r\n2\r\nEquine International, Cambridge CB22 5LD, UK\r\n3\r\nTUM School of Computation, Information and Technology, Technical University of Munich, Arcisstraße 21, 80333 Munich, Germany\r\n*\r\nAuthor to whom correspondence should be addressed.\r\nSensors 2025, 25(3), 859; https://doi.org/10.3390/s25030859\r\nSubmission received: 5 January 2025 / Revised: 22 January 2025 / Accepted: 30 January 2025 / Published: 31 January 2025\r\n(This article belongs to the Special Issue Emotion Recognition and Cognitive Behavior Analysis Based on Sensors)\r\nDownloadkeyboard_arrow_down Browse Figures Review Reports Versions Notes\r\n\r\nAbstract\r\nThis research applies unsupervised learning on a large original dataset of horses in the wild to identify previously unidentified horse emotions. We construct a novel, high-quality, diverse dataset of 3929 images consisting of five wild horse breeds worldwide at different geographical locations. We base our analysis on the seven Panksepp emotions of mammals “Exploring”, “Sadness”, “Playing”, “Rage”, “Fear”, “Affectionate” and “Lust”, along with one additional emotion “Pain” which has been shown to be highly relevant for horses. We apply the contrastive learning framework MoCo (Momentum Contrast for Unsupervised Visual Representation Learning) on our dataset to predict the seven Panksepp emotions and “Pain” using unsupervised learning. We significantly modify the MoCo framework, building a custom downstream classifier network that connects with a frozen CNN encoder that is pretrained using MoCo. Our method allows the encoder network to learn similarities and differences within image groups on its own without labels. The clusters thus formed are indicative of deeper nuances and complexities within a horse’s mood, which can possibly hint towards the existence of novel and complex equine emotions.",
        "authors": [
            "Aarya Bhave",
            "Emily Kieson",
            "Alina Hafner",
            "Peter A. Gloor"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158247",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Biomanufacturing in the U.S.: A MIT Policy Brief",
        "abstract": "",
        "authors": [
            "J. Christopher Love",
            "Elisabeth B. Reynolds",
            "David Goldston",
            "Hannah E. Frye"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158134",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Energy Burden in the United States: An Analysis Using Decision Trees",
        "abstract": "The concept of energy burden (EB) continues to gain prominence in energy and associated policy research as energy prices rise and electricity and heating options diversify. This research offers a deeper understanding of EB dynamics and how EB can be addressed more effectively by discerning the interplay between regional environmental, social, and economic factors. Using decision trees (DTs), a powerful machine learning technique, we explore the multifaceted dynamics that shape EB across the United States (U.S.) by examining how factors like housing quality, demographic variations, access to energy sources, and regional economic conditions interact, creating distinct EB profiles across communities. Following a comprehensive review of existing literature and DT analysis, we map the results to identify the most significant factors influencing EB. We find that no single variable has a determinant effect on EB levels. While there is no uniform regional pattern, regions with higher population density exhibit a stronger correlation between EB and socioeconomic and other demographic factors such as educational attainment levels and racial segregation. Our findings underscore the significance of regional ecologies in shaping EB, revealing how localized environmental and economic contexts amplify or mitigate systemic inequities. Specifically, our analysis reveals significant regional disparities, highlighting the need for localized policies and interventions. We find that a one-size-fits-all approach is insufficient and that targeted, place-based strategies are necessary to address the specific needs of different communities. Policy interventions should prioritize energy democracy, address systemic inequities, and ensure universal energy access through participatory planning, financial assistance, and targeted initiatives such as housing rehabilitation, energy efficiency improvements, and incentives for underrepresented communities.",
        "authors": [
            "Jungwoo Chun",
            "Dania Ortiz",
            "Brooke Jin",
            "Nikita Kulkarni",
            "Stephen Hart",
            "Janelle Knox-Hayes"
        ],
        "journal_conference_name": "Energies",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158246",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Hypergeometric L-functions in average polynomial time, II",
        "abstract": "We describe an algorithm for computing, for all primes p ≤ X , the trace of Frobenius at p of a hypergeometric motive over Q in time quasilinear in X. This involves computing the trace modulo p e for suitable e; as in our previous work treating the case e = 1 , we combine the Beukers–Cohen–Mellit trace formula with average polynomial time techniques of Harvey and Harvey–Sutherland. The key new ingredient for e > 1 is an expanded version of Harvey’s “generic prime” construction, making it possible to incorporate certain p-adic transcendental functions into the computation; one of these is the p-adic Gamma function, whose average polynomial time computation is an intermediate step which may be of independent interest. We also provide an implementation in Sage and discuss the remaining computational issues around tabulating hypergeometric L-series.",
        "authors": [
            "Edgar Costa",
            "Kiran S. Kedlaya",
            "David Roe"
        ],
        "journal_conference_name": "Research in Number Theory",
        "publisher": "Springer International Publishing",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159062",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Review of Pnictogenides for Next-Generation Anode Materials for Sodium-Ion Batteries",
        "abstract": "With the growing market of secondary batteries for electric vehicles (EVs) and grid-scale energy storage systems (ESS), driven by environmental challenges, the commercialization of sodium-ion batteries (SIBs) has emerged to address the high price of lithium resources used in lithium-ion batteries (LIBs). However, achieving competitive energy densities of SIBs to LIBs remains challenging due to the absence of high-capacity anodes in SIBs such as the group-14 elements, Si or Ge, which are highly abundant in LIBs. This review presents potential candidates in metal pnictogenides as promising anode materials for SIBs to overcome the energy density bottleneck. The sodium-ion storage mechanisms and electrochemical performance across various compositions and intrinsic physical and chemical properties of pnictogenide have been summarized. By correlating these properties, strategic frameworks for designing advanced anode materials for next-generation SIBs were suggested. The trade-off relation in pnictogenides between the high specific capacities and the failure mechanism due to large volume expansion has been considered in this paper to address the current issues. This review covers several emerging strategies focused on improving both high reversible capacity and cycle stability.",
        "authors": [
            "Sion Ha",
            "Junhee Kim",
            "Dong Won Kim",
            "Jun Min Suh",
            "Kyeong-Ho Kim"
        ],
        "journal_conference_name": "Batteries",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158294",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Molecular and cellular characteristics of cerebrovascular cell types and their contribution to neurodegenerative diseases",
        "abstract": "Many diseases and disorders of the nervous system suffer from a lack of adequate therapeutics to halt or slow disease progression, and to this day, no cure exists for any of the fatal neurodegenerative diseases. In part this is due to the incredible diversity of cell types that comprise the brain, knowledge gaps in understanding basic mechanisms of disease, as well as a lack of reliable strategies for delivering new therapeutic modalities to affected areas. With the advent of single cell genomics, it is now possible to interrogate the molecular characteristics of diverse cell populations and their alterations in diseased states. More recently, much attention has been devoted to cell populations that have historically been difficult to profile with bulk single cell technologies. In particular, cell types that comprise the cerebrovasculature have become increasingly better characterized in normal and neurodegenerative disease contexts. In this review, we describe the current understanding of cerebrovasculature structure, function, and cell type diversity and its role in the mechanisms underlying various neurodegenerative diseases. We focus on human and mouse cerebrovasculature studies and discuss both origins and consequences of cerebrovascular dysfunction, emphasizing known cell type-specific vulnerabilities in neuronal and cerebrovascular cell populations. Lastly, we highlight how novel insights into cerebrovascular biology have impacted the development of modern therapeutic approaches and discuss outstanding questions in the field.",
        "authors": [
            "Francisco J. Garcia",
            "Myriam Heiman"
        ],
        "journal_conference_name": "Molecular Neurodegeneration",
        "publisher": "BioMed Central",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158284",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Noisy-channel language comprehension in aphasia: A Bayesian mixture modeling approach",
        "abstract": "Individuals with “agrammatic” receptive aphasia have long been known to rely on semantic plausibility rather than syntactic cues when interpreting sentences. In contrast to early interpretations of this pattern as indicative of a deficit in syntactic knowledge, a recent proposal views agrammatic comprehension as a case of “noisy-channel” language processing with an increased expectation of noise in the input relative to healthy adults. Here, we investigate the nature of the noise model in aphasia and whether it is adapted to the statistics of the environment. We first replicate findings that a) healthy adults (N = 40) make inferences about the intended meaning of a sentence by weighing the prior probability of an intended sentence against the likelihood of a noise corruption and b) their estimate of the probability of noise increases when there are more errors in the input (manipulated via exposure sentences). We then extend prior findings that adults with chronic post-stroke aphasia (N = 28) and healthy age-matched adults (N = 19) similarly engage in noisy-channel inference during comprehension. We use a hierarchical latent mixture modeling approach to account for the fact that rates of guessing are likely to differ between healthy controls and individuals with aphasia and capture individual differences in the tendency to make inferences. We show that individuals with aphasia are more likely than healthy controls to draw noisy-channel inferences when interpreting semantically implausible sentences, even when group differences in the tendency to guess are accounted for. While healthy adults rapidly adapt their inference rates to an increase in noise in their input, whether individuals with aphasia do the same remains equivocal. Further investigation of comprehension through a noisy-channel lens holds promise for a parsimonious understanding of language processing in aphasia and may suggest potential avenues for treatment.",
        "authors": [
            "Rachel Ryskin",
            "Edward Gibson",
            "Swathi Kiran"
        ],
        "journal_conference_name": "Psychonomic Bulletin & Review",
        "publisher": "Springer US",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158279",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Half-Covered ‘Glitter-Cake’ AM@SE Composite: A Novel Electrode Design for High Energy Density All-Solid-State Batteries",
        "abstract": "All-solid-state batteries (ASSBs) are pursued due to their potential for better safety and high energy density. However, the energy density of the cathode for ASSBs does not seem to be satisfactory due to the low utilization of active materials (AMs) at high loading. With small amount of solid electrolyte (SE) powder in the cathode, poor electrochemical performance is often observed due to contact loss and non-homogeneous distribution of AMs and SEs, leading to high tortuosity and limitation of lithium and electron transport pathways. Here, we propose a novel cathode design that can achieve high volumetric energy density of 1258 Wh L−1 at high AM content of 85 wt% by synergizing the merits of AM@SE core–shell composite particles with conformally coated thin SE shell prepared from mechanofusion process and small SE particles. The core–shell structure with an intimate and thin SE shell guarantees high ionic conduction pathway while unharming the electronic conduction. In addition, small SE particles play the role of a filler that reduces the packing porosity in the cathode composite electrode as well as between the cathode and the SE separator layer. The systematic demonstration of the optimization process may provide understanding and guidance on the design of electrodes for ASSBs with high electrode density, capacity, and ultimately energy density.",
        "authors": [
            "Min J. Kim",
            "Jin-Sung Park",
            "Jin W. Lee",
            "Sung E. Wang",
            "Dowoong Yoon",
            "Jong D. Lee",
            "Jung H. Kim",
            "Taeseup Song",
            "Ju Li",
            "Yun C. Kang",
            "Dae S. Jung"
        ],
        "journal_conference_name": "Nano-Micro Letters",
        "publisher": "Springer Nature Singapore",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158290",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Nature-inspired orientation-dependent toughening mechanism for TPMS ceramic architectures",
        "abstract": "Triply periodic minimal surfaces (TPMSs) have been extensively studied in many fields of engineering, including bone tissue scaffolds. Recent advancements in manufacturing have enabled the three-dimensional printing of ceramic porous architectures; however, their intrinsic brittleness limits its practical applications. It has been observed that the ossicles of the knobby starfish exhibit a mineralized TPMS structure with lattice distortions (i.e., dislocations), which effectively deviate the crack propagation and enhance the fracture energy. In this article, the aforementioned toughening mechanism has been introduced in a TPMS architecture. We employed finite element models to analyze the effective mechanical properties of the structures under compression, both in the elastic and post-elastic regimes. Our analysis reveals that the introduction of the dislocation induces variations in both elastic and fracture properties of the structures. With particular reference to the fracture behavior, a suitable oriented edge dislocation is able to alter the crack nucleation and propagation, resulting in a tougher structure. Both the elastic and fracture phenomena can be enhanced or reduced by changing the dislocation density.",
        "authors": [
            "Luca D’Andrea",
            "Ting Yang",
            "Ming Dao",
            "Pasquale Vena"
        ],
        "journal_conference_name": "MRS Bulletin",
        "publisher": "Springer International Publishing",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159073",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design and Validation of a High-Fidelity Left Atrial Cardiac Simulator for the Study and Advancement of Left Atrial Appendage Occlusion",
        "abstract": "Purpose Atrial fibrillation (AF) is the most common chronic cardiac arrhythmia that increases the risk of stroke, primarily due to thrombus formation in the left atrial appendage (LAA). Left atrial appendage occlusion (LAAO) devices offer an alternative to oral anticoagulation for stroke prevention. However, the complex and variable anatomy of the LAA presents significant challenges to device design and deployment. Current benchtop models fail to replicate both anatomical variability and physiological hemodynamics, limiting their utility. This study introduces a novel left atrial cardiac simulator that incorporates patient-derived LAA models within a benchtop circulatory flow loop, enabling high-fidelity LAAO device testing and development. Methods A rigid, patient-derived left atrium (LA) model was 3D printed from segmented MRI data and modified to accommodate attachment of patient-specific LAA models. A library of LAA geometries was fabricated using silicone casting techniques to replicate the mechanical properties of native tissue. The LA-LAA model was integrated into a circulatory flow loop equipped with a pulsatile pump, pressure sensors, and flow probes, allowing real-time hemodynamic analysis. System tunability was demonstrated by varying heart rate, stroke volume, resistance, and compliance to simulate physiological and pathological conditions. Results The simulator accurately replicated LA pressure and flow waveforms, closely approximating physiological conditions. Changes in heart rate, stroke volume, and compliance effectively modulated LAP and LA inflow before and after LAAO. Distinct pressure and flow waveforms were observed with different LAA geometries. Hemodynamic analysis revealed increased left atrial pulse pressure after occlusion, with the greatest increase occurring after complete exclusion of the LAA. The simulator facilitated the evaluation of LAAO device performance, including metrics such as seal and PDL, and served as an effective training tool for iterative device deployment and recapture with visual and imaging-guided feedback. Conclusions The left atrial cardiac simulator offers a highly tunable and realistic platform for testing and developing LAAO devices. It also serves as an effective procedural training tool, allowing for the simulation of patient-specific anatomical and hemodynamic conditions. By enabling these advanced simulations, the simulator enhances pre-procedural planning, device sizing, and placement. This innovation represents a significant step toward advancing personalized medicine in atrial fibrillation management and improving LAAO outcomes.",
        "authors": [
            "Keegan Mendez",
            "Manisha Singh",
            "Patrick Willoughby",
            "Beatrice Ncho",
            "Aileen Liao",
            "Susan Su",
            "Megan Lim",
            "Elijah Lee",
            "Mohamad Alkhouli",
            "Hasan Alarouri",
            "Ellen T. Roche"
        ],
        "journal_conference_name": "Cardiovascular Engineering and Technology",
        "publisher": "Springer International Publishing",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158278",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Recycling of Tantalum Capacitors Via Sulfide Chemistry",
        "abstract": "The fabrication of tantalum capacitors represents more than 35 pct of the total consumption of metallic tantalum with an increasing demand for the high-technology sector. Tantalum capacitors contain a large concentration of tantalum, and the absence of niobium leads to interesting economic outcomes for potential recycling processes. The article discusses such recycling using sulfur, where an AB2O6 crystal structure analogous to the orthorhombic columbite-tantalite series is sulfidized. Sulfide affinities differences between A (Mn, Fe) and B (Nb, Ta) effectively separate the ternary oxide, capitalizing on the distinct chemical properties between A and B elements, in the absence of fluoridic acids. To bypass the fluoride-based chemistry process entirely, a proof of concept of tantalum disulfide (TaS2) production via sulfidation of Ta2O5 and its subsequent metallic reduction via molten sulfide electrolysis are also presented.",
        "authors": [
            "Charles Boury",
            "Antoine Allanore"
        ],
        "journal_conference_name": "Metallurgical and Materials Transactions B",
        "publisher": "Springer US",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158275",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Measurement of multidifferential cross sections for dijet production in proton–proton collisions at √s = 13 TeV",
        "abstract": "A measurement of the dijet production cross section is reported based on proton–proton collision data collected in 2016 at s = 13 Te V by the CMS experiment at the CERN LHC, corresponding to an integrated luminosity of up to 36.3 fb - 1 . Jets are reconstructed with the anti- k T algorithm for distance parameters of R = 0.4 and 0.8. Cross sections are measured double-differentially (2D) as a function of the largest absolute rapidity | y | max of the two jets with the highest transverse momenta p T and their invariant mass m 1 , 2 , and triple-differentially (3D) as a function of the rapidity separation y ∗ , the total boost y b , and either m 1 , 2 or the average p T of the two jets. The cross sections are unfolded to correct for detector effects and are compared with fixed-order calculations derived at next-to-next-to-leading order in perturbative quantum chromodynamics. The impact of the measurements on the parton distribution functions and the strong coupling constant at the mass of the Z boson is investigated, yielding a value of α S ( m Z ) = 0.1179 ± 0.0019.",
        "authors": [
            "Unknown author"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158258",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Uniform volumetric single-cell processing for organ-scale molecular phenotyping",
        "abstract": "Extending single-cell analysis to intact tissues while maintaining organ-scale spatial information poses a major challenge due to unequal chemical processing of densely packed cells. Here we introduce Continuous Redispersion of Volumetric Equilibrium (CuRVE) in nanoporous matrices, a framework to address this challenge. CuRVE ensures uniform processing of all cells in organ-scale tissues by perpetually maintaining dynamic equilibrium of the tissue's gradually shifting chemical environment. The tissue chemical reaction environment changes at a continuous, slow rate, allowing redispersion of unevenly distributed chemicals and preserving chemical equilibrium tissue wide at any given moment. We implemented CuRVE to immunologically label whole mouse and rat brains and marmoset and human tissue blocks within 1 day. We discovered highly variable regionalized reduction of parvalbumin immunoreactive cells in wild-type adult mice, a phenotype missed by the commonly used genetic labeling. We envision that our platform will advance volumetric single-cell processing and analysis, facilitating comprehensive single-cell level investigations within their spatial context in organ-scale tissues.",
        "authors": [
            "Dae Hee Yun",
            "Young-Gyun Park",
            "Jae Hun Cho",
            "Lee Kamentsky",
            "Nicholas B Evans",
            "Nicholas DiNapoli",
            "Katherine Xie",
            "Seo Woo Choi",
            "Alexandre Albanese",
            "Yuxuan Tian",
            "Chang Ho Sohn",
            "Qiangge Zhang",
            "Minyoung E Kim",
            "Justin Swaney",
            "Webster Guan",
            "Juhyuk Park",
            "Gabi Drummond",
            "Heejin Choi",
            "Luzdary Ruelas",
            "Guoping Feng",
            "Kwanghun Chung"
        ],
        "journal_conference_name": "Nature Biotechnology",
        "publisher": "Springer Science and Business Media LLC",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158176",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Cryptographic Censorship",
        "abstract": "We formulate and take two large strides towards proving a quantum version of the weak cosmic censorship conjecture. We first prove “Cryptographic Censorship”: a theorem showing that when the time evolution operator of a holographic CFT is approximately pseudorandom (or Haar random) on some code subspace, then there must be an event horizon in the corresponding bulk dual. This result provides a general condition that guarantees (in finite time) event horizon formation, with minimal assumptions about the global spacetime structure. Our theorem relies on an extension of a recent quantum learning no-go theorem and is proved using new techniques of pseudorandom measure concentration. To apply this result to cosmic censorship, we separate singularities into classical, semi-Planckian, and Planckian types. We illustrate that classical and semi-Planckian singularities are compatible with approximately pseudorandom CFT time evolution; thus, if such singularities are indeed approximately pseudorandom, by Cryptographic Censorship, they cannot exist in the absence of event horizons. This result provides a sufficient condition guaranteeing that seminal holographic results on quantum chaos and thermalization, whose general applicability relies on typicality of horizons, will not be invalidated by the formation of naked singularities in AdS/CFT.",
        "authors": [
            "Netta Engelhardt",
            "Åsmund Folkestad",
            "Adam Levine",
            "Evita Verheijden",
            "Lisa Yang"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158276",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Microhardness, Young’s and Shear Modulus in Tetrahedrally Bonded Novel II-Oxides and III-Nitrides",
        "abstract": "Direct wide-bandgap III-Ns and II-Os have recently gained considerable attention due to their unique electrical and chemical properties. These novel semiconductors are being explored to design short-wavelength light-emitting diodes, sensors/biosensors, photodetectors for integration into flexible transparent nanoelectronics/photonics to achieve high-power radio-frequency modules, and heat-resistant optical switches for communication networks. Knowledge of the elastic constants structural and mechanical properties has played crucial roles both in the basic understanding and assessing materials’ use in thermal management applications. In the absence of experimental structural, elastic constants, and mechanical traits, many theoretical simulations have yielded inconsistent results. This work aims to investigate the basic characteristics of tetrahedrally coordinated, partially ionic BeO, MgO, ZnO, and CdO, and partially covalent BN, AlN, GaN, and InN materials. By incorporating a bond-orbital and a valance force field model, we have reported comparative results of our systematic calculations for the bond length d\r\n, bond polarity αP\r\n, covalency αC\r\n, bulk modulus B\r\n, elastic stiffness C(=[c11−c12]2)\r\n, bond-stretching α\r\n and bond-bending β\r\n force constants, Kleinmann’s internal displacement ζ, and Born’s transverse effective charge e∗T\r\n. Correlations between C/B\r\n, β\r\n/α\r\n, c12c11,\r\n ζ, and\r\n αC \r\nrevealed valuable trends of structural, elastic, and bonding characteristics. The study noticed AlN and GaN (MgO and ZnO) showing nearly comparable features, while BN (BeO) is much harder compared to InN (CdO) material, with drastically softer bonding. Calculations of microhardness H\r\n, shear modulus G,\r\n and Young’s modulus Y\r\n have predicted BN (BeO) satisfying a criterion of super hardness. III-Ns (II-Os) could be vital in electronics, aerospace, defense, nuclear reactors, and automotive industries, providing integrity and performance at high temperature in high-power applications, ranging from heat sinks to electronic substrates to insulators in high-power devices.",
        "authors": [
            "Devki N. Talwar",
            "Piotr Becla"
        ],
        "journal_conference_name": "Department of Materials Science and Engineering",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158245",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "X-Mapper: fast and accurate sequence alignment via gapped x-mers",
        "abstract": "Sequence alignment is foundational to many bioinformatic analyses. Many aligners start by splitting sequences into contiguous, fixed-length seeds, called k-mers. Alignment is faster with longer, unique seeds, but more accurate with shorter seeds avoiding mutations. Here, we introduce X-Mapper, aiming to offer high speed and accuracy via dynamic-length seeds containing gaps, called gapped x-mers. We observe 11–24-fold fewer suboptimal alignments analyzing a human reference and 3–579-fold lower inconsistency across bacterial references than other aligners, improving on 53% and 30% of reads aligned to non-target strains and species, respectively. Other seed-based analysis algorithms might benefit from gapped x-mers too.",
        "authors": [
            "Jeffry M. Gaston",
            "Eric J. Alm",
            "An-Ni Zhang"
        ],
        "journal_conference_name": "Genome Biology",
        "publisher": "BioMed Central",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158285",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Thermogelation of nanoemulsions stabilized by a commercial pea protein isolate: high-pressure homogenization defines gel strength",
        "abstract": "The impact of animal-based food production on climate change drives the development of plant-based alternatives. We demonstrate the use of colloidal thermogelation on a real nanoemulsion system to create structured gels that could be of interest for thermo-mechanical processing of next-generation plant-based food applications. We use a commercial pea protein isolate (PPI) without further purification to stabilize a 20 vol% peanut oil-in-water nanoemulsion at pH = 7 by high-pressure homogenization (HPH) and demonstrate the temperature induced gelation behavior of the nanoemulsion as a function of the HPH processing parameters. Bright-field and laser scanning confocal fluorescence microscopy reveals a diverse microstructure of the aqueous PPI dispersions, with a large amount of insoluble protein particles, cell-wall debris particles, and lipid inclusions. Sedimentation of particulates is prevented by HPH treatment and leads to a loss of the dispersion's thermogelation properties. The non-gelling PPI dispersion stabilizes nanoemulsions and the insoluble components of the PPI dispersions persist throughout the HPH processing. We perform a systematic rheological investigation of the effect of HPH processing on thermogelation and demonstrate that the number of HPH passes n and HPH pressure P control the average nanoemulsion droplet size measured by DLS at a 90° scattering angle. We show that the droplet size defines the final gel strength with a strong inverse dependence of the elastic modulus on droplet size. Furthermore, processing can lead to heterogeneously structured gels that yield over a large strain amplitude range.",
        "authors": [
            "Damian Renggli",
            "Patrick S Doyle"
        ],
        "journal_conference_name": "Soft Matter",
        "publisher": "Royal Society of Chemistry",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158249",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Impact of lesion preparation-induced calcified plaque defects in vascular intervention for atherosclerotic disease: in silico assessment",
        "abstract": "Percutaneous coronary interventions in highly calcified atherosclerotic lesions are challenging due to the high mechanical stiffness that significantly restricts stent expansion. Intravascular lithotripsy (IVL) is a novel vessel preparation technique with the potential to improve interventional outcomes by inducing microscopic and macroscopic cracks to enhance stent expansion. However, the exact mechanism of action for IVL is poorly understood, and it remains unclear whether the improvement in-stent expansion is caused by either the macro-cracks allowing the vessel to open or the micro-cracks altering the bulk material properties. In silico models offer a robust means to examine (a) diverse lesion morphologies, (b) a range of lesion modifications to address these deficiencies, and (c) the correlation between calcium morphology alteration and improved stenting outcomes. These models also help identify which lesions would benefit the most from IVL. In this study, we develop an in silico model of stent expansion to study the effect of macro-crack morphology on interventional outcomes in clinically inspired geometries. Larger IVL-induced defects promote more post-stent lumen gain. IVL seems to induce better stenting outcomes for large calcified lesions. IVL defects that split calcified plaque in two parts are the most beneficial for stenting angioplasty, regardless of the calcified plaque size. Location of the IVL defect does not seem to matter with respect to lumen gain. These findings underscore the potential of IVL to enhance lesion compliance and improve clinical outcomes in PCI. The macroscopic defects induced by IVL seem to have a substantial impact on post-stent outcomes.",
        "authors": [
            "Jonas Sogbadji",
            "Karim Kadry",
            "Gianluca Poletti",
            "Francesca Berti",
            "Elazer R. Edelman",
            "Farhad R. Nezami"
        ],
        "journal_conference_name": "Biomechanics and Modeling in Mechanobiology",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158262",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "From burst to controlled release: using hydrogel crosslinking chemistry to tune release of micro-crystalline active pharmaceutical ingredients",
        "abstract": "Hydrogels have been widely studied as substrates for drug delivery and tissue engineering owing to their biocompatibility and ability to swell in aqueous media. Encapsulation of lipophilic active pharmaceutical ingredients (API) as crystalline micro-/nanoparticles within hydrogel formulations has shown promise for improving their bioavailability and achieving high drug load. Despite the size reduction of the API within the hydrogel mesh, the bioavailability of these formulations is largely governed by the inherent ability of the hydrogel polymer backbone to release the API. In this work, Michael addition-based Polyethylene glycol (PEG) hydrogels are developed for micro-crystalline fenofibrate (Fen) encapsulation. Using a parallelized step emulsification device, API nanoemulsion (NE) loaded micro-hydrogels are fabricated and subsequently subjected to anti-solvent extraction for API crystallization. The bi-molecular nature of the Michael addition reaction provides modular incorporation of crosslinking functional groups leading to precise temporal control over hydrogel degradation, thereby offering a sensitive handle on the release of micro-crystalline fenofibrate. By merely changing the chemical identity of the hydrogel cross-link, complete Fen release could be tuned from 4 hours to 10 days. Furthermore, the interaction of crystallizing Fen and PEG within the micro-hydrogel environment led to eutectic formation. This unique feature offered a second handle on the Fen release from the composite micro-hydrogels.",
        "authors": [
            "Purnima N Manghnani",
            "Arif Z Nelson",
            "Kelvin Wong",
            "Yi Wei Lee",
            "Saif A Khan",
            "Patrick S Doyle"
        ],
        "journal_conference_name": "RSC Pharmaceutics",
        "publisher": "Royal Society of Chemistry",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158253",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Injectable sustained-release hydrogel for high-concentration antibody delivery",
        "abstract": "There is an increasing interest in subcutaneous (SC) delivery as an alternative to the traditional intravenous (IV) for immunotherapies and other advanced therapies. High-concentration formulations of antibodies are needed to meet the limited-volume requirements of subcutaneous SC delivery. Despite this need, there remain challenges in delivering stable and injectable antibodies in these high concentrations. Hydrogel encapsulation of amorphous solid antibodies has been proven to improve the stability and injectability of high-concentration antibody formulations. However, the antibody is quickly released from the hydrogel due to the material's porosity, leading to rapid, uncontrolled drug release kinetics undesirable for the drug's efficacy and safety. In this paper, we propose a dual-network composite hydrogel which leverages interactions between the two polymer networks to achieve controlled release of the antibody. We load the solid form of the antibody at high concentrations within alginate hydrogel microparticles which are then suspended in thermogelling methylcellulose solution to formulate the in situ gelling composite hydrogel. By facile chemical modification of the alginate to tune the microparticles’ gel properties and alginate–methylcellulose interactions, we demonstrate how the composite system can delay release of the drug in a tunable manner and achieve a near-zero order release profile for improved therapeutic efficacy. We show acceptable injectability properties of the composite hydrogel at high antibody concentrations, highlighting the functionalities of dualnetwork encapsulation. We imagine this composite system to be applicable for the sustained delivery of various therapeutic protein forms, especially for high-loading SC formulations.",
        "authors": [
            "Talia Zheng",
            "Patrick S Doyle"
        ],
        "journal_conference_name": "RSC Pharmaceutics",
        "publisher": "Royal Society of Chemistry",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158252",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Measurement of inclusive and diferential cross sections of single top quark production in association with a W boson in proton-proton collisions at √s = 13.6 TeV",
        "abstract": "The first measurement of the inclusive and normalised differential cross sections of single top quark production in association with a W boson in proton-proton collisions at a centre-of-mass energy of 13.6 TeV is presented. The data were recorded with the CMS detector at the LHC in 2022, and correspond to an integrated luminosity of 34.7 fb−1. The analysed events contain one muon and one electron in the final state. For the inclusive measurement, multivariate discriminants exploiting the kinematic properties of the events are used to separate the signal from the dominant top quark-antiquark production background. A cross section of 82.3 ± 2.1 stat − 9.7 + 9.9 syst ± 3.3 lumi pb is obtained, consistent with the predictions of the standard model. A fiducial region is defined according to the detector acceptance to perform the differential measurements. The resulting differential distributions are unfolded to particle level and show good agreement with the predictions at next-to-leading order in perturbative quantum chromodynamics.",
        "authors": [
            "A. Hayrapetyan",
            "A. Tumasyan",
            "W. Adam",
            "J. W. Andrejkovic",
            "T. Bergauer",
            "S. Chatterjee",
            "K. Damanakis",
            "M. Dragicevic",
            "P. S. Hussain",
            "M. Jeitler",
            "N. Krammer",
            "A. Li",
            "D. Liko",
            "I. Mikulec",
            "J. Schieck",
            "R. Schöfbeck",
            "D. Schwarz",
            "M. Sonawane",
            "The CMS collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158260",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Waves dangerous, domesticated, and diagnostic",
        "abstract": "This paper, based on a keynote presented at the MARE People and the Sea Conference 2023 as well as on material from A Book of Waves, examines how oceanographers and coastal engineers in the United States, the Netherlands, Australia, Japan, and Bangladesh study and represent waves. Waves, seen as both chaotic and ordered, ephemeral and enduring, offer insights into how science engages with environmental, national, and planetary futures. The discussion begins in the Netherlands, where centuries-old efforts to resist waves in a nation below sea level have evolved into “building-with-nature” strategies, reframing waves as collaborators in environmental resilience. Historical contexts, from wave folklore to physical scale models, underpin this shift in Dutch wave science. Next, I explore the wave simulation laboratory at Oregon State University, where researchers model tsunami risks from the Cascadia fault line. These experiments connect the Pacific Northwest with Japan’s tsunami research, highlighting challenges in adapting wave knowledge across regions. Finally, I turn to Bangladesh’s Ganges Delta, where Dutch hydrological expertise was applied in mid-20th-century development projects, often with uneven results. This case illustrates the complexities of transposing wave science into diverse settings. I conclude by reflecting on how these scientific practices contribute to understanding the Anthropocene, particularly from the perspective of the Global South’s oceans.",
        "authors": [
            "Stefan Helmreich"
        ],
        "journal_conference_name": "Maritime Studies",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158288",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Instrument stiffness artifacts: avoiding bad data with operational limit lines of G max and E max",
        "abstract": "We derive an operating limit line for the non-ideal artifacts caused by machine stiffness (instrument compliance) which causes measured apparent viscoelastic moduli to be systematically lower than the true values. The limit is represented as a maximum measurable apparent shear modulus G max , or tensile modulus E max , which can be shown explicitly on plots of viscoelastic moduli independent of the applied displacement, load, or frequency. Uncorrected data should be much lower than these limits. Corrected data can be above these limits and credible. These interpretations are supported by studying how correction equations can be re-written in terms of G max or E max and how error propagates in the corrections. We also show how the dynamic compliance representation leads to simpler corrections and how machine stiffness can be calibrated from apparent dynamic compliance measurements of a single sample at two different geometry conditions. Equations are provided for rotational rheometers as well as linear displacement dynamic mechanical analyzers. Used as an operational limit line, G max or E max , the method can assess the credibility of data from others—even without access to their primary data of displacement, force, torque, or amount of correction, which are rarely reported. The method can also anticipate future issues before data are taken, e.g., to understand operational limits when selecting instruments and test geometries.",
        "authors": [
            "Mohammad T. Hossain",
            "Christopher W. Macosko",
            "Gareth H. McKinley",
            "Randy H. Ewoldt"
        ],
        "journal_conference_name": "Rheologica Acta",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159054",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Maximizing Free Energy Gain",
        "abstract": "Maximizing the amount of work harvested from an environment is important for a wide variety of biological and technological processes, from energy-harvesting processes such as photosynthesis to energy storage systems such as fuels and batteries. Here, we consider the maximization of free energy&mdash;and by extension, the maximum extractable work&mdash;that can be gained by a classical or quantum system that undergoes driving by its environment. We consider how the free energy gain depends on the initial state of the system while also accounting for the cost of preparing the system. We provide simple necessary and sufficient conditions for increasing the gain of free energy by varying the initial state. We also derive simple formulae that relate the free energy gained using the optimal initial state rather than another suboptimal initial state. Finally, we demonstrate that the problem of finding the optimal initial state may have two distinct regimes, one easy and one difficult, depending on the temperatures used for preparation and work extraction. We illustrate our results on a simple model of an information engine.",
        "authors": [
            "Artemy Kolchinsky",
            "Iman Marvian",
            "Can Gokler",
            "Zi-Wen Liu",
            "Peter Shor",
            "Oles Shtanko",
            "Kevin Thompson",
            "David Wolpert",
            "Seth Lloyd"
        ],
        "journal_conference_name": "Entropy",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158156",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On Modular Invariance of Quantum Affine W-Algebras",
        "abstract": "Abstract We find modular transformations of normalized characters for the following W-algebras: (a) W k min ( g ) , where g = D n ( n ≥ 4 ) , or E 6 , E 7 , E 8 , and k is a negative integer ≥ - 2 , or ≥ - h ∨ 6 - 1 , respectively; (b) quantum Hamiltonian reduction of the g ^ -module L ( k Λ 0 ) , where g is a simple Lie algebra, f is its non-zero nilpotent element, and k is a principal admissible level with the denominator u > θ ( x ) , where 2x is the Dynkin characteristic of f, and θ is the highest root of g . We prove that these vertex algebras are modular invariant. A conformal vertex algebra V is called modular invariant if its character t r V q L 0 - c / 24 converges to a holomorphic modular function in the complex upper half-plane on a congruence subgroup. We find explicit formulas for their characters. Modular invariance of V is important since, in particular, conjecturally it implies that V is simple, and that V is rational, provided that it is lisse.",
        "authors": [
            "Victor G. Kac",
            "Minoru Wakimoto"
        ],
        "journal_conference_name": "Communications in Mathematical Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159039",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Shift invariance of half space integrable models",
        "abstract": "We formulate and establish symmetries of certain integrable half space models, analogous to recent results on symmetries for models in a full space. Our starting point is the colored stochastic six vertex model in a half space, from which we obtain results on the asymmetric simple exclusion process, as well as for the beta polymer through a fusion procedure which may be of independent interest. As an application, we establish a distributional identity between the absorption time in a type B analogue of the oriented swap process and last passage times in a half space, establishing the Baik–Ben Arous–Péché phase transition for the absorption time. The proof uses Hecke algebras and integrability of the six vertex model through the Yang–Baxter and reflection equations.",
        "authors": [
            "Jimmy He"
        ],
        "journal_conference_name": "Probability Theory and Related Fields",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158248",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Preliminary results on the long-term operation of RPCs with eco-friendly gas mixtures under irradiation at the CERN Gamma Irradiation Facility",
        "abstract": "Since 2019, a collaboration between researchers from various institutes and experiments (i.e., ATLAS, CMS, ALICE, LHCb/SHiP and the CERN EP-DT group) has been operating several RPCs with diverse electronics, gas gap thicknesses and detector layouts at the CERN Gamma Irradiation Facility (GIF++). The studies aim at assessing the performance of RPCs when filled with new eco-friendly gas mixtures in avalanche mode and in view of evaluating possible aging effects after long high background irradiation periods, for example, high-luminosity LHC phase. This challenging research is also part of a task of the European AidaInnova project. A promising eco-friendly gas identified for RPC operation is the tetrafluoruropropene (C 3 H 2 F 4 , commercially known as HFO-1234ze) that has been studied at the CERN GIF++ in combination with different percentages of CO 2 . Between the end of 2021 and 2022, several beam tests have been carried out to establish the performance of RPCs operated with such mixtures before starting the irradiation campaign for the aging study. Results of these tests for different RPCs layouts and different gas mixtures, under increasing background rates are presented here, together with the preliminary outcome of the detector aging tests.",
        "authors": [
            "L. Quaglia",
            "D. Ramos",
            "M. Abbrescia",
            "G. Aielli",
            "R. Aly",
            "M. C. Arena",
            "M. Barroso",
            "L. Benussi",
            "S. Bianco",
            "D. Boscherini",
            "F. Bordon",
            "A. Bruni",
            "S. Buontempo",
            "M. Busato",
            "P. Camarri",
            "R. Cardarelli",
            "L. Congedo",
            "D. De Jesus Damiao",
            "M. De Serio",
            "A. Di Ciacco"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158287",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Decoding Codon Bias: The Role of tRNA Modifications in Tissue-Specific Translation",
        "abstract": "The tRNA epitranscriptome has been recognized as an important player in mRNA translation regulation. Our knowledge of the role of the tRNA epitranscriptome in fine-tuning translation via codon decoding at tissue or cell levels remains incomplete. We analyzed tRNA expression and modifications as well as codon optimality across seven mouse tissues. Our analysis revealed distinct enrichment patterns of tRNA modifications in different tissues. Queuosine (Q) tRNA modification was most enriched in the brain compared to other tissues, while mitochondrial tRNA modifications and tRNA expression were highest in the heart. Using this observation, we synthesized, and delivered in vivo, codon-mutated EGFP for Q-codons, where the C-ending Q-codons were replaced with U-ending codons. The protein levels of mutant EGFP were downregulated in liver, which is poor in Q, while in brain EGFP, levels did not change. These data show that understanding tRNA modification enrichments across tissues is not only essential for understanding codon decoding and bias but can also be utilized for optimizing gene and mRNA therapeutics to be more tissue-, cell-, or condition-specific.",
        "authors": [
            "Daisuke Ando",
            "Sherif Rashad",
            "Thomas J. Begley",
            "Hidenori Endo",
            "Masashi Aoki",
            "Peter C. Dedon",
            "Kuniyasu Niizuma"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158155",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Rapid prediction of conformationally-dependent DFT-level descriptors using graph neural networks for carboxylic acids and alkyl amines",
        "abstract": "Data-driven reaction discovery and development is a growing field that relies on the use of molecular descriptors to capture key information about substrates, ligands, and targets. Broad adaptation of this strategy is hindered by the associated computational cost of descriptor calculation, especially when considering conformational flexibility. Descriptor libraries can be precomputed agnostic of application to reduce the computational burden of data-driven reaction development. However, as one often applies these models to evaluate novel hypothetical structures, it would be ideal to predict the descriptors of compounds on-the-fly. Herein, we report DFT-level descriptor libraries for conformational ensembles of 8528 carboxylic acids and 8172 alkyl amines towards this goal. Employing 2D and 3D graph neural network architectures trained on these libraries culminated in the development of predictive models for molecule-level descriptors, as well as the bond- and atom-level descriptors for the conserved reactive site (carboxylic acid or amine). The predictions were confirmed to be robust for an external validation set of medicinally-relevant carboxylic acids and alkyl amines. Additionally, a retrospective study correlating the rate of amide coupling reactions demonstrated the suitability of the predicted DFT-level descriptors for downstream applications. Ultimately, these models enable high-fidelity predictions for a vast number of potential substrates, greatly increasing accessibility to the field of data-driven reaction development.",
        "authors": [
            "Brittany C Haas",
            "Melissa A Hardy",
            "Shree Sowndarya S. V.",
            "Keir Adams",
            "Connor W Coley",
            "Robert S Paton",
            "Matthew S Sigman"
        ],
        "journal_conference_name": "Digital Discovery",
        "publisher": "Royal Society of Chemistry",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158096",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "GCBF+: A Neural Graph Control Barrier Function Framework for Distributed Safe Multi-Agent Control",
        "abstract": "Distributed, scalable, and safe control of large-scale multi-agent systems is a challenging problem. In this paper, we design a distributed framework for safe multi-agent control in large-scale environments with obstacles, where a large number of agents are required to maintain safety using only local information and reach their goal locations. We introduce a new class of certificates, termed graph control barrier function (GCBF), which are based on the well-established control barrier function theory for safety guarantees and utilize a graph structure for scalable and generalizable distributed control of MAS. We develop a novel theoretical framework to prove the safety of an arbitrary-sized MAS with a single GCBF. We propose a new training framework GCBF+ that uses graph neural networks to parameterize a candidate GCBF and a distributed control policy. The proposed framework is distributed and is capable of taking point clouds from LiDAR, instead of actual state information, for real-world robotic applications. We illustrate the efficacy of the proposed method through various hardware experiments on a swarm of drones with objectives ranging from exchanging positions to docking on a moving target without collision. Additionally, we perform extensive numerical experiments, where the number and density of agents, as well as the number of obstacles, increase. Empirical results show that in complex environments with agents with nonlinear dynamics (e.g., Crazyflie drones), GCBF+ outperforms the hand-crafted CBF-based method with the best performance by up to 20% for relatively small-scale MAS with up to 256 agents, and leading reinforcement learning (RL) methods by up to 40% for MAS with 1024 agents. Furthermore, the proposed method does not compromise on the performance, in terms of goal reaching, for achieving high safety rates, which is a common trade-off in RL-based methods.",
        "authors": [
            "Songyuan Zhang",
            "Oswin So",
            "Kunal Garg",
            "Chuchu Fan"
        ],
        "journal_conference_name": "IEEE Transactions on Robotics",
        "publisher": "Institute of Electrical and Electronics Engineers",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158072",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Contactless Multi-Modal Sensing Approach for Material Assessment and Recovery in Building Deconstruction",
        "abstract": "As material scarcity and environmental concerns grow, material reuse and waste reduction are gaining attention based on their potential to reduce carbon emissions and promote net-zero buildings. This study develops an innovative approach that combines multi-modal sensing technologies with machine learning to enable contactless assessment of in situ building materials for reuse potential. By integrating thermal imaging, red, green, and blue (RGB) cameras, as well as depth sensors, the system analyzes material conditions and reveals hidden geometries within existing buildings. This approach enhances material understanding by analyzing existing materials, including their compositions, histories, and assemblies. A case study on drywall deconstruction demonstrates that these technologies can effectively guide the deconstruction process, potentially reducing material costs and carbon emissions significantly. The findings highlight feasible scenarios for drywall reuse and offer insights into improving existing deconstruction techniques through automated feedback and visualization of cut lines and fastener positions. This research indicates that contactless assessment and automated deconstruction methods are technically viable, economically advantageous, and environmentally beneficial. Serving as an initial step toward novel methods to view and classify existing building materials, this study lays a foundation for future research, promoting sustainable construction practices that optimize material reuse and reduce negative environmental impact.",
        "authors": [
            "Sophia Cabral",
            "Mikita Klimenka",
            "Fopefoluwa Bademosi",
            "Damon Lau",
            "Stefanie Pender",
            "Lorenzo Villaggi",
            "James Stoddart",
            "James Donnelly",
            "Peter Storey",
            "David Benjamin"
        ],
        "journal_conference_name": "Sustainability",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158153",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Response Regulator OmpR Negatively Controls the Expression of Genes Implicated in Tilimycin and Tilivalline Cytotoxin Production in Klebsiella oxytoca",
        "abstract": "Klebsiella oxytoca toxigenic strains represent a critical health threat, mainly due to their link to antibiotic-associated hemorrhagic colitis. This serious condition results from the bacteria’s ability to produce tilimycin and tilivalline cytotoxins. Our research highlights the pivotal role of OmpR, a key regulator within the EnvZ/OmpR two-component system, in controlling the virulence factors associated with K. oxytoca. Our findings strongly indicate that OmpR is a repressor of the aroX and npsA genes, the first genes of aroX and NRPS operons, respectively, which are indispensable for producing these enterotoxins. Notably, in the absence of OmpR, we observe a significant increase in cytotoxic effects on Caco-2 cells. These observations identify OmpR as a crucial negative transcription regulator for both operons, effectively managing the release of these cytotoxins. This research deepens our understanding of the mechanisms of toxigenic K. oxytoca and opens promising avenues for targeting OmpR for new therapeutic interventions. By focusing on this innovative approach, we can develop more effective solutions to combat this pressing health challenge, ultimately improving patient outcomes against this pathogen.",
        "authors": [
            "Ramón G. Varela-Nájera",
            "Miguel A. De la Cruz",
            "Jorge Soria-Bustos",
            "Carmen González-Horta",
            "Ma Carmen E. Delgado-Gardea",
            "Jorge A. Yáñez-Santos",
            "María L. Cedillo",
            "Hidetada Hirakawa",
            "James G. Fox",
            "Blanca Sánchez-Ramírez",
            "Miguel A. Ares"
        ],
        "journal_conference_name": "Microorganisms",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158154",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Visual Acuity Outcomes and Influencing Factors in a Cohort of UK Real-World Diabetic Macular Oedema Patients During the First Two Years of Anti-VEGF Treatment",
        "abstract": "Background/Objectives: The visual acuity (VA) outcomes after the first and second years of anti-vascular endothelial growth factor (anti-VEGF) treatment in patients with diabetic macular oedema (DMO) were evaluated, and the factors associated with treatment success were investigated. Methods: Using Medisoft electronic medical records (UK), this retrospective cohort study analysed VA outcomes, changes, and determinants in DMO patients at year 1 and year 2 after initial anti-VEGF injection. Descriptive analysis examined baseline demographics and clinical characteristics, while regression models were used to assess associations between these factors and changes in VA. Results: 728 DMO patients (1035 eyes) treated with anti-VEGFs (ranibizumab, aflibercept, or bevacizumab) at the Northern Ireland Mater Macular Clinic from 2008 to 2021 were evaluated. The mean age was 64.5 (SD 12.8) years, and 59.6% were male. In the first year, the median annual injection number and interval were 6.0 (IQR 5.0&ndash;8.0) and 6.1 weeks (IQR 5.4&ndash;7.8), respectively, and in the second year, they were 3.0 (IQR 2.0&ndash;5.0) and 10.0 weeks (IQR 6.5&ndash;20.1). In the first two treatment years, 83.4% and 79.8% of eyes had improved/stable VA (ISVA) respectively. The injection number, interval, baseline VA, age, and proliferative diabetic retinopathy (PDR) significantly impacted VA outcomes. Conclusions: Our study confirms the effectiveness of anti-VEGF treatments in improving or maintaining vision for DMO patients, consistent with previous real-world clinical data. An elder age, a better baseline VA, low annual injection numbers (&lt;5), and less frequent injection intervals (&ge;12 weeks) were negatively associated with ISVA success in the first two years. These findings have implications for managing patient expectations, allocating resources, and understanding DMO clinical management.",
        "authors": [
            "Qing Wen",
            "Helene Karcher",
            "David M. Wright",
            "Samriddhi Buxy Sinha",
            "Usha Chakravarthy",
            "Catarina Santos",
            "Franklin Igwe",
            "Recivall Salongcay",
            "Katie Curran",
            "Tunde Peto"
        ],
        "journal_conference_name": "Pharmaceutics",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158152",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Search for B ( s ) ∗ 0 → μ + μ - in B c + → π + μ + μ - decays",
        "abstract": "A search for the very rare B ∗ 0 → μ + μ - and B s ∗ 0 → μ + μ - decays is conducted by analysing the B c + → π + μ + μ - process. The analysis uses proton-proton collision data collected with the LHCb detector between 2011 and 2018, corresponding to an integrated luminosity of 9 \\,fb - 1 . The signal signatures correspond to simultaneous peaks in the μ + μ - and π + μ + μ - invariant masses. No evidence for an excess of events over background is observed for either signal decay mode. Upper limits at the 90 % confidence level are set on the branching fractions relative to that for B c + → J / ψ π + decays, R B ∗ 0 ( μ + μ - ) π + / J / ψ π + < 3.8 × 10 - 5 and R B s ∗ 0 ( μ + μ - ) π + / J / ψ π + < 5.0 × 10 - 5.",
        "authors": [
            "LHCb Collaboration"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158251",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Global surgery and climate change: how global surgery can prioritise both the health of the planet and its people",
        "abstract": "Climate change is an emerging global health crisis, disproportionately affecting low- and middle-income countries (LMICs) where health outcomes are increasingly compromised by environmental stressors such as pollution, natural disasters, and human migration. With a focus on promoting health equity, Global Surgery advocates for expanding access to surgical care and enhancing health outcomes, particularly in resource-limited and disaster-affected areas like LMICs. The healthcare industry—and more specifically, surgical care—significantly contributes to the global carbon footprint, primarily through resource-intensive settings, i.e. operating rooms that generate greenhouse gases and substantial medical waste. Therefore, Global Surgery efforts aimed at improving surgical access through an increase in surgical volumes may inadvertently exacerbate health challenges for vulnerable populations by further contributing to environmental degradation. This predicament is particularly pronounced in LMICs, who already suffer from a disproportionate share of the global burden of disease, and where the demand for surgery is rising without corresponding resilient infrastructure. LMICs face a double jeopardy of health inequity coupled with climate vulnerability. As a movement positioned to improve health around the world, Global Surgery has an increasingly significant role in envisioning and ensuring a sustainable future. Global Surgery initiatives must prioritise sustainable infrastructure in both high-income countries (HICs) and LMICs, all while accounting for the unequal polluting contributions between HICs and LMICs and, consequently, moral responsibilities moving forward. Moreover, through targeting upstream causes of poor health at urban and perioperative levels, Global Surgery’s interventions may help to reduce the global burden of disease—avoiding preventable surgeries and their carbon footprints from the outset. Altogether, Global Surgery and climate change are two matters of social justice whose solutions must synergistically centralise the health of both the planet and its most vulnerable people.",
        "authors": [
            "Sophia Chen",
            "Yvan Zolo",
            "Lumbani Ngulube",
            "Moses Isiagi",
            "Salome Maswime"
        ],
        "journal_conference_name": "BMC Surgery",
        "publisher": "BioMed Central",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158061",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Taurine prevents mitochondrial dysfunction and protects mitochondria from reactive oxygen species and deuterium toxicity",
        "abstract": "Taurine, although not a coding amino acid, is the most common free amino acid in the body. Taurine has multiple and complex functions in protecting mitochondria against oxidative-nitrosative stress. In this comprehensive review paper, we introduce a novel potential role for taurine in protecting from deuterium (heavy hydrogen) toxicity. This can be of crucial impact to either normal or cancer cells that have highly different mitochondrial redox status. Deuterium is an isotope of hydrogen with a neutron as well as a proton, making it about twice as heavy as hydrogen. We first explain the important role that the gut microbiome and the gut sulfomucin barrier play in deuterium management. We describe the synergistic effects of taurine in the gut to protect against the deleterious accumulation of deuterium in the mitochondria, which disrupts ATP synthesis by ATPase pumps. Moreover, taurine’s derivatives, N-chlorotaurine (NCT) and N-bromotaurine (NBrT), produced through spontaneous reaction of taurine with hypochlorite and hypobromite, have fascinating regulatory roles to protect from oxidative stress and beyond. We describe how taurine could potentially alleviate deuterium stress, primarily through metabolic collaboration among various gut microflora to produce deuterium depleted nutrients and deuterium depleted water, and in this way protect against leaky gut barrier, inflammatory bowel disease, and colon cancer.",
        "authors": [
            "Stephanie Seneff",
            "Anthony M. Kyriakopoulos"
        ],
        "journal_conference_name": "Amino Acids",
        "publisher": "Springer Vienna",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158031",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "\"My Very Subjective Human Interpretation\": Domain Expert Perspectives on Navigating the Text Analysis Loop for Topic Models",
        "abstract": "Practitioners dealing with large text collections frequently use topic models such as Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF) in their projects to explore trends. Despite twenty years of accrued advancement in natural language processing tools, these models are found to be slow and challenging to apply to text exploration projects. In our work, we engaged with practitioners (n=15) who use topic modeling to explore trends in large text collections to understand their project workflows and investigate which factors often slow down the processes and how they deal with such errors and interruptions in automated topic modeling. Our findings show that practitioners are required to diagnose and resolve context-specific problems with preparing data and models and need control for these steps, especially for data cleaning and parameter selection. Our major findings resonate with existing work across CSCW, computational social science, machine learning, data science, and digital humanities. They also leave us questioning whether automation is actually a useful goal for tools designed for topic models and text exploration.",
        "authors": [
            "Alexandra Schofield",
            "Siqi Wu",
            "Theo Bayard de Volo",
            "Tatsuki Kuze",
            "Alfredo Gomez",
            "Sharifa Sultana"
        ],
        "journal_conference_name": "Proceedings of the ACM on Human-Computer Interaction",
        "publisher": "Association for Computing Machinery",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158190",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Effects of maze appearance on maze solving",
        "abstract": "As mazes are typically complex, cluttered stimuli, solving them is likely limited by visual crowding. Thus, several aspects of the appearance of the maze – the thickness, spacing, and curvature of the paths, as well as the texture of both paths and walls – likely influence the performance. In the current study, we investigate the effects of perceptual aspects of maze design on maze-solving performance to understand the role of crowding and visual complexity. We conducted two experiments using a set of controlled stimuli to examine the effects of path and wall thickness, as well as the style of rendering used for both paths and walls. Experiment 1 finds that maze-solving time increases with thicker paths (thus thinner walls). Experiment 2 replicates this finding while also showing that maze-solving time increases when mazes have wavy walls, which are likely more crowded, rather than straight walls. Our findings imply a role of both crowding and figure/ground segmentation in mental maze solving and suggest reformulating the growth cone models.",
        "authors": [
            "Yelda Semizer",
            "Dian Yu",
            "Qianqian Wan",
            "Benjamin Balas",
            "Ruth Rosenholtz"
        ],
        "journal_conference_name": "Attention, Perception, & Psychophysics",
        "publisher": "Springer US",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158073",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Sequence‐Sensitivity in Functional Synthetic Polymer Properties",
        "abstract": "Recently, a new class of synthetic methyl methacrylate‐based random heteropolymers (MMA‐based RHPs) has displayed protein‐like properties. Their function appears to be insensitive to the precise sequence. Here, through atomistic molecular dynamics simulation, we show that there are universal protein‐like features of MMA‐based RHPs that are insensitive to the sequence, and mostly depend on the overall composition. In particular, we find that MMA‐based RHPs “fold” into globules with heterogeneous hydration patterns. However, the insensitivity to sequence identity observed in MMA‐based RHPs dramatically changes when we substitute the backbone architecture with acrylate or replace the oxygen atom in the side chain with a nitrogen atom (methacrylamide or acrylamide). In such scenarios, the sequence contributes significantly to the compactness and the hydration of monomers. Using principal component analysis and an intersection‐over‐union based index, we demonstrate that different sequences may not overlap in the property space, meaning that their properties are controlled by the sequence rather than fixed composition. We further investigate the sequence‐insensitive capability of the MMA‐based RHPs as previously reported on bacterial phospholipase OmpLA stabilization through heterodimerization. As experimentally observed, such polymers enhance the stability of OmpLA as reliably as its native bilayer environment. The design of such MMA‐based RHPs provides a sequence‐insensitive alternative to protein‐mimetic biomaterials that is orthogonal to the sequence‐structure‐function paradigm of proteins.",
        "authors": [
            "Tianyi Jin",
            "Connor W Coley",
            "Alfredo Alexander‐Katz"
        ],
        "journal_conference_name": "Angewandte Chemie International Edition",
        "publisher": "Wiley",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158097",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Magnetoelectric Extracellular Vesicle Latency-Targeting (MELT) Nanotherapeutic for the Block-Lock-and-Kill HIV Eradication Strategy",
        "abstract": "Background: Human immunodeficiency virus (HIV) establishes latent infections in cellular reservoirs, including microglia. HC69 cells, a microglial model of HIV latency, contain an HIV promoter long terminal repeat (LTR)-GFP reporter and were used for testing the efficacy of a two-step magnetoelectric nanoparticle (MENP) and extracellular vesicle (xEV) latency-targeting (MELT) nanotherapeutic. GFP expression in HC69 at rest is low (GFPLo), and upon exposure to LTR, transcription-activating agents (i.e., TNF-α) are induced to be high expressing (GFPHi). Methods: The first step of MELT utilized ZL0580, an HIV Tat inhibitor loaded into EVs (80%) via incubation. ZL0580-EVs were taken up by GFPLo and blocked LTR transcriptional reactivation by 50% and were 90% less toxic than ZL0580 alone. The second step in MELT involved conjugation of monomethyl auristatin E (MMAE) to MENPs. HPLC measurements showed 80% MMAE attachment to MENPs. Flow cytometry-based measurements of the membrane potential indicated that the membranes of GFPHi HC69 were 60% more polarized than GFPLo HC69 cells. More MMAE–MENPs were internalized by GFPLo HC69. Results: Using a mixed-cell blood–brain barrier (BBB) Transwell model, we demonstrated that 20% of MELT crossed the BBB, was taken up by HC69 cells, and reduced LTR reactivation by 10%. Conclusions: Overall, this study demonstrated that MELT can potentially be utilized as a nanotherapeutic to target HIV latency in microglia.",
        "authors": [
            "Mickensone Andre",
            "Nagesh Kolishetti",
            "Adriana Yndart",
            "Arti Vashist",
            "Madhavan Nair",
            "Andrea D. Raymond"
        ],
        "journal_conference_name": "Biomedicines",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158143",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Finite- and infinite-volume study of DDπ scattering",
        "abstract": "We develop a comprehensive framework for extracting the pole position and properties of the doubly-charmed tetraquark T cc + 3875 from lattice QCD data using the relativistic three-particle formalism. This approach incorporates the effect of the one-pion exchange diagram in DDπ and DD∗ scattering, making it applicable at energies coinciding with the left-hand cut in the partial-wave projected DD∗ amplitude. We present an example application of this framework to existing lattice QCD data at mπ = 280 MeV. We solve the integral equations describing the DDπ reaction, use LSZ reduction to determine the corresponding DD∗ amplitude, and find the values of the infinite-volume two- and three-body K matrices that lead to agreement with lattice DD∗ phase shifts within their uncertainties. Using these K matrices in the three-particle quantization condition, we describe the finite- volume DD∗ spectrum and find good agreement with the lattice QCD energies. Our results suggest that, at this pion mass, the tetraquark appears as a pair of subthreshold complex poles whose precise location strongly depends on the value of the DDπ three-particle K matrix.",
        "authors": [
            "Sebastian M. Dawid",
            "Fernando Romero-López",
            "Stephen R. Sharpe"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158062",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Passive Monitoring of Parkinson Tremor in Daily Life: A Prototypical Network Approach",
        "abstract": "Objective and continuous monitoring of Parkinson’s disease (PD) tremor in free-living conditions could benefit both individual patient care and clinical trials, by overcoming the snapshot nature of clinical assessments. To enable robust detection of tremor in the context of limited amounts of labeled training data, we propose to use prototypical networks, which can embed domain expertise about the heterogeneous tremor and non-tremor sub-classes. We evaluated our approach using data from the Parkinson@Home Validation study, including 8 PD patients with tremor, 16 PD patients without tremor, and 24 age-matched controls. We used wrist accelerometer data and synchronous expert video annotations for the presence of tremor, captured during unscripted daily life activities in and around the participants’ own homes. Based on leave-one-subject-out cross-validation, we demonstrate the ability of prototypical networks to capture free-living tremor episodes. Specifically, we demonstrate that prototypical networks can be used to enforce robust performance across domain-informed sub-classes, including different tremor phenotypes and daily life activities.",
        "authors": [
            "Luc J. W. Evers",
            "Yordan P. Raykov",
            "Tom M. Heskes",
            "Jesse H. Krijthe",
            "Bastiaan R. Bloem",
            "Max A. Little"
        ],
        "journal_conference_name": "Sensors",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158145",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Amplitude analysis of B+ → ψ(2S)K+π+π− decays",
        "abstract": "he first full amplitude analysis of B+ → ψ(2S)K+π+π− decays is performed using proton-proton collision data corresponding to an integrated luminosity of 9 fb−1 recorded with the LHCb detector. The rich K+π+π− spectrum is studied and the branching fractions of the resonant substructure associated with the prominent K1(1270)+ contribution are measured. The data cannot be described by conventional strange and charmonium resonances only. An amplitude model with 53 components is developed comprising 11 hidden-charm exotic hadrons. New production mechanisms for charged charmonium-like states are observed. Significant resonant activity with spin-parity JP = 1+ in the ψ(2S)π+ system is confirmed and a multi-pole structure is demonstrated. The spectral decomposition of the ψ(2S)π+π− invariant-mass structure, dominated by X0 → ψ(2S)ρ(770)0 decays, broadly resembles the J/ψϕ spectrum observed in B+ → J/ψϕK+ decays. Exotic ψ(2S)K+π− resonances are observed for the first time.",
        "authors": [
            "R. Aaij",
            "A. S. W. Abdelmotteleb",
            "C. Abellan Beteta",
            "F. Abudinén",
            "T. Ackernley",
            "A. A. Adefisoye",
            "B. Adeva",
            "M. Adinolfi",
            "P. Adlarson",
            "C. Agapopoulou",
            "C. A. Aidala",
            "Z. Ajaltouni",
            "S. Akar",
            "K. Akiba",
            "P. Albicocco",
            "J. Albrecht",
            "The LHCb collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158259",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Evaluating the reliability of a microperimetry-based method for assessing visual function in the junctional zone of geographic atrophy lesions",
        "abstract": "Purpose To assess the repeatability of a microperimetry methodology for quantifying visual function changes in the junctional zone of eyes with geographic atrophy (GA) in the clinical trial context. Methods A post hoc analysis of the OAKS phase III trial was conducted, which enrolled patients with GA secondary to age-related macular degeneration. Microperimetry using a standard 10 − 2 fovea centered grid was performed at baseline and follow-up visits. GA regions were traced on fundus autofluorescence (FAF) images. Two graders independently registered baseline microperimetry images with baseline FAF images in a sampling of 30 eyes from the OAKS study. Agreement between the two graders’ assessments of mean sensitivity and the number of scotomatous points within a ± 250 𝜇m GA junctional zone was assessed. Results The intraclass correlation (ICC) and coefficient of repeatability (CoR) for the mean junctional zone sensitivity were 0.987 and 0.214 dB, respectively. The ICC and CoR for the total number of scotomatous points within the junctional zone were 0.991 and 1.42, respectively. Conclusions The repeatability of the methodology and its compatibility with standard MP acquisitions appear to make it well-suited for identifying and analyzing retinal sensitivity within high-risk areas of the retina. Summary We present a microperimetry-based methodology for assessing visual function changes in the junctional zone of geographic atrophy lesions using a standard 10 − 2 fovea centered grid in a clinical trial context. The approach’s repeatability and compatibility with standard microperimetry grids may make it useful for assessing the effects of GA therapeutics.",
        "authors": [
            "A. Y. Alibhai",
            "Eric  M. Moult",
            "Muhammad U. Jamil",
            "Khadija Raza",
            "Marco U. Morales",
            "Ramiro Ribeiro",
            "Caroline R. Baumal",
            "James G. Fujimoto",
            "Nadia K. Waheed"
        ],
        "journal_conference_name": "International Journal of Retina and Vitreous",
        "publisher": "BioMed Central",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158526",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Inference Plans for Hybrid Particle Filtering",
        "abstract": "Advanced probabilistic programming languages (PPLs) using hybrid particle filtering combine symbolic exact inference and Monte Carlo methods to improve inference performance. These systems use heuristics to partition random variables within the program into variables that are encoded symbolically and variables that are encoded with sampled values, and the heuristics are not necessarily aligned with the developer's performance evaluation metrics. In this work, we present inference plans, a programming interface that enables developers to control the partitioning of random variables during hybrid particle filtering. We further present Siren, a new PPL that enables developers to use annotations to specify inference plans the inference system must implement. To assist developers with statically reasoning about whether an inference plan can be implemented, we present an abstract-interpretation-based static analysis for Siren for determining inference plan satisfiability. We prove the analysis is sound with respect to Siren's semantics. Our evaluation applies inference plans to three different hybrid particle filtering algorithms on a suite of benchmarks. It shows that the control provided by inference plans enables speed ups of 1.76x on average and up to 206x to reach a target accuracy, compared to the inference plans implemented by default heuristics; the results also show that inference plans improve accuracy by 1.83x on average and up to 595x with less or equal runtime, compared to the default inference plans. We further show that our static analysis is precise in practice, identifying all satisfiable inference plans in 27 out of the 33 benchmark-algorithm evaluation settings.",
        "authors": [
            "Ellie Cheng",
            "Eric Atkinson",
            "Guillaume Baudart",
            "Louis Mandel",
            "Michael Carbin"
        ],
        "journal_conference_name": "Proceedings of the ACM on Programming Languages",
        "publisher": "Association for Computing Machinery",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158236",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Global Bioenergy Availability",
        "abstract": "In efforts to decarbonize and mitigate climate change impacts, some sectors require unprecedented levels of technological innovation to substantially reduce greenhouse gas emissions. This is especially true for air transportation and maritime shipping, where vehicle electrification is currently infeasible due to limitations in available energy storage technologies. The use of biomass to produce readily substitutable (“drop-in”) and low-carbon liquid fuels presents a lever to reduce emissions along the lifecycle of the fuel. The potential to scale-up production of such fuels depends on establishing a robust supply chain for biogenic feedstocks and on building and operating cost-competitive, high-throughput biorefineries. The ultimate ‘ceiling’ for displacing fossil fuels with bio-based fuels is set by the supply of primary biomass. Existing estimates of biomass potential are heterogenous, ranging from 2 to 1200 EJ/yr for energy crops and 8-215 EJ/yr for biomass sourced as waste or residue. We provide a summary of current understanding and outline the underlying assumptions to evaluate whether these conditions are realistic, sustainable, and comprise a future which is worth pursuing. Motifs within the analysis point towards sizable barriers limiting the development of bioenergy to cover future energy demand, such as the logistics of collection and the implications of large-scale land use. For the case of agricultural residues, where such challenges can be partially mitigated, we conservatively estimate a global potential of 18±15 EJ/yr. for 2050, compared to a median estimate from meta-analysis of 27.5 EJ/yr. We dive into this apparent discrepancy, identifying multiple sources of uncertainty. Overall, our findings show that bio-energy alone is unlikely to cover the needs of decarbonizing tough-to-decarbonize transportation modes such as maritime shipping and aviation in a sustainable way.",
        "authors": [
            "Katie Daehn",
            "Evan Coleman",
            "Florian Allrogen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/157942",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Long-lived particle reconstruction downstream of the LHCb magnet",
        "abstract": "Charged-particle trajectories are usually reconstructed with the LHCb detector using combined information\r\nfrom the tracking devices placed upstream and downstream\r\nof the 4 T m dipole magnet. Trajectories reconstructed using\r\nonly information from the tracker downstream of the dipole\r\nmagnet, which are referred to as T tracks, have not been used\r\nfor physics analysis to date. The challenges of the reconstruction of long-lived particles with T tracks for physics use are\r\ndiscussed and solutions are proposed. The feasibility and the\r\ntracking performance are studied using samples of long-lived\r\n and K0\r\nS hadrons decaying between 6.0 and 7.6 m downstream of the proton–proton collision point, thereby traversing most of the magnetic field region and providing maximal sensitivity to magnetic and electric dipole moments. The\r\nreconstruction can be expanded upstream to about 2.5 m for\r\nuse in direct searches of exotic long-lived particles. The data\r\nused in this analysis have been recorded between 2015 and\r\n2018 and correspond to an integrated luminosity of 6 fb−1.\r\nThe results obtained demonstrate the possibility to further\r\nextend the decay volume and the physics reach of the LHCb\r\nexperiment.",
        "authors": [
            "LHCb collaboration"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158039",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Multiple testing for signal-agnostic searches for new physics with machine learning",
        "abstract": "In this work, we address the question of how to enhance signal-agnostic searches by leveraging multiple testing strategies. Specifically, we consider hypothesis tests relying on machine learning, where model selection can introduce a bias towards specific families of new physics signals. Focusing on the New Physics Learning Machine, a methodology to perform a signal-agnostic likelihood-ratio test, we explore a number of approaches to multiple testing, such as combining p-values and aggregating test statistics. Our findings show that it is beneficial to combine different tests, characterised by distinct choices of hyperparameters, and that performances comparable to the best available test are generally achieved, while also providing a more uniform response to various types of anomalies. This study proposes a methodology that is valid beyond machine learning approaches and could in principle be applied to a larger class model-agnostic analyses based on hypothesis testing.",
        "authors": [
            "Gaia Grosso",
            "Marco Letizia"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/157946",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "An empirical design theory for compact drip irrigation emitters",
        "abstract": "With freshwater reserves rapidly diminishing, sustainable irrigation technologies such as drip irrigation must be widely adopted to meet the food demand of a growing global population. Drip irrigation uses a network of pressurized tubes with flow-regulating devices called emitters to minimize conveyance losses, saving up to 65% water compared to flood and furrow irrigation. However, its widespread adoption remains limited due to its high initial capital costs, up to 55% of which are driven by the emitters and tubes. The plastic material consumed by the emitters and tubes is a major driver of their cost. To directly address this cost barrier, this paper details a hydraulic design theory for compact emitters having a common commercial architecture: uniform depth labyrinths with symmetric, triangular teeth. The theory uses geometric symmetry, manufacturing considerations, and clogging constraints to identify three design parameters in emitters that can be used to tune their hydraulic performance without significantly affecting their material volume: the tooth tip gap, labyrinth depth, and the number of tooth pairs. This knowledge allows designers to minimize emitter volume and set architecture a priori, and then use an empirically derived hydraulic model that uses the selected parameters as input arguments to tune flow rate independently. This ensures faster and simpler design iterations. The theory enabled a reduction in emitter material consumption by 67% compared to at least one commercial emitter, potentially cutting the initial capital cost of drip irrigation by up to 10%, making this already sustainable irrigation technology more globally accessible.",
        "authors": [
            "Aditya Ghodgaonkar",
            "Emily Welsh",
            "Benjamin Judge",
            "Amos G. Winter V"
        ],
        "journal_conference_name": "Irrigation Science",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/157945",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Digital Phenotypic Assessment in Neuro-Oncology (DANO): A Pilot Study on Sociability Changes in Patients Undergoing Treatment for Brain Malignancies",
        "abstract": "first_pageDownload PDFsettingsOrder Article Reprints\r\nOpen AccessArticle\r\nA Digital Phenotypic Assessment in Neuro-Oncology (DANO): A Pilot Study on Sociability Changes in Patients Undergoing Treatment for Brain Malignancies †\r\nby Francesca Siddi 1,2,*,Patrick Emedom-Nnamdi 3,Michael P. Catalino 4,Aakanksha Rana 1,5ORCID,Alessandro Boaro 1,2ORCID,Hassan Y. Dawood 1ORCID,Francesco Sala 2,Jukka-Pekka Onnela 3,‡ andTimothy R. Smith 1,‡\r\n1\r\nComputational Neuroscience Outcomes Center, Department of Neurosurgery, Brigham and Women’s Hospital, and Harvard Medical School, Boston, MA 02115, USA\r\n2\r\nSection of Neurosurgery, Department of Neurosciences, Biomedicine and Movement Sciences, University of Verona, 37129 Verona, Italy\r\n3\r\nDepartment of Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA 02115, USA\r\n4\r\nDepartment of Neurosurgery, University of Virginia, Charlottesville, VA 22908, USA\r\n5\r\nMcGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, MA 02139, USA\r\n*\r\nAuthor to whom correspondence should be addressed.\r\n†\r\nPrevious Presentations: This work was virtually presented as an oral poster presentation at the 2021 Annual Meeting of the European Association of Neurosurgical Societies (eEANS), Virtual Congress, 1–7 October 2021; EP13028.\r\n‡\r\nThese authors contributed equally to this work.\r\nCancers 2025, 17(1), 139; https://doi.org/10.3390/cancers17010139\r\nSubmission received: 15 October 2024 / Revised: 24 December 2024 / Accepted: 3 January 2025 / Published: 4 January 2025\r\n(This article belongs to the Special Issue Novel Diagnostic and Therapeutic Approaches in Diffuse Gliomas)\r\nDownloadkeyboard_arrow_down Browse Figures Versions Notes\r\n\r\nSimple Summary\r\nNowadays, smartphones are the principal tool for interactions between people. Mobile health applications might be used to study the cognitive functions in the neuro-oncological population. Many brain tumor patients have cognitive challenges that have an impact on sociability. Digital phenotyping is able to characterize social and spatial dimensions of human behavior from mobile phone call records. The aim of this study was to start to explore this technology in brain cancer patients, focusing on sociability data. The results of this pilot study indicate that a digital assessment in neuro-oncology can be used to characterize and follow the social activity of patients’ lives. Changes in the patient’s social network relate to disease progression, suggesting a new tool to improve the complex evaluation of underserved brain cancer patients.\r\nAbstract\r\nBackground: The digital phenotyping tool has great potential for the deep characterization of neurological and quality-of-life assessments in brain tumor patients. Phone communication activities (details on call and text use) can provide insight into the patients’ sociability. Methods: We prospectively collected digital-phenotyping data from six brain tumor patients. The data were collected using the Beiwe application installed on their personal smartphones. We constructed several daily sociability features from phone communication logs, including the number of incoming and outgoing text messages and calls, the length of messages and duration of calls, message reciprocity, the number of communication partners, and number of missed calls. We compared variability in these sociability features against those obtained from a control group, matched for age and sex, selected among patients with a herniated disc. Results: In brain tumor patients, phone-based communication appears to deteriorate with time, as evident in the trend for total outgoing minutes, total outgoing calls, and call out-degree. Conclusions: These measures indicate a possible decrease in sociability over time in brain tumor patients that may correlate with survival. This exploratory analysis suggests that a quantifiable digital sociability phenotype exists and is comparable for patients with different survival outcomes. Overall, assessing neurocognitive function using digital phenotyping appears promising.",
        "authors": [
            "Francesca Siddi",
            "Patrick Emedom-Nnamdi",
            "Michael P. Catalino",
            "Aakanksha Rana",
            "Alessandro Boaro",
            "Hassan Y. Dawood",
            "Francesco Sala",
            "Jukka-Pekka Onnela",
            "Timothy R. Smith"
        ],
        "journal_conference_name": "Cancers",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/157962",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Microbial methanogenesis fueled by freshwater infiltration and oil biodegradation in the Siljan impact structure, Sweden",
        "abstract": "Deeply fractured rocks of meteorite impact craters are suggested as prime niches for subsurface microbial colonization. Methane can be a product of such microbial communities and seeps of methane from impact craters on Earth are of strong interest as they act as analogs for Mars. Previous studies report signs of ancient microbial methanogenesis in the Devonian Siljan meteorite impact structure in Sweden, but the proportion of microbial methane, metabolic pathways, and potential modern activity remain elusive. In this study, gas composition, hydrochemistry, oil organic geochemistry, and microbial community analyses are reported in 400 m deep fractures of the Siljan impact structure. The results showed a dominantly microbial origin for methane, which was supported by highly negative δ13CCH4 and positive δ13CCO2 values along with multiply substituted isotopologues (Δ13CH3D) that indicated disequilibrium fractionation due to microbial kinetic isotope effects. The presence of C2 to C5 hydrocarbons suggested a minor thermogenic input in the gas mix. Characterization of the microbial community via 16S rRNA gene amplicon sequencing and real-time PCR indicated a low abundance of several methanogenic archaeal populations, which is common for settings with active methanogenesis. Evidence of oil biodegradation suggested that secondary microbial hydrocarbon utilization was involved in the methanogenesis. Low sulfate and high alkalinity in the groundwaters also suggested a dominantly microbial methane formation driven by infiltration of freshwater that was coupled to sulfate reduction and secondary utilization of early mature thermogenic hydrocarbons.",
        "authors": [
            "Femke van Dam",
            "Riikka Kietäväinen",
            "George Westmeijer",
            "Manuel Reinhardt",
            "Shuhei Ono",
            "Mark Dopson",
            "Marcelo Ketzer",
            "Jennifer C. McIntosh",
            "Henrik Drake"
        ],
        "journal_conference_name": "Discover Applied Sciences",
        "publisher": "Springer International Publishing",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/157943",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Workforce Readiness Level (WRL) Deskbook",
        "abstract": "This document describes how Workforce Readiness Levels (WRLs) can be used as a critical part of the strategic development of manufacturing processes for emerging technology. WRLs are designed to align with TRLs and MRLs, and conducting Workforce Readiness Assessments will enable organizations to intentionally prepare their manufacturing workforce. Our Deskbook is offered as a companion to the MRL and TRL Deskbooks to assist industry leaders in following a holistic and integrated approach to technology and workforce readiness.\r\nWe anticipate that WRL assessments will be conducted by both industry leaders as they prepare for workforce transition and expansion, and by researchers and consultants supporting the emergence of innovative and cutting-edge technologies. The outcomes of these assessments will inform not only recruiting and hiring for manufacturing organizations, but also training and development both within organizations and in community colleges and vocational schools.",
        "authors": [
            "Juliet Aiken",
            "Elizabeth Moore",
            "Autumn Fedewa",
            "Taylor Jordan",
            "Frank Field",
            "Elizabeth Unger",
            "Randolph Kirchain"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/157940",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Photonic Integrated Devices Manufacturing Workforce Preparation Assessment Report: Middle Skilled Technicians",
        "abstract": "The Conducere-MIT Collaboratory conducted a Manufacturing Workforce Preparation assessment (MWP) for photonic integrated devices, particularly bio-photonic devices. Leveraging data from O*NET and expert interviews, the team identified the need for two manufacturing technician positions, the Photonic Integrated Circuit Technician and the Functionalization Technician. The team identified tasks, skills, and competencies that are critical and needed at entry for manufacturing technicians producing photonic integrated devices. While tasks between these positions are expected to vary somewhat, the skills and competencies needed to be successful are not expected to vary much. Additionally, needed skills and competencies will likely differ for technicians working predominantly with automated equipment and technologists who conduct their work more manually. Much of what technicians need to be able to do will be learned on the job.\r\nThis report first shares what photonic integrated devices are and summarizes our findings relevant to the market for this emerging technology. The report then details role, task, skill, and competency findings for the technician positions, and provides insight into equipment, education, recruiting, hiring, and training.",
        "authors": [
            "Juliet Aiken",
            "Elizabeth Moore",
            "Autumn Fedewa",
            "Taylor Jordan",
            "Frank Field",
            "Elizabeth Unger",
            "Anuradha Argarwal",
            "Randolph Kirchain"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/157941",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Vehicle Routing Problem Formulation for Efficient Tracking of Objects in Low Earth Orbit",
        "abstract": "The increasing number of resident space objects (RSOs) in low Earth orbit (LEO) endangers the sustainable use of space and necessitates continuous surveillance to prevent collisions. The U.S. Space Surveillance Network (SSN) tracks tens of thousands of LEO RSOs using a suite of ground-based sensors; however, the algorithms that task and schedule these sensors have not improved significantly in the last twenty years. In that time, the number of catalogued LEO RSOs has more than doubled, calling for more efficient tasking algorithms. Prior research has primarily focused on improving the tasking of ground-based sensors for tracking RSOs in geosynchronous Earth orbit (GEO). In this paper, we extend recent work on a vehicle routing problem (VRP) formulation for optimal tasking and scheduling of ground-based radars for tracking GEO RSOs and apply it to tracking LEO RSOs. We introduce a modified VRP formulation, which features discrete time indexing and leverages sparse, binary feasibility matrices for reduced computation time, and present results for several simulations. We show that our approach can compute global and regional optima for tracking (a) 100 targets using 4 ground-based sensors over a 5-hour time horizon in under 5 minutes on a laptop computer and (b) 10,000 targets using 27 ground-based sensors over a 24-hour time horizon in about 4 hours on a high-performance computing cluster.",
        "authors": [
            "Allan Shtofenmakher",
            "Hamsa Balakrishnan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "American Institute of Aeronautics and Astronautics",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158022",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Resurgence in Liouville theory",
        "abstract": "Liouville conformal field theory is a prototypical example of an exactly solvable quantum field theory, in the sense that the correlation functions in an arbitrary background can be determined exactly using only the constraints of unitarity and crossing symmetry. For example, the three point correlation functions are given by the famous formula of Dorn-Otto-Zamolodchikov-Zamolodchikov (DOZZ). Unlike many other exactly solvable theories, Liouville theory has a continuously tunable parameter — essentially ℏ — which is related to the central charge of the theory. Here we investigate the nature of the perturbative expansion in powers of ℏ, which is the loop expansion around a semi-classical solution. We show that the perturbative coefficients grow factorially, as expected of a Feynman diagram expansion, and take the form of an asymptotic series. We identify the singularities in the Borel plane, and show that they are associated with complex instanton solutions of Liouville theory; they correspond precisely to the complex solutions described by Harlow, Maltz, and Witten. Both single- and multi-valued solutions of Liouville appear. We show that the perturbative loop expansions around these different saddle points mix in the way expected for a trans-series expansion. Thus Liouville theory provides a calculable example of a quantum field theory where perturbative and instanton contributions can be summed up and assembled into a finite answer.",
        "authors": [
            "Nathan Benjamin",
            "Scott Collier",
            "Alexander Maloney",
            "Viraj Meruliya"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/157950",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Measurement of the double-diferential inclusive jet cross section in proton-proton collisions at √s = 5.02 TeV",
        "abstract": "The inclusive jet cross section is measured as a function of jet transverse momentum pT and rapidity y. The measurement is performed using proton-proton collision data at s = 5.02 TeV, recorded by the CMS experiment at the LHC, corresponding to an integrated luminosity of 27.4 pb−1. The jets are reconstructed with the anti-kT algorithm using a distance parameter of R = 0.4, within the rapidity interval |y| < 2, and across the kinematic range 0.06 < pT < 1 TeV. The jet cross section is unfolded from detector to particle level using the determined jet response and resolution. The results are compared to predictions of perturbative quantum chromodynamics, calculated at both next-to-leading order and next-to-next-to-leading order. The predictions are corrected for nonperturbative effects, and presented for a variety of parton distribution functions and choices of the renormalization/factorization scales and the strong coupling αS.",
        "authors": [
            "A. Hayrapetyan",
            "A. Tumasyan",
            "W. Adam",
            "J. W. Andrejkovic",
            "T. Bergauer",
            "S. Chatterjee",
            "K. Damanakis",
            "M. Dragicevic",
            "A. Escalante Del Valle",
            "P. S. Hussain",
            "M. Jeitler",
            "N. Krammer",
            "L. Lechner",
            "D. Liko",
            "I. Mikulec",
            "J. Schieck",
            "R. Schöfbeck",
            "The CMS collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158044",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Uniacute Spherical Codes",
        "abstract": "A spherical L-code, where L ⊆ [−1,∞), consists of unit vectors in Rd whose pairwise inner products are contained in L. Determining the maximum cardinality NL (d)\r\nof an L-code in Rd is a fundamental question in discrete geometry and has been\r\nextensively investigated for various choices of L. Our understanding in high dimensions is generally quite poor. Equiangular lines, corresponding to L = {−α, α}, is\r\na rare and notable solved case. Bukh studied an extension of equiangular lines and\r\nshowed that NL (d) = OL (d) for L = [−1, −β]∪{α} with α, β > 0 (we call such\r\nL-codes “uniacute”), leaving open the question of determining the leading constant\r\nfactor. Balla, Dräxler, Keevash, and Sudakov proved a “uniform bound” showing\r\nlim supd→∞ NL (d)/d ≤ 2p for L = [−1, −β]∪{α} and p =  α/β  + 1. For which\r\n(α, β) is this uniform bound tight? We completely answer this question. We develop a\r\nframework for studying uniacute codes, including a global structure theorem showing\r\nthat the Gram matrix has an approximate p-block structure. We also formulate a notion\r\nof “modular codes,” which we conjecture to be optimal in high dimensions.",
        "authors": [
            "Saba Lepsveridze",
            "Aleksandre Saatashvili",
            "Yufei Zhao"
        ],
        "journal_conference_name": "Combinatorica",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159064",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Intel’s Fall from Grace",
        "abstract": "Intel continues to dominate the microprocessor market for personal computers and datacenter servers for cloud services but it has fallen sharply behind Nvidia, the new platform leader for AI applications and GPU servers.  Intel's decline has two main causes, apart from the innovations at Nvidia. One involves the inability to adapt to new technologies and customers (i.e., mobile and AI) as they emerged.  A second involves the commitment to manufacture its own microprocessors even though advanced semiconductor manufacturing, led by Taiwan Semiconductor Manufacturing Corp. (TSMC), has evolved into a highly specialized capability, separable from design.",
        "authors": [
            "Michael Cusumano"
        ],
        "journal_conference_name": "Communications of the ACM",
        "publisher": "ACM",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158131",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "TeleAbsence: A Vision of Past and Afterlife Telepresence",
        "abstract": "This paper presents our vision of TeleAbsence, extending the concept of telepresence to the past and the afterlife to address the vast emotional and temporal distance caused by the memory of loved ones who drifted apart and faded away. Instead of explicit and literal representations of loved ones, TeleAbsence describes poetic encounters with digital and physical traces left by the absence of others. TeleAbsence fosters illusory communications to conjure the feeling of being there with those no longer with us without using synthetic or generative representations and utterances. Our vision is deeply inspired by the Portuguese concept “Saudade”—the “desire for the beloved thing, people, place, and moment, made painful by its absence.” We present our vision through five design principles: presence of absence, illusory communication, the materiality of memory, traces of reflection, and remote time, grounded in historical and cultural contexts. We present exploratory narratives to illustrate these principles and the concept of ambient co-presence using poetry, phone, piano, and pen as mediums. We discuss challenges and opportunities for future work, including representational strategies to depict lost loved ones, ethical issues, and the possible extension of TeleAbsence to historical public figures.",
        "authors": [
            "Hiroshi Ishii",
            "Daniel Pillis",
            "Pat Pataranutaporn",
            "Xiao Xiao",
            "Hayoun Noh",
            "Lucy Li",
            "Alaa Algargoosh",
            "Jean-Baptiste Labrune"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "MIT Press",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158451",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Reversibly Switching Hydrogen-Responsive Palladium-Graphene Composite Membranes",
        "abstract": "",
        "authors": [
            "Lohyun Kim",
            "Aaron Persad",
            "Chi Cheng",
            "Randall Field",
            "Rohit Karnik"
        ],
        "journal_conference_name": "Advanced Functional Materials",
        "publisher": "Wiley",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/157990",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Estudios feministas de seguridad desde América Latina y el Caribe",
        "abstract": "",
        "authors": [
            "Alessandra Jungs de Almeida",
            "Catherine D'Ignazio"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Biblioteca Universitária/Universidade Federal de Santa Catarina",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158241",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "SPECTER: efficient evaluation of the spectral EMD",
        "abstract": "The Energy Mover’s Distance (EMD) has seen use in collider physics as a metric between events and as a geometric method of defining infrared and collinear safe observables. Recently, the Spectral Energy Mover’s Distance (SEMD) has been proposed as a more analytically tractable alternative to the EMD. In this work, we obtain a closed-form expression for the Riemannian-like p = 2 SEMD metric between events, eliminating the need to numerically solve an optimal transport problem. Additionally, we show how the SEMD can be used to define event and jet shape observables by minimizing the distance between events and parameterized energy flows (similar to the EMD), and we obtain closed-form expressions for several of these observables. We also present the Specter framework, an efficient and highly parallelized implementation of the SEMD metric and SEMD-derived shape observables as an analogue of the previously-introduced Shaper for EMD-based computations. We demonstrate that computing the SEMD with Specter can be up to a thousand times faster than computing the EMD with standard optimal transport libraries.",
        "authors": [
            "Rikab Gambhir",
            "Andrew J. Larkoski",
            "Jesse Thaler"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157951",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Ortho-Unit Polygons can be Guarded with at most n - 4 8 Guards",
        "abstract": "Abstract An orthogonal polygon is called an ortho-unit polygon if its vertices have integer coordinates, and all of its edges have length one. In this paper we prove that any ortho-unit polygon with n ≥ 12 vertices can be guarded with at most ⌊ n - 4 8 ⌋ guards, which is a tight bound.",
        "authors": [
            "J. M. Díaz-Báñez",
            "P. Horn",
            "M. A. Lopez",
            "N. Marín",
            "A. Ramírez-Vigueras",
            "O. Solé-Pi",
            "A. Stevens",
            "J. Urrutia"
        ],
        "journal_conference_name": "Graphs and Combinatorics",
        "publisher": "Springer Japan",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/159047",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Bias in machine learning applications to address non-communicable diseases at a population-level: a scoping review",
        "abstract": "Background Machine learning (ML) is increasingly used in population and public health to support epidemiological studies, surveillance, and evaluation. Our objective was to conduct a scoping review to identify studies that use ML in population health, with a focus on its use in non-communicable diseases (NCDs). We also examine potential algorithmic biases in model design, training, and implementation, as well as efforts to mitigate these biases. Methods We searched the peer-reviewed, indexed literature using Medline, Embase, Cochrane Central Register of Controlled Trials and Cochrane Database of Systematic Reviews, CINAHL, Scopus, ACM Digital Library, Inspec, Web of Science’s Science Citation Index, Social Sciences Citation Index, and the Emerging Sources Citation Index, up to March 2022. Results The search identified 27 310 studies and 65 were included. Study aims were separated into algorithm comparison (n = 13, 20%) or disease modelling for population-health-related outputs (n = 52, 80%). We extracted data on NCD type, data sources, technical approach, possible algorithmic bias, and jurisdiction. Type 2 diabetes was the most studied NCD. The most common use of ML was for risk modeling. Mitigating bias was not extensively addressed, with most methods focused on mitigating sex-related bias. Conclusion This review examines current applications of ML in NCDs, highlighting potential biases and strategies for mitigation. Future research should focus on communicable diseases and the transferability of ML models in low and middle-income settings. Our findings can guide the development of guidelines for the equitable use of ML to improve population health outcomes.",
        "authors": [
            "Sharon Birdi",
            "Roxana Rabet",
            "Steve Durant",
            "Atushi Patel",
            "Tina Vosoughi",
            "Mahek Shergill",
            "Christy Costanian",
            "Carolyn P. Ziegler",
            "Shehzad Ali",
            "David Buckeridge",
            "Marzyeh Ghassemi",
            "Jennifer Gibson",
            "Ava John-Baptiste",
            "Jillian Macklin",
            "Melissa McCradden",
            "Kwame McKenzie"
        ],
        "journal_conference_name": "BMC Public Health",
        "publisher": "BioMed Central",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157937",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Self-Assembly of a Biologically Plausible Learning Circuit",
        "abstract": "Over the last four decades, the amazing success of deep learning has been driven by the use of Stochastic Gradient Descent (SGD) as the main optimization technique. The default implementation for the computation of the gradient for SGD is backpropagation, which, with its variations, is used to this day in almost all computer implementations. From the perspective of neuroscientists, however, the consensus is that backpropagation is unlikely to be used by the brain. Though several alternatives have been discussed, none is so far supported by experimental evidence. Here we propose a circuit for updating the weights in a network that is biologically plausible, works as well as backpropagation, and leads to verifiable predictions about the anatomy and the physiology of a characteristic motif of four plastic synapses between ascending and descending cortical streams. A key prediction of our proposal is a surprising property of self-assembly of the basic circuit, emerging from initial random connectivity and heterosynaptic plasticity rules.",
        "authors": [
            "Qianli Liao",
            "Liu Ziyin",
            "Yulu Gan",
            "Brian Cheung",
            "Mark Harnett",
            "Tomaso Poggio"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Center for Brains, Minds and Machines (CBMM)",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157934",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Approaching coupled-cluster accuracy for molecular electronic structures with multi-task learning (preprint)",
        "abstract": "",
        "authors": [
            "Hao Tang",
            "Brian Xiao",
            "Wenhao He",
            "Pero Subasic",
            "Avetik R. Harutyunyan",
            "Yao Wang",
            "Fang Liu",
            "Haowei Xu",
            "Ju Li"
        ],
        "journal_conference_name": "Nature Computational Science",
        "publisher": "Springer Science and Business Media LLC",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157988",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Principles and Guidelines for Evaluating Social Robot Navigation Algorithms",
        "abstract": "A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this paper, we pave the road towards common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributions include (a) a definition of a socially navigating robot as one that respects the principles of safety, comfort, legibility, politeness, social competency, agent understanding, proactivity, and responsiveness to context, (b) guidelines for the use of metrics, development of scenarios, benchmarks, datasets, and simulators to evaluate social navigation, and (c) a design of a social navigation metrics framework to make it easier to compare results from different simulators, robots and datasets.",
        "authors": [
            "Anthony Francis",
            "Claudia P?rez-D'Arpino",
            "Chengshu Li",
            "Fei Xia",
            "Alexandre Alahi",
            "Rachid Alami",
            "Aniket Bera",
            "Abhijat Biswas",
            "Joydeep Biswas",
            "Rohan Chandra",
            "Hao-Tien Chiang",
            "Michael Everett",
            "Sehoon Ha",
            "Justin Hart",
            "Jonathan How",
            "Haresh Karnan",
            "Tsang-Wei Lee",
            "Luis Manso",
            "Reuth Mirsky",
            "S?ren Pirk"
        ],
        "journal_conference_name": "ACM Transactions on Human-Robot Interaction",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158075",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "NiNC Catalysts in CO2-to-CO Electrolysis",
        "abstract": "CO2-to-CO electrolyzer technology converts carbon dioxide into carbon monoxide using electrochemical methods, offering significant environmental and energy benefits by aiding in greenhouse gas mitigation and promoting a carbon circular economy. Recent study by Strasser et al. in Nature Chemical Engineering presents a high-performance CO2-to-CO electrolyzer utilizing a NiNC catalyst with nearly 100% faradaic efficiency, employing innovative diagnostic tools like the carbon crossover coefficient (CCC) to address transport-related failures and optimize overall efficiency. Strasser’s research demonstrates the potential of NiNC catalysts, particularly NiNC-IMI, for efficient CO production in CO2-to-CO electrolyzers, highlighting their high selectivity and performance. However, challenges such as localized CO2 depletion and mass transport limitations underscore the need for further optimization and development of diagnostic tools like CCC. Strategies for optimizing catalyst structure and operational parameters offer avenues for enhancing the performance and reliability of electrochemical CO2 reduction catalysts.",
        "authors": [
            "Hao Zhang",
            "Menghui Qi",
            "Yong Wang"
        ],
        "journal_conference_name": "Nano-Micro Letters",
        "publisher": "Springer Nature Singapore",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157938",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Walk of Guilt: Multimodal Deception Detection from Nonverbal Motion Behaviour",
        "abstract": "Detecting deceptive behaviour for surveillance and border protection is critical for a country’s security. With the advancement of technology in relation to sensors and artificial intelligence, recognising deceptive behaviour could be performed automatically. Following the success of affective computing in emotion recognition from verbal and nonverbal cues, we aim to apply a similar concept for deception detection. Recognising deceptive behaviour has been attempted; however, only a few studies have analysed this behaviour from gait and body movement. This research involves a multimodal approach for deception detection from gait, where we fuse features extracted from body movement behaviours from a video signal, acoustic features from walking steps from an audio signal, and the dynamics of walking movement using an accelerometer sensor. Using the video recording of walking from the Whodunnit deception dataset, which contains 49 subjects performing scenarios that elicit deceptive behaviour, we conduct multimodal two-category (guilty/not guilty) subject-independent classification. The classification results obtained reached an accuracy of up to 88% through feature fusion, with an average of 60% from both single and multimodal signals. Analysing body movement using single modality showed that the visual signal had the highest performance followed by the accelerometer and acoustic signals. Several fusion techniques were explored, including early, late, and hybrid fusion, where hybrid fusion not only achieved the highest classification results, but also increased the confidence of the results. Moreover, using a systematic framework for selecting the most distinguishing features of guilty gait behaviour, we were able to interpret the performance of our models. From these baseline results, we can conclude that pattern recognition techniques could help in characterising deceptive behaviour, where future work will focus on exploring the tuning and enhancement of the results and techniques.",
        "authors": [
            "Sharifa Alghowinem",
            "Sabrina Caldwell",
            "Ibrahim Radwan",
            "Michael Wagner",
            "Tom Gedeon"
        ],
        "journal_conference_name": "Information",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158142",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Investigating the Applicability of the Peak Density Thickness Parameter over the Equatorial Region",
        "abstract": "The Peak Density Thickness (PDT) refers to a vertical region in the ionosphere encompassing the F2 peak, where electron density is at its maximum, and extending upward—maintaining a constant density—for a fixed altitude beyond this peak. This study builds on the previously established PDT concept, initially explored at midlatitudes using data from Millstone Hill, by evaluating its applicability and effectiveness over equatorial latitudes using data from the Jicamarca Incoherent Scatter Radar (ISR) in Lima, Peru. A comprehensive analysis of electron density profiles measured by the Jicamarca ISR, spanning 1997 to 2020, was conducted using the Madrigal database to extract the PDT parameter for the F2 layer. Findings from the Jicamarca ISR indicate that the PDT parameter peaks around solar noon, aligning with observations from Millstone Hill. For selected case studies, the Vary-Chap topside model was employed to reconstruct the ionospheric profile above the F2 peak and PDT, demonstrating the model’s enhanced effectiveness when incorporating the PDT parameter over equatorial regions. This research confirms the presence of PDT in equatorial regions, consistent with its behavior at midlatitudes, and underscores the importance of PDT in refining predictive ionospheric models across different latitudes.",
        "authors": [
            "Mohamed O. Shammat",
            "Bodo W. Reinisch",
            "Ivan Galkin",
            "Philip J. Erickson",
            "Jay A. Weitzen",
            "William C. Rideout"
        ],
        "journal_conference_name": "Atmosphere",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158141",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Smart City Products and Their Materials Assessment Using the Pentagon Framework",
        "abstract": "Smart cities are complex urban environments that rely on advanced technology and data analytics to enhance city services&rsquo; quality of life, sustainability, and efficiency. As these cities continue to evolve, there is a growing need for a structured framework to evaluate and integrate products that align with smart city objectives. This paper introduces the Pentagon Framework, a comprehensive evaluation method designed to ensure that products and their materials meet the specific needs of smart cities. The framework focuses on five key features&mdash;smart, sustainable, sensing, social, and safe&mdash;collectively called the Penta-S concept. These features provide a structured approach to categorizing and assessing products, ensuring alignment with the city&rsquo;s goals for efficiency, sustainability, and user experience. The <i>Smart City Pentagon Framework Analyzer</i> is also presented, a dedicated web application that facilitates interaction with the framework. It allows product data input, provides feedback on alignment with the Penta-S features, and suggests personality traits based on the OCEAN model. Complementing the web application, the <i>Smart City Penta-S Compliance Assistant</i> API, developed through ChatGPT, offers a more profound, personalized evaluation of products, including the life cycle phase recommendations using the IPPMD model. This paper contributes to the development of smart city solutions by providing a flexible framework that can be applied to any product type, optimizing its life cycle, and ensuring compliance with the Pentagon Framework. This approach improves product integration and fosters user satisfaction by tailoring products and their materials to meet specific user preferences and needs within the smart city environment. The proposed framework emphasizes citizen-centric design and highlights its advantages over conventional evaluation methods, ultimately enhancing urban planning and smart city development.",
        "authors": [
            "Pedro Ponce",
            "Mario Rojas",
            "Juana Isabel Mendez",
            "Brian Anthony",
            "Russel Bradley",
            "Aminah Robinson Fayek"
        ],
        "journal_conference_name": "Multimodal Technologies and Interaction",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158140",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Model-independent search for pair production of new bosons decaying into muons in proton-proton collisions a √s = 13 TeV",
        "abstract": "The results of a model-independent search for the pair production of new bosons within a mass range of 0.21 < m < 60 GeV, are presented. This study utilizes events with a four-muon final state. We use two data sets, comprising 41.5 fb−1 and 59.7 fb−1 of proton-proton collisions at s = 13 TeV, recorded in 2017 and 2018 by the CMS experiment at the CERN LHC. The study of the 2018 data set includes a search for displaced signatures of a new boson within the proper decay length range of 0 < cτ < 100 mm. Our results are combined with a previous CMS result, based on 35.9 fb−1 of proton-proton collisions at s = 13 TeV collected in 2016. No significant deviation from the expected background is observed. Results are presented in terms of a model-independent upper limit on the product of cross section, branching fraction, and acceptance. The findings are interpreted across various benchmark models, such as an axion-like particle model, a vector portal model, the next-to-minimal supersymmetric standard model, and a dark supersymmetric scenario, including those predicting a non-negligible proper decay length of the new boson. In all considered scenarios, substantial portions of the parameter space are excluded, expanding upon prior results.",
        "authors": [
            "A. Hayrapetyan",
            "A. Tumasyan",
            "W. Adam",
            "J. W. Andrejkovic",
            "T. Bergauer",
            "S. Chatterjee",
            "K. Damanakis",
            "M. Dragicevic",
            "P. S. Hussain",
            "M. Jeitler",
            "N. Krammer",
            "A. Li",
            "D. Liko",
            "I. Mikulec",
            "J. Schieck",
            "R. Schöfbeck",
            "D. Schwarz",
            "M. Sonawane",
            "The CMS collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157949",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Evaluation and Analysis of Next-Generation FY-4A LPW Products over Various Climatic Regions in China",
        "abstract": "Atmospheric water vapor, a significant constituent of the atmosphere, affects the energy balance between Earth’s atmosphere and space, and its changes play a crucial role in the greenhouse effect. Layer precipitable water (LPW), which represents the column-integral water vapor within a vertical range, is increasingly recognized as a key indicator of atmospheric water vapor distributions and variations. Due to its capability for layer-wise monitoring, LPW products have the potential to offer valuable insights into the characteristics and evolution of climatic regions through refined atmospheric spatiotemporal information. However, the observational quality and spatiotemporal variations of LPW products across different climate zones, e.g., the diverse climatic regions in China, have not been systematically assessed. In this paper, we aim to evaluate and analyze the climatic and seasonal variations of FY-4A LPW products across five climatic regions in China, contributing to a deeper understanding of water vapor variability and providing valuable data for climate change research. A surface pressure calibration algorithm for ERA5 data is developed to calculate accurate ERA5 LPW products. The results show that all four FY-4A LPWs are consistent with ERA5 LPWs, with an overall root mean square error (RMSE) of 2.58, 0.90, 1.30, and 1.01 mm, respectively. Furthermore, FY-4A LPWs are underestimated in the temperate monsoon area and overestimated in the subtropical and tropical monsoon regions, while FY-4A observations agree well with ERA5 reanalysis in temperate continental and plateau mountain zones. These analyses highlight the remarkable climate dependency of FY-4A LPWs and their potential for climate-related studies.",
        "authors": [
            "Wenyuan Zhang",
            "Xinyu Xiao",
            "Jinsong Peng",
            "Shubi Zhang",
            "Endrit Shehaj",
            "Gregor Moeller"
        ],
        "journal_conference_name": "Atmosphere",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157961",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Age- and Sex-Based Developmental Biomarkers in Eye Movements",
        "abstract": "Background: Eye movement research serves as a critical tool for assessing brain function, diagnosing neurological and psychiatric disorders, and understanding cognition and behavior. Sex differences have largely been under reported or ignored in neurological research. However, eye movement features provide biomarkers that are useful for disease classification with superior accuracy and robustness compared to previous classifiers for neurological diseases. Neurological diseases have a sex specificity, yet eye movement analysis has not been specific to our understanding of sex differences. Methods: The study involved subjects recruited from 804 sites equipped with RightEye Vision Systems, primarily located in optometry practices across the United States. Subjects completed six eye movement assessments: circular smooth pursuit (CSP), horizontal smooth pursuit (HSP), vertical smooth pursuit (VSP), horizontal saccades (HS), vertical saccades (VS), and fixation stability (FS). Eye movements were analyzed and classified in accordance with age and sex by multiple t-tests and linear regression models. Results: This study represented a large sample size of 23,557 subjects, with 11,871 males and 11,686 females representing ages from birth through 80 years of age. We observed statistically significant differences for all eye movement functions between males and females. Conclusions: We demonstrate that eye movements are sex-specific and offer normative data to compare sex-specific eye movement function by age. Novel baseline metrics can be compared to individual performance, regardless of sex. This study represents significant progress in linking eye movements with brain function and clinical syndromes, allowing researchers and clinicians to stratify individuals by age and sex.",
        "authors": [
            "Frederick Robert Carrick",
            "Melissa Hunfalvay",
            "Takumi Bolte",
            "Sergio F. Azzolino",
            "Mahera Abdulrahman",
            "Ahmed Hankir",
            "Matthew M. Antonucci",
            "Nouf Al-Rumaihi"
        ],
        "journal_conference_name": "Brain Sciences",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157960",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Analysis of Λ b 0 → pK−μ+μ− decays",
        "abstract": "The differential branching fraction and angular coefficients of Λ b 0 → pK−μ+μ− decays are measured in bins of the dimuon mass squared and dihadron mass. The analysis is performed using a data set corresponding to 9 fb−1 of integrated luminosity collected with the LHCb detector between 2011 and 2018. The data are consistent with receiving contributions from a mixture of Λ resonances with different spin-parity quantum numbers. The angular coefficients show a pattern of vector-axial vector interference that is a characteristic of the type of flavour-changing neutral-current transition relevant for these decays.",
        "authors": [
            "R. Aaij",
            "A. S. W. Abdelmotteleb",
            "C. Abellan Beteta",
            "F. Abudinén",
            "T. Ackernley",
            "A. A. Adefisoye",
            "B. Adeva",
            "M. Adinolfi",
            "P. Adlarson",
            "C. Agapopoulou",
            "C. A. Aidala",
            "Z. Ajaltouni",
            "S. Akar",
            "K. Akiba",
            "P. Albicocco",
            "J. Albrecht",
            "The LHCb collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157948",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Learning Probabilistic Boolean Network Model of a Smart Grid with Applications in System Maintenance",
        "abstract": "Probabilistic Boolean Networks can capture the dynamics of complex biological systems as well as other non-biological systems, such as manufacturing systems and smart grids. In this proof-of-concept manuscript, we propose a Probabilistic Boolean Network architecture with a learning process that significantly improves the prediction of the occurrence of faults and failures in smart-grid systems. This idea was tested in a Probabilistic Boolean Network model of the WSCC nine-bus system that incorporates Intelligent Power Routers on every bus. The model learned the equality and negation functions in the different experiments performed. We take advantage of the complex properties of Probabilistic Boolean Networks to use them as a positive feedback adaptive learning tool and to illustrate that these networks could have a more general use than previously thought. This multi-layered PBN architecture provides a significant improvement in terms of performance for fault detection, within a positive-feedback network structure that is more tolerant of noise than other techniques.",
        "authors": [
            "Pedro Juan Rivera Torres",
            "Chen Chen",
            "Jaime Macías-Aguayo",
            "Sara Rodríguez González",
            "Javier Prieto Tejedor",
            "Orestes Llanes Santiago",
            "Carlos Gershenson García",
            "Samir Kanaan Izquierdo"
        ],
        "journal_conference_name": "Energies",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157959",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "UFO Instruction Graphs Are Machine Knittable",
        "abstract": "Programming low-level controls for knitting machines is a meticulous, time-consuming task that demands specialized expertise. Recently, there has been a shift towards automatically generating low-level knitting machine programs from high-level knit representations that describe knit objects in a more intuitive, user-friendly way. Current high-level systems trade off\r\nexpressivity for ease-of-use, requiring ad-hoc trapdoors to access the full space of machine capabilities, or eschewing completeness in the name of utility. Thus, advanced techniques either require ad-hoc extensions from domain experts, or are entirely unsupported. Furthermore, errors may emerge during the compilation from knit object representations to machine instructions. While the generated program may describe a valid machine control sequence, the fabricated object is topologically different from the specified input, with little recourse for understanding and fixing the issue.\r\n\r\nTo address these limitations, we introduce instruction graphs, an intermediate representation capable of capturing the full range of machine knitting programs. We define a semantic mapping from instruction graphs to fenced tangles, which make them compatible with the established formal semantics for machine knitting instructions. We establish a semantics-preserving bijection between machine knittable instruction graphs and knit programs that proves three properties &#8211; upward, forward, and ordered (UFO) &#8211; are both necessary and sufficient to ensure the existence of a machine knitting program that can fabricate the fenced tangle denoted by the graph. As a proof-of-concept, we implement an instruction graph editor and compiler that allows a user to transform an instruction graph into UFO presentation and then compile it to a machine program, all while maintaining semantic equivalence. In addition, we use the UFO properties to more precisely characterize the limitations of existing compilers. This work lays the groundwork for more expressive and reliable automated knitting machine programming systems by providing a formal characterization of machine knittability.",
        "authors": [
            "Jenny Lin",
            "Yuka Ikarashi",
            "Gilbert Bernstein",
            "James McCann"
        ],
        "journal_conference_name": "ACM Transactions on Graphics",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157853",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Procedural Material Generation with Reinforcement Learning",
        "abstract": "Modern 3D content creation heavily relies on procedural assets. In particular, procedural materials are ubiquitous in the industry, but their manipulation remains challenging. Previous work conditionally generates procedural graphs that match a given input image. However, the parameter generation step limits how accurately the generated graph matches the input image, due to a reliance on supervision with scarcely available procedural data. We propose to improve parameter prediction accuracy for image-conditioned procedural material generation by leveraging reinforcement learning (RL) and present the first RL approach for procedural materials. RL circumvents the limited availability of procedural data, the domain gap between real and synthetic materials, and the need for end-to-end differentiable loss functions. Given a target image, we retrieve a procedural material and use an RL-trained transformer model to predict a set of parameters that reconstruct the target image as closely as possible. We show that using RL significantly improves parameter prediction to match a given target image compared to supervised methods on both synthetic and real target images.",
        "authors": [
            "Beichen Li",
            "Yiwei Hu",
            "Paul Guerrero",
            "Milos Hasan",
            "Liang Shi",
            "Valentin Deschaintre",
            "Wojciech Matusik"
        ],
        "journal_conference_name": "ACM Transactions on Graphics",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157855",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "All you need is rotation: Construction of developable strips",
        "abstract": "We present a novel approach to generate developable strips along a space curve. The key idea of the new method is to use the rotation angle between the Frenet frame of the input space curve, and its Darboux frame of the curve on the resulting developable strip as a free design parameter, thereby revolving the strip around the tangential axis of the input space curve. This angle is not restricted to be constant but it can be any differentiable function defined on the curve, thereby creating a large design space of developable strips that share a common directrix curve. The range of possibilities for choosing the rotation angle is diverse, encompassing constant angles, linearly varying angles, sinusoidal patterns, and even solutions derived from initial value problems involving ordinary differential equations. This enables the potential of the proposed method to be used for a wide range of practical applications, spanning fields such as architectural design, industrial design, and papercraft modeling. In our computational and physical examples, we demonstrate the flexibility of the method by constructing, among others, toroidal and helical windmill blades for papercraft models, curved foldings, triply orthogonal structures, and developable strips featuring a log-aesthetic directrix curve.",
        "authors": [
            "Takashi Maekawa",
            "Felix Scholz"
        ],
        "journal_conference_name": "ACM Transactions on Graphics",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157852",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Medial Skeletal Diagram: A Generalized Medial Axis Approach for Compact 3D Shape Representation",
        "abstract": "We propose the Medial Skeletal Diagram, a novel skeletal representation that tackles the prevailing issues around skeleton sparsity and reconstruction accuracy in existing skeletal representations. Our approach augments the continuous elements in the medial axis representation to effectively shift the complexity away from the discrete elements. To that end, we introduce generalized enveloping primitives, an enhancement over the standard primitives in the medial axis, which ensure efficient coverage of intricate local features of the input shape and substantially reduce the number of discrete elements required. Moreover, we present a computational framework for constructing a medial skeletal diagram from an arbitrary closed manifold mesh. Our optimization pipeline ensures that the resulting medial skeletal diagram comprehensively covers the input shape with the fewest primitives. Additionally, each optimized primitive undergoes a post-refinement process to guarantee an accurate match with the source mesh in both geometry and tessellation. We validate our approach on a comprehensive benchmark of 100 shapes, demonstrating the sparsity of the discrete elements and superior reconstruction accuracy across a variety of cases. Finally, we exemplify the versatility of our representation in downstream applications such as shape generation, mesh decomposition, shape optimization, mesh alignment, mesh compression, and user-interactive design.",
        "authors": [
            "Minghao Guo",
            "Bohan Wang",
            "Wojciech Matusik"
        ],
        "journal_conference_name": "ACM Transactions on Graphics",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157854",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Memory Checking Requires Logarithmic Overhead",
        "abstract": "We study the complexity of memory checkers with computational security and prove the first general tight lower bound.         Memory checkers, first introduced over 30 years ago by Blum, Evans, Gemmel, Kannan, and Naor (FOCS '91, Algorithmica '94), allow a user to store and maintain a large memory on a remote and unreliable server by using small trusted local storage. The user can issue instructions to the server and after every instruction, obtain either the correct value or a failure (but not an incorrect answer) with high probability. The main complexity measure of interest is the size of the local storage and the number of queries the memory checker makes upon every logical instruction. The most efficient known construction has query complexity $O(\\log n/\\log\\log n)$ and local space proportional to a computational security parameter, assuming one-way functions, where $n$ is the logical memory size. Dwork, Naor, Rothblum, and Vaikuntanathan (TCC '09) showed that for a restricted class of ``deterministic and non-adaptive' memory checkers, this construction is optimal, up to constant factors. However, going beyond the small class of deterministic and non-adaptive constructions has remained a major open problem.     In this work, we fully resolve the complexity of memory checkers by showing that \\emph{any} construction with local space $p$ and query complexity $q$ must satisfy       $$ p \\ge \\frac{n}{(\\log n)^{O(q)}} \\;. $$      This implies, as a special case, that $q\\ge \\Omega(\\log n/\\log\\log n)$ in any scheme, assuming that $p\\le n^{1-\\varepsilon}$ for $\\varepsilon>0$. The bound applies to any scheme with computational security, completeness $2/3$, and inverse polynomial in $n$ soundness (all of which make our lower bound only stronger). We further extend the lower bound to schemes where the read complexity $q_r$ and write complexity $q_w$ differ. For instance, we show the tight bound that if $q_r=O(1)$ and $p\\le n^{1-\\varepsilon}$ for $\\varepsilon>0$, then $q_w\\ge n^{\\Omega(1)}$. This is the first lower bound, for any non-trivial class of constructions, showing a read-write query complexity trade-off.        Our proof is via a delicate compression argument showing that a ``too good to be true' memory checker can be used to compress random bits of information. We draw inspiration from tools recently developed for lower bounds for relaxed locally decodable codes. However, our proof itself significantly departs from these works, necessitated by the differences between settings.",
        "authors": [
            "Elette Boyle",
            "Ilan Komargodski",
            "Neekon Vafa"
        ],
        "journal_conference_name": "Journal of the ACM",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158076",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Innovative Approach to Sustainable Fertilizer Production: Leveraging Electrically Assisted Conversion of Sewage Sludge for Nutrient Recovery",
        "abstract": "Efforts addressing sludge management, food security, and resource recovery have led to novel approaches in these areas. Electrically assisted conversion of sludge stands out as a promising technology for sewage sludge valorization, producing nitrogen and phosphorus-based fertilizers. The adoption of this technology, which could lead to a fertilizer circular economy, holds the potential to catalyze a transformative change in wastewater treatment facilities toward process intensification, innovation, and sustainability. This paper provides insights into the economic aspects of the technology, policy considerations, and challenges involved in realizing the potential of electrified processes for sludge valorization. To demonstrate the impact of the technology, a case study for its implementation in the United States assuming the municipal wastewater treatment plants market is discussed. It was found that electrically assisted sludge conversion could enable the recovery of nitrogen and phosphorus from waste, representing up to 9% of the nitrogen and 32% of the phosphorus consumption of the U.S. for fertilizer use. This technology also enables full electrification and modularization of the process, thereby presenting significant economic and environmental opportunities.",
        "authors": [
            "Gerardine G Botte",
            "Dayana Donneys-Victoria",
            "Christian E Alvarez-Pugliese",
            "Jedidian Adjei",
            "Selin Sahin",
            "Nathan W Wilson",
            "Kayleigh Millerick",
            "Amy Hardberger",
            "Ariel L Furst",
            "Nicole Hu",
            "Andrew J Medford"
        ],
        "journal_conference_name": "ACS Omega",
        "publisher": "American Chemical Society",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158291",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploring the Dynamics of Canine-Assisted Interactions: A Wearable Approach to Understanding Interspecies Well-Being",
        "abstract": "first_pageDownload PDFsettingsOrder Article Reprints\r\nOpen AccessFeature PaperArticle\r\nExploring the Dynamics of Canine-Assisted Interactions: A Wearable Approach to Understanding Interspecies Well-Being\r\nby Timothy R. N. Holder 1ORCID,Colt Nichols 2ORCID,Emily Summers 2,David L. Roberts 3ORCID andAlper Bozkurt 2,*ORCID\r\n1\r\nDepartment of Aeronautics and Astronautics, Massachusetts Institute of Technology, Cambridge, MA 02139, USA\r\n2\r\nDepartment of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC 27695, USA\r\n3\r\nDepartment of Computer Science, North Carolina State University, Raleigh, NC 27695, USA\r\n*\r\nAuthor to whom correspondence should be addressed.\r\nAnimals 2024, 14(24), 3628; https://doi.org/10.3390/ani14243628\r\nSubmission received: 11 October 2024 / Revised: 15 November 2024 / Accepted: 27 November 2024 / Published: 16 December 2024\r\n(This article belongs to the Special Issue Animal–Computer Interaction: New Horizons in Animal Welfare)\r\nDownloadkeyboard_arrow_down Browse Figures Versions Notes\r\n\r\nSimple Summary\r\nThis study utilizes electronic sensors to investigate the outcomes of Canine Assisted Interactions (CAI), a growing therapeutic field, for both human and animal participants. It represents the first attempt to deploy synchronized wearable systems on both humans and dogs, allowing for the continuous and simultaneous collection of physiological and behavioral data during interactions. Leveraging this data, the research examines the real-time dynamics of CAIs, moving beyond traditional survey-based pre- and post-session evaluations. Three innovative visualization tools—a subsession heatmap, a synchrony table, and a metric correlation matrix—are introduced to better characterize the interactions and bonding within human-dog dyads. Preliminary exploratory analyses provide insights that inspire further investigation into CAI mechanisms. This research marks a significant step forward in using multimodal data collection to deepen our understanding of human-animal interactions, particularly in therapeutic settings.\r\nAbstract\r\nCanine-assisted interactions (CAIs) have been explored to offer therapeutic benefits to human participants in various contexts, from addressing cancer-related fatigue to treating post-traumatic stress disorder. Despite their widespread adoption, there are still unresolved questions regarding the outcomes for both humans and animals involved in these interactions. Previous attempts to address these questions have suffered from core methodological weaknesses, especially due to absence of tools for an efficient objective evaluation and lack of focus on the canine perspective. In this article, we present a first-of-its-kind system and study to collect simultaneous and continuous physiological data from both of the CAI interactants. Motivated by our extensive field reviews and stakeholder feedback, this comprehensive wearable system is composed of custom-designed and commercially available sensor devices. We performed a repeated-measures pilot study, to combine data collected via this system with a novel dyadic behavioral coding method and short- and long-term surveys. We evaluated these multimodal data streams independently, and we further correlated the psychological, physiological, and behavioral metrics to better elucidate the outcomes and dynamics of CAIs. Confirming previous field results, human electrodermal activity is the measure most strongly distinguished between the dyads’ non-interaction and interaction periods. Valence, arousal, and the positive affect of the human participant significantly increased during interaction with the canine participant. Also, we observed in our pilot study that (a) the canine heart rate was more dynamic than the human’s during interactions, (b) the surveys proved to be the best indicator of the subjects’ affective state, and (c) the behavior coding approaches best tracked the bond quality between the interacting dyads. Notably, we found that most of the interaction sessions were characterized by extended neutral periods with some positive and negative peaks, where the bonded pairs might display decreased behavioral synchrony. We also present three new representations of the internal and overall dynamics of CAIs for adoption by the broader field. Lastly, this paper discusses ongoing options for further dyadic analysis, interspecies emotion prediction, integration of contextually relevant environmental data, and standardization of human–animal interaction equipment and analytical approaches. Altogether, this work takes a significant step forward on a promising path to our better understanding of how CAIs improve well-being and how interspecies psychophysiological states can be appropriately measured.",
        "authors": [
            "Timothy R. N. Holder",
            "Colt Nichols",
            "Emily Summers",
            "David L. Roberts",
            "Alper Bozkurt"
        ],
        "journal_conference_name": "Animals",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157958",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Higher dimensional Fourier quasicrystals from Lee–Yang varieties",
        "abstract": "In this paper, we construct Fourier quasicrystals with unit masses in arbitrary dimensions. This generalizes a one-dimensional construction of Kurasov and Sarnak. To do this, we employ a class of complex algebraic varieties avoiding certain regions in C n , which generalize hypersurfaces defined by Lee–Yang polynomials. We show that these are Delone almost periodic sets that have at most finite intersection with every discrete periodic set.",
        "authors": [
            "Lior Alon",
            "Mario Kummer",
            "Pavel Kurasov",
            "Cynthia Vinzant"
        ],
        "journal_conference_name": "Inventiones mathematicae",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/159044",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Domain Adversarial Convolutional Neural Network Improves the Accuracy and Generalizability of Wearable Sleep Assessment Technology",
        "abstract": "Wearable accelerometers are widely used as an ecologically valid and scalable solution for long-term at-home sleep monitoring in both clinical research and care. In this study, we applied a deep learning domain adversarial convolutional neural network (DACNN) model to this task and demonstrated that this new model outperformed existing sleep algorithms in classifying sleep–wake and estimating sleep outcomes based on wrist-worn accelerometry. This model generalized well to another dataset based on different wearable devices and activity counts, achieving an accuracy of 80.1% (sensitivity 84% and specificity 58%). Compared to commonly used sleep algorithms, this model resulted in the smallest error in wake after sleep onset (MAE of 48.7, Cole–Kripke of 86.2, Sadeh of 108.2, z-angle of 57.5) and sleep efficiency (MAE of 11.8, Cole–Kripke of 18.4, Sadeh of 23.3, z-angle of 9.3) outcomes. Despite being around for many years, accelerometer-alone devices continue to be useful due to their low cost, long battery life, and ease of use. Improving the accuracy and generalizability of sleep algorithms for accelerometer wrist devices is of utmost importance. We here demonstrated that domain adversarial convolutional neural networks can improve the overall accuracy, especially the specificity, of sleep–wake classification using wrist-worn accelerometer data, substantiating its use as a scalable and valid approach for sleep outcome assessment in real life.",
        "authors": [
            "Adonay S. Nunes",
            "Matthew R. Patterson",
            "Dawid Gerstel",
            "Sheraz Khan",
            "Christine C. Guo",
            "Ali Neishabouri"
        ],
        "journal_conference_name": "Sensors",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157956",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Sampling from convex sets with a cold start using multiscale decompositions",
        "abstract": "A standard approach for sampling approximately uniformly from a convex body K ⊆ R n is to run a random walk within K. The requirement is that starting from a suitable initial distribution, the random walk should “mix rapidly”, i.e., after a number of steps that is polynomial in n and the aspect ratio R/r (here, K is assumed to contain a ball of radius r and to be contained within a ball of radius R), the distribution of the random walk should come close to the uniform distribution π K on K. Different random walks differ in aspects such as the ease of implementation of each step, or suitability for a specific class of convex bodies. Therefore, the rapid mixing of a wide variety of random walks on convex bodies has been studied. Many proofs of rapid mixing of such random walks however require that the initial distribution of the random walk is not too different from the target distribution π K . In particular, they require that the probability density function of the initial distribution with respect to the uniform distribution π K on K must be bounded above by poly ( n ) : this is called a warm start. Achieving such a warm start often requires a non-trivial pre-processing step before the random walk can be started. This motivates the problem of proving rapid mixing from “cold starts”, i.e., when the density of the initial distribution with respect to π K can be as high as exp ( poly ( n ) ) . In contrast to warm starts, a cold start is usually trivial to achieve. However, rapid mixing from a cold start may not hold for every random walk, e.g., the well-known “ball walk” does not have rapid mixing from an arbitrary cold start. On the other hand, for the “hit-and-run” random walk, Lovász and Vempala proved rapid mixing from a cold start. For the related coordinate hit-and-run (CHR) random walk, which has been found to be promising in computational experiments, a rapid mixing result starting from a warm start was proven only recently, while the question of whether CHR mixes rapidly from a cold start remained open. In this paper, we construct a family of Markov chains inspired by classical multiscale decompositions of subsets of R n into countably many axis-aligned cubes. We show that even with a cold start, the mixing times of these chains are bounded by a polynomial in n and the aspect ratio of the body. Our main technical ingredient is an isoperimetric inequality for K for a metric that magnifies distances between points that are close to the boundary of K. As a byproduct of the analysis of this new family of chains, we show that the coordinate hit-and-run (CHR) random walk also mixes rapidly from a cold start, and also from any point that is not too close to the boundary of the body.",
        "authors": [
            "Hariharan Narayanan",
            "Amit Rajaraman",
            "Piyush Srivastava"
        ],
        "journal_conference_name": "Probability Theory and Related Fields",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157860",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Adversarial Network Optimization under Bandit Feedback: Maximizing Utility in Non-Stationary Multi-Hop Networks",
        "abstract": "Stochastic Network Optimization (SNO) concerns scheduling in stochastic queueing systems and has been widely studied in network theory. Classical SNO algorithms require network conditions to be stationary w.r.t. time, which fails to capture the non-stationary components in increasingly many real-world scenarios. Moreover, most existing algorithms in network optimization assume perfect knowledge of network conditions before decision, which again rules out applications where unpredictability in network conditions presents.\r\nMotivated by these issues, this paper considers Adversarial Network Optimization (ANO) under bandit feedback. Specifically, we consider the task of i) maximizing some unknown and time-varying utility function associated with scheduler's actions, where ii) the underlying network topology is a non-stationary multi-hop network whose conditions change arbitrarily with time, and iii) only bandit feedback (the effect of actually deployed actions) is revealed after decision-making. We propose the UMO2 algorithm, which does not require any pre-decision knowledge or counterfactual feedback, ensures network stability, and also matches the utility maximization performance of any \"mildly varying\" reference policy up to a polynomially decaying gap. To our knowledge, no previous algorithm can handle multi-hop networks or achieve utility maximization guarantees in ANO problems with bandit feedback, whereas ours is able to do both.\r\nTechnically, our method builds upon a novel integration of online learning techniques into the Lyapunov drift-plus-penalty method. Specifically, we propose meticulous analytical techniques to jointly balance online learning and Lyapunov arguments, which is used to handle the complex inter-dependency among queues in multi-hop networks. To tackle the learning obstacles due to potentially unbounded queue sizes and negative queue differences, we design a new online linear optimization algorithm that automatically adapts to the unknown (potentially negative) loss magnitudes. Finally, we also propose a bandit convex optimization algorithm with novel queue-dependent learning rate scheduling that suites drastically varying queue lengths in utility maximization. Our new insights and techniques in online learning can also be of independent interest.",
        "authors": [
            "Yan Dai",
            "Longbo Huang"
        ],
        "journal_conference_name": "Proceedings of the ACM on Measurement and Analysis of Computing Systems",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158129",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Acceleration by Stepsize Hedging: Multi-Step Descent and the Silver Stepsize Schedule",
        "abstract": "Can we accelerate the convergence of gradient descent without changing the algorithmÐjust by judiciously choosing stepsizes?\r\nSurprisingly, we show that the answer is yes. Our proposed Silver Stepsize Schedule optimizes strongly convex functions in\r\n�\r\nlog�\r\n2 ≈ �\r\n0.7864 iterations, where � = 1 +\r\n√\r\n2 is the silver ratio and � is the condition number. This is intermediate between\r\nthe textbook unaccelerated rate � and the accelerated rate �\r\n1/2 due to Nesterov in 1983. The non-strongly convex setting is\r\nconceptually identical, and standard black-box reductions imply an analogous partially accelerated rate �\r\n− log�\r\n2 ≈ �\r\n−0.7864\r\n.\r\nWe conjecture and provide partial evidence that these rates are optimal among all stepsize schedules.\r\nThe Silver Stepsize Schedule is constructed recursively in a fully explicit way. It is non-monotonic, fractal-like, and\r\napproximately periodic of period �\r\nlog�\r\n2\r\n. This leads to a phase transition in the convergence rate: initially super-exponential\r\n(acceleration regime), then exponential (saturation regime).\r\nThe core algorithmic intuition is hedging between individually suboptimal strategiesÐshort steps and long stepsÐsince bad\r\ncases for the former are good cases for the latter, and vice versa. Properly combining these stepsizes yields faster convergence\r\ndue to the misalignment of worst-case functions. The key challenge in proving this speedup is enforcing long-range consistency\r\nconditions along the algorithm’s trajectory. We do this by developing a technique that recursively glues constraints from\r\ndiferent portions of the trajectory, thus removing a key stumbling block in previous analyses of optimization algorithms.\r\nMore broadly, we believe that the concepts of hedging and multi-step descent have the potential to be powerful algorithmic\r\nparadigms in a variety of contexts in optimization and beyond.\r\nThis paper publishes and extends the irst author’s 2018 Master’s Thesis (advised by the second author)Ðwhich established\r\nfor the irst time that judiciously choosing stepsizes can enable acceleration in convex optimization. Prior to this thesis, the\r\nonly such result was for the special case of quadratic optimization, due to Young in 1953.",
        "authors": [
            "Jason Altschuler",
            "Pablo Parrilo"
        ],
        "journal_conference_name": "Journal of the ACM",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158132",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Anomaly-aware summary statistic from data batches",
        "abstract": "Signal-agnostic data exploration based on machine learning could unveil very subtle statistical deviations of collider data from the expected Standard Model of particle physics. The beneficial impact of a large training sample on machine learning solutions motivates the exploration of increasingly large and inclusive samples of acquired data with resource efficient computational methods. In this work we consider the New Physics Learning Machine (NPLM), a multivariate goodness-of-fit test built on the Neyman-Pearson maximum-likelihood-ratio construction, and we address the problem of testing large size samples under computational and storage resource constraints. We propose to perform parallel NPLM routines over batches of the data, and to combine them by locally aggregating over the data-to-reference density ratios learnt by each batch. The resulting data hypothesis defining the likelihood-ratio test is thus shared over the batches, and complies with the assumption that the expected rate of new physical processes is time invariant. We show that this method outperforms the simple sum of the independent tests run over the batches, and can recover, or even surpass, the sensitivity of the single test run over the full data. Beside the significant advantage for the offline application of NPLM to large size samples, the proposed approach offers new prospects toward the use of NPLM to construct anomaly-aware summary statistics in quasi-online data streaming scenarios.",
        "authors": [
            "G. Grosso"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157890",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Deep Water Subsea Energy Storage, Lessons Learned from the Offshore Oil and Gas Industry",
        "abstract": "In a future where a large portion of power will be supplied by highly intermittent sources such as solar- and wind-power, energy storage will form a crucial part of the power mix ensuring that there is enough flexibility in the system to cope with the intermittency. With further development of pumped storage hydro constrained by the lack of remaining suitable topography, a novel Subsea Pumped Hydro Storage concept has emerged as a promising solution to utilize the ocean space for large-scale energy storage. While previous publications address thermodynamic efficiency limits, there is a notable lack of research on turbine selection, design, and cost estimation based on best practices. This paper presents a comprehensive overview of current state-of-the-art subsea engineering and its significant achievements pioneered by the oil and gas industry. This paper introduces a robust methodological framework for calculating the costs of concrete SPHS tanks, factoring in longevity and best installation practices for structures designed to endure for half a century. The results indicate that with an optimized design, the cost of an SPSH concrete storage tank is approximately $0.15/Wh. This work lays the groundwork for future advancements in SPHS, building on the substantial progress within subsea engineering over recent decades, and marks a significant step towards realizing the potential of this concept in the renewable energy landscape.",
        "authors": [
            "Rasmus Juhlin",
            "Alexander H. Slocum",
            "Mohsen Assadi"
        ],
        "journal_conference_name": "Journal of Marine Science and Engineering",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157955",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Imaging the initial condition of heavy-ion collisions and nuclear structure across the nuclide chart",
        "abstract": "High-energy nuclear collisions encompass three key stages: the structure of the colliding nuclei, informed by low-energy nuclear physics, the initial condition, leading to the formation of quark–gluon plasma (QGP), and the hydrodynamic expansion and hadronization of the QGP, leading to final-state hadron distributions that are observed experimentally. Recent advances in both experimental and theoretical methods have ushered in a precision era of heavy-ion collisions, enabling an increasingly accurate understanding of these stages. However, most approaches involve simultaneously determining both QGP properties and initial conditions from a single collision system, creating complexity due to the coupled contributions of these stages to the final-state observables. To avoid this, we propose leveraging established knowledge of low-energy nuclear structures and hydrodynamic observables to independently constrain the QGP’s initial condition. By conducting comparative studies of collisions involving isobar-like nuclei—species with similar mass numbers but different ground-state geometries—we can disentangle the initial condition’s impacts from the QGP properties. This approach not only refines our understanding of the initial stages of the collisions but also turns high-energy nuclear experiments into a precision tool for imaging nuclear structures, offering insights that complement traditional low-energy approaches. Opportunities for carrying out such comparative experiments at the Large Hadron Collider and other facilities could significantly advance both high-energy and low-energy nuclear physics. Additionally, this approach has implications for the future electron-ion collider. While the possibilities are extensive, we focus on selected proposals that could benefit both the high-energy and low-energy nuclear physics communities. Originally prepared as input for the long-range plan of U.S. nuclear physics, this white paper reflects the status as of September 2022, with a brief update on developments since then.",
        "authors": [
            "Jiangyong Jia",
            "Giuliano Giacalone",
            "Benjamin Bally",
            "James D. Brandenburg",
            "Ulrich Heinz",
            "Shengli Huang",
            "Dean Lee",
            "Yen-Jie Lee",
            "Constantin Loizides",
            "Wei Li",
            "Matthew Luzum",
            "Govert Nijs",
            "Jacquelyn Noronha-Hostler",
            "Mateusz Ploskon",
            "Wilke van der Schee",
            "Bjoern Schenke"
        ],
        "journal_conference_name": "Nuclear Science and Techniques",
        "publisher": "Springer Nature Singapore",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157891",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "How information about historic carbon emissions affects support for climate aid: evidence from a survey experiment",
        "abstract": "In recent years, international climate negotiations have reached increasing consensus that the wealthiest countries should make significant financial contributions to offset the damages caused by the climate crisis in poorer countries. Proponents have justified such action based on wealthy countries’ disproportionate responsibility for global warming in the form of past emissions. However, in democratic countries such as the United States, it remains uncertain whether such messages can affect public opinion, especially across partisan lines. We conducted a pre-registered survey from a national online pool (N = 5,002) with a built-in experiment to evaluate the effectiveness of alternative communications strategies associated with historic carbon emissions in increasing support for climate aid. We find that specific attribution claims that reflect a climate justice perspective do boost support for more generous climate aid, but the effects are largely driven by Democrats. We also find that global solidarity frames emphasizing shared responsibility did not affect support for climate aid. Our results have important implications for climate advocacy and our understanding of climate-related attitudes.",
        "authors": [
            "Volha Charnysh",
            "Jared Kalow",
            "Evan Lieberman",
            "Erin Walk"
        ],
        "journal_conference_name": "Climatic Change",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157886",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Study of the rare decay J/ψ → μ+μ−μ+μ−",
        "abstract": "The rare electromagnetic J/ψ → μ+μ−μ+μ− decay is observed with a significance greatly exceeding the discovery threshold, using proton-proton collision data collected by the LHCb experiment during 2016–2018 at a center-of-mass energy of 13 TeV, corresponding to an integrated luminosity of 5.4 fb−1. The rate of this decay is measured relative to that of the J/ψ → μ+μ− mode. Using the QED model for the four-muon decay in the efficiency estimation, its branching fraction is determined to be B J / ψ → μ + μ − μ + μ − = 1.13 ± 0.10 ± 0.05 ± 0.01 × 10 − 6 , where the uncertainties are statistical, systematic and due to the uncertainty on the branching fraction of the J/ψ → μ+μ− decay.",
        "authors": [
            "R. Aaij",
            "A. S. W. Abdelmotteleb",
            "C. Abellan Beteta",
            "F. Abudinén",
            "T. Ackernley",
            "A. A. Adefisoye",
            "B. Adeva",
            "M. Adinolfi",
            "P. Adlarson",
            "C. Agapopoulou",
            "C. A. Aidala",
            "Z. Ajaltouni",
            "S. Akar",
            "K. Akiba",
            "P. Albicocco",
            "J. Albrecht",
            "The LHCb collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157889",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Imaging the wakes of jets with energy-energy-energy correlators",
        "abstract": "As the partons in a high energy jet propagate through the droplet of quark-gluon plasma (QGP) produced in a heavy-ion collision they lose energy to, kick, and are kicked by the medium. The resulting modifications to the parton shower encode information about the microscopic nature of QGP. A direct consequence, however, is that the momentum and energy lost by the parton shower are gained by the medium and, since QGP is a strongly coupled liquid, this means that the jet excites a wake in the droplet of QGP. After freezeout, this wake becomes soft hadrons with net momentum in the jet direction meaning that what an experimentalist later reconstructs as a jet includes hadrons originating from both the modified parton shower and its wake. This has made it challenging to find experimental observables that provide an unambiguous view of the dynamical response of a droplet of QGP to a jet shooting through it. Recent years have seen significant substantial advances in the theoretical and experimental understanding of the substructure of jets, in particular, using correlation functions, E n → 1 ⋯ E n → k , of the energy flux operator in proton-proton collisions and, recently, in heavy-ion collisions. So far, such studies have focused primarily on the two-point correlator, which allows for the identification of the angular scale of the underlying dynamics. Higher-point correlators hold the promise of mapping out the dynamics themselves. In this paper we perform the first study of the shape-dependent three-point energy-energy-energy correlator in heavy-ion collisions. Using the Hybrid Model to simulate the interactions of high energy jets with the QGP medium, we show that the three-point correlator presents us with a striking new opportunity. We find that hadrons originating from wakes are the dominant contribution to the three-point correlator in the kinematic regime in which the three points are well-separated in angle, forming a roughly equilateral triangle. This equilateral region of the correlator is far from the region populated by collinear vacuum emissions, making it a canvas on which jet wakes are laid out, where experimentalists can map their shapes. Our work provides a key step towards the systematic use of energy correlators to image and unravel the dynamical response of a droplet of QGP that has been probed by a passing jet, and motivates numerous experimental and theoretical studies.",
        "authors": [
            "Hannah Bossi",
            "Arjun S. Kudinoor",
            "Ian Moult",
            "Daniel Pablos",
            "Ananya Rai",
            "Krishna Rajagopal"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157947",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "JWST sighting of decameter main-belt asteroids and view on meteorite sources",
        "abstract": "Asteroid discoveries are essential for planetary-defense efforts aiming to prevent impacts\r\nwith Earth, including the more frequent megaton explosions from decameter impactors.\r\nWhile large asteroids (≥100 km) have remained in the main belt since their formation,\r\nsmall asteroids are commonly transported to the near-Earth object (NEO) population.\r\nHowever, due to the lack of direct observational constraints, their size-frequency distribution —which informs our understanding of the NEOs and the delivery of meteorite\r\nsamples to Earth—varies significantly among models. Here, we report 138 detections\r\nof the smallest asteroids (⪆10 m) ever observed in the main belt, which were enabled by JWST’s infrared capabilities covering the asteroids’ emission peaks and synthetic tracking techniques. Despite small orbital arcs, we constrain the objects’ distances and\r\nphase angles using known asteroids as proxies, allowing us to derive sizes via radiometric\r\ntechniques. Their size-frequency distribution exhibits a break at ∼100 m (debiased cumulative slopes of q = −2.66 ± 0.60 and −0.97 ± 0.14 for diameters smaller and larger than\r\n∼100 m, respectively), suggestive of a population driven by collisional cascade. These\r\nasteroids were sampled from multiple asteroid families —most likely Nysa, Polana and\r\nMassalia— according to the geometry of pointings considered here. Through additional\r\nlong-stare infrared observations, JWST is poised to serendipitously detect thousands of\r\ndecameter-scale asteroids across the sky, probing individual asteroid families and the\r\nsource regions of meteorites “in-situ”.",
        "authors": [
            "Artem Y. Burdanov",
            "Julien de Wit",
            "Miroslav Broz",
            "Thomas G. Muller",
            "Tobias Hoffmann",
            "Marin Ferrais",
            "Marco Micheli",
            "Emmanuel Jehin",
            "Daniel Parrott",
            "Samantha N. Hasler",
            "Richard P. Binzel",
            "Elsa Ducrot",
            "Laura Kreidberg",
            "Michael Gillon",
            "Thomas P. Greene",
            "Will M. Grundy",
            "Theodore Kareta",
            "Pierre-Olivier Lagage",
            "Nicholas Moskovitz",
            "Audrey Thirouin",
            "Cristina A. Thomas",
            "Sebastian Zieba"
        ],
        "journal_conference_name": "Nature",
        "publisher": "Springer Nature",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157797",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Classical correspondence beyond the Ehrenfest time for open quantum systems with general Lindbladians",
        "abstract": "Quantum and classical systems evolving under the same formal Hamiltonian H may exhibit dramatically different behavior after the Ehrenfest timescale t E ∼ log ( ħ - 1 ) , even as ħ → 0 . Coupling the system to a Markovian environment results in a Lindblad equation for the quantum evolution. Its classical counterpart is given by the Fokker–Planck equation on phase space, which describes Hamiltonian flow with friction and diffusive noise. The quantum and classical evolutions may be compared via the Wigner-Weyl representation. Due to decoherence, they are conjectured to match closely for times far beyond the Ehrenfest timescale as ħ → 0 . We prove a version of this correspondence, bounding the error between the quantum and classical evolutions for any sufficiently regular Hamiltonian H(x, p) and Lindblad functions L k ( x , p ) . The error is small when the strength of the diffusion D associated to the Lindblad functions satisfies D ≫ ħ 4 / 3 , in particular allowing vanishing noise in the classical limit. Our method uses a time-dependent semiclassical mixture of variably squeezed Gaussian states. The states evolve according to a local harmonic approximation to the Lindblad dynamics constructed from a second-order Taylor expansion of the Lindbladian. Both the exact quantum trajectory and its classical counterpart can be expressed as perturbations of this semiclassical mixture, with the errors bounded using Duhamel’s principle. We present heuristic arguments suggesting the 4/3 exponent is optimal and defines a boundary in the sense that asymptotically weaker diffusion permits a breakdown of quantum-classical correspondence at the Ehrenfest timescale. Our presentation aims to be comprehensive and accessible to both mathematicians and physicists. In a shorter companion paper, we treat the special case of Hamiltonians that decompose into kinetic and potential energy with linear Lindblad operators, with explicit bounds that can be applied directly to physical systems.",
        "authors": [
            "Felipe Hernández",
            "Daniel Ranard",
            "C. J. Riedel"
        ],
        "journal_conference_name": "Communications in Mathematical Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157859",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Study of charmonium production via the decay to p p¯ at √s = 13 TeV",
        "abstract": "Charmonium production cross-section in proton–proton collisions is measured at the centre-of-mass energy s = 13 TeV using decays to p p ¯ final state. The study is performed using a data sample corresponding to an integrated luminosity of 2.2 fb - 1 collected in 2018 with the LHCb detector. The production cross-section of the η c meson is measured in a rapidity range of 2.0 < y < 4.0 and in a transverse momentum range of 5.0 < p T < 20.0 GeV / c , which is extended compared with previous LHCb analyses. The differential cross-section is measured in bins of p T and, for the first time, of y. Upper limits, at 90% and 95% confidence levels, on the η c ( 2 S ) and h c ( 1 P ) prompt production cross-sections are determined for the first time.",
        "authors": [
            "LHCb Collaboration"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157885",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Measurement of the electric potential and the magnetic field in the shifted analysing plane of the KATRIN experiment",
        "abstract": "The projected sensitivity of the effective electron neutrino-mass measurement with the KATRIN experiment is below 0.3 eV (90 % CL) after 5 years of data acquisition. The sensitivity is affected by the increased rate of the background electrons from KATRIN’s main spectrometer. A special shifted-analysing-plane (SAP) configuration was developed to reduce this background by a factor of two. The complex layout of electromagnetic fields in the SAP configuration requires a robust method of estimating these fields. We present in this paper a dedicated calibration measurement of the fields using conversion electrons of gaseous 83m Kr, which enables the neutrino-mass measurements in the SAP configuration.",
        "authors": [
            "KATRIN Collaboration"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157799",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Search for CP violation in D0 → K0 SK0 S decays in proton–proton collisions at √s = 13 TeV",
        "abstract": "A search is reported for charge-parity CP violation in D 0 → K S 0 K S 0 decays, using data collected in proton–proton collisions at s = 13 Te V recorded by the CMS experiment in 2018. The analysis uses a dedicated data set that corresponds to an integrated luminosity of 41.6 fb - 1 , which consists of about 10 billion events containing a pair of b hadrons, nearly all of which decay to charm hadrons. The flavor of the neutral D meson is determined by the pion charge in the reconstructed decays D ∗ + → D 0 π + and D ∗ - → D ¯ 0 π - . The CP asymmetry in D 0 → K S 0 K S 0 is measured to be A CP ( K S 0 K S 0 ) = ( 6.2 ± 3.0 ± 0.2 ± 0.8 ) % , where the three uncertainties represent the statistical uncertainty, the systematic uncertainty, and the uncertainty in the measurement of the CP asymmetry in the D 0 → K S 0 π + π - decay. This is the first CP asymmetry measurement by CMS in the charm sector as well as the first to utilize a fully hadronic final state.",
        "authors": [
            "CMS Collaboration"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157800",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Using magnetic resonance relaxometry to evaluate the safety and quality of induced pluripotent stem cell-derived spinal cord progenitor cells",
        "abstract": "Background The emergence of induced pluripotent stem cells (iPSCs) offers a promising approach for replacing damaged neurons and glial cells, particularly in spinal cord injuries (SCI). Despite its merits, iPSC differentiation into spinal cord progenitor cells (SCPCs) is variable, necessitating reliable assessment of differentiation and validation of cell quality and safety. Phenotyping is often performed via label-based methods including immunofluorescent staining or flow cytometry analysis. These approaches are often expensive, laborious, time-consuming, destructive, and severely limits their use in large scale cell therapy manufacturing settings. On the other hand, cellular biophysical properties have demonstrated a strong correlation to cell state, quality and functionality and can be measured with ingenious label-free technologies in a rapid and non-destructive manner. Method In this study, we report the use of Magnetic Resonance Relaxometry (MRR), a rapid and label-free method that indicates iron levels based on its readout (T2). Briefly, we differentiated human iPSCs into SCPCs and compared key iPSC and SCPC cellular markers to their intracellular iron content (Fe3+) at different stages of the differentiation process. Results With MRR, we found that intracellular iron of iPSCs and SCPCs were distinctively different allowing us to accurately reflect varying levels of residual undifferentiated iPSCs (i.e., OCT4+ cells) in any given population of SCPCs. MRR was also able to predict Day 10 SCPC OCT4 levels from Day 1 undifferentiated iPSC T2 values and identified poorly differentiated SCPCs with lower T2, indicative of lower neural progenitor (SOX1) and stem cell (Nestin) marker expression levels. Lastly, MRR was able to provide predictive indications for the extent of differentiation to Day 28 spinal cord motor neurons (ISL-1/SMI-32) based on the T2 values of Day 10 SCPCs. Conclusion MRR measurements of iPSCs and SCPCs has clearly indicated its capabilities to identify and quantify key phenotypes of iPSCs and SCPCs for end-point validation of safety and quality parameters. Thus, our technology provides a rapid label-free method to determine critical quality attributes in iPSC-derived progenies and is ideally suited as a quality control tool in cell therapy manufacturing.",
        "authors": [
            "Jerome Tan",
            "Jiahui Chen",
            "Daniel Roxby",
            "Wai H. Chooi",
            "Tan D. Nguyen",
            "Shi Y. Ng",
            "Jongyoon Han",
            "Sing Y. Chew"
        ],
        "journal_conference_name": "Stem Cell Research & Therapy",
        "publisher": "BioMed Central",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157837",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Advances in 2D Molybdenum Disulfide Transistors for Flexible and Wearable Electronics",
        "abstract": "As the trajectory of developing advanced electronics is shifting towards wearable electronics, various methods for implementing flexible and bendable devices capable of conforming to curvilinear surfaces have been widely investigated. In particular, achieving high-performance and stable flexible transistors remains a significant technical challenge, as transistors are fundamental components of electronics, playing a key role in overall performance. Among the wide range of candidates for flexible transistors, two-dimensional (2D) molybdenum disulfide (MoS2)-based transistors have emerged as potential solutions to address these challenges. Unlike other 2D materials, the 2D MoS2 offers numerous advantages, such as high carrier mobility, a tunable bandgap, superior mechanical strength, and exceptional chemical stability. This review emphasizes the novel techniques of the fabrication process, structure, and material to achieve flexible MoS2 transistor-based applications. Furthermore, the distinctive feature of this review is its focus on studies published in high-impact journals over the past decade, emphasizing their methods for developing MoS2 transistors into various applications. Finally, the review addresses technical challenges and provides an outlook for flexible and wearable MoS2 transistors.",
        "authors": [
            "Kyoungwon Kwak",
            "Hyewon Yoon",
            "Seongin Hong",
            "Byung Ha Kang"
        ],
        "journal_conference_name": "Micromachines",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157954",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "\"Data comes from the real world\": A Constructionist Approach to Mainstreaming K12 Data Science Education",
        "abstract": "Data science is emerging as a crucial 21st-century competence, influencing professional practices from citing evidence when advocating for social change to developing artificial intelligence (AI) models. For middle and high school students, data science can put formerly decontextualized subjects into real-world scenarios. Many existing curricula, however, lack authenticity and personal relevance for students. A critique of data science courseware cites the lack of \"author proximity,\" in which students do not contribute to the data's production or see their personal experiences reflected in the data. This paper introduces a novel data science curriculum to scaffold middle and high school students in undertaking real-world data science practices. Through project-based learning modules, the curriculum engages students in investigating solutions to community-based problems through visualization and analysis of live sensor data and public data sets. Materials include formative assessments to help educators (especially those from non-math and computing backgrounds) measure their students' abilities to identify statistical patterns, critically evaluate data biases, and make predictions. As we pilot and co-design with teachers, we will look closely at whether the curriculum's resources can successfully support non-technical practitioners engaging in an integrated curriculum.",
        "authors": [
            "Prerna Ravi",
            "Robert Parks",
            "John Masla",
            "Hal Abelson",
            "Cynthia Breazeal"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 2024 ACM Virtual Global Computing Education Conference V. 1",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158080",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "AI Mastery May Not Be For Everyone, But AI Literacy Should Be",
        "abstract": "Despite the abundance of advice from policy bodies, professional associations, advocacy groups, and scholars on how K-12 schools should assimilate AI and provide AI education, practical plans are lacking from K-12 education leaders themselves. Education leaders must make strategic decisions about how to prepare teachers and students for an AI-infused future. Simultaneously, educators need immediate support and guidance on how to manage the arrival of tools that render some existing educational practices obsolete and prompt the need to teach new skills and awareness. Near term, it may be unrealistic to expect all students to master the ability to develop AI applications; universal AI literacy is a more feasible goal. We introduce a set of short-format, modular AI literacy courses and report how they were implemented and affected teachers' and students' knowledge and perceptions of AI. Using an online questionnaire, we collected data from 265 individuals worldwide who accessed the courses, including 190 teachers who implemented them with over 11,800 students. We conducted 17 teacher interviews to gather feedback and to better understand how courses were adapted for local contexts. Teachers reported an increase in their own and their students' knowledge of AI concepts; and increased optimism about the potential benefits of AI to society and their ability to influence the future of AI. Key takeaways are that AI literacy instruction should be designed for adaptability to local contexts and cultures and that steps should be taken to institutionalize the integration of AI literacy into the regular school curriculum.",
        "authors": [
            "Fiona Hollands",
            "Daniella DiPaola",
            "Cynthia Breazeal",
            "Safinah Ali"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 2024 ACM Virtual Global Computing Education Conference V. 1",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158079",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A low-cost, open-source cylindrical Couette rheometer",
        "abstract": "Rheology describes the flow of fluids from food and plastics, to coatings, adhesives, and 3D printing inks, and is commonly denoted by viscosity alone as a simplification. While viscometers adequately probe Newtonian (constant) viscosity, most fluids have complex viscosity, requiring tests over multiple shear rates, and transient measurements. As a result, rheometers are typically large, expensive, and require additional infrastructure (e.g., gas lines), rendering them inaccessible for regular use by many individuals, small organizations, and educators. Here, we introduce a low-cost (under USD$200 bill of materials) Open Source Rheometer (OSR), constructed entirely from thermoplastic 3D printed components and off-the-shelf electromechanical components. A sample fluid rests in a cup while a micro stepping motor rotates a tool inside the cup, applying strain-controlled shear flow. A loadcell measures reaction torque exerted on the cup, and viscosity is calculated. To establish the measurement range, the viscosity of four Newtonian samples of 0.1–10 Pa.s were measured with the OSR and compared to benchmark values from a laboratory rheometer, showing under 23% error. Building on this, flow curves of three complex fluids – a microgel (hand sanitizer), foam (Gillette), and biopolymer solution (1% Xanthan Gum) – were measured with a similar error range. Stress relaxation, a transient test, was demonstrated on the biopolymer solution to extract the nonlinear damping function. We finally include detailed exposition of measurement windows, sources of error, and future design suggestions. The OSR cost is ∼1/25th that of commercially available devices with comparable minimum torque (200 µN.m), and provides a fully open-source platform for further innovation in customized rheometry.",
        "authors": [
            "Makita Erni",
            "A. John Hart",
            "David Trumper",
            "Crystal E. Owens"
        ],
        "journal_conference_name": "Scientific Reports",
        "publisher": "Springer Nature",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157788",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Measurement of boosted Higgs bosons produced via vector boson fusion or gluon fusion in the H → bb¯ decay mode using LHC proton-proton collision data at √s = 13 TeV",
        "abstract": "A measurement is performed of Higgs bosons produced with high transverse momentum (pT) via vector boson or gluon fusion in proton-proton collisions. The result is based on a data set with a center-of-mass energy of 13 TeV collected in 2016–2018 with the CMS detector at the LHC and corresponds to an integrated luminosity of 138 fb−1. The decay of a high-pT Higgs boson to a boosted bottom quark-antiquark pair is selected using large-radius jets and employing jet substructure and heavy-flavor taggers based on machine learning techniques. Independent regions targeting the vector boson and gluon fusion mechanisms are defined based on the topology of two quark-initiated jets with large pseudorapidity separation. The signal strengths for both processes are extracted simultaneously by performing a maximum likelihood fit to data in the large-radius jet mass distribution. The observed signal strengths relative to the standard model expectation are 4.9 − 1.6 + 1.9 and 1.6 − 1.5 + 1.7 for the vector boson and gluon fusion mechanisms, respectively. A differential cross section measurement is also reported in the simplified template cross section framework.",
        "authors": [
            "A. Hayrapetyan",
            "A. Tumasyan",
            "W. Adam",
            "J. W. Andrejkovic",
            "T. Bergauer",
            "S. Chatterjee",
            "K. Damanakis",
            "M. Dragicevic",
            "P. S. Hussain",
            "M. Jeitler",
            "N. Krammer",
            "A. Li",
            "D. Liko",
            "I. Mikulec",
            "J. Schieck",
            "R. Schöfbeck",
            "D. Schwarz",
            "M. Sonawane",
            "The CMS collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157887",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "How J-chain ensures the assembly of immunoglobulin IgM pentamers",
        "abstract": "Polymeric IgM immunoglobulins have high avidity for antigen and complement, and dominate primary antibody responses. They are produced either as assemblies of six µ2L2 subunits (i.e., hexamers), or as pentamers of two µ2L2 subunits and an additional protein termed J-chain (JC), which allows transcytosis across epithelia. The molecular mechanism of IgM assembly with the desired stoichiometry remained unknown. Here, we show in vitro and in cellula that JC outcompetes the sixth IgM subunit during assembly. Before insertion into IgM, JC exists as an ensemble of largely unstructured, protease-sensitive species with heterogeneous, non-native disulfide bonds. The J-chain interacts with the hydrophobic β-sheets selectively exposed by nascent pentamers. Completion of an amyloid-like core triggers JC folding and drives disulfide rearrangements that covalently stabilize JC-containing pentamers. In cells, the quality control factor ERp44 surveys IgM assembly and prevents the secretion of aberrant conformers. This mechanism allows the efficient production of high-avidity IgM for systemic or mucosal immunity.",
        "authors": [
            "Chiara Giannone",
            "Xenia Mess",
            "Ruiming He",
            "Maria R. Chelazzi",
            "Annika Mayer",
            "Anush Bakunts",
            "Tuan Nguyen",
            "Yevheniia Bushman",
            "Andrea Orsi",
            "Benedikt Gansen",
            "Massimo Degano",
            "Johannes Buchner",
            "Roberto Sitia"
        ],
        "journal_conference_name": "The EMBO Journal",
        "publisher": "Nature Publishing Group UK",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157840",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Around the Corner mmWave Imaging in Practical Environments",
        "abstract": "We present the design, implementation, and evaluation of RFlect, a mmWave imaging system capable of producing around-the-corner high-resolution images in practical environments. RFlect leverages signals reflected off complex surfaces (e.g., poles, concave surfaces, or composition of multiple surfaces) to image objects that are not in the RF line-of-sight. RFlect models the reflections and introduces reconstruction algorithms for different types of surfaces. It also leverages a novel method for precisely mapping the location and geometry of the reflecting surface. We also derive the theoretical resolution and coverage for different reflecting surface geometries. We built a prototype of RFlect and performed extensive evaluations to demonstrate its ability to reconstruct the shape of objects around the corner, with an average Chamfer Distance of 2cm and 3D F-Score of 88.6%.",
        "authors": [
            "Laura Dodds",
            "Hailan Shanbhag",
            "Junfeng Guan",
            "Saurabh Gupta",
            "Haitham Hassanieh"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 30th Annual International Conference on Mobile Computing and Networking",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158077",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Automated and Blind Detection of Low Probability of Intercept RF Anomaly Signals",
        "abstract": "Automated spectrum monitoring necessitates the accurate detection of low probability of intercept (LPI) radio frequency (RF) anomaly signals to identify unwanted interference in wireless networks. However, detecting these unforeseen low-power RF signals is fundamentally challenging due to the scarcity of labeled RF anomaly data. In this paper, we introduce WANDA (Wireless ANomaly Detection Algorithm), an automated framework designed to detect LPI RF anomaly signals in low signal-to-interference ratio (SIR) environments without relying on labeled data. WANDA operates through a two-step process: (i) Information extraction, where a convolutional neural network (CNN) utilizing soft Hirschfeld-Gebelein-Rényi correlation (HGR) as the loss function extracts informative features from RF spectrograms; and (ii) Anomaly detection, where the extracted features are applied to a one-class support vector machine (SVM) classifier to infer RF anomalies. To validate the effectiveness of WANDA, we present a case study focused on detecting unknown Bluetooth signals within the WiFi spectrum using a practical dataset. Experimental results demonstrate that WANDA outperforms other methods in detecting anomaly signals across a range of SIR values (-10 dB to 20 dB).",
        "authors": [
            "Kuanl Gusain",
            "Zoheb Hassan",
            "David Couto",
            "Mai Abdel Malek",
            "Vijay K Shah",
            "Lizhong Zheng",
            "Jeffrey H. Reed"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 30th Annual International Conference on Mobile Computing and Networking",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158078",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "SeaScan: An Energy-Efficient Underwater Camera for Wireless 3D Color Imaging",
        "abstract": "We present the design, implementation, and evaluation of SeaScan, an energy-efficient camera for 3D imaging of underwater environments. At the core of SeaScan's design is a trinocular lensing system, which employs three ultra-low-power monochromatic image sensors to reconstruct color images. Each of the sensors is equipped with a different filter (red, green, and blue) for color capture. The design introduces multiple innovations to enable reconstructing 3D color images from the captured monochromatic ones. This includes an ML-based cross-color alignment architecture to combine the monochromatic images. It also includes a cross-refractive compensation technique that overcomes the distortion of the wide-angle imaging of the low-power CMOS sensors in underwater environments. We built an end-to-end prototype of SeaScan, including color filter integration, 3D reconstruction, compression, and underwater backscatter communication. Our evaluation in real-world underwater environments demonstrates that SeaScan can capture underwater color images with as little as 23.6 mJ, which represents 37X reduction in energy consumption in comparison to the lowest-energy state-of-the-art underwater imaging system. We also report qualitative and quantitative evaluation of SeaScan's color reconstruction and demonstrate its success in comparison to multiple potential alternative techniques (both geometric and ML-based) in the literature. SeaScan's ability to image underwater environments at such low energy opens up important applications in long-term monitoring for ocean climate change, seafood production, and scientific discovery.",
        "authors": [
            "Nazish Naeem",
            "Jack Rademacher",
            "Ritik Patnaik",
            "Tara Boroushaki",
            "Fadel Adib"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 30th Annual International Conference on Mobile Computing and Networking",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158065",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "SURF: Eavesdropping on Underwater Communications from the Air",
        "abstract": "This paper investigates how an airborne node can eavesdrop on the underwater acoustic communication between submerged nodes. Conventionally, such eavesdropping has been assumed impossible as acoustic signals do not cross the water-air boundary. Here, we demonstrate that underwater acoustic communications signals can be picked up and (under certain conditions) decoded using an airborne mmWave radar due to the minute vibrations induced by the communication signals on the water surface. We implemented and evaluated a proof-of-concept prototype of our method and tested it in controlled (pool) and uncontrolled environments (lake). Our results demonstrate that an airborne device can identify the modulation and bitrate of acoustic transmissions from an uncooperative underwater transmitter (victim), and even decode the transmitted symbols. Unlike conventional over-the-air communications, our results indicate that the secrecy of underwater links varies depending on the modulation type and provide insights into the underlying reasons behind these differences. We also highlight the theoretical limitations of such a threat model, and how these results may have a significant impact on the stealthiness of underwater communications, with particular concern to submarine warfare, underwater operations (e.g., oil & gas, search & rescue, mining), and conservation of endangered species. Finally, our investigation uncovers countermeasures that can be used to improve or restore the stealthiness of underwater acoustic communications against such threats.",
        "authors": [
            "Poorya Mollahosseini",
            "Sayed Saad Afzal",
            "Fadel Adib",
            "Yasaman Ghasempour"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 30th Annual International Conference on Mobile Computing and Networking",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158064",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Snooping Underwater Communications via Low-Cost mmWave Radars",
        "abstract": "This study examines how an airborne device can intercept underwater acoustic signals exchanged between submerged nodes. It challenges the conventional belief that acoustic communications under the water are safe against eavesdropping since acoustics do not cross the water-air boundary. We show that an airborne mmWave radar can detect and decode underwater acoustic signals by picking up minute surface vibrations induced by these signals. The proof-of-concept was tested in controlled (pool) and uncontrolled (lake) environments, proving that an airborne adversary can identify modulation type, bitrate, and decode symbols from an uncooperative underwater transmitter using its radar sensing capabilities. We demonstrate that the secrecy of underwater links depends on modulation type, providing insights into countermeasures to enhance the security of underwater acoustic communications.",
        "authors": [
            "Poorya Mollahosseini",
            "Sayed Saad Afzal",
            "Fadel Adib",
            "Yasaman Ghasempour"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 30th Annual International Conference on Mobile Computing and Networking",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158074",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Europa Imaging System (EIS) Investigation",
        "abstract": "The Europa Imaging System (EIS) consists of a Narrow-Angle Camera (NAC) and a Wide-Angle Camera (WAC) that are designed to work together to address high-priority science objectives regarding Europa’s geology, composition, and the nature of its ice shell. EIS accommodates variable geometry and illumination during rapid, low-altitude flybys with both framing and pushbroom imaging capability using rapid-readout, 8-megapixel (4k × 2k) detectors. Color observations are acquired using pushbroom imaging with up to six broadband filters. The data processing units (DPUs) perform digital time delay integration (TDI) to enhance signal-to-noise ratios and use readout strategies to measure and correct spacecraft jitter. The NAC has a 2.3° × 1.2° field of view (FOV) with a 10-μrad instantaneous FOV (IFOV), thus achieving 0.5-m pixel scale over a swath that is 2 km wide and several km long from a range of 50 km. The NAC is mounted on a 2-axis gimbal, ±30° cross- and along-track, that enables independent targeting and near-global (≥90%) mapping of Europa at ≤100-m pixel scale (to date, only ∼15% of Europa has been imaged at ≤900 m/pixel), as well as stereo imaging from as close as 50-km altitude to generate digital terrain models (DTMs) with ≤4-m ground sample distance (GSD) and ≤0.5-m vertical precision. The NAC will also perform observations at long range to search for potential erupting plumes, achieving 10-km pixel scale at a distance of one million kilometers. The WAC has a 48° × 24° FOV with a 218-μrad IFOV, achieving 11-m pixel scale at the center of a 44-km-wide swath from a range of 50 km, and generating DTMs with 32-m GSD and ≤4-m vertical precision. The WAC is designed to acquire three-line pushbroom stereo and color swaths along flyby ground-tracks.",
        "authors": [
            "E. P. Turtle",
            "A. S. McEwen",
            "G. W. Patterson",
            "C. M. Ernst",
            "C. M. Elder",
            "K. A. Slack",
            "S. E. Hawkins",
            "J. McDermott",
            "H. Meyer",
            "R. DeMajistre",
            "R. Espiritu",
            "H. Seifert",
            "J. Niewola"
        ],
        "journal_conference_name": "Space Science Reviews",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157798",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions",
        "abstract": "Computer-Generated Holography (CGH) is a set of algorithmic methods for identifying holograms that reconstruct Three-Dimensio-nal (3D) scenes in holographic displays. CGH algorithms decompose 3D scenes into multiplanes at different depth levels and rely on simulations of light that propagated from a source plane to a targeted plane. Thus, for n planes, CGH typically optimizes holograms using n plane-to-plane light transport simulations, leading to major time and computational demands. Our work replaces multiple planes with a focal surface and introduces a learned light transport model that could propagate a light field from a source plane to the focal surface in a single inference. Our model leverages spatially adaptive convolution to achieve depth-varying propagation demanded by targeted focal surfaces. The proposed model reduces the hologram optimization process up to 1.5x, which contributes to hologram dataset generation and the training of future learned CGH models.",
        "authors": [
            "Chuanjun Zheng",
            "Yicheng Zhan",
            "Liang Shi",
            "Ozan Cakmakci",
            "Kaan Ak?it"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|SIGGRAPH Asia 2024 Technical Communications",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157841",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Measurement of the effective leptonic weak mixing angle",
        "abstract": "Using pp collision data at s = 13 TeV, recorded by the LHCb experiment between 2016 and 2018 and corresponding to an integrated luminosity of 5.4 fb−1, the forward-backward asymmetry in the pp → Z/γ* → μ+μ− process is measured. The measurement is carried out in ten intervals of the difference between the muon pseudorapidities, within a fiducial region covering dimuon masses between 66 and 116 GeV, muon pseudorapidities between 2.0 and 4.5 and muon transverse momenta above 20 GeV. These forward-backward asymmetries are compared with predictions, at next-to-leading order in the strong and electroweak couplings. The measured effective leptonic weak mixing angle is sin 2 θ eff ℓ = 0.23147 ± 0.00044 ± 0.00005 ± 0.00023 , where the first uncertainty is statistical, the second arises from systematic uncertainties associated with the asymmetry measurement, and the third arises from uncertainties in the fit model used to extract sin 2 θ eff ℓ from the asymmetry measurement. This result is based on an arithmetic average of results using the CT18, MSHT20, and NNPDF31 parameterisations of the proton internal structure, and is consistent with previous measurements and with predictions from the global electroweak fit.",
        "authors": [
            "R. Aaij",
            "A. S. W. Abdelmotteleb",
            "C. Abellan Beteta",
            "F. Abudinén",
            "T. Ackernley",
            "A. A. Adefisoye",
            "B. Adeva",
            "M. Adinolfi",
            "P. Adlarson",
            "C. Agapopoulou",
            "C. A. Aidala",
            "Z. Ajaltouni",
            "S. Akar",
            "K. Akiba",
            "P. Albicocco",
            "J. Albrecht",
            "F. Alessio",
            "M. Alexander",
            "The LHCb collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157836",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Thermochromorph: Dynamic Relief Printing with Thermochromic Inks",
        "abstract": "Thermochromorph is a novel relief printing technique that produces multicolored images that transition into each other through changes in temperature. Our process utilizes two sets of CMYK thermochromic inks that exhibit complementary color-changing behaviors: one shifting from color to transparency, the other from transparency to color at the same activation temperature. We describe our printmaking workflow, provide an open-source software toolkit, showcase prints made with our system, and facilitate an artist workshop. By incorporating new materials and technology with the rich history of printmaking, our work extends the expressive capabilities of relief printing as the medium continues to evolve.",
        "authors": [
            "Ticha Sethapakdi",
            "Paris Myers",
            "Tianyu Yu",
            "Juliana Covarrubias",
            "Mackenzie Leake",
            "Stefanie Mueller"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|SIGGRAPH Asia 2024 Art Papers",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157795",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "KnitworkVR: Dual-reality Experience through Distributed Sensor-Actuator Networks in the Living Knitwork Pavilion",
        "abstract": "KnitworkVR integrates dual-reality and digital twin platforms to simulate the Living Knitwork Pavilion in a desert landscape, using real-time sensor data. The sensor network captures movements, interactions, and spatial positioning of occupants, linking electric field sensor data with VR positioning. This creates a sensor-driven immersive experience with dynamic lighting, live animations, and adaptive soundscapes, enabling telepresence and collaborative interaction in both digital and physical environments. This paper explores the functional textile design, sensing hardware, audiovisual system, and VR framework, highlighting the applications of immersive spaces with knitted electronic textiles and distributed physical-digital systems.",
        "authors": [
            "Irmandy Wicaksono",
            "Lancelot Blanchard",
            "Sam Chin",
            "Cristian Colon",
            "Joseph Paradiso"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|SIGGRAPH Asia 2024 Art Papers",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157796",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Manifold Sampling for Differentiable Uncertainty in Radiance Fields",
        "abstract": "SA Conference Papers ’24, December 03–06, 2024, Tokyo, Japan",
        "authors": [
            "Linjie Lyu",
            "Ayush Tewari",
            "Marc Habermann",
            "Shunsuke Saito",
            "Michael Zollh?fer",
            "Thomas Leimk?hler",
            "Christian Theobalt"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|SIGGRAPH Asia 2024 Conference Papers",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158127",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Large Étendue 3D Holographic Display with Content-adaptive Dynamic Fourier Modulation",
        "abstract": "Emerging holographic display technology offers unique capabilities for next-generation virtual reality systems. Current holographic near-eye displays, however, only support a small étendue, which results in a direct tradeoff between achievable field of view and eyebox size. Étendue expansion has recently been explored, but existing approaches are either fundamentally limited in the image quality that can be achieved or they require extremely high-speed spatial light modulators. We describe a new étendue expansion approach that combines multiple coherent sources with content-adaptive amplitude modulation of the hologram spectrum in the Fourier plane. To generate time-multiplexed phase and amplitude patterns for our spatial light modulators, we devise a pupil-aware gradient-descent-based computer-generated holography algorithm that is supervised by a large-baseline target light field. Compared with relevant baseline approaches, ours demonstrates significant improvements in image quality and étendue in simulation and with an experimental holographic display prototype.",
        "authors": [
            "Brian Chao",
            "Manu Gopakumar",
            "Suyeon Choi",
            "Jonghyun Kim",
            "Liang Shi",
            "Gordon Wetzstein"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|SIGGRAPH Asia 2024 Conference Papers",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158089",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Markov-Chain Monte Carlo Sampling of Visibility Boundaries for Differentiable Rendering",
        "abstract": "Physics-based differentiable rendering requires estimating boundary path integrals emerging from the shift of discontinuities (e.g., visibility boundaries). Previously, although the mathematical formulation of boundary path integrals has been established, efficient and robust estimation of these integrals has remained challenging. Specifically, state-of-the-art boundary sampling methods all rely on primary-sample-space guiding precomputed using sophisticated data structures—whose performance tends to degrade for finely tessellated geometries.\r\nIn this paper, we address this problem by introducing a new Markov-Chain-Monte-Carlo (MCMC) method. At the core of our technique is a local perturbation step capable of efficiently exploring highly fragmented primary sample spaces via specifically designed jumping rules. We compare the performance of our technique with several state-of-the-art baselines using synthetic differentiable-rendering and inverse-rendering experiments.",
        "authors": [
            "Peiyu Xu",
            "Sai Bangaru",
            "Tzu-Mao Li",
            "Shuang Zhao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|SIGGRAPH Asia 2024 Conference Papers",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158126",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Sketching With Your Voice: \"Non-Phonorealistic\" Rendering of Sounds via Vocal Imitation",
        "abstract": "We present a method for automatically producing human-like vocal imitations of sounds: the equivalent of “sketching,” but for auditory rather than visual representation. Starting with a simulated model of the human vocal tract, we first try generating vocal imitations by tuning the model’s control parameters to make the synthesized vocalization match the target sound in terms of perceptually-salient auditory features. Then, to better match human intuitions, we apply a cognitive theory of communication to take into account how human speakers reason strategically about their listeners. Finally, we show through several experiments and user studies that when we add this type of communicative reasoning to our method, it aligns with human intuitions better than matching auditory features alone does. This observation has broad implications for the study of depiction in computer graphics.",
        "authors": [
            "Matthew Caren",
            "Kartik Chandra",
            "Joshua Tenenbaum",
            "Jonathan Ragan-Kelley",
            "Karima Ma"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|SIGGRAPH Asia 2024 Conference Papers",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158128",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Listeria monocytogenes aptasensor on laser inscribed graphene for food safety monitoring in hydroponic water",
        "abstract": "Consumption of fresh produce, such as leafy greens, is often encouraged as part of a healthy diet. Hence, indoor facilities for hydroponic production of leafy greens are increasingly being established. However, fresh produce entails a higher risk of microbial foodborne illnesses than processed foods. Listeria monocytogenes is a major source of fresh produce contamination and is among the leading causes of severe foodborne illnesses in the United States, with a 16% mortality rate. Tools for rapid monitoring are needed for pathogens such as L. monocytogenes to prevent outbreaks. In this manuscript, we have demonstrated the feasibility of a multi-aptamer approach for development of label-free aptasensors targeting L. monocytogenes in irrigation water for lettuce hydroponic production. We use screening studies with surface plasmon resonance to rationally develop mixtures of relevant aptamers for targeting L. monocytogenes. Based on this screening, multiple aptamers targeting extracellular structures on intact L. monocytogenes were tethered to platinum-modified laser inscribed graphene electrodes. This is the first report of a L. monocytogenes biosensor based on laser inscribed graphene. We show that mixing multiple aptamers with varying affinity improves the diagnostic performance over one aptamer alone in complex sample matrices (lettuce hydroponic water). Multi-aptamer biosensors showed high accuracy for L. monocytogenes and were at least three times more selective than Escherichia coli (Crooks, K12, O157:H7) with an accuracy of 85%. The limit of detection (10 CFU/10 mL) is based on data which were significantly different after calibration toward L. monocytogenes or E. coli (Crooks) and validated against gold standard molecular analysis (polymerase chain reaction). Rapid screening of pathogens is a global need to meet food safety and water quality regulations. This study shows the importance of sensors targeting more than one bacterial surface structure in complex samples relevant to the food-water nexus.",
        "authors": [
            "Nicholas Cavallaro",
            "Geisianny Moreira",
            "Diana Vanegas",
            "Dong Xiang",
            "Shoumen P. A. Datta",
            "Carmen Gomes",
            "Eric S. McLamore"
        ],
        "journal_conference_name": "Discover Food",
        "publisher": "Springer International Publishing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157839",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Do We Learn From Each Other: Understanding the Human-AI Co-Learning Process Embedded in Human-AI Collaboration",
        "abstract": "Beyond collaborating in the AI-supported decision-making setting to achieve complementary performance, human and AI should learn from each other and internalize knowledge from their collaboration. This can enhance their individual performance when working independently after their collaboration. However, this expected dual-pathway co-learning process, including both “human learns from AI” and “AI learns from human”, does not occur spontaneously. Human-AI collaboration designs could have inconsistent and intertwined influences on the co-learning process. Based on the learning cycle theory, this study conducted three online, two-stage, and between-subject behavioral experiments to reveal how human and AI learn from each other. By developing a context where human and AI have comparable and moderate performance on emotion classification tasks, our study provides the first empirical evidence of an effective human-AI co-learning process within human-AI collaboration. However, the AI feedback and collaborative workflow design can lead to unequal and potentially negative impacts on both pathways of the co-learning process in groups with varying levels of cognitive reflection capability. These findings highlight three design principles to facilitate the co-learning process embedded in human-AI collaboration rather than naively deploying a complex AI system.",
        "authors": [
            "Jinwei Lu",
            "Yikuan Yan",
            "Keman Huang",
            "Ming Yin",
            "Fang Zhang"
        ],
        "journal_conference_name": "Group Decision and Negotiation",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/159157",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Opening the AI Black Box: Distilling Machine-Learned Algorithms into Code",
        "abstract": "Can we turn AI black boxes into code? Although this mission sounds extremely challenging, we show that it is not entirely impossible by presenting a proof-of-concept method, MIPS, that can synthesize programs based on the automated mechanistic interpretability of neural networks trained to perform the desired task, auto-distilling the learned algorithm into Python code. We test MIPS on a benchmark of 62 algorithmic tasks that can be learned by an RNN and find it highly complementary to GPT-4: MIPS solves 32 of them, including 13 that are not solved by GPT-4 (which also solves 30). MIPS uses an integer autoencoder to convert the RNN into a finite state machine, then applies Boolean or integer symbolic regression to capture the learned algorithm. As opposed to large language models, this program synthesis technique makes no use of (and is therefore not limited by) human training data such as algorithms and code from GitHub. We discuss opportunities and challenges for scaling up this approach to make machine-learned models more interpretable and trustworthy.",
        "authors": [
            "Eric J. Michaud",
            "Isaac Liao",
            "Vedang Lad",
            "Ziming Liu",
            "Anish Mudide",
            "Chloe Loughridge",
            "Zifan Carl Guo",
            "Tara Rezaei Kheirkhah",
            "Mateja Vukelić",
            "Max Tegmark"
        ],
        "journal_conference_name": "Entropy",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157939",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Beam heating explains critical current suppression measured during ion irradiation of REBCO tapes",
        "abstract": "Reports of critical current (Ic) suppression during cryogenic ion\r\nirradiation of REBCO tapes have raised concerns for the operational margins\r\nof fusion power plant (FPP) magnets. However, the data remain inconclusive\r\nregarding beam heating due to the difficulty of measuring local temperatures\r\nwith contact probes. This leaves a critical knowledge gap concerning the\r\nmechanism behind Ic suppression, and whether the so-called beam on effect is\r\nto be expected under neutron irradiation during FPP operation. In this paper,\r\nwe show that Ic suppression is independent of atomic displacement rate in the\r\nREBCO layer, the latter of which increases twelve-fold as we reduce the beam\r\nenergy from 2400 to 800 keV. At fixed power, we observe statistically identical\r\nsuppression with 150 keV protons, which do not have enough energy to reach\r\nthe REBCO layer, refuting hypotheses about beam on effects being caused by\r\nnuclear displacements or direct ion-Cooper pair interactions. These results show\r\nthat REBCO temperature rise alone can explain Ic suppression, leaving little to no\r\nmargin for alternative mechanisms. With this insight, we developed a method to\r\nmeasure beam spot temperature that does not depend on the specific installation\r\nof our temperature sensor. With this new method, we measured the temperature\r\ngradient across the tape during irradiation and found that thermal resistance at\r\nthe tape/target interface is the controlling variable in Ic suppression. As such,\r\naccelerator-based facilities aiming to reproduce the operation of REBCO magnets\r\nin a nuclear fusion environment should find strategies to minimize interface\r\nthermal resistance. Most importantly, we find that the dose rates expected\r\nin a FPP will not change Ic due to ballistic radiation damage or ion-Cooper\r\npair interactions, allowing us to safely ignore these effects when designing FPP\r\nmagnets.",
        "authors": [
            "Alexis Devitre",
            "David Fischer",
            "N. Riva",
            "M. Rae",
            "Lauryn Kortman",
            "Kevin Woller",
            "Zoe Fisher",
            "Michael Short",
            "Dennis Whyte",
            "Zachary Hartwig"
        ],
        "journal_conference_name": "Superconductor Science and Technology",
        "publisher": "IOP Publishing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157858",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Materials approaches for next-generation encapsulated cell therapies",
        "abstract": "Transplanted cells can act as living drug factories capable of secreting therapeutic proteins in vivo, with applications in the treatment of Type 1 diabetes (T1D), blood borne disease, vision disorders, and degenerative neural disease, potentially representing functional cures for chronic conditions. However, attack from the host immune system represents a major challenge, requiring chronic immunosuppression to enable long-lived cell transplantation in vivo. Encapsulating cells in engineered biomaterials capable of excluding components of the host immune system while allowing for the transport of therapeutic proteins, oxygen, nutrients, metabolites, and waste products represents a potential solution. However, the foreign-body response can lead to isolation from native vasculature and hypoxia leading to cell death. In this prospective article, we highlight materials-based solutions to three important challenges in the field: (i) improving biocompatibility and reducing fibrosis; (ii) enhancing transport of secreted protein drugs and key nutrients and oxygen via engineered, semipermeable membranes; and (iii) improving oxygenation. These efforts draw on several disciplines in materials’ research, including polymer science, surfaces, membranes, biomaterials’ microfabrication, and flexible electronics. If successful, these efforts could lead to new therapies for chronic disease and are a rich space for both fundamental materials’ discovery and applied translational science.",
        "authors": [
            "Siddharth R. Krishnan",
            "Robert Langer",
            "Daniel G. Anderson"
        ],
        "journal_conference_name": "MRS Communications",
        "publisher": "Springer International Publishing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157838",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Graphical vs. Deep Generative Models: Measuring the Impact of Differentially Private Mechanisms and Budgets on Utility",
        "abstract": "Generative models trained with Differential Privacy (DP) can produce synthetic data while reducing privacy risks. However, navigating their privacy-utility tradeoffs makes finding the best models for specific settings/tasks challenging. This paper bridges this gap by profiling how DP generative models for tabular data distribute privacy budgets across rows and columns, which is one of the primary sources of utility degradation. We compare graphical and deep generative models, focusing on the key factors contributing to how privacy budgets are spent, i.e., underlying modeling techniques, DP mechanisms, and data dimensionality.\r\nThrough our measurement study, we shed light on the characteristics that make different models suitable for various settings and tasks. For instance, we find that graphical models distribute privacy budgets horizontally and thus cannot handle relatively wide datasets for a fixed training time; also, the performance on the task they were optimized for monotonically increases with more data but could also overfit. Deep generative models spend their budgets per iteration, so their behavior is less predictable with varying dataset dimensions, but are more flexible as they could perform better if trained on more features. Moreover, low levels of privacy (ε≥100) could help some models generalize, achieving better results than without applying DP. We believe our work will aid the deployment of DP synthetic data techniques by navigating through the best candidate models vis-à-vis the dataset features, desired privacy levels, and downstream tasks.",
        "authors": [
            "Georgi Ganev",
            "Kai Xu",
            "Emiliano De Cristofaro"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158085",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Manipulative Interference Attacks",
        "abstract": "A μ-kernel is an operating system (OS) paradigm that facilitates a strong cybersecurity posture for embedded systems. Unlike a monolithic OS such as Linux, a μ-kernel reduces overall system privilege by deploying most OS functionality within isolated, userspace protection domains. Moreover, a μ-kernel ensures confidentiality and integrity between protection domains (i.e., spatial isolation), and offers timing predictability for real-time tasks in mixed-criticality systems (i.e., temporal isolation). One popular μ-kernel is seL4 which offers extensive formal guarantees of implementation correctness and flexible temporal budgeting mechanisms.\r\nHowever, we show that an untrusted protection domain on a μ-kernel can abuse service requests to other protection domains in order to corrode system availability. We generalize this denial-of-service (DoS) attack strategy as Manipulative Interference Attacks (MIAs) and introduce techniques to efficiently identify instances of MIAs within a configured system. Specifically, we propose a novel hybrid approach that first leverages static analysis to identify software components with influenceable execution times, and second, uses an automatically generated model-based analysis to determine which compromised protection domains can manipulate the influenceable components and trigger MIAs. We investigate the risk of MIAs in several representative system examples including the seL4 Microkit, as well as a case study of seL4 software artifacts from the DARPA Cyber Assured Systems Engineering (CASE) program. In particular, we demonstrate that our analysis is efficient enough to discover practical instances of MIAs in real-world systems.",
        "authors": [
            "Samuel Mergendahl",
            "Stephen Fickas",
            "Boyana Norris",
            "Richard Skowyra"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158086",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Formal Privacy Proof of Data Encoding: The Possibility and Impossibility of Learnable Encryption",
        "abstract": "We initiate a formal study on the concept of learnable obfuscation and aim to answer the following question: is there a type of data encoding that maintains the \"learnability\" of encoded samples, thereby enabling direct model training on transformed data, while ensuring the privacy of both plaintext and the secret encoding function? This long-standing open problem has prompted many efforts to design such an encryption function, for example, NeuraCrypt and TransNet. Nonetheless, all existing constructions are heuristic without formal privacy guarantees, and many successful reconstruction attacks are known on these constructions assuming an adversary with substantial prior knowledge.\r\nWe present both generic possibility and impossibility results pertaining to learnable obfuscation. On one hand, we demonstrate that any non-trivial, property-preserving transformation which enables effectively learning over encoded samples cannot offer cryptographic computational security in the worst case. On the other hand, from the lens of information-theoretical security, we devise a series of new tools to produce provable and useful privacy guarantees from a set of heuristic obfuscation methods, including matrix masking, data mixing and permutation, through noise perturbation. Under the framework of PAC Privacy, we show how to quantify the leakage from the learnable obfuscation built upon obfuscation and perturbation methods against adversarial inference. Significantly sharpened utility-privacy tradeoffs are achieved compared to state-of-the-art accounting methods when measuring privacy against data reconstruction and membership inference attacks.",
        "authors": [
            "Hanshen Xiao",
            "G. Edward Suh",
            "Srinivas Devadas"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158081",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploiting Temporal Vulnerabilities for Unauthorized Access in Intent-based Networking",
        "abstract": "Intent-based networking (IBN) enables network administrators to express high-level goals and network policies without needing to specify low-level forwarding configurations, topologies, or protocols. Administrators can define intents that capture the overall behavior they want from the network, and an IBN controller compiles such intents into low-level configurations that get installed in the network and implement the desired behavior.\r\nWe discovered that current IBN specifications and implementations do not specify that flow rule installation orderings should be enforced, which leads to temporal vulnerabilities where, for a limited time, attackers can exploit indeterminate connectivity behavior to gain unauthorized network access.\r\nIn this paper, we analyze the causes of such temporal vulnerabilities and their security impacts with a representative case study via the ONOS IBN implementation. We devise the Phantom Link attack and demonstrate a working exploit to highlight the security impacts. To defend against such attacks, we propose Spotlight, a detection method that can alert a system administrator of risky intent updates prone to exploitable temporal vulnerabilities. Spotlight is effective in identifying risky updates using realistic network topologies and policies. We show that Spotlight can detect risky updates in a mean time of 0.65 seconds for topologies of over 1,300 nodes.",
        "authors": [
            "Ben Weintraub",
            "Jiwon Kim",
            "Ran Tao",
            "Cristina Nita-Rotaru",
            "Hamed Okhravi",
            "Dave (Jing) Tian",
            "Benjamin Ujcich"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158083",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Secure Sorting and Selection via Function Secret Sharing",
        "abstract": "We revisit the problem of concretely efficient secure computation of sorting and selection (e.g., maximum, median, or top-k) on secret-shared data, focusing on the case of security against a single semi-honest party. Previous solutions either have a high communication overhead or many rounds of interaction, even when allowing input-independent preprocessing.\r\nWe propose a suite of 2-party and 3-party offline-online protocols that exploit the efficient aggregation feature of function secret sharing to minimize the online communication and rounds. In particular, most of our protocols are optimal in terms of both online communication and online rounds up to small constant factors.\r\nWe compare the performance of our protocols with prior works for different input parameters (number of items, bit length of items, batch size) and system parameters (CPU cores, network) and obtain up to 14x improvement in online run time for sorting and selection under some settings.",
        "authors": [
            "Amit Agarwal",
            "Elette Boyle",
            "Nishanth Chandran",
            "Niv Gilboa",
            "Divya Gupta",
            "Yuval Ishai",
            "Mahimna Kelkar",
            "Yiping Ma"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158087",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "High-Throughput Three-Party DPFs with Applications to ORAM and Digital Currencies",
        "abstract": "specific and general secure computation. While two-party DPF constructions are readily available for those applications with satisfiable performance, the three-party ones are left behind in both security and efficiency. In this paper we close this gap and propose the first three-party DPF construction that matches the state-of-the-art two-party DPF on all metrics. Namely, it is secure against a malicious adversary corrupting both the dealer and one out of the three evaluators, its function's shares are of the same size and evaluation takes the same time as in the best two-party DPF. Compared to the state-of-the-art three-party DPF, our construction enjoys 40-120× smaller function's share size and shorter evaluation time, for function domains of 216 -240, respectively.\r\nApart from DPFs as a stand-alone tool, our construction finds immediate applications to private information retrieval (PIR), writing (PIW) and oblivious RAM (ORAM). To further showcase its applicability, we design and implement an ORAM with access policy, an extension to ORAMs where a policy is being checked before accessing the underlying database. The policy we plug-in is the one suitable for account-based digital currencies, and in particular to central bank digital currencies (CBDCs). Our protocol offers the first design and implementation of a large scale privacy-preserving account-based digital currency. While previous works supported anonymity sets of 64-256 clients and less than 10 transactions per second (tps), our protocol supports anonymity sets in the millions, performing {500,200,58} tps for anonymity sets of {216, 218, 220}, respectively.\r\nToward that application, we introduce a new primitive called updatable DPF, which enables a direct computation of a dot product between a DPF and a vector; we believe that updatable DPF and the new dot-product protocol will find interest in other applications.",
        "authors": [
            "Guy Zyskind",
            "Avishay Yanai",
            "Alex Pentland"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158082",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Specification and Verification of Strong Timing Isolation of Hardware Enclaves",
        "abstract": "The process isolation enforceable by commodity hardware and operating systems is too weak to protect secrets from malicious code running on the same machine: attacks exploit timing side channels derived from contention on shared microarchitectural resources to extract secrets. With appropriate hardware support, however, we can construct isolated enclaves and safeguard independent processes from interference through timing side channels, a step towards confidentiality and integrity guarantees.\r\nIn this paper, we describe our work on formally specifying and verifying that a synthesizable hardware architecture implements strong timing isolation for enclaves. We reason about the cycle-accurate semantics of circuits with respect to a trustworthy formulation of strong isolation based on \"air-gapped machines\" and develop a modular proof strategy that sidesteps the need to prove functional correctness of processors. We apply our method on a synthesizable, multicore, pipelined RISC-V design formalized in Coq.",
        "authors": [
            "Stella Lau",
            "Thomas Bourgeat",
            "Cl?ment Pit-Claudel",
            "Adam Chlipala"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158084",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Enabling Perspective-Aware Ai with Contextual Scene Graph Generation",
        "abstract": "This paper advances contextual image understanding within perspective-aware Ai (PAi), an emerging paradigm in human–computer interaction that enables users to perceive and interact through each other’s perspectives. While PAi relies on multimodal data—such as text, audio, and images—challenges in data collection, alignment, and privacy have led us to focus on enabling the contextual understanding of images. To achieve this, we developed perspective-aware scene graph generation with LLM post-processing (PASGG-LM). This framework extends traditional scene graph generation (SGG) by incorporating large language models (LLMs) to enhance contextual understanding. PASGG-LM integrates classical scene graph outputs with LLM post-processing to infer richer contextual information, such as emotions, activities, and social contexts. To test PASGG-LM, we introduce the context-aware scene graph generation task, where the goal is to generate a context-aware situation graph describing the input image. We evaluated PASGG-LM pipelines using state-of-the-art SGG models, including Motifs, Motifs-TDE, and RelTR, and showed that fine-tuning LLMs, particularly GPT-4o-mini and Llama-3.1-8B, improves performance in terms of R@K, mR@K, and mAP. Our method is capable of generating scene graphs that capture complex contextual aspects, advancing human–machine interaction by enhancing the representation of diverse perspectives. Future directions include refining contextual scene graph models and expanding multi-modal data integration for PAi applications in domains such as healthcare, education, and social robotics.",
        "authors": [
            "Daniel Platnick",
            "Marjan Alirezaie",
            "Hossein Rahnama"
        ],
        "journal_conference_name": "Information",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157953",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Gênero e Feminismos no Ensino de Relações Internacionais no Brasil",
        "abstract": "",
        "authors": [
            "Alessandra Jungs de Almeida"
        ],
        "journal_conference_name": "Revista Brasileira de Políticas Públicas e Internacionais",
        "publisher": "No Publisher",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157847",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Expanding the landscape of antibody discovery",
        "abstract": "Library:library screening technologies hold substantial promise for paired antibody:antigen discovery, but challenges have persisted. In this issue of Cell Reports Methods, Wagner et al. introduce a method that combines antibody-ribosome-mRNA complexes, antigen cell surface display, and single-cell RNA sequencing to successfully screen diverse antibody gene libraries against a library of viral receptor proteins.",
        "authors": [
            "Shelbe Johnson",
            "Brandon J DeKosky"
        ],
        "journal_conference_name": "Cell Reports Methods",
        "publisher": "Elsevier BV",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158238",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Data for \"Variations on five-dimensional sphere packings\"",
        "abstract": "This data set includes all the code and data from the paper \"Variations on five-dimensional sphere packings\" by Cohn and Rajagopal.",
        "authors": [
            "Henry Cohn",
            "Isaac Rajagopal"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157699",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "From My Vantage Point: Exploring The Effect of First-Person and Third-Person Perspectives on Social Acceptance in VR Roleplaying Games",
        "abstract": "Virtual reality (VR) roleplaying games designed to promote perspective taking typically involve players assuming the perspective of others from different backgrounds and experiencing a simulated scenario from their everyday life, with the goal of facilitating and enhancing empathy and social acceptance toward marginalized groups. One key question pertains to the extent to which players’ perspective during VR roleplaying games affects their social acceptance of the other. To address this question, we examined the effect of first-person vs. third-person perspective on presence, co-presence, and social acceptance during a VR roleplaying game. Two groups of participants played the same VR roleplaying game from either a first-person perspective or a third-person perspective. Results showed that compared to third-person perspective, first-person perspective led to greater co-presence during the game and engendered higher levels of social acceptance toward the character whose role participants played. These results highlight the importance of using first-person perspective in VR roleplaying games focusing on facilitating and enhancing social acceptance.",
        "authors": [
            "Caglar Yildirim",
            "Sercan Sengun",
            "Eyup Kucuk",
            "Mehmet Akhoroz",
            "D. Fox Harrell"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|International Conference on Mobile and Ubiquitous Multimedia",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158130",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Retrieval of refractivity fields from GNSS tropospheric delays: theoretical and data-based evaluation of collocation methods and comparisons with GNSS tomography",
        "abstract": "This paper focuses on the retrieval of refractivity fields from GNSS measurements by means of least-squares collocation. Collocation adjustment estimates parameters that relate delays and refractivity without relying on a grid. It contains functional and stochastic models that define the characteristics of the retrieved refractivity fields. This work aims at emphasizing the capabilities and limitations of the collocation method in modeling refractivity and to present it as a valuable alternative to GNSS tomography. Initially, we analyze the stochastic models in collocation and compare the theoretical errors of collocation with those of tomography. We emphasize the low variability of collocation formal variances/covariances compared to tomography and its lower dependence on a-priori fields. Then, based on real and simulated data, we investigate the importance of station resolution and station heights for collocation. Increasing the network resolution, for example, from 10 to 2 km, results in improved a-posteriori statistics, including a 10% reduction in the error statistic for the retrieved refractivity up to 6 km. In addition, using additional stations at higher altitudes has an impact on the retrieved refractivity fields of about 1 ppm in terms of standard deviation up to 6 km, and a bias reduction of more than 3 ppm up to 3 km. Furthermore, we compare refractivity fields retrieved through tomography and collocation, where data of the COSMO weather model are utilized in a closed-loop validation mode to simulate tropospheric delays and validate the retrieved profiles. While tomography estimates are less biased, collocation captures relative changes in refractivity more effectively among the voxels within one height level. Finally, we apply tomography and collocation to test their capabilities to detect an approaching weather front. Both methods can sense the weather front, but their atmospheric structures appear more similar when the GNSS network has a well-distributed height coverage.",
        "authors": [
            "Endrit Shehaj",
            "Alain Geiger",
            "Markus Rothacher",
            "Gregor Moeller"
        ],
        "journal_conference_name": "Journal of Geodesy",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157746",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Robust Reinforcement Learning Strategies with Evolving Curriculum for Efficient Bus Operations in Smart Cities",
        "abstract": "Public transit systems are critical to the quality of urban life, and enhancing their efficiency is essential for building cost-effective and sustainable smart cities. Historically, researchers sought reinforcement learning (RL) applications to mitigate bus bunching issues with holding strategies. Nonetheless, these attempts often led to oversimplifications and misalignment with the goal of reducing the total time passengers spent in the system, resulting in less robust or non-optimal solutions. In this study, we introduce a novel setting where each bus, supervised by an RL agent, can appropriately form aggregated policies from three strategies (holding, skipping station, and turning around to serve the opposite direction). It&rsquo;s difficult to learn them all together, due to learning complexity, we employ domain knowledge and develop a gradually expanding action space curriculum, enabling agents to learn these strategies incrementally. We incorporate Long Short-Term Memory (LSTM) in our model considering the temporal interrelation among these actions. To address the inherent uncertainties of real-world traffic systems, we impose Domain Randomization (DR) on variables such as passenger demand and bus schedules. We conduct extensive numerical experiments with the integration of synthetic and real-world data to evaluate our model. Our methodology proves effective, enhancing bus schedule reliability and reducing total passenger waiting time by over 15%, thereby improving bus operation efficiency and smoothering operations of buses that align with sustainable goals. This work highlights the potential of robust RL combined with curriculum learning for optimizing public transport in smart cities, offering a scalable solution for real-world multi-agent systems.",
        "authors": [
            "Yuhan Tang",
            "Ao Qu",
            "Xuan Jiang",
            "Baichuan Mo",
            "Shangqing Cao",
            "Joseph Rodriguez",
            "Haris N Koutsopoulos",
            "Cathy Wu",
            "Jinhua Zhao"
        ],
        "journal_conference_name": "Smart Cities",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157936",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Structural molecular modeling of bacterial integral membrane protein enzymes and their AlphaFold2 predicted water-soluble QTY variants",
        "abstract": "Context Beta-barrel enzymes are an important area of study in the field of structural biology. These proteins serve crucial roles, acting as porins, transporters, enzymes, virulence factors, and receptors. Recent research has unveiled a novel role for beta-barrel enzymes in the bacterial integral membrane as sentinels. They remain inactive when the integral membrane is intact but activate to carry out enzymatic catalysis in response to host immune responses and antibiotics that breach this barrier. Understanding their structure and function is pivotal in grasping their sentinel role in the bacterial integral membrane. Here we present our structural molecular modeling analyses on four bacterial integral membrane beta-barrel enzymes: (a) OMPLA, (b) OmpT, (c) PagP from E. coli, and (d) PagL from Pseudomonas aeruginosa. We superposed the structures of native beta-barrel integral membrane enzymes with their AlphaFold2-predicted QTY variant structures that showed remarkable similarity despite the replacement of at least 22.95% amino acids in transmembrane regions, the superposed structures displayed notable structural similarity, indicated by RMSD values ranging from 0.181 Å to 0.286 Å. We also analyze the hydrophobicity patches and the enhanced hydrophilic surfaces. Our research provide insights into the structural similarity of hydrophobic and hydrophilic beta-barrel enzymes, validating the utility of the QTY code for investigating beta-barrel membrane enzymes. Our results not only demonstrate that the QTY code serves as a straightforward tool for designing water-soluble membrane proteins across various biological contexts, but it may also stimulate experiments to validate our molecular modeling studies. Methods All the QTY variant beta-barrel enzyme structure prediction was performed using the AlphaFold2 program ( https://github.com/sokrypton/ColabFold ) following the provided instructions. Computations were carried out on 11th Gen Intel Core i5-11300H processor with 16 GB RAM and Iris Xe Graphics, 512 GB NVMe SSD. The structures are publicly available on the AlphaFold2 database ( https://alphafold.ebi.ac.uk ) at the European Bioinformatics Institute (EBI). A custom Python script was used to extract the relevant information from the UniProt database. To predict the structures of the QTY variants, AlphaFold2 was utilized. The native sequences for these enzymes were retrieved from UniProt https://www.uniprot.org , and AlphaFold2 structural predictions were performed using the open-source implementation at https://github.com/sokrypton/ColabFold . The predicted variant structures were then superposed with the native structures using PyMOL https://pymol.org/2/ for structural analysis and comparison. This work leverages public databases PDB, UniProt and open-source software AlphaFold2 and PyMOL to computationally model and analyze QTY variant integral membrane beta-barrel enzyme structures. Graphical abstract",
        "authors": [
            "Akash Sajeev-Sheeja",
            "Shuguang Zhang"
        ],
        "journal_conference_name": "Journal of Proteins and Proteomics",
        "publisher": "Springer Nature Singapore",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157745",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The price elasticity of natural gas demand of small consumers in Germany during the energy crisis 2022",
        "abstract": "Understanding how consumers respond to turbulent market conditions is crucial for planning security of natural gas supply. This paper estimates the price elasticity of demand of small consumers in Germany in the period with both high price fluctuations and a fear of natural gas shortage in the aftermath of the Russian invasion of Ukraine. Using granular data between 2018 and 2023, we estimate an Auto Regressive Distributed Lag (ARDL) time series cointegrating model. We find a price elasticity of demand for natural gas of -0.01 for wholesale prices and -0.04 for retail prices. Additionally, we quantify the effects of weather conditions and public awareness on the energy crisis. The results suggest i) that extreme price changes would be required to trigger short-term demand adjustments, and ii) demonstrate the importance of public attention on the crisis situation.",
        "authors": [
            "David Jamissen",
            "Johanne Vatne",
            "Franziska Holz",
            "Anne Neumann"
        ],
        "journal_conference_name": "Energy Efficiency",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157701",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Multifunctional lightweight autonomous vehicles: an agent-based study",
        "abstract": "In mobility-on-demand services, the number of vehicles needed is often determined by peak demand during rush hours, leading to prolonged vehicle idle times during off-peak periods. This surplus capacity presents an opportunity for vehicles to perform additional tasks, potentially enhancing system efficiency and reducing the overall number of vehicles needed in cities. Leveraging agent-based modeling, we evaluate the effectiveness of vehicles catering to on-demand rides and food deliveries in two real-life scenarios: Cambridge, MA, USA, and San Sebastian, Gipuzkoa, Spain. The results show that multifunctional behavior can lead to reduced fleet sizes, with context-specific exceptions. Additionally, a strategic dispatching algorithm is introduced that demonstrates reductions in wait times and overall distances traveled. This research contributes to the understanding of the performance of multifunctional fleets in diverse urban contexts, informing the development of sustainable and resource-efficient mobility systems.",
        "authors": [
            "Naroa Coretti Sanchez",
            "Kent Larson"
        ],
        "journal_conference_name": "Transportation",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157703",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exact algorithms for continuous pricing with advanced discrete choice demand models",
        "abstract": "We present a spatial Branch and Bound and spatial Branch and Benders Decomposition approach together with the Breakpoint Exact Algorithm (BEA) to tackle the uncapacitated choice-based pricing problem (CPP) where demand is captured by a discrete choice model (DCM) based on the random utility principle. We leverage problem characteristics to reformulate the state-of-the-art simulation-based formulation of the CPP as a mixed-integer linear program (MILP) into a non-convex quadratically constrained quadratic program (QCQP), and then into a non-convex QCQP with linear objective (QCQP-L). We solve this reformulation with an efficient spatial Branch and Bound procedure utilizing the McCormick envelope for relaxations, which are then solved using Benders decomposition. We further exploit utility breakpoints to develop the BEA, which scales polynomially in the number of customers and draws, providing a fast option for low numbers of prices. Our methods are evaluated against solving the MILP, QCQP, or QCQP-L with GUROBI on a mixed logit (ML) parking space operator case study. We outspeed the MILP by several orders of magnitude when optimizing one or two prices and reduce computational time drastically for larger numbers of prices. When comparing to algorithms tailored for the CPP with ML demand specifically, our approaches significantly outperform the state of the art. Our methodology suits all choice-based optimization problems with linear-in-price utilities, given any DCM.",
        "authors": [
            "Tom Haering",
            "Robin Legault",
            "Fabian Torres",
            "Ivana Ljubić",
            "Michel Bierlaire"
        ],
        "journal_conference_name": "OR Spectrum",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157700",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "When Cities Go Nuclear: Exploring the Applications of Nuclear Batteries Toward Energy Transformation",
        "abstract": "Global society faces the pressing question of how to eliminate reliance on fossil fuels while meeting increasing energy demand. In comparison to solar and wind energy, nuclear power has been largely ignored in urban studies research. However, nuclear energy has recently regained attention through the emergence of Small Modular Reactors (SMRs), and as the stakes of decarbonization become increasingly essential. To evaluate situations in which SMRs bring value to urban energy mixes, this paper focuses on Nuclear Batteries (NBs), a specific class of SMRs, that can fit in standard shipping containers. First, we outline an evaluation framework for the use and application of NBs; second, we present use cases for NBs in real-world situations, from disaster relief to grid reinforcement; and third, we discuss the social challenges around this technology.",
        "authors": [
            "Sanjana Paul",
            "Mikita Klimenka",
            "Fabio Duarte",
            "Carmen Crawford",
            "Claire Gorman",
            "Carlo Ratti",
            "Jacopo Buongiorno"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157935",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Acceleration by stepsize hedging: Silver Stepsize Schedule for smooth convex optimization",
        "abstract": "We provide a concise, self-contained proof that the Silver Stepsize Schedule proposed in our companion paper directly applies to smooth (non-strongly) convex optimization. Specifically, we show that with these stepsizes, gradient descent computes an ε -minimizer in O ( ε - log ρ 2 ) = O ( ε - 0.7864 ) iterations, where ρ = 1 + 2 is the silver ratio. This is intermediate between the textbook unaccelerated rate O ( ε - 1 ) and the accelerated rate O ( ε - 1 / 2 ) due to Nesterov in 1983. The Silver Stepsize Schedule is a simple explicit fractal: the i-th stepsize is 1 + ρ ν ( i ) - 1 where ν ( i ) is the 2-adic valuation of i. The design and analysis are conceptually identical to the strongly convex setting in our companion paper, but simplify remarkably in this specific setting.",
        "authors": [
            "Jason M. Altschuler",
            "Pablo A. Parrilo"
        ],
        "journal_conference_name": "Mathematical Programming",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157702",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "“Better Networks” Project is Working to Improve Cybersecurity",
        "abstract": "In collaboration with the MIT Sociotechnical Systems Research Center and the Air Force, the Lincoln Laboratory Supercomputing Center is working to develop better sensors using Laboratory developed Dynamic Distributed Dimensional Data Model (D4M) technology and artificial intelligence algorithms to support defensive cyber operations.",
        "authors": [
            "Kailen Comeau"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157666",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Lp -Hardy identities and inequalities with respect to the distance and mean distance to the boundary",
        "abstract": "Firstly, this paper establishes useful forms of the remainder term of Hardy-type inequalities on general domains where the weights are functions of the distance to the boundary. For weakly mean convex domains we use the resulting identities to establish nonexistence of extremizers for and improve known sharp Hardy inequalities. Secondly, we establish geometrically interesting remainders for the Davies-Hardy-Tidblom inequalities for the mean distance function, as well as generalize and improve several Hardy type inequalities in the spirit of Brezis and Marcus and spectral estimates of Davies. Lastly, we apply our results to obtain Sobolev inequalities for non-regular Riemannian metrics on geometric exterior domains.",
        "authors": [
            "Joshua Flynn",
            "Nguyen Lam",
            "Guozhen Lu"
        ],
        "journal_conference_name": "Calculus of Variations and Partial Differential Equations",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/159155",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "MindScape Study: Integrating LLM and Behavioral Sensing for Personalized AI-Driven Journaling Experiences",
        "abstract": "Mental health concerns are prevalent among college students, highlighting the need for effective interventions that promote self-awareness and holistic well-being. MindScape pioneers a novel approach to AI-powered journaling by integrating passively collected behavioral patterns such as conversational engagement, sleep, and location with Large Language Models (LLMs). This integration creates a highly personalized and context-aware journaling experience, enhancing self-awareness and well-being by embedding behavioral intelligence into AI. We present an 8-week exploratory study with 20 college students, demonstrating the MindScape app's efficacy in enhancing positive affect (7%), reducing negative affect (11%), loneliness (6%), and anxiety and depression, with a significant week-over-week decrease in PHQ-4 scores (-0.25 coefficient), alongside improvements in mindfulness (7%) and self-reflection (6%). The study highlights the advantages of contextual AI journaling, with participants particularly appreciating the tailored prompts and insights provided by the MindScape app. Our analysis also includes a comparison of responses to AI-driven contextual versus generic prompts, participant feedback insights, and proposed strategies for leveraging contextual AI journaling to improve well-being on college campuses. By showcasing the potential of contextual AI journaling to support mental health, we provide a foundation for further investigation into the effects of contextual AI journaling on mental health and well-being.",
        "authors": [
            "Subigya Nepal",
            "Arvind Pillai",
            "William Campbell",
            "Talie Massachi",
            "Michael Heinz",
            "Ashmita Kunwar",
            "Eunsol Soul Choi",
            "Xuhai \"Orson\" Xu",
            "Joanna Kuc",
            "Jeremy Huckins",
            "Jason Holden",
            "Sarah M. Preum",
            "Colin Depp",
            "Nicholas Jacobson",
            "Mary Czerwinski",
            "Eric Granholm",
            "Andrew Campbell"
        ],
        "journal_conference_name": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157901",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Beyond Detection: Towards Actionable Sensing Research in Clinical Mental Healthcare",
        "abstract": "Researchers in ubiquitous computing have long promised that passive sensing will revolutionize mental health measurement by detecting individuals in a population experiencing a mental health disorder or specific symptoms. Recent work suggests that detection tools do not generalize well when trained and tested in more heterogeneous samples. In this work, we contribute a narrative review and findings from two studies with 41 mental health clinicians to understand these generalization challenges. Our findings motivate research on actionable sensing, as an alternative to detection research, studying how passive sensing can be used alongside traditional mental health measures to support actions in clinical care. Specifically, we identify how passive sensing can support clinical actions by revealing patients' presenting problems for treatment and identifying targets for behavior change and symptom reduction, but passive data needs to be contextualized with patients to be appropriately interpreted and used in care. We conclude by suggesting research at the intersection of actionable sensing and mental healthcare, to align technical research in ubiquitous computing with clinical actions and needs.",
        "authors": [
            "Daniel Adler",
            "Yuewen Yang",
            "Thalia Viranda",
            "Xuhai Xu",
            "David Mohr",
            "Anna Van Meter",
            "Julia Tartaglia",
            "Nicholas Jacobson",
            "Fei Wang",
            "Deborah Estrin",
            "Tanzeem Choudhury"
        ],
        "journal_conference_name": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157900",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Sensor2Text: Enabling Natural Language Interactions for Daily Activity Tracking Using Wearable Sensors",
        "abstract": "Visual Question-Answering, a technology that generates textual responses from an image and natural language question, has progressed significantly. Notably, it can aid in tracking and inquiring about daily activities, crucial in healthcare monitoring, especially for elderly patients or those with memory disabilities. However, video poses privacy concerns and has a limited field of view. This paper presents Sensor2Text, a model proficient in tracking daily activities and engaging in conversations using wearable sensors. The approach outlined here tackles several challenges, including low information density in wearable sensor data, insufficiency of single wearable sensors in human activities recognition, and model's limited capacity for Question-Answering and interactive conversations. To resolve these obstacles, transfer learning and student-teacher networks are utilized to leverage knowledge from visual-language models. Additionally, an encoder-decoder neural network model is devised to jointly process language and sensor data for conversational purposes. Furthermore, Large Language Models are also utilized to enable interactive capabilities. The model showcases the ability to identify human activities and engage in Q&A dialogues using various wearable sensor modalities. It performs comparably to or better than existing visual-language models in both captioning and conversational tasks. To our knowledge, this represents the first model capable of conversing about wearable sensor data, offering an innovative approach to daily activity tracking that addresses privacy and field-of-view limitations associated with current vision-based solutions.",
        "authors": [
            "Wenqiang Chen",
            "Jiaxuan Cheng",
            "Leyao Wang",
            "Wei Zhao",
            "Wojciech Matusik"
        ],
        "journal_conference_name": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157899",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "EyeTrAES: Fine-grained, Low-Latency Eye Tracking via Adaptive Event Slicing",
        "abstract": "Eye-tracking technology has gained significant attention in recent years due to its wide range of applications in human-computer interaction, virtual and augmented reality, and wearable health. Traditional RGB camera-based eye-tracking systems often struggle with poor temporal resolution and computational constraints, limiting their effectiveness in capturing rapid eye movements. To address these limitations, we propose EyeTrAES, a novel approach using neuromorphic event cameras for high-fidelity tracking of natural pupillary movement that shows significant kinematic variance. One of EyeTrAES's highlights is the use of a novel adaptive windowing/slicing algorithm that ensures just the right amount of descriptive asynchronous event data accumulation within an event frame, across a wide range of eye movement patterns. EyeTrAES then applies lightweight image processing functions over accumulated event frames from just a single eye to perform pupil segmentation and tracking (as opposed to gaze-based techniques that require simultaneous tracking of both eyes). We show that these two techniques boost pupil tracking fidelity by 6+%, achieving IoU~=92%, while incurring at least 3x lower latency than competing pure event-based eye tracking alternatives. We additionally demonstrate that the microscopic pupillary motion captured by EyeTrAES exhibits distinctive variations across individuals and can thus serve as a biometric fingerprint. For robust user authentication, we train a lightweight per-user Random Forest classifier using a novel feature vector of short-term pupillary kinematics, comprising a sliding window of pupil (location, velocity, acceleration) triples. Experimental studies with two different datasets (capturing eye movement across a range of environmental contexts) demonstrate that the EyeTrAES-based authentication technique can simultaneously achieve high authentication accuracy (~=0.82) and low processing latency (~=12ms), and significantly outperform multiple state-of-the-art competitive baselines.",
        "authors": [
            "Argha Sen",
            "Nuwan Bandara",
            "Ila Gokarn",
            "Thivya Kandappu",
            "Archan Misra"
        ],
        "journal_conference_name": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157898",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Probing the nature of the χc1(3872) state using radiative decays",
        "abstract": "The radiative decays χc1(3872) → ψ(2S) γ and χc1(3872) → J/ψγ are used to probe the nature of the χc1(3872) state using proton-proton collision data collected with the LHCb detector, corresponding to an integrated luminosity of 9 fb−1. Using the B+ → χc1(3872)K+ decay, the χc1(3872) → ψ(2S) γ process is observed for the first time and the ratio of its partial width to that of the χc1(3872) → J/ψγ decay is measured to be Γ χ c 1 3872 → ψ 2 S γ Γ χ c 1 3872 → J / ψ γ = 1.67 ± 0.21 ± 0.12 ± 0.04 , where the first uncertainty is statistical, the second systematic and the third is due to the uncertainties on the branching fractions of the ψ(2S) and J/ψ mesons. The measured ratio makes the interpretation of the χc1(3872) state as a pure D0 D ¯ ∗ 0 + D ¯ 0 D*0 molecule questionable and strongly indicates a sizeable compact charmonium or tetraquark component within the χc1(3872) state.",
        "authors": [
            "R. Aaij",
            "A. S. W. Abdelmotteleb",
            "C. Abellan Beteta",
            "F. Abudinén",
            "T. Ackernley",
            "A. A. Adefisoye",
            "B. Adeva",
            "M. Adinolfi",
            "P. Adlarson",
            "C. Agapopoulou",
            "C. A. Aidala",
            "Z. Ajaltouni",
            "S. Akar",
            "K. Akiba",
            "P. Albicocco",
            "J. Albrecht"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157704",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Vista: Machine Learning based Database Performance Troubleshooting Framework in Amazon RDS",
        "abstract": "Database performance troubleshooting is a complex multi-step process that broadly involves three key stages- (a) Detection: determining what's wrong and when; (b) Root Cause Analysis (RCA): reasoning about why is the performance poor; (c) Resolution: identifying a fix. A plethora of techniques exist to address each of these problems, but they hardly work in real-world at scale. First, real-world customer workloads are noisy, non-stationary and quasi-periodic in nature rendering traditional detectors ineffective. Second, real-world production databases execute a highly diverse set of queries that skew the database statistics into long-tail distributions causing traditional RCA methods to fail. Third, these databases typically execute millions of such diverse queries every minute rendering traditional methods inefficient when deployed at scale.\r\nIn this paper we describe Vista, a machine learning based performance troubleshooting framework for databases, and dive-deep into how it addresses the 3 real-world problems outlined above. Vista deploys a deep auto-regressive model trained on a large and diverse Amazon Relational Database Service (RDS) fleet with custom skip connections and periodicity alignment features to model long range and varying periodicity in customer workloads, and detects performance bottlenecks in the form of outliers. Furthermore, it efficiently filters only a top few dominating SQL queries from millions in a problematic workload, and uses a robust causal inference framework to identify the culprit queries and their statistics leading to a low false-positive and false-negative rate. Currently, Vista runs on hundreds of thousands of RDS databases, analyzes millions of workloads every day bringing down the troubleshooting time for RDS customers from hours to seconds. At the end, we also describe several challenges and learnings from implementing and deploying Vista at Amazon scale.",
        "authors": [
            "Vikramank Singh",
            "Zhao Song",
            "Balakrishnan (Murali) Narayanaswamy",
            "Kapil Eknath Vaidya",
            "Tim Kraska"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|ACM Symposium on Cloud Computing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157897",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Clinical Validation of Non-invasive Simulation-Based Determination of Vascular Impedance, Wave Intensity, and Hydraulic Work in Patients Undergoing Transcatheter Aortic Valve Replacement",
        "abstract": "Purpose The impact of Aortic Stenosis (AS) on the left ventricle (LV) extends beyond the influence of the pressure drop across the stenotic valve, but also includes the additional serial afterload imposed by the vascular system. Aortic input impedance is the gold standard for comprehensively studying the contribution of the vascular system to total myocardial afterload, but in the past measurement has been challenging arising from the need for invasive catheterization or specialized equipment to precisely record time-resolved blood pressure and flow signals. The goal of this work was to develop and validate a novel simulation-based method for determining aortic input impedance using only clinically available echocardiographic data and a simple blood pressure measurement. Methods A simulation-based method to determine vascular impedance was developed using echocardiographic data and a brachial blood pressure measurement. Simulation-based impedance was compared to impedance calculated from echocardiographic flow data and pressure data from a non-invasive central pressure measurement device. Results In validation analysis comparing patient-specific simulation-based vascular impedance to non-invasively measured impedance, correlation between methods across a range of vascular parameters varied between R2 = 0.40 and 0.99. A tendency was seen toward underestimation of pressure waveforms in point-by-point comparison of measured and simulated waveforms with an overall mean difference of 4.01 mmHg. Conclusions Requiring only non-invasive clinical data that are widely available, simulation-based vascular impedance has the potential to allow for easier, more widespread, and larger-scale investigation of the effect of vascular impedance on total LV afterload.",
        "authors": [
            "Jonathan Y. Brown",
            "Gabriela V. Fernandez",
            "Jose M. De La Torre Hernández",
            "Michael Murphy",
            "Benjamin S. Wessler",
            "Elazer R. Edelman"
        ],
        "journal_conference_name": "Annals of Biomedical Engineering",
        "publisher": "Springer International Publishing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157670",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "How Do Transformers Model Physics? Investigating the Simple Harmonic Oscillator",
        "abstract": "ow do transformers model physics? Do transformers model systems with interpretable analytical solutions or do they create an “alien physics” that is difficult for humans to decipher? We have taken a step towards demystifying this larger puzzle by investigating the simple harmonic oscillator (SHO), 𝑥¨+2𝛾𝑥˙+𝜔20𝑥=0\r\n, one of the most fundamental systems in physics. Our goal was to identify the methods transformers use to model the SHO, and to do so we hypothesized and evaluated possible methods by analyzing the encoding of these methods’ intermediates. We developed four criteria for the use of a method within the simple test bed of linear regression, where our method was 𝑦=𝑤𝑥\r\n and our intermediate was w: (1) Can the intermediate be predicted from hidden states? (2) Is the intermediate’s encoding quality correlated with the model performance? (3) Can the majority of variance in hidden states be explained by the intermediate? (4) Can we intervene on hidden states to produce predictable outcomes? Armed with these two correlational (1,2), weak causal (3), and strong causal (4) criteria, we determined that transformers use known numerical methods to model the trajectories of the simple harmonic oscillator, specifically, the matrix exponential method. Our analysis framework can conveniently extend to high-dimensional linear systems and nonlinear systems, which we hope will help reveal the “world model” hidden in transformers.",
        "authors": [
            "Subhash Kantamneni",
            "Ziming Liu",
            "Max Tegmark"
        ],
        "journal_conference_name": "Entropy",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157692",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Towards Safer Heuristics With Xplain",
        "abstract": "Many problems that cloud operators solve are computationally expensive, and operators often use heuristic algorithms (that are faster and scale better than optimal) to solve them more efficiently. Heuristic analyzers enable operators to find when and by how much their heuristics underperform. However, these tools do not provide enough detail for operators to mitigate the heuristic's impact in practice: they only discover a single input instance that causes the heuristic to underperform (and not the full set) and they do not explain why.\r\nWe propose XPlain, a tool that extends these analyzers and helps operators understand when and why their heuristics underperform. We present promising initial results that show such an extension is viable.",
        "authors": [
            "Pantea Karimi",
            "Solal Pirelli",
            "Siva Kesava Reddy Kakarla",
            "Ryan Beckett",
            "Santiago Segarra",
            "Beibin Li",
            "Pooria Namyar",
            "Behnaz Arzani"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 23rd ACM Workshop on Hot Topics in Networks",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157896",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Dynamic Expansion and Merging of the Equatorial Ionization Anomaly During the 10–11 May 2024 Super Geomagnetic Storm",
        "abstract": "first_pagesettingsOrder Article Reprints\r\nOpen AccessTechnical Note\r\nDynamic Expansion and Merging of the Equatorial Ionization Anomaly During the 10–11 May 2024 Super Geomagnetic Storm\r\nby Ercha Aa 1,2,*ORCID,Yanhong Chen 2ORCID andBingxian Luo 2ORCID\r\n1\r\nHaystack Observatory, Massachusetts Institute of Technology, Westford, MA 01886, USA\r\n2\r\nNational Space Science Center, Chinese Academy of Sciences, Beijing 100190, China\r\n*\r\nAuthor to whom correspondence should be addressed.\r\nRemote Sens. 2024, 16(22), 4290; https://doi.org/10.3390/rs16224290\r\nSubmission received: 24 October 2024 / Revised: 14 November 2024 / Accepted: 15 November 2024 / Published: 18 November 2024\r\n(This article belongs to the Special Issue Ionosphere Monitoring with Remote Sensing (3rd Edition))\r\nDownloadkeyboard_arrow_down Browse Figures Versions Notes\r\n\r\nAbstract\r\nThis study investigates the responses of the equatorial and low-latitude ionosphere in the American–Atlantic longitude sector during the super geomagnetic storm that occurred on 10–11 May 2024. The investigation utilizes multi-instrument datasets, including ground-based observations (GNSS TEC, ionosonde, and Fabry–Perot interferometer) as well as space-borne satellite measurements (GOLD, Swarm, DMSP, and TIMED). Our findings reveal significant day-to-day variations in the storm-time equatorial ionization anomaly (EIA), summarized as follows: (1) During the main phase of the storm, the low- and mid-latitude ionosphere experienced a positive storm, with TEC drastically enhanced by 50–100% within a few hours. The EIA crests exhibited a substantial poleward expansion, reaching as high as ±35° MLAT. This expansion was caused by the enhanced fountain effect driven by penetration electric fields, along with increased ambipolar diffusion due to transient meridional wind surges. (2) During the recovery phase of the storm, the global ionosphere was characterized by a substantial negative storm with a 50–80% depletion in TEC. The EIA crests were notably suppressed and merged into a single equatorial band, which can be attributed to the composition change effect and the influence of disturbance dynamo electric fields. These results illustrate the complex processes of magnetosphere–ionosphere–thermosphere coupling during a superstorm, highlighting the significant impacts of space weather on the global ionosphere.",
        "authors": [
            "Ercha Aa",
            "Yanhong Chen",
            "Bingxian Luo"
        ],
        "journal_conference_name": "Remote Sensing",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157691",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "MLTCP: A Distributed Technique to Approximate Centralized Flow Scheduling For Machine Learning",
        "abstract": "This paper argues that congestion control protocols in machine learning datacenters sit at a sweet spot between centralized and distributed flow scheduling solutions. We present MLTCP, a technique to augment today's congestion control algorithms to approximate an interleaved centralized flow schedule. At the heart of MLTCP lies a straight-forward principle based on a key conceptual insight: by scaling the congestion window size (or sending rate) based on the number of bytes sent at each iteration, MLTCP flows eventually converge into a schedule that reduces network contention. We demonstrate that MLTCP uses a gradient descent trend with a step taken at every training (or fine-tuning) iteration towards reducing network congestion among competing jobs.",
        "authors": [
            "Sudarsanan Rajasekaran",
            "Sanjoli Narang",
            "Anton A. Zabreyko",
            "Manya Ghobadi"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 23rd ACM Workshop on Hot Topics in Networks",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157894",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Case for Decentralized Fallback Networks",
        "abstract": "This paper argues that network and application delivery infrastructures have become highly centralized and are more vulnerable to attacks and disasters than is desirable. It proposes a research agenda for decentralized fallback networks and focuses on a key component---a city-scale decentralized network using existing Wi-Fi access points, which are deployed across almost all buildings in cities. It proposes a routing system that uses information about buildings from geospatial maps instead of traditional routing mechanisms to scale well to millions of Wi-Fi nodes.",
        "authors": [
            "James Lynch",
            "Ziqian Liu",
            "Chenning Li",
            "Manya Ghobadi",
            "Hari Balakrishnan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 23rd ACM Workshop on Hot Topics in Networks",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157895",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Modeling Inertia-Driven Oil Transport Inside the Three-Piece Oil Control Ring of Internal Combustion Engines",
        "abstract": "The three-piece oil control ring (TPOCR), traditionally used in light-duty gasoline engines, is becoming a viable option for heavy-duty gas and hydrogen engines due to its ability to control lubricating oil consumption (LOC) under throttled conditions. Understanding the distribution of oil inside the TPOCR groove, as well as the effects of rail gap and drain hole positions, is critical for optimizing TPOCR and groove designs. In this work, a one-dimensional oil distribution model was developed to simulate inertia-driven oil transport in the TPOCR groove. A novel approach was proposed by first dividing the TPOCR into units composed of a pair of expander pitches. Then, the relationship between the oil outflow rate of the unit and its oil mass was established with the help of three-dimensional two-phase computational fluid dynamics (CFD) simulations. This relationship was then used to model one-dimensional oil transport along the circumference of the TPOCR groove. Incorporating the boundary conditions at the rail gaps and drain holes, this simple model can complete computations for 10,000 cycles within a few seconds, allowing for quick the evaluation of transient behavior and design iterations. Studies on low-load conditions show that the model, with reasonable adjustment for the boundary conditions, can match the oil distribution patterns observed in visualization experiments. This is the first step toward studying oil transport in the TPOCR groove before involving the effects of gas flows.",
        "authors": [
            "Tsung-Yu Yang",
            "Mo Li",
            "Tian Tian"
        ],
        "journal_conference_name": "Lubricants",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157690",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Unsupervised Canine Emotion Recognition Using Momentum Contrast",
        "abstract": "We describe a system for identifying dog emotions based on dogs’ facial expressions and body posture. Towards that goal, we built a dataset with 2184 images of ten popular dog breeds, grouped into seven similarly sized primal mammalian emotion categories defined by neuroscientist and psychobiologist Jaak Panksepp as ‘Exploring’, ‘Sadness’, ‘Playing’, ‘Rage’, ‘Fear’, ‘Affectionate’ and ‘Lust’. We modified the contrastive learning framework MoCo (Momentum Contrast for Unsupervised Visual Representation Learning) to train it on our original dataset and achieved an accuracy of 43.2% and a baseline of 14%. We also trained this model on a second publicly available dataset that resulted in an accuracy of 48.46% but had a baseline of 25%. We compared our unsupervised approach with a supervised model based on a ResNet50 architecture. This model, when tested on our dataset with the seven Panksepp labels, resulted in an accuracy of 74.32%",
        "authors": [
            "Aarya Bhave",
            "Alina Hafner",
            "Anushka Bhave",
            "Peter A. Gloor"
        ],
        "journal_conference_name": "Sensors",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157689",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploring the impact of COVID-19 on the grammar of schools in project-based learning contexts",
        "abstract": "While scholars and public figures have positioned the COVID-19 pandemic as an opportunity for school reform, the response to this potential for change by teachers remains underexplored. In turn, we attend to the following research question: how do teachers at project-based learning high schools conceptualize the changes to education that have occurred in response to the COVID-19 pandemic? In analyzing temporally dispersed interviews with eight teachers from four different schools in the United States between 2020 and 2022, we found that participants recognized changes in the pedagogies, curricula, assessments, and structures in their school systems. In particular, teachers conceptualized these educational shifts through the lenses of technological change, a push for student-centered practices, and an embrace of real world applications of learning. However, they also described a reversal of these changes once in person schooling returned, illustrating an inability of the pandemic to affect the “grammar of schools” (Tyack & Tobin, 1994).",
        "authors": [
            "Peter J. Woods",
            "Emma Anderson",
            "Avneet Hira"
        ],
        "journal_conference_name": "Journal of Educational Change",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157560",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "VI-VS: calibrated identification of feature dependencies in single-cell multiomics",
        "abstract": "Unveiling functional relationships between various molecular cell phenotypes from data using machine learning models is a key promise of multiomics. Existing methods either use flexible but hard-to-interpret models or simpler, misspecified models. VI-VS (Variational Inference for Variable Selection) balances flexibility and interpretability to identify relevant feature relationships in multiomic data. It uses deep generative models to identify conditionally dependent features, with false discovery rate control. VI-VS is available as an open-source Python package, providing a robust solution to identify features more likely representing genuine causal relationships.",
        "authors": [
            "Pierre Boyeau",
            "Stephen Bates",
            "Can Ergen",
            "Michael I. Jordan",
            "Nir Yosef"
        ],
        "journal_conference_name": "Genome Biology",
        "publisher": "BioMed Central",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157562",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Learning reaction-transport coupling from thermal waves",
        "abstract": "Although thermal waves are ubiquitous in nature and engineering, the development of diagnostic tools capable of elucidating the roles of reaction and transport remains an unmet need. This limits our comprehension of the physics and ability to predict wave dynamics. Here we demonstrate that thermal properties and chemical kinetics can be learned directly from observing thermal wave dynamics, using partial differential equation-constrained optimization. This enables the determination of unobserved reaction rates without the need for a comprehensive measurement of all state variables, given the model space constrained by governing equations. Examples include steady planar waves and unsteady pulsating waves of which dynamics are commonly observed in nature. We show successful learning of thermal properties and chemical kinetics and reconstruction of wave dynamics with the inferred properties, which enables the comprehension of the intricate reaction-transport coupling from thermal data.",
        "authors": [
            "Suyong Kim",
            "Sili Deng"
        ],
        "journal_conference_name": "Nature Communications",
        "publisher": "Springer Science and Business Media LLC",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158171",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Identifying Money Laundering Subgraphs on the Blockchain",
        "abstract": "Anti-Money Laundering (AML) involves the identification of money laundering crimes in financial activities, such as cryptocurrency transactions. Recent studies advanced AML through the lens of graph-based machine learning, modeling the web of financial transactions as a graph and developing graph methods to identify suspicious activities. For instance, a recent effort on opensourcing datasets and benchmarks, Elliptic2, treats a set of Bitcoin addresses, considered to be controlled by the same entity, as a graph node and transactions among entities as graph edges. This modeling reveals the “shape” of a money laundering scheme—a subgraph on the blockchain, such as a peeling chain or a nested service. Despite the attractive subgraph classification results benchmarked by the paper, competitive methods remain expensive to apply due to the massive size of the graph; moreover, existing methods require candidate subgraphs as inputs which may not be available in practice.\r\nIn this work, we introduce RevTrack, a graph-based framework that enables large-scale AML analysis with a lower cost and a higher accuracy. The key idea is to track the initial senders and the final receivers of funds; these entities offer a strong indication of the nature (licit vs. suspicious) of their respective subgraph. Based on this framework, we propose RevClassify, which is a neural network model for subgraph classification. Additionally, we address the practical problem where subgraph candidates are not given, by proposing RevFilter. This method identifies new suspicious subgraphs by iteratively filtering licit transactions, using RevClassify. Benchmarking these methods on Elliptic2, a new standard for AML, we show that RevClassify outperforms state-of-the-art subgraph classification techniques in both cost and accuracy. Furthermore, we demonstrate the effectiveness of RevFilter in discovering new suspicious subgraphs, confirming its utility for practical AML.",
        "authors": [
            "Kiwhan Song",
            "Mohamed Ali Dhraief",
            "Muhua Xu",
            "Locke Cai",
            "Xuhao Chen",
            "Arvind Mithal",
            "Jie Chen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|5th ACM International Conference on AI in Finance",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157760",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Renewing Our Focus on Vulnerable Populations Among People Living with HIV",
        "abstract": "The global HIV landscape has changed over the past few decades, with great milestones achieved in both HIV treatment and prevention. Access to lifesaving antiretroviral therapy (ART) has markedly expanded, with a total of 30.7 million (27 million–31.9 million) out of 39.9 million (36.1 million–44.6 million) people living with HIV accessing the medication in 2023 [1]. Continued expansion of access to, initiation of, and adherence to treatment is crucial in achieving control of the HIV pandemic, given the strong evidence that treatment is prevention [2]. Despite these marked advances, 28% of people living with HIV (PLHIV) are reported to be virally unsuppressed [1]. Viral non-suppression is associated with increased risk of progression to AIDS and portends poor outcomes for PLHIV [3,4]. Additionally, viral non-suppression increases the risk of onward transmission of HIV, reversing the gains made in combating the pandemic [3]. The risk of viral non-suppression is greater in certain groups. This Special Issue focuses on exploring HIV support, care, and treatment for vulnerable populations, or those at elevated risk of viral non-suppression and poor health outcomes.\r\nWe solicited articles on this topic and received submissions from diverse settings and authors of different backgrounds and training. The interest and importance of this topic are revealed in the diversity of articles that were submitted and the disciplines that showed interest. This Special Issue contains ten articles that advance our understanding of vulnerable populations, challenge the current thinking about vulnerable populations, and propose bold interventions to address the barriers to HIV care engagement throughout the cascade.\r\nThe articles in this Special Issue bring to the fore three critical questions about vulnerable groups: What makes one vulnerable? What are the threats to care engagement for vulnerable people? And what health care system changes are needed to accommodate vulnerable people? These questions must be addressed to improve outcomes among vulnerable groups, especially to design interventions that address their concerns.",
        "authors": [
            "James Ayieko",
            "Marguerite Thorp",
            "Musie Ghebremichael"
        ],
        "journal_conference_name": "Tropical Medicine and Infectious Disease",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157688",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Local geometry of NAE-SAT solutions in the condensation regime",
        "abstract": "The local behavior of typical solutions of random constraint satisfaction problems (csp) describes many important phenomena including clustering thresholds, decay of correlations, and the behavior of message passing algorithms. When the constraint density is low, studying the planted model is a powerful technique for determining this local behavior which in many examples has a simple Markovian structure. The work of Coja-Oghlan, Kapetanopoulos, Müller (Comb Prob Comput 29:346-422, 2020) showed that for a wide class of models, this description applies up to the so-called condensation threshold. Understanding the local behavior after the condensation threshold is more complex due to long-range correlations. In this work, we revisit the random regular nae-sat model in the condensation regime and determine the local weak limit which describes a random solution around a typical variable. This limit exhibits a complicated non-Markovian structure arising from the space of solutions being dominated by a small number of large clusters. This is the first description of the local weak limit in the condensation regime for any sparse random csps in the one-step replica symmetry breaking (1rsb) class. Our result is non-asymptotic and characterizes the tight fluctuation O ( n - 1 / 2 ) around the limit. Our proof is based on coupling the local neighborhoods of an infinite spin system, which encodes the structure of the clusters, to a broadcast model on trees whose channel is given by the 1rsb belief-propagation fixed point. We believe that our proof technique has broad applicability to random csps in the 1rsb class.",
        "authors": [
            "Allan Sly",
            "Youngtak Sohn"
        ],
        "journal_conference_name": "Probability Theory and Related Fields",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157559",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "FraudGT: A Simple, Effective, and Efficient Graph Transformer for Financial Fraud Detection",
        "abstract": "Fraud detection plays a crucial role in the financial industry, preventing significant financial losses. Traditional rule-based systems and manual audits often struggle with the evolving nature of fraud schemes and the vast volume of transactions. Recent advances in machine learning, particularly graph neural networks (GNNs), have shown promise in addressing these challenges. However, GNNs still face limitations in learning intricate patterns, effectively utilizing edge attributes, and maintaining efficiency on large financial graphs. To address these limitations, we introduce FraudGT, a simple, effective, and efficient graph transformer (GT) model specifically designed for fraud detection in financial transaction graphs. FraudGT leverages edge-based message passing gates and an edge attribute-based attention bias to enhance its ability to discern important transactional features and differentiate between normal and fraudulent transactions. Our model achieves state-of-the-art performance in detecting fraudulent activities while demonstrating high throughput and significantly lower latency compared to existing methods. We validate the effectiveness of FraudGT through extensive experiments on multiple large-scale synthetic financial datasets. FraudGT consistently outperforms other models, achieving 7.8–17.8% higher F1 scores, while delivering an average of 2.4 × greater throughput and reduced latency. Our code and datasets are available at https://github.com/junhongmit/FraudGT.",
        "authors": [
            "Junhong Lin",
            "Xiaojie Guo",
            "Yada Zhu",
            "Samuel Mitchell",
            "Erik Altman",
            "Julian Shun"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|5th ACM International Conference on AI in Finance",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157762",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A 35-Year Analysis of Vegetation Cover in Rare-Earth Mining Areas Using Landsat Data",
        "abstract": "Fractional vegetation cover (FVC) plays a significant role in assessing ecological quality and protection, as well as soil and water conservation. As a typical rare-earth resource county in China, Dingnan County has experienced rapid development due to rare-earth mining, resulting in significant alterations to vegetation cover. To elucidate the spatio-temporal changes in vegetation within Dingnan County over the past 35 years and the effects of natural and human factors on these changes, the spatial and temporal variations in FVC were analyzed using Landsat-TM/OLI multispectral images taken in 1988, 1995, 1997, 2002, 2006, 2013, 2017, and 2023. The findings indicate that (1) vegetation coverage in Dingnan County decreased from 1988 to 2002, followed by a gradual increase; (2) high vegetation cover is predominantly found in forested areas that maintain their natural state, while the central town and mining areas exhibit generally low coverage; (3) there are regional differences in the relationship between vegetation cover and environmental factors in Dingnan County. This research facilitates the alignment of ion-type rare-earth mining with ecological protection, thereby promoting the sustainable development of the mining area and providing scientific guidance for local governments to formulate more effective management and protection strategies for the mining ecosystem. Additionally, this research offers a scientific foundation for mining areas globally to develop sustainable policies and informed decision-making regarding environmental protection and sustainable development.",
        "authors": [
            "Zhubin Zheng",
            "Yuqing Liu",
            "Na Chen",
            "Ge Liu",
            "Shaohua Lei",
            "Jie Xu",
            "Jianzhong Li",
            "Jingli Ren",
            "Chao Huang"
        ],
        "journal_conference_name": "Forests",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157687",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Hydrodynamic forces on a side-by-side ellipse pair with and without relative motion",
        "abstract": "Motivated by flow interactions in schooling biological swimmers as well as in unmanned underwater vehicle fleets, we investigate the flow past two identical 6 : 1 ellipses using two-dimensional simulations at Reynolds numbers of  (103). When both ellipses move at the same velocity, overall drag reductions of 10 %–20 % can be achieved in staggered formations, with the strongest drag reductions occurring at the smallest lateral distances. In side-by-side configurations, the drag on both bodies increases by 10 %–20 %. Lift coefficients are repulsive and up to four times larger than the total drag coefficients. During overtaking manoeuvres, increasing the relative speed of the overtaking ellipse predominantly affects the forces on the overtaken ellipse. The mean drag force on the overtaken ellipse increases with increasing speed difference. Mean lift forces during the overtaking manoeuvre are repulsive for both bodies; as the speed difference increases, the repulsive force increases on the overtaken body and decreases on the overtaking body. Overall, these results highlight that the lateral forces in hydrodynamic interactions between bodies in formation dominate the hydrodynamic interactions. Further, the results indicate that future work is needed to investigate how viscous and three-dimensional effects change the lateral forces between side-by-side submerged bodies.",
        "authors": [
            "Preston Rhodes",
            "Wim M. van Rees"
        ],
        "journal_conference_name": "Flow",
        "publisher": "Cambridge University Press",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157676",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Hierarchically conductive electrodes unlock stable and scalable CO2 electrolysis",
        "abstract": "Electrochemical CO2 reduction has emerged as a promising CO2 utilization technology, with Gas Diffusion Electrodes becoming the predominant architecture to maximize performance. Such electrodes must maintain robust hydrophobicity to prevent flooding, while also ensuring high conductivity to minimize ohmic losses. Intrinsic material tradeoffs have led to two main architectures: carbon paper is highly conductive but floods easily; while expanded Polytetrafluoroethylene is flooding resistant but non-conductive, limiting electrode sizes to just 5 cm2. Here we demonstrate a hierarchically conductive electrode architecture which overcomes these scaling limitations by employing inter-woven microscale conductors within a hydrophobic expanded Polytetrafluoroethylene membrane. We develop a model which captures the spatial variability in voltage and product distribution on electrodes due to ohmic losses and use it to rationally design the hierarchical architecture which can be applied independent of catalyst chemistry or morphology. We demonstrate C2+ Faradaic efficiencies of ~75% and reduce cell voltage by as much as 0.9 V for electrodes as large as 50 cm2 by employing our hierarchically conductive electrode architecture.",
        "authors": [
            "Simon Rufer",
            "Michael P Nitzsche",
            "Sanjay Garimella",
            "Jack R Lake",
            "Kripa K Varanasi"
        ],
        "journal_conference_name": "Nature Communications",
        "publisher": "Springer Science and Business Media LLC",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158170",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Theory of Quantum Anomalous Hall Phases in Pentalayer Rhombohedral Graphene Moiré Structures",
        "abstract": "Remarkable recent experiments on the moiré structure formed by pentalayer rhombohedral graphene aligned with a hexagonal boron nitride substrate report the discovery of a zero field fractional quantum Hall effect. These “(fractional) quantum anomalous Hall” [(F)QAH] phases occur for one sign of a perpendicular displacement field, and correspond, experimentally, to full or partial filling of a valley polarized Chern-1 band. Such a band is absent in the noninteracting band structure. Here we show that electron-electron interactions play a crucial role, and present microscopic theoretical calculations demonstrating the emergence of a nearly flat, isolated, Chern-1 band and FQAH phases in this system. We also study the four- and six-layer analogs and identify parameters where a nearly flat isolated Chern-1 band emerges which may be suitable to host FQAH physics.",
        "authors": [
            "Zhihuan Dong",
            "Adarsh S. Patri",
            "Todadri Senthil"
        ],
        "journal_conference_name": "Physical Review Letters",
        "publisher": "American Physical Society",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157541",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Bridging Dictionary: AI-Generated Dictionary of Partisan Language Use",
        "abstract": "Words often carry different meanings for people from diverse backgrounds. Today's era of social polarization demands that we choose words carefully to prevent miscommunication, especially in political communication and journalism. To address this issue, we introduce the Bridging Dictionary, an interactive tool designed to illuminate how words are perceived by people with different political views. The Bridging Dictionary includes a static, printable document featuring 796 terms with summaries generated by a large language model. These summaries highlight how the terms are used distinctively by Republicans and Democrats. Additionally, the Bridging Dictionary offers an interactive interface that lets users explore selected words, visualizing their frequency, sentiment, summaries, and examples across political divides. We present a use case for journalists and emphasize the importance of human agency and trust in further enhancing this tool. The deployed version of Bridging Dictionary is available at https://dictionary.ccc-mit.org/.",
        "authors": [
            "Hang Jiang",
            "Doug Beeferman",
            "William Brannon",
            "Andrew Heyward",
            "Deb Roy"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Companion of the 2024 Computer-Supported Cooperative Work and Social Computing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157764",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Excess Mortality and its Determinants During the COVID-19 Pandemic in 21 Countries: An Ecological Study from the C-MOR Project, 2020 and 2021",
        "abstract": "Introduction The COVID-19 pandemic overwhelmed health systems, resulting in a surge in excess deaths. This study clustered countries based on excess mortality to understand their response to the pandemic and the influence of various factors on excess mortality within each cluster. Materials and Methods This ecological study is part of the COVID-19 MORtality (C-MOR) Consortium. Mortality data were gathered from 21 countries and were previously used to calculate weekly all-cause excess mortality. Thirty exposure variables were considered in five categories as factors potentially associated with excess mortality: population factors, health care resources, socioeconomic factors, air pollution, and COVID-19 policy. Estimation of Latent Class Linear Mixed Model (LCMM) was used to cluster countries based on response trajectory and Generalized Linear Mixture Model (GLMM) for each cluster was run separately. Results Using LCMM, two clusters were reached. Among 21 countries, Brazil, the USA, Georgia, and Poland were assigned to a separate cluster, with the mean of excess mortality z-score in 2020 and 2021 around 4.4, compared to 1.5 for all other countries assigned to the second cluster. In both clusters the population incidence of COVID-19 had the greatest positive relationship with excess mortality while interactions between the incidence of COVID-19, fully vaccinated people, and stringency index were negatively associated with excess mortality. Moreover, governmental variables (government revenue and government effectiveness) were the most protective against excess mortality. Conclusion This study highlighted that clustering countries based on excess mortality can provide insights to gain a broader understanding of countries' responses to the pandemic and their effectiveness.",
        "authors": [
            "Mohammad Reza Rahmanian Haghighi",
            "Chryso T. Pallari",
            "Souzana Achilleos",
            "Annalisa Quattrocchi",
            "John Gabel",
            "Andreas Artemiou",
            "Maria Athanasiadou",
            "Stefania Papatheodorou",
            "Tianyu Liu",
            "José Antonio Cernuda Martínez",
            "Gleb Denissov",
            "Błażej Łyszczarz",
            "Qian Huang",
            "Kostas Athanasakis",
            "Catherine M. Bennett"
        ],
        "journal_conference_name": "Journal of Epidemiology and Global Health",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157558",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Van der Waals magnetic materials for current-induced control toward spintronic applications",
        "abstract": "Spintronics, leveraging electron spin for information processing, promises substantial advancements in energy-efficient computing. Van der Waals (vdW) magnetic materials, with their unique-layered structures and exceptional magnetic properties, have emerged as pivotal components in this field. This report explores the current-based control of vdW magnets, focusing on the spin–orbit torque (SOT) mechanism, which is crucial for spintronic applications. Key studies on Fe3GaTe2/Pt and Fe3GaTe2/WTe2 heterostructures are highlighted, demonstrating efficient SOT switching at room temperature. The advantages of vdW magnets for SOT switching, including high spin-torque efficiencies and superior interface quality, are discussed. The report also examines future directions, such as wafer-scale growth techniques, materials design for enhanced Curie temperatures (Tc), and the development of magneto tunnel junctions using all-vdW materials. These advancements underscore the potential of vdW magnetic materials in developing scalable, high-performance spintronic devices, paving the way for significant breakthroughs in energy-efficient computing. Graphical abstract",
        "authors": [
            "Jeongchun Ryu",
            "Shivam N. Kajale",
            "Deblina Sarkar"
        ],
        "journal_conference_name": "MRS Communications",
        "publisher": "Springer International Publishing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157557",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Identifying the Values that Shape HCI and CSCW Research with Latin American Communities: A Collaborative Autoethnography",
        "abstract": "Over the past decade, community collaborations have come into focus within the HCI and CSCW fields. Largely the result of increased concern for social and contextual dimensions of practice, these partnerships facilitate a pathway for researchers and practitioners to foreground the nuances of technology as it takes place in the real world. How these collaborations are engaged, what values mediate them, and how practices might vary across geographies remain active research questions. In this paper, we contribute by zooming into the experience of four HCI and CSCW researchers engaging in community collaborations in Latin America (LATAM). Through a collaborative autoethnography (CAE), we identify three main value tensions impacting HCI practices and methods in research collaborations with LATAM communities: camaraderie vs. cautiousness, informality vs. formality and hopefulness vs. transparency. Building on our findings, we provide three recommendations for researchers interested in engaging in community-based research in similar contexts.",
        "authors": [
            "Carla Griggio",
            "Mayra Barrera Machuca",
            "Marisol Wong-Villacres",
            "Laura Gayt?n-Lugo",
            "Karla Badillo-Urquiola",
            "Adriana Alvarado Garcia",
            "Monica Perusquia-Hernandez",
            "Marianela Ciolfi Felice",
            "Franceli Cibrian",
            "Michaelanne Thomas",
            "Carolina Fuentes",
            "Pedro Reynolds-Cu?llar"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Companion of the 2024 Computer-Supported Cooperative Work and Social Computing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157793",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Harnessing LLMs for Automated Video Content Analysis: An Exploratory Workflow of Short Videos on Depression",
        "abstract": "Despite the growing interest in leveraging Large Language Models (LLMs) for content analysis, current studies have primarily focused on text-based content. In the present work, we explored the potential of LLMs in assisting video content analysis by conducting a case study that followed a new workflow of LLM-assisted multimodal content analysis. The workflow encompasses codebook design, prompt engineering, LLM processing, and human evaluation. We strategically crafted annotation prompts to get LLM Annotations in structured form and explanation prompts to generate LLM Explanations for a better understanding of LLM reasoning and transparency. To test LLM's video annotation capabilities, we analyzed 203 keyframes extracted from 25 YouTube short videos about depression. We compared the LLM Annotations with those of two human coders and found that LLM has higher accuracy in object and activity Annotations than emotion and genre Annotations. Moreover, we identified the potential and limitations of LLM's capabilities in annotating videos. Based on the findings, we explore opportunities and challenges for future research and improvements to the workflow. We also discuss ethical concerns surrounding future studies based on LLM-assisted video analysis.",
        "authors": [
            "Jiaying (Lizzy) Liu",
            "Yunlong Wang",
            "Yao Lyu",
            "Yiheng Su",
            "Shuo Niu",
            "Xuhai Xu",
            "Yan Zhang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Companion of the 2024 Computer-Supported Cooperative Work and Social Computing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157763",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "AudienceView: AI-Assisted Interpretation of Audience Feedback in Journalism",
        "abstract": "Understanding and making use of audience feedback is important but difficult for journalists, who now face an impractically large volume of audience comments online. We introduce AudienceView, an online tool to help journalists categorize and interpret this feedback by leveraging large language models (LLMs). AudienceView identifies themes and topics, connects them back to specific comments, provides ways to visualize the sentiment and distribution of the comments, and helps users develop ideas for subsequent reporting projects. We consider how such tools can be useful in a journalist's workflow, and emphasize the importance of contextual awareness and human judgment.",
        "authors": [
            "William Brannon",
            "Doug Beeferman",
            "Hang Jiang",
            "Andrew Heyward",
            "Deb Roy"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Companion of the 2024 Computer-Supported Cooperative Work and Social Computing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157792",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Beyond Preferences in AI Alignment",
        "abstract": "The dominant practice of AI alignment assumes (1) that preferences are an adequate representation of human values, (2) that human rationality can be understood in terms of maximizing the satisfaction of preferences, and (3) that AI systems should be aligned with the preferences of one or more humans to ensure that they behave safely and in accordance with our values. Whether implicitly followed or explicitly endorsed, these commitments constitute what we term a preferentist approach to AI alignment. In this paper, we characterize and challenge the preferentist approach, describing conceptual and technical alternatives that are ripe for further research. We first survey the limits of rational choice theory as a descriptive model, explaining how preferences fail to capture the thick semantic content of human values, and how utility representations neglect the possible incommensurability of those values. We then critique the normativity of expected utility theory (EUT) for humans and AI, drawing upon arguments showing how rational agents need not comply with EUT, while highlighting how EUT is silent on which preferences are normatively acceptable. Finally, we argue that these limitations motivate a reframing of the targets of AI alignment: Instead of alignment with the preferences of a human user, developer, or humanity-writ-large, AI systems should be aligned with normative standards appropriate to their social roles, such as the role of a general-purpose assistant. Furthermore, these standards should be negotiated and agreed upon by all relevant stakeholders. On this alternative conception of alignment, a multiplicity of AI systems will be able to serve diverse ends, aligned with normative standards that promote mutual benefit and limit harm despite our plural and divergent values.",
        "authors": [
            "Tan Zhi-Xuan",
            "Micah Carroll",
            "Matija Franklin",
            "Hal Ashton"
        ],
        "journal_conference_name": "Philosophical Studies",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157530",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Arthroscopic Bone Block and Arthroscopic Latarjet for Anterior Shoulder Dislocation&mdash;Technical Note with Tricks and Tips for Conversion and Successful Surgery",
        "abstract": "Background: The treatment of patients affected by recurrent anterior shoulder instability has received more attention in the last ten years, focusing on the management of bone loss, which is crucial in predicting postoperative recurrence risk. Recently, various bone grafting techniques and different fixation methods have been developed to preserve native anatomy and reduce complications. Nowadays, glenoid bone reconstruction is usually carried out via the Latarjet procedure or free bone block technique. While the Latarjet procedure has traditionally been considered the best option, the bone block has been demonstrated to be a successful procedure. Even though the indication to perform a free bone block or a Latarjet procedure may be given preoperatively, in cases where the choice between the two procedures is unclear, the decision can be made intraoperatively, given the possibility to switch from one to another. This technical note aims to outline our techniques for the arthroscopic Latarjet procedure and the arthroscopic free bone block, as well as discuss the indications, benefits and downsides of each procedure. Technical tips and tricks are provided. Methods: A step-by-step thorough description of bone block and Latarjet procedures is provided, as well as a comparison of advantages and disadvantages of each technique and tips to avoid complications. Respective indications are discussed. Results: Both the procedures have benefits and downsides. The arthroscopic Latarjet procedure is the most effective in addressing anterior shoulder instability, but is more elaborate, has a shallow learning curve and can have a high complication rate. The bone block technique is an anatomic procedure with a shorter learning curve but has fewer indications. Conclusion: The Latarjet is currently considered the gold standard for glenoid bone grafting. The bone block technique can allegedly be seen as being “in the middle” of the soft tissue repair and Latarjet procedures. Many factors should be considered when choosing the right surgical technique, and treatment plans must be customized for each patient. More studies with long-term follow-up are needed to evaluate the efficacy of arthroscopic bone grafting procedures in various subtypes of patients based on bipolar bone loss assessment and individual risk factors.",
        "authors": [
            "Umile Giuseppe Longo",
            "Gianmarco Marcello",
            "Ara Nazarian",
            "Joseph DeAngelis",
            "Margaux D’Hooghe",
            "Pieter D’Hooghe"
        ],
        "journal_conference_name": "Osteology",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157957",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Insights from an Experiment Crowdsourcing Data from Thousands of US Amazon Users: The importance of transparency, money, and data use",
        "abstract": "Data generated by users on digital platforms are a crucial resource for advocates and researchers interested in uncovering digital inequities, auditing algorithms, and understanding human behavior. Yet data access is often restricted. How can researchers both effectively and ethically collect user data? This paper shares an innovative approach to crowdsourcing user data to collect otherwise inaccessible Amazon purchase histories, spanning 5 years, from more than 5,000 U.S. users. We developed a data collection tool that prioritizes participant consent and includes an experimental study design. The design allows us to study multiple important aspects of privacy perception and user data sharing behavior, including how socio-demographics, monetary incentives and transparency can impact share rates. Experiment results (N=6,325) reveal both monetary incentives and transparency can significantly increase data sharing. Age, race, education, and gender also played a role, where female and less-educated participants were more likely to share. Our study design enables a unique empirical evaluation of the &#8220;privacy paradox&#8221;, where users claim to value their privacy more than they do in practice. We set up both real and hypothetical data sharing scenarios and find measurable similarities and differences in share rates across these contexts. For example, increasing monetary incentives had a 6 times higher impact on share rates in real scenarios. In addition, we study participants' opinions on how data should be used by various third parties, again finding that gender, age, education, and race have a significant impact. Notably, the majority of participants disapproved of government agencies using purchase data yet the majority approved of use by researchers. Overall, our findings highlight the critical role that transparency, incentive design, and user demographics play in ethical data collection practices, and provide guidance for future researchers seeking to crowdsource user generated data.",
        "authors": [
            "Alex Berke",
            "Robert Mahari",
            "Sandy Pentland",
            "Kent Larson",
            "Dana Calacci"
        ],
        "journal_conference_name": "Proceedings of the ACM on Human-Computer Interaction",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157844",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Expediting treatments in the 21st century: orphan drugs and accelerated approvals",
        "abstract": "Background In response to activated patient communities’ catalyzation, two significant efforts by the FDA to expedite treatments have now been in place for multiple decades. In 1983, the United States Congress passed the Orphan Drug Act to provide financial incentives for development of drugs for rare diseases. In 1992, partly in response to the HIV epidemic, the FDA implemented Accelerated Approval (AA) to expedite access to promising new therapies to treat serious conditions with unmet medical need based on surrogate marker efficacy while additional clinical data is confirmed. The uses of these regulatory approaches over time are assessed in this study. Methods The following U.S. FDA CDER published lists were used in this analysis: 1. all orphan designations and approvals; 2. all AA and their details updated through December 31, 2022; new molecular entities (NMEs). Results Orphan drug designations and approvals have increased several-fold over the past four decades. The largest increase recently has been in therapies targeting oncological diseases (comprised of both oncology and malignant hematology). Although orphan drug approvals based on NMEs are the minority of orphan drug designations, the count of approved orphan drug NMEs has increased in recent years. The characteristics of orphan drug approvals show notable differences by disease area with rare diseases and medical genetics (49%) having a relatively large fraction of orphan drug approvals with NMEs compared to the oncological diseases (32%). Similar to the use of orphan drug designation, oncological disease therapies have been the largest utilizers of AA. Many therapies targeting these diseases address unmet medical need and can leverage surrogate markers that have previously been used in similar trials. The timings of conversion of AA (confirmed or withdrawn) were assessed and found to be consistent across decades and to have some dependency upon the broad disease area (when assessed by three large groups: HIV conversions were fastest; followed by oncology; followed by all others). By the end of 2022, 98% of the first 105 (approved in 2010 or earlier) AA had been converted to confirmed or withdrawn. Conclusions Although the typical timings for AA to be confirmed or withdrawn has not changed significantly over the decades, the disease areas utilizing orphan drug designation and AA have changed significantly over time. Both programs have had increases in their use for therapies targeting oncological diseases. The re-use of surrogate markers for oncological diseases has been an advantage in a way that may not be scientifically feasible in many other disease areas that have greater differentiation across disease etiology. For non-oncological diseases, applicability of AA is, in part, dependent upon greater focus on characterization and acceptance of novel surrogate markers.",
        "authors": [
            "Reuben Domike",
            "G. K. Raju",
            "Jamie Sullivan",
            "Annie Kennedy"
        ],
        "journal_conference_name": "Orphanet Journal of Rare Diseases",
        "publisher": "BioMed Central",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157535",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The CMS Statistical Analysis and Combination Tool: Combine",
        "abstract": "This paper describes the Combine software package used for statistical analyses by the CMS Collaboration. The package, originally designed to perform searches for a Higgs boson and the combined analysis of those searches, has evolved to become the statistical analysis tool presently used in the majority of measurements and searches performed by the CMS Collaboration. It is not specific to the CMS experiment, and this paper is intended to serve as a reference for users outside of the CMS Collaboration, providing an outline of the most salient features and capabilities. Readers are provided with the possibility to run Combine and reproduce examples provided in this paper using a publicly available container image. Since the package is constantly evolving to meet the demands of ever-increasing data sets and analysis sophistication, this paper cannot cover all details of Combine. However, the online documentation referenced within this paper provides an up-to-date and complete user guide.",
        "authors": [
            "Unknown author"
        ],
        "journal_conference_name": "Computing and Software for Big Science",
        "publisher": "Springer International Publishing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157537",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Dittos: Personalized, Embodied Agents That Participate in Meetings When You Are Unavailable",
        "abstract": "Imagine being able to send a personalized embodied agent to meetings you are unable to attend. This paper explores the idea of a Ditto—an agent that visually resembles a person, sounds like them, possesses knowledge about them, and can represent them in meetings. This paper reports on results from two empirical investigations: 1) focus group sessions with six groups (n=24) and 2) a Wizard of Oz (WOz) study with 10 groups (n=39) recruited from within a large technology company. Results from the focus group sessions provide insights on what contexts are appropriate for Dittos, and issues around social acceptability and representation risk. The focus group results also provide feedback on visual design characteristics for Dittos. In the WOz study, teams participated in meetings with two different embodied agents: a Ditto and a Delegate (an agent which did not resemble the absent person). Insights from this research demonstrate the impact these embodied agents can have in meetings and highlight that Dittos in particular show promise in evoking feelings of presence and trust, as well as informing decision making. These results also highlight issues related to relationship dynamics such as maintaining social etiquette, managing one's professional reputation, and upholding accountability. Overall, our investigation provides early evidence that Dittos could be beneficial to represent users when they are unable to be present but also outlines many factors that need to be carefully considered to successfully realize this vision.",
        "authors": [
            "Joanne Leong",
            "John Tang",
            "Edward Cutrell",
            "Sasa Junuzovic",
            "Gregory Baribault",
            "Kori Inkpen"
        ],
        "journal_conference_name": "Proceedings of the ACM on Human-Computer Interaction",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157850",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Chillbot: Content Moderation in the Backchannel",
        "abstract": "Moderating online spaces effectively is not a matter of simply taking down content: moderators also provide private feedback and defuse situations before they cross the line into harm. However, moderators have little tool support for these activities, which often occur in the backchannel rather than in front of the entire community. In this paper, we introduce Chillbot, a moderation tool for Discord designed to facilitate backchanneling from moderators to users. With Chillbot, moderators gain the ability to send rapid anonymous feedback responses to situations where removal or formal punishment is too heavy-handed to be appropriate, helping educate users about how to improve their behavior while avoiding direct confrontations that can put moderators at risk. We evaluated Chillbot through a two week field deployment on eleven Discord servers ranging in size from 25 to over 240,000 members. Moderators in these communities used Chillbot more than four hundred times during the study, and moderators from six of the eleven servers continued using the tool past the end of the formal study period. Based on this deployment, we describe implications for the design of a broader variety of means by which moderation tools can help shape communities' norms and behavior.",
        "authors": [
            "Joseph Seering",
            "Manas Khadka",
            "Nava Haghighi",
            "Tanya Yang",
            "Zachary Xi",
            "Michael Bernstein"
        ],
        "journal_conference_name": "Proceedings of the ACM on Human-Computer Interaction",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157849",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "\"Come to us first\": Centering Community Organizations in Artificial Intelligence for Social Good Partnerships",
        "abstract": "Artificial Intelligence for Social Good (AI4SG) has emerged as a growing body of research and practice exploring the potential of AI technologies to tackle social issues. This area emphasizes interdisciplinary partnerships with community organizations, such as non-profits and government agencies. However, amidst excitement about new advances in AI and their potential impact, the needs, expectations, and aspirations of these community organizations--and whether they are being met--are not well understood. Understanding these factors is important to ensure that the considerable efforts by AI teams and community organizations can actually achieve the positive social impact they strive for. Drawing on the Data Feminism framework, we explored the perspectives of community organization members on their partnerships with AI teams through 16 semi-structured interviews. Our study highlights the pervasive influence of funding agendas and the optimism surrounding AI's potential. Despite the significant intellectual contributions and labor provided by community organization members, their goals were frequently sidelined in favor of other stakeholders, including AI teams. While many community organization members expected tangible project deployment, only two out of 14 projects we studied reached the deployment stage. However, community organization members sustained their belief in the potential of the projects, still seeing diminished goals as valuable. To enhance the efficacy of future collaborations, our participants shared their aspirations for success, calling for co-leadership starting from the early stages of projects. We propose data co-liberation as a grounding principle for approaching AI4SG moving forward, positing that community organizations' co-leadership is essential for fostering more effective, sustainable, and ethical development of AI.",
        "authors": [
            "Hongjin Lin",
            "Naveena Karusala",
            "Chinasa Okolo",
            "Catherine D'Ignazio",
            "Krzysztof Gajos"
        ],
        "journal_conference_name": "Proceedings of the ACM on Human-Computer Interaction",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157845",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Entangled Amid Misaligned Seams: Limitations to Technology-Mediated Care for Repairing Infrastructural Breakdowns in a Youth Empowerment Program",
        "abstract": "The COVID-19 pandemic broke down the human infrastructure of many community-based programs, disrupting in-person care services for low-resourced families. Yet, minimal work has explored how actors repair these breakdowns and how other infrastructures may interfere with repairs in such contexts. Interviewing adolescents and adults affiliated with a youth empowerment program, we used the pandemic to examine how a human infrastructure that previously facilitated a sense of community broke down and how members attempted to repair this infrastructure. While organized activities, resources, and interpersonal interactions aligned to facilitate in-person care that established a sense of community, incorporating information and communication technologies to align a sociotechnical infrastructure during social restrictions could not overcome multiple constraints imposed by other infrastructures that limited this sense of community. We discuss limitations to care and aligning together multiple disjointed infrastructures, calling for CSCW researchers to critically consider asset-based design as a methodology that might help sustain a community's well-being.",
        "authors": [
            "Adrian Choi",
            "Grace Pfohl",
            "Catherine D'Ignazio",
            "Brooke Foucault Welles",
            "Andrea Parker"
        ],
        "journal_conference_name": "Proceedings of the ACM on Human-Computer Interaction",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157843",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Anonymization of Voices in Spaces for Civic Dialogue: Measuring Impact on Empathy, Trust, and Feeling Heard",
        "abstract": "Anonymity is a powerful component of many participatory media platforms that can afford people greater freedom of expression and protection from external coercion and interference. However, it can be difficult to effectively implement on platforms that leverage spoken language due to distinct biomarkers present in the human voice. In this work, we explore the use of voice anonymization methods within the context of a technology-enhanced civic dialogue network based in the United States, whose purpose is to increase feelings of agency and being heard within civic processes. Specifically, we investigate the use of two different speech transformation and synthesis methods for anonymization: voice conversion (VC) and text-to-speech (TTS). Through a series of two studies, we examine the impact that each method has on 1) the empathy and trust that listeners feel towards a person sharing a personal story, and 2) a speaker's own perception of being heard, finding that voice conversion is an especially suitable method for our purposes. Our findings open up interesting potential research directions related to anonymous spoken discourse, as well as additional ways of engaging with voice-based civic technologies.",
        "authors": [
            "Wonjune Kang",
            "Margaret Hughes",
            "Deb Roy"
        ],
        "journal_conference_name": "Proceedings of the ACM on Human-Computer Interaction",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157848",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Paradoxes of Openness: Trans Experiences in Open Source Software",
        "abstract": "In recent years, concerns have increased over the lack of contributor diversity in open source software (OSS), despite its status as a paragon of open collaboration. OSS is an important form of digital infrastructure and part of a career path for many developers. While there exists a growing body of literature on cisgender women&#8217;s under-representation in OSS, the experiences of contributors from other marginalized groups are comparatively absent from the literature. Such is the case for trans contributors, a historically influential group in OSS. In this study, we interviewed 21 trans participants to understand and represent their experiences in the OSS literature. From their experiences, we theorize two related paradoxes of openness in OSS: the paradox of openness and display and the paradox of openness and governance. In an increasingly violent world for trans people, we draw on our theorizing to build recommendations for more inclusive and safer OSS projects for contributors.",
        "authors": [
            "Hana Frluckaj",
            "Nikki Stevens",
            "James Howison",
            "Laura Dabbish"
        ],
        "journal_conference_name": "Proceedings of the ACM on Human-Computer Interaction",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157851",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Local Search Heuristic for the Two-Echelon Capacitated Vehicle Routing Problem in Educational Decision Support Systems",
        "abstract": "This study focuses on developing a heuristic for Decision Support Systems (DSS) in e-commerce logistics education, specifically addressing the Two-Echelon Capacitated Vehicle Routing Problem (2E-CVRP). The 2E-CVRP involves using Urban Transshipment Points (UTPs) to optimize deliveries. To tackle the complexity of the 2E-CVRP, DSS can employ fast and effective techniques for visual problem-solving. Therefore, the objective of this work is to develop a local search heuristic to solve the 2E-CVRP quickly and efficiently for implementation in DSS. The efficiency of the heuristic is assessed through benchmarks from the literature and applied to real-world problems from a Brazilian e-commerce retailer, contributing to advancements in the 2E-CVRP approach and promoting operational efficiency in e-commerce logistics education. The heuristic yielded promising results, solving problems almost instantly, for instances in the literature on average in 1.06 s, with average gaps of 6.3% in relation to the best-known solutions and, for real problems with hundreds of customers, in 1.4 s, with gaps of 8.3%, demonstrating its effectiveness in achieving the study’s objectives.",
        "authors": [
            "José Pedro Gomes da Cruz",
            "Matthias Winkenbach",
            "Hugo Tsugunobu Yoshida Yoshizaki"
        ],
        "journal_conference_name": "Algorithms",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157686",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Materials beyond monolayers: The magnetic quasi-1D semiconductor CrSBr",
        "abstract": "The all-surface nature of atomically thin van der Waals materials can present challenges for practical applications. Fortunately, new layered materials are on the horizon that preserve their useful properties even when thicker than a monolayer. Here, we summarize our interest in one of these emergent materials, the magnetic semiconductor CrSBr. We describe monolayer properties exhibited by this material in its bulk form, discussing how the quasi-1D electronic structure of CrSBr allows mono- or bilayer physics to be displayed even in thick crystals. Long-range magnetic order offers additional tuning with the coupled lattice, spin, orbit, and charge degrees of freedom enabling magneto-correlated phenomena. We discuss the stability of CrSBr in air and show atomic scale structural manipulation through electron beam-driven transformations. We conclude that the stability and structural amenability of CrSBr provide opportunities for imagining devices that use bulk crystals yet exploit unique magnetic and quantum confinement effects.",
        "authors": [
            "Julian Klein",
            "Frances M. Ross"
        ],
        "journal_conference_name": "Journal of Materials Research",
        "publisher": "Springer International Publishing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157536",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Topological recursion for hyperbolic string field theory",
        "abstract": "We derive an analog of Mirzakhani’s recursion relation for hyperbolic string vertices and investigate its implications for closed string field theory. Central to our construction are systolic volumes: the Weil-Petersson volumes of regions in moduli spaces of Riemann surfaces whose elements have systoles L ≥ 0. These volumes can be shown to satisfy a recursion relation through a modification of Mirzakhani’s recursion as long as L ≤ 2 sinh−1 1. Applying the pants decomposition of Riemann surfaces to off-shell string amplitudes, we promote this recursion to hyperbolic string field theory and demonstrate the higher order vertices are determined by the cubic vertex iteratively for any background. Such structure implies the solutions of closed string field theory obey a quadratic integral equation. We illustrate the utility of our approach in an example of a stubbed scalar theory.",
        "authors": [
            "Atakan H. Fırat",
            "Nico Valdes-Meller"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157529",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "What the visual system can learn from the non-dominant hand: The effect of graphomotor engagement on visual discrimination",
        "abstract": "Previous studies have demonstrated that engaging in graphomotor activity for creating graphemes can enhance their subsequent visual discrimination. This suggests a positive influence of the motor system on visual learning. However, existing studies have emphasized the dominant hand, which is superiorly dexterous in fine-motor movements. This near-exclusive focus prompts the inquiry of whether the observed perceptual facilitation is a general characteristic of the motor system, or specific to pathways controlling the skilled over-trained dominant hand. Furthermore, the mechanistic underpinning of visual facilitation from graphomotor training (i.e., the individual contribution of motor activity, temporal evolution of the visual trace, variability of visual output) remain unclear. To address these questions, we assessed visual discrimination capabilities of healthy right-handed participants (N = 60) before and after graphomotor or visual training. Contrary to our initial expectation, graphomotor engagement with the non-dominant hand did not yield additional benefits to visual learning beyond those attainable through visual training alone. Moreover, graphomotor training with the non-dominant hand resulted in visual discrimination improvements comparable to those of dominant hand training, despite the inherent differences between hands in motor performance and in the amount of improvement in shape tracing throughout training. We conclude that the motor components of graphomotor activity may not be critical for visual learning of shapes through tracing activity. Instead, our results are in agreement with the symbolic theoretical account, suggesting that basic shape features required for discrimination can be acquired through visual inspection alone, providing a perspective on the improvements observed in prior studies.",
        "authors": [
            "Shlomit Ben-Ami",
            "Batel Buaron",
            "Ori Yaron",
            "Kyle Keane",
            "Virginia H. Sun",
            "Flip Phillips",
            "Jason Friedman",
            "Pawan Sinha",
            "Roy Mukamel"
        ],
        "journal_conference_name": "Memory & Cognition",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157534",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Understanding Non-Verbal Irony Markers: Machine Learning Insights Versus Human Judgment",
        "abstract": "rony detection is a complex task that often stumps both humans, who frequently misinterpret ironic statements, and artificial intelligence (AI) systems. While the majority of AI research on irony detection has concentrated on linguistic cues, the role of non-verbal cues like facial expressions and auditory signals has been largely overlooked. This paper investigates the effectiveness of machine learning models in recognizing irony using solely non-verbal cues. To this end, we conducted the following experiments and analysis: (i) we trained and evaluated some machine-learning models to detect irony; (ii) we compared the results with human interpretations; and (iii) we analysed and identified multi-modal non-verbal irony markers. Our research demonstrates that machine learning models trained on nonverbal data have shown significant promise in detecting irony, outperforming human judgments in this task. Specifically, we found that certain facial action units and acoustic characteristics of speech are key indicators of irony expression. These non-verbal cues, often overlooked in traditional irony detection methods, were effectively identified by machine learning models, leading to improved accuracy in detecting irony.",
        "authors": [
            "Micol Spitale",
            "Fabio Catania",
            "Francesca Panzeri"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157794",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Modular Verification of Secure and Leakage-Free Systems: From Application Specification to Circuit-Level Implementation",
        "abstract": "Parfait is a framework for proving that an implementation of a hardware security module (HSM) leaks nothing more than what is mandated by an application specification. Parfait proofs cover the software and the hardware of an HSM, which catches bugs above the cycle-level digital circuit abstraction, including timing side channels. Parfait's contribution is a scalable approach to proving security and non-leakage by using intermediate levels of abstraction and relating them with transitive information-preserving refinement. This enables Parfait to use different techniques to verify the implementation at different levels of abstraction, reuse existing verified components such as CompCert, and automate parts of the proof, while still providing end-to-end guarantees. We use Parfait to verify four HSMs, including an ECDSA certificate-signing HSM and a password-hashing HSM, on top of the OpenTitan Ibex and PicoRV32 processors. Parfait provides strong guarantees for these HSMs: for instance, it proves that the ECDSA-on-Ibex HSM implementation---2,300 lines of code and 13,500 lines of Verilog---leaks nothing more than what is allowed by a 40-line specification of its behavior.",
        "authors": [
            "Anish Athalye",
            "Henry Corrigan-Gibbs",
            "Frans Kaashoek",
            "Joseph Tassarotti",
            "Nickolai Zeldovich"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|ACM SIGOPS 30th Symposium on Operating Systems Principles",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157857",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Multiplicative resonant enhancement of chemical detection",
        "abstract": "Optical resonances can increase the sensitivity of measurements to material perturbations and also accelerate photochemical reactions. Here, we show that these two effects can be combined multiplicatively, to enhance the detection via weak or low-concentration photochemical reactions far beyond what could previously be attained. For an optical resonance with quality factor 𝑄, the sensitivity of our detection scheme is enhanced by ∼𝑄2 (where ∼ denotes approximate proportionality), as demonstrated by both theoretical arguments and numerical simulations of a simple optical-grating resonance coupled with reaction-diffusion equations. Such an approach opens a door to further improvements by careful design of the resonance: even a three-parameter optimization of the grating resonance yields an additional ≈7 times improvement.",
        "authors": [
            "Wenchao Ma",
            "Raphaël Pestourie",
            "Zin Lin",
            "Alan Aguirre-Soto",
            "Hadley D. Sikes",
            "Steven G. Johnson"
        ],
        "journal_conference_name": "Physical Review Applied",
        "publisher": "American Physical Society",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157510",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Unifying serverless and microservice workloads with SigmaOS",
        "abstract": "Many cloud applications use both serverless functions, for bursts of stateless parallel computation, and container orchestration, for long-running microservices and tasks that need to interact. Ideally a single platform would offer the union of these systems' capabilities, but neither is sufficient to act as that single platform: serverless functions are lightweight but cannot act as servers with long-term state, while container orchestration offers general-purpose computation but instance start-up takes too long to support burst parallelism.\r\nσOS is a new multi-tenant cloud operating system that combines the best of container orchestration and serverless in one platform with one API. σOS computations, called procs, can be long-running, stateful, and interact with each other, making them a good match for both serverless and microservice tasks. A key aspect of the σOS design is its cloud-centric API, which provides flexible management of computation, a novel abstraction for communication endpoints, σEPs---which allow procs of a tenant to communicate efficiently but prohibits procs from sending packets to other tenants---and a flexible naming system to name, for example, σEPs.\r\nQuick proc start-up is important for serverless uses. A key enabling observation is that both serverless and microservice applications rely on cloud services for much of the work traditionally done by the local OS (e.g., access to durable storage and additional compute resources). σOS exploits this observation by providing only a small and generic local operating system image to each proc, which can be created much more quickly than a container orchestration instance since σOS need not install application-specific filesystem content or (due to σOS's σEPs) configure an isolated overlay network.\r\nMicrobenchmarks show that σOS can cold start a proc in 7.7 msec and can create 36,650 procs per second, distributing them over a 24-machine cluster. An evaluation of σOS with two microservice applications from DeathStarBench, a MapReduce application, and an image processing benchmark, shows that the σOS API supports both microservices and lambda-style computations, and provides better performance than corresponding versions on AWS Lambda and Kubernetes.",
        "authors": [
            "Ariel Szekely",
            "Adam Belay",
            "Robert Morris",
            "M. Frans Kaashoek"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|ACM SIGOPS 30th Symposium on Operating Systems Principles",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157856",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Urban site characterization using DAS dark fibers on the MIT campus in Cambridge, Massachusetts",
        "abstract": "Telecommunication dark fibers with distributed acoustic sensing (DAS) are a useful survey tool for site characterization in urban environments. In this paper, we introduce our five-day student-led DAS experiment using dark fibers at the Massachusetts Institute of Technology campus in the city of Cambridge. The campus has been identified as an area that is highly susceptible to seismic hazards due to subsurface structure and soil properties. The experiment included survey planning, data acquisition, data analysis, subsurface characterization, and site-response estimations. Rayleigh waves collected by dark fibers in the urban environment are mostly from human activities and contain abundant higher-mode energies. We invert the phase velocity dispersions to resolve the shear-wave velocity (VS) in the top 120 m of the subsurface. The VS profiles show low VS (0.1–0.3 km/s) corresponding to unconsolidated materials such as artificial fills and clays overlying a hard bedrock (1.5–1.8 km/s). The depth to bedrock is 75–95 m on the west campus. The site near the waterfront has a lower VS and deeper bedrock. The 1D site-response modeling for shear waves suggests that the fundamental resonance frequency is at 0.6 and 1 Hz, with a sediment-to-bedrock amplitude ratio of 6–7. This should be considered in building design to mitigate seismic hazards. Our results agree with previous studies and can bridge the gap between measurements at nearby sites.",
        "authors": [
            "Hilary Chang",
            "Nori Nakata"
        ],
        "journal_conference_name": "The Leading Edge",
        "publisher": "Society of Exploration Geophysicists",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157498",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Real-time estimation of bound water concentration during lyophilization with temperature-based state observers",
        "abstract": "Lyophilization (aka freeze drying) has been shown to provide long-term stability for many crucial biotherapeutics, e.g., mRNA vaccines for COVID-19, allowing for higher storage temperature. The final stage of lyophilization, namely secondary drying, entails bound water removal via desorption, in which accurate prediction of bound water concentration is vital to ensuring the quality of the lyophilized product. This article proposes a novel technique for real-time estimation of the bound water concentration during secondary drying in lyophilization. A state observer is employed, which combines temperature measurement and mechanistic understanding of heat transfer and desorption kinetics, without requiring any online concentration measurement. Results from both simulations and experimental data show that the observer can accurately estimate the concentration of bound water in real time for all possible concentration levels, operating conditions, and measurement noise. This framework can also be applied for monitoring and control of the residual moisture in other desorption-related processes.",
        "authors": [
            "Prakitr Srisuma",
            "George Barbastathis",
            "Richard D Braatz"
        ],
        "journal_conference_name": "International Journal of Pharmaceutics",
        "publisher": "Elsevier BV",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157660",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Inhibitory Potential of the Truncated Isoforms on Glutamate Transporter Oligomerization Identified by Computational Analysis of Gene-Centric Isoform Maps",
        "abstract": "Objective Glutamate transporters play a key role in central nervous system physiology by maintaining excitatory neurotransmitter homeostasis. Biological assemblies of the transporters, consisting of cyclic homotrimers, emerge as a crucial aspect of glutamate transporter modulation. Hence targeting heteromerization promises an effective approach for modulator design. On the other hand, the dynamic nature of transcription allows for the generation of transporter isoforms in structurally distinct manners. Methods The potential isoforms were identified through the analysis of computationally generated gene-centric isoform maps. The conserved features of isoform sequences were revealed by computational chemistry methods and subsequent structural analysis of AlphaFold2 predictions. Truncated isoforms were further subjected to a wide range of docking analyses, 50ns molecular dynamics simulations, and evolutionary coupling analyses. Results Energetic landscapes of isoform-canonical transporter complexes suggested an inhibitory potential of truncated isoforms on glutamate transporter bio-assembly. Moreover, isoforms that mimic the trimerization domain (in particular, TM2 helices) exhibited stronger interactions with canonical transporters, underscoring the role of transmembrane helices in isoform interactions. Additionally, self-assembly dynamics observed in truncated isoforms mimicking canonical TM5 helices indicate a potential protective role against unwanted interactions with canonical transporters. Conclusion Our computational studies on glutamate transporters offer insights into the roles of alternative splicing on protein interactions and identifies potential drug targets for physiological or pathological processes.",
        "authors": [
            "Alper Karagöl",
            "Taner Karagöl",
            "Mengke Li",
            "Shuguang Zhang"
        ],
        "journal_conference_name": "Pharmaceutical Research",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157515",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Sitetack: a deep learning model that improves PTM prediction by using known PTMs",
        "abstract": "Motivation\r\nPost-translational modifications (PTMs) increase the diversity of the proteome and are vital to organismal life and therapeutic strategies. Deep learning has been used to predict PTM locations. Still, limitations in datasets and their analyses compromise success.\r\n\r\nResults\r\nWe evaluated the use of known PTM sites in prediction via sequence-based deep learning algorithms. For each PTM, known locations of that PTM were encoded as a separate amino acid before sequences were encoded via word embedding and passed into a convolutional neural network that predicts the probability of that PTM at a given site. Without labeling known PTMs, our models are on par with others. With labeling, however, we improved significantly upon extant models. Moreover, knowing PTM locations can increase the predictability of a different PTM. Our findings highlight the importance of PTMs for the installation of additional PTMs. We anticipate that including known PTM locations will enhance the performance of other proteomic machine learning algorithms.\r\n\r\nAvailability and implementation\r\nSitetack is available as a web tool at https://sitetack.net; the source code, representative datasets, instructions for local use, and select models are available at https://github.com/clair-gutierrez/sitetack.",
        "authors": [
            "Clair S Gutierrez",
            "Alia A Kassim",
            "Benjamin D Gutierrez",
            "Ronald T Raines"
        ],
        "journal_conference_name": "Bioinformatics",
        "publisher": "Oxford University Press",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158166",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A more flexible design for MDSplus Device drivers",
        "abstract": "The traditional approach to building MDSplus Device drivers is rigid and lacks the ability to meet changing needs. We introduce a novel paradigm for Device driver development that allows the tree structure to dynamically change. This allows device drivers that can reconfigure to automatically reflect the hardware it represents, or a device that implements a variable number of queries to an external database. We have created a driver using this paradigm that communicates with a digitizer, queries the modules attached, and builds a MDSplus tree structure to utilize them. Additionally, this driver can reconfigure to match changes in the digitizer, by adding or deleting nodes using overwrite and/or delete modes. We also wrote a method for verifying both the setting provided and that the hardware matches the last known state. We have added fields to help validate settings such as min/max limits, and a list of allowed values. The definitions of the nodes which make of the device have been augmented to include help, tool tips and validation ranges. This will facilitate automated user interface generation.",
        "authors": [
            "Fernando Santoro",
            "Stephen Lane-Walsh",
            "Joshua Stiller",
            "Mark Winkel"
        ],
        "journal_conference_name": "Fusion Engineering and Design",
        "publisher": "Elsevier",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158538",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Lincoln Scholars and Military Fellows Programs Foster Collaboration and Research to Prepare for the Future",
        "abstract": "",
        "authors": [
            "Rachel Ornitz"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157457",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploring the Mysterious Alphabet of Sperm Whales",
        "abstract": "MIT CSAIL and Project CETI researchers reveal complex communication patterns in sperm whales, deepening our\r\nunderstanding of animal language systems.",
        "authors": [
            "Rachel Gordon"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157452",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Measurement of the B s 0 → J / ψK S 0 effective lifetime from proton-proton collisions at s = 13 TeV",
        "abstract": "The effective lifetime of the B s 0 meson in the decay B s 0 → J / ψK S 0 is measured using data collected during 2016–2018 with the CMS detector in s = 13 TeV proton-proton collisions at the LHC, corresponding to an integrated luminosity of 140 fb−1. The effective lifetime is determined by performing a two-dimensional unbinned maximum likelihood fit to the B s 0 meson invariant mass and proper decay time distributions. The resulting value of 1.59 ± 0.07(stat) ± 0.03(syst) ps is the most precise measurement to date and is in good agreement with the expected value.",
        "authors": [
            "A. Hayrapetyan",
            "A. Tumasyan",
            "W. Adam",
            "J. W. Andrejkovic",
            "T. Bergauer",
            "S. Chatterjee",
            "K. Damanakis",
            "M. Dragicevic",
            "P. S. Hussain",
            "M. Jeitler",
            "N. Krammer",
            "A. Li",
            "D. Liko",
            "I. Mikulec",
            "J. Schieck",
            "R. Schöfbeck",
            "D. Schwarz"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157561",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Microfluidic Hanging Droplet as a Programmable Platform for Mammalian Egg Vitrification",
        "abstract": "Egg (oocyte) vitrification is the dominant method for preserving fertility for women of reproductive age. However, the method is typically performed by hand, requiring precise (∼0.1 to 10 μL) and time-sensitive (∼1 s) liquid exchange of cryoprotectants (CPA) around eggs as well as fine handling of eggs (∼100 μm) for immersion into liquid nitrogen (LN2). Here, we developed a microfluidic platform for programmable vitrification. Our platform is based on a millimeter-sized hanging droplet inside which a given egg is suspended and subjected to liquid exchanges within seconds. After programmable exposures to CPA, the egg is extracted from the liquid–air interface of the droplet using a motorized fine-tip instrument and immersed into LN2 for vitrification. To benchmark our platform with the manual method, we vitrified over a hundred mouse eggs and found comparable percentages (∼95%) for post-vitrification survivability. In addition, our platform performs real-time microscopy of the egg thereby enabling future studies where its morphology may be linked to functional outcomes. Our study contributes to the ongoing efforts to enhance the automation of embryology techniques towards broader applications in reproductive medicine both for clinical and research purposes.",
        "authors": [
            "Haidong Feng",
            "Georgios Katsikis",
            "India Napier",
            "Gong Du",
            "Josh Lim",
            "Joseph Doyle",
            "Scott R Manalis",
            "Linda G Griffith"
        ],
        "journal_conference_name": "Lab on a Chip",
        "publisher": "Royal Society of Chemistry",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157523",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Learning from and about scientists: Consensus messaging shapes perceptions of climate change and climate scientists",
        "abstract": "Despite overwhelming scientific consensus on the existence of human-caused climate change, public opinion among Americans remains split. Directly informing people of scientific consensus is among the most prominent strategies for climate communication, yet the reasons for its effectiveness and its limitations are not fully understood. Here, we propose that consensus messaging provides information not only about the existence of climate change but also traits of climate scientists themselves. In a large (n=2,545) nationally representative survey experiment, we examine how consensus information affects belief in human-caused climate change by shaping perceptions of climate scientist credibility. In the control group (n=847), we first show that people learn both from and about climate scientists when presented with consensus and that perceived scientist credibility (especially skill) mediates up to about 40% of the total effect of consensus information on climate belief. We demonstrate that perceptions of climate scientists are malleable with two novel interventions that increase belief in climate change above and beyond consensus information.",
        "authors": [
            "Reed Orchinik",
            "Rachit Dubey",
            "Samuel J Gershman",
            "Derek M Powell",
            "Rahul Bhui"
        ],
        "journal_conference_name": "PNAS Nexus",
        "publisher": "Oxford University Press",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158172",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Telepresence Robots in the Context of Dementia Caregiving: Caregivers’ and Care Recipients’ Perspectives",
        "abstract": "As a result of a rapidly aging population and the increasing prevalence of dementia among older adults, technological solutions are increasingly being considered to facilitate caregiving. This research investigates the perspectives of 20 caregiving dyads on VGo, a telepresence social robot with features designed to support caregiving. Care recipients (CRs), aged 65 and older, diagnosed with Alzheimer’s disease and related dementias, along with their primary caregivers (CGs), evaluated the robot through an online interview study. The interviews integrated informative videos showcasing VGo’s features and functions. Insights from the interviews revealed diverse expectations, interests, and reservations. The majority of CGs and their CRs perceived the robot’s features as beneficial. In particular, the voice command capability was appreciated as an alternative to using smartphones and as a way to manage home appliances. The community feature, however, did not align well with many participants’ lifestyles, and participants had a number of suggestions to enhance the robot’s notification function. Based on the interview results, the study offers a set of design recommendations for telepresence social robots in home caregiving contexts. This investigation highlights the promise of social robots in caregiving contexts and underscores the need for further improvements to ensure they fit users’ needs.",
        "authors": [
            "Shabnam FakhrHosseini",
            "Lauren Cerino",
            "Lisa D’Ambrosio",
            "Lexi Balmuth",
            "Chaiwoo Lee",
            "Mengke Wu",
            "Joseph Coughlin"
        ],
        "journal_conference_name": "Robotics",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157684",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Health-related quality of life dynamics: modeling insights from immunotherapy",
        "abstract": "Understanding how treatments affect patients’ quality of life over time is crucial, but capturing the complex interactions of health factors poses a challenge for clinical and observational research. To overcome this, we have turned to simulation modeling, a method that allows for a more thorough exploration of these dynamics. Our study focuses on cancer immunotherapy, a treatment that, despite its potential to prolong survival, also comes with life-threatening risks. We evaluated the effectiveness of two strategies aimed at improving quality of life: reducing the time to treatment infusion and enhancing social support. These strategies were assessed across three different patient scenarios: those not initially eligible for treatment, patients experiencing a relapse, and patients showing a complete response. By using simulation modeling, we demonstrated how this approach can help explore the dynamics and interactions of various health factors and the impact of specific strategies.",
        "authors": [
            "Zeynep Hasgul",
            "Anne Spanjaart",
            "Sumreen Javed",
            "Ali Akhavan",
            "Marie J. Kersten",
            "Mohammad S. Jalali"
        ],
        "journal_conference_name": "Quality of Life Research",
        "publisher": "Springer International Publishing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157516",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Examining the adoption of electromobility concepts across social contexts for energy transition",
        "abstract": "The impact of mobility decisions not only shapes urban traffic patterns and planning, but also its associated effects, such as greenhouse gas (GHG) emissions. Although e-bike sharing is not a new concept, it has shown significant strides in technological progress in recent years due to the ongoing process of digitalization, specifically towards decarbonization effects. Past studies have shown that e-bike sharing shows a potential as a fast, mobile, and environmentally friendly alternative to cars and public transport. Although e-bikes represent a viable alternative to traditional means of transportation, there is a lack of quantification in understanding the impact and acceptance of e-bikes towards social contexts as well as its adoption as a type of sharing concept. In this paper, we employ the Unified Theory of Acceptance and Use of Technology (UTAUT) model as an analytical framework to discern the use and acceptance of e-bike sharing as an emerging technological concept across different cities and social contexts. Our findings reveal that the e-bike sharing system's utilization is skewed towards a small percentage of \"frequent users\", and overall usage is biased towards younger, more-educated, and higher-income populations who live in bike-friendly areas. Our work contributes to the feasibility of embedding the e-bike sharing concept in the scope of the energy transition.",
        "authors": [
            "Julia K?hlke",
            "Adam Lechowicz",
            "Oluwole Fabikun",
            "Noman Bashir",
            "Abel Souza",
            "Prashant Shenoy",
            "Sebastian Lehnhoff"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 11th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157618",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On the non-perturbative bulk Hilbert space of JT gravity",
        "abstract": "What is the bulk Hilbert space of quantum gravity? In this paper, we resolve this problem in 2d JT gravity, both with and without matter, providing an explicit definition of a non-perturbative Hilbert space specified in terms of metric variables. The states are wavefunctions of the length and matter state, but with a non-trivial and highly degenerate inner product. We explicitly identify the null states, and discuss their importance for defining operators non-perturbatively. To highlight the power of the formalism we developed, we study the non-perturbative effects for two bulk linear operators that may serve as proxies for the experience of an observer falling into a two-sided black hole: one captures the length of an Einstein-Rosen bridge and the other captures the center-of-mass collision energy between two particles falling from opposite sides. We track the behavior of these operators up to times of order e S BH , at which point the wavefunction spreads to the complete set of eigenstates of these operators. If these observables are indeed good proxies for the experience of an infalling observer, our results indicate an O(1) probability of detecting a firewall at late times that is self-averaging and universal.",
        "authors": [
            "Luca V. Iliesiu",
            "Adam Levine",
            "Henry W. Lin",
            "Henry Maxfield",
            "Márk Mezei"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157517",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Targeted hematopoietic stem cell depletion through SCF-blockade",
        "abstract": "Abstract Background Hematopoietic stem cell transplantation (HSCT) is a curative treatment for many diverse blood and immune diseases. However, HSCT regimens currently commonly utilize genotoxic chemotherapy and/or total body irradiation (TBI) conditioning which causes significant morbidity and mortality through inducing broad tissue damage triggering infections, graft vs. host disease, infertility, and secondary cancers. We previously demonstrated that targeted monoclonal antibody (mAb)-based HSC depletion with anti(α)-CD117 mAbs could be an effective alternative conditioning approach for HSCT without toxicity in severe combined immunodeficiency (SCID) mouse models, which has prompted parallel clinical αCD117 mAbs to be developed and tested as conditioning agents in clinical trials starting with treatment of patients with SCID. Subsequent efforts have built upon this work to develop various combination approaches, though none are optimal and how any of these mAbs fully function is unknown. Methods To improve efficacy of mAb-based conditioning as a stand-alone conditioning approach for all HSCT settings, it is critical to understand the mechanistic action of αCD117 mAbs on HSCs. Here, we compare the antagonistic properties of αCD117 mAb clones including ACK2, 2B8, and 3C11 as well as ACK2 fragments in vitro and in vivo in both SCID and wildtype (WT) mouse models. Further, to augment efficacy, combination regimens were also explored. Results We confirm that only ACK2 inhibits SCF binding fully and prevents HSC proliferation in vitro. Further, we verify that this corresponds to HSC depletion in vivo and donor engraftment post HSCT in SCID mice. We also show that SCF-blocking αCD117 mAb fragment derivatives retain similar HSC depletion capacity with enhanced engraftment post HSCT in SCID settings, but only full αCD117 mAb ACK2 in combination with αCD47 mAb enables enhanced donor HSC engraftment in WT settings, highlighting that the Fc region is not required for single-agent efficacy in SCID settings but is required in immunocompetent settings. This combination was the only non-genotoxic conditioning approach that enabled robust donor engraftment post HSCT in WT mice. Conclusion These findings shed new insights into the mechanism of αCD117 mAb-mediated HSC depletion. Further, they highlight multiple approaches for efficacy in SCID settings and optimal combinations for WT settings. This work is likely to aid in the development of clinical non-genotoxic HSCT conditioning approaches that could benefit millions of people world-wide.",
        "authors": [
            "Yan Y. Chan",
            "Pui Y. Ho",
            "Carla Dib",
            "Leah Swartzrock",
            "Maire Rayburn",
            "Hana Willner",
            "Ethan Ko",
            "Katie Ho",
            "Julian D. Down",
            "Adam C. Wilkinson",
            "Hiro Nakauchi",
            "Morgane Denis",
            "Taylor Cool",
            "Agnieszka Czechowicz"
        ],
        "journal_conference_name": "Stem Cell Research & Therapy",
        "publisher": "BioMed Central",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157518",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "SAUC: Sparsity-Aware Uncertainty Calibration for Spatiotemporal Prediction with Graph Neural Networks",
        "abstract": "Quantifying uncertainty is crucial for robust and reliable predictions. However, existing spatiotemporal deep learning mostly focuses on deterministic prediction, overlooking the inherent uncertainty in such prediction. Particularly, highly-granular spatiotemporal datasets are often sparse, posing extra challenges in prediction and uncertainty quantification. To address these issues, this paper introduces a novel post-hoc Sparsity-aware Uncertainty Calibration (SAUC) framework, which calibrates uncertainty in both zero and non-zero values. To develop SAUC, we firstly modify the state-of-the-art deterministic spatiotemporal Graph Neural Networks (ST-GNNs) to probabilistic ones in the pre-calibration phase. Then we calibrate the probabilistic ST-GNNs for zero and non-zero values using quantile approaches. Through extensive experiments, we demonstrate that SAUC can effectively fit the variance of sparse data and generalize across two real-world spatiotemporal datasets at various granularities. Specifically, our empirical experiments show a 20% reduction in calibration errors in zero entries on the sparse traffic accident and urban crime prediction. Overall, this work demonstrates the theoretical and empirical values of the SAUC framework, thus bridging a significant gap between uncertainty quantification and spatiotemporal prediction.",
        "authors": [
            "Dingyi Zhuang",
            "Yuheng Bu",
            "Guang Wang",
            "Shenhao Wang",
            "Jinhua Zhao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 32nd ACM International Conference on Advances in Geographic Information Systems",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157749",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Mind the Hazard: Modeling and Interpreting Comfort with Personalized Sensing",
        "abstract": "Recent advances in personalized sensing and comfort feedback have spurred the development of data-driven comfort models tailored to individual needs. However, because current models treat sequential comfort feedback independently, they are subject to unstable predictions and limited interpretability, hindering their deployment in building management. This study introduces a dynamic modeling framework that utilizes a Neural Ordinary Differential Equations-based Continuous-time Markov Chain to model the transitions in comfort states over time. Our modeling approach, developed through a field study utilizing smart glasses and mobile app feedback, tracks occupants' comfort transitions across daily activities and contexts. The results demonstrate that this model not only predicts comfort states more accurately and stably than conventional classification models but also uniquely provides a representation of how the hazards of state transitions are influenced by changing ambient and contextual conditions. This approach, therefore, offers a new perspective on personalized building control, where predictions of comfort transition hazards can preemptively suggest building management interventions to avoid occupants experiencing discomfort. In addition, insights into how environmental and contextual characteristics relate to these hazards can guide holistic management strategies that dynamically balance comfort with energy targets in response to the occupants' activities and contexts.",
        "authors": [
            "Yufei Zhang",
            "Matteo Favero",
            "Patrick Chwalek",
            "Sailin Zhong",
            "Denis Lalanne",
            "Joseph A. Paradiso",
            "Clayton Miller",
            "Andrew Sonta"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 11th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157619",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Measurement of diferential ZZ + jets production cross sections in pp collisions at √s = 13 TeV",
        "abstract": "Diboson production in association with jets is studied in the fully leptonic final states, pp → (Z/γ*)(Z/γ*) + jets → 2ℓ2ℓ′ + jets, (ℓ, ℓ′ = e or μ) in proton-proton collisions at a center-of-mass energy of 13 TeV. The data sample corresponds to an integrated luminosity of 138 fb−1 collected with the CMS detector at the LHC. Differential distributions and normalized differential cross sections are measured as a function of jet multiplicity, transverse momentum pT, pseudorapidity η, invariant mass and ∆η of the highest-pT and second-highest-pT jets, and as a function of invariant mass of the four-lepton system for events with various jet multiplicities. These differential cross sections are compared with theoretical predictions that mostly agree with the experimental data. However, in a few regions we observe discrepancies between the predicted and measured values. Further improvement of the predictions is required to describe the ZZ+jets production in the whole phase space.",
        "authors": [
            "A. Hayrapetyan",
            "A. Tumasyan",
            "W. Adam",
            "J. W. Andrejkovic",
            "T. Bergauer",
            "S. Chatterjee",
            "K. Damanakis",
            "M. Dragicevic",
            "A. Escalante Del Valle",
            "P. S. Hussain",
            "M. Jeitler",
            "N. Krammer",
            "D. Liko",
            "I. Mikulec",
            "J. Schieck",
            "R. Schöfbeck",
            "D. Schwarz",
            "M. Sonawane"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157528",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "From Transparency to Accountability and Back",
        "abstract": "Artificial intelligence (AI) is increasingly intervening in our lives, raising widespread concern about its unintended and undeclared side effects. These developments have brought attention to the problem of AI auditing: the systematic evaluation and analysis of an AI system, its development, and its behavior relative to a set of predetermined criteria. Auditing can take many forms, including pre-deployment risk assessments, ongoing monitoring, and compliance testing. It plays a critical role in providing assurances to various AI stakeholders, from developers to end users. Audits may, for instance, be used to verify that an algorithm complies with the law, is consistent with industry standards, and meets the developer’s claimed specifications. However, AI developers and companies will rarely grant auditors unfettered access to their systems.\r\nIn this work, we examine a key consideration in AI auditing: what type of access to an AI system is needed to perform a meaningful audit? Addressing this question has direct policy relevance, as it can inform AI audit guidelines and requirements. We begin by discussing the factors that auditors balance when determining the appropriate type of access, and unpack the benefits and drawbacks of four types of access. We conclude that, at minimum, black-box access—providing query access to a model without exposing its internal implementation—should be granted to auditors. In particular, we argue that black-box access effectively balances concerns related to proprietary technology, data privacy, audit standardization, and audit efficiency. We then suggest a framework for determining how much further access (on top of black-box access) to provide to auditors. We show that auditing can be cast as a natural hypothesis test and argue that this framing provides clear and interpretable guidance on the implementation of AI audits. In particular, we draw parallels between aspects of hypothesis testing and those of legal procedure, such as legal presumption and burden of proof. As a result, hypothesis testing provides an approach to AI auditing that is both interpretable and effective, offering a potential path forward despite the challenges posed by AI’s opacity.",
        "authors": [
            "Sarah Cen",
            "Rohan Alur"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Equity and Access in Algorithms, Mechanisms, and Optimization",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157655",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Pico-Scale Science for Pedestrian-Scale Solutions (PSS4PSS): A Computational Toolbox Leveraging Molecular Simulation for Pedestrian Dynamics",
        "abstract": "Efficient and accurate simulations of pedestrian dynamics are critical for the smart cities of the future. In this work, we present a computational toolbox that accelerates such simulations relative to a popularly used pedestrian simulation tool by leveraging computational frameworks initially developed for molecular simulation. We make the argument that the field of pedestrian dynamics could benefit to a significant extent from a serendipitous interdisciplinary synergy with the molecular-simulation community. We provide arguments and representative examples in support of this premise, demonstrating that molecular simulation tools can be repurposed to solve precisely the same governing equations as traditional pedestrian-dynamics simulation tools, yielding the same results in significantly reduced computational time. We also describe a computational tool that we have developed that streamlines the conversion of indoor maps into boundary conditions for pedestrian simulations.",
        "authors": [
            "Samuel Chen",
            "Emerson Collins",
            "Vincent Cheng",
            "Kelby Kramer",
            "Gerald Wang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 11th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157615",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Graph Deep Learning Model for Station Ridership Prediction in Expanding Metro Networks",
        "abstract": "Due to their reliability, efficiency, and environmental friendliness, metro systems have become a crucial solution to transportation challenges associated with urbanization. Many countries have constructed or expanded their metro networks over the past decades. During the planning stage, accurately predicting station ridership post-expansion, particularly for new stations, is essential to enhance the effectiveness of infrastructure investments. However, station-level metro ridership prediction under expansion scenarios (MRP-E) has not been thoroughly explored, as most advanced models currently focus on short-term predictions. MRP-E presents significant challenges due to the absence of historical data for newly built stations and the dynamic, complex spatiotemporal relationships between stations during expansion phases. In this study, we propose a Metro-specific Multi-Graph Attention Network model (Metro-MGAT) to address these issues. Our model leverages multi-sourced urban context data and network topology information to generate station features. Multi-relation graphs are constructed to capture the spatial correlations between stations, and an attention mechanism is employed to facilitate graph encoding. The model has been evaluated through realistic experiments using multi-year metro ridership data from Shanghai, China. The results validate the superior performance of our approach compared to existing methods, particularly in predicting ridership at new stations.",
        "authors": [
            "Fangyi Ding",
            "Yuebing Liang",
            "Yamin Wang",
            "Yan Tang",
            "Yang Zhou",
            "Zhan Zhao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|2nd ACM SIGSPATIAL International Workshop on Advances in Urban-AI",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157842",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Improvements to Quantum Interior Point Method for Linear Optimization",
        "abstract": "Quantum linear system algorithms (QLSA) have the potential to speed up Interior Point Methods (IPM). However, a major bottleneck is the inexactness of quantum Tomography to extract classical solutions from quantum states. In addition, QLSAs are sensitive to the condition number, and this sensitivity is exacerbated when the Newton systems arising in IPMs converge to a singular matrix. Recently, an Inexact Feasible Quantum IPM (IF-QIPM) has been developed that addresses the inexactness of QLSAs. However, this method requires a large number of gates and qubits to be implemented. Here, we propose a new IF-QIPM using the normal equation system, which requires less number of gates and qubits. To mitigate the sensitivity to the condition number and other input data-related parameters, we use preconditioning coupled with iterative refinement to obtain better complexity. Finally, we demonstrate the effectiveness of our approach on IBM Qiskit simulators.",
        "authors": [
            "Mohammadhossein Mohammadisiahroudi",
            "Zeguan Wu",
            "Brandon Augustino",
            "Arielle Carr",
            "Tam?s Terlaky"
        ],
        "journal_conference_name": "ACM Transactions on Quantum Computing",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157545",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Cooperation and Fairness in Multi-Agent Reinforcement Learning",
        "abstract": "Multi-agent systems are trained to maximize shared cost objectives, which typically reflect system-level efficiency. However, in the resource-constrained environments of mobility and transportation systems, efficiency may be achieved at the expense of fairness --- certain agents may incur significantly greater costs or lower rewards compared to others. Tasks could be distributed inequitably, leading to some agents receiving an unfair advantage while others incur disproportionately high costs. It is, therefore, important to consider the tradeoffs between efficiency and fairness in such settings.     We consider the problem of fair multi-agent navigation for a group of decentralized agents using multi-agent reinforcement learning (MARL). We consider the reciprocal of the coefficient of variation of the distances traveled by different agents as a measure of fairness and investigate whether agents can learn to be fair without significantly sacrificing efficiency (i.e., increasing the total distance traveled). We find that by training agents using min-max fair distance goal assignments along with a reward term that incentivizes fairness as they move towards their goals, the agents (1) learn a fair assignment of goals and (2) achieve almost perfect goal coverage in navigation scenarios using only local observations. For goal coverage scenarios, we find that, on average, the proposed model yields a 14% improvement in efficiency and a 5% improvement in fairness over a baseline model that is trained using random assignments. Furthermore, an average of 21% improvement in fairness can be achieved by the proposed model as compared to a model trained on optimally efficient assignments; this increase in fairness comes at the expense of only a 7% decrease in efficiency. Finally, we extend our method to environments in which agents must complete coverage tasks in prescribed formations and show that it is possible to do so without tailoring the models to specific formation shapes.",
        "authors": [
            "Jasmine Aloor",
            "Siddharth Nagar Nayak",
            "Sydney Dolan",
            "Hamsa Balakrishnan"
        ],
        "journal_conference_name": "ACM Journal on Autonomous Transportation Systems",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157544",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Beyond MELD Score: Association of Machine Learning-derived CT Body Composition with 90-Day Mortality Post Transjugular Intrahepatic Portosystemic Shunt Placement",
        "abstract": "Purpose To determine the association of machine learning-derived CT body composition and 90-day mortality after transjugular intrahepatic portosystemic shunt (TIPS) and to assess its predictive performance as a complement to Model for End-Stage Liver Disease (MELD) score for mortality risk prediction. Materials and Methods This retrospective multi-center cohort study included patients who underwent TIPS from 1995 to 2018 and had a contrast-enhanced CT abdomen within 9 months prior to TIPS and at least 90 days of post-procedural clinical follow-up. A machine learning algorithm extracted CT body composition metrics at L3 vertebral level including skeletal muscle area (SMA), skeletal muscle index (SMI), skeletal muscle density (SMD), subcutaneous fat area (SFA), subcutaneous fat index (SFI), visceral fat area (VFA), visceral fat index (VFI), and visceral-to-subcutaneous fat ratio (VSR). Independent t-tests, logistic regression models, and ROC curve analysis were utilized to assess the association of those metrics in predicting 90-day mortality. Results A total of 122 patients (58 ± 11.8, 68% male) were included. Patients who died within 90 days of TIPS had significantly higher MELD (18.9 vs. 11.9, p < 0.001) and lower SMA (123 vs. 144.5, p = 0.002), SMI (43.7 vs. 50.5, p = 0.03), SFA (122.4 vs. 190.8, p = 0.009), SFI (44.2 vs. 66.7, p = 0.04), VFA (105.5 vs. 171.2, p = 0.003), and VFI (35.7 vs. 57.5, p = 0.02) compared to those who survived past 90 days. There were no significant associations between 90-day mortality and BMI (26 vs. 27.1, p = 0.63), SMD (30.1 vs. 31.7, p = 0.44), or VSR (0.97 vs. 1.03, p = 0.66). Multivariable logistic regression showed that SMA (OR = 0.97, p < 0.01), SMI (OR = 0.94, p = 0.03), SFA (OR = 0.99, p = 0.01), and VFA (OR = 0.99, p = 0.02) remained significant predictors of 90-day mortality when adjusted for MELD score. ROC curve analysis demonstrated that including SMA, SFA, and VFA improves the predictive power of MELD score in predicting 90-day mortality after TIPS (AUC, 0.84; 95% CI: 0.77, 0.91; p = 0.02). Conclusion CT body composition is positively predictive of 90-day mortality after TIPS and improves the predictive performance of MELD score. Level of Evidence: Level 3, Retrospective multi-center cohort study. Graphical Abstract",
        "authors": [
            "Tarig Elhakim",
            "Arian Mansur",
            "Jordan Kondo",
            "Omar M. F. Omar",
            "Khalid Ahmed",
            "Azadeh Tabari",
            "Allison Brea",
            "Gabriel Ndakwah",
            "Shams Iqbal",
            "Andrew S. Allegretti",
            "Florian J. Fintelmann",
            "Eric Wehrenberg-Klee",
            "Christopher Bridge",
            "Dania Daye"
        ],
        "journal_conference_name": "CardioVascular and Interventional Radiology",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157506",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Racial Steering by Large Language Models: A Prospective Audit of GPT-4 on Housing Recommendations",
        "abstract": "The integration of Large Language Models (LLMs) into a wide range of rental and real estate platforms could exacerbate historical inequalities in housing, particularly given that LLMs have exhibited gender, racial, ethnic, nationality, and language-based biases in other contexts. Examples of use cases already exist, with real estate listing platforms having launched ChatGPT plugins in 2023. In response to the critical need to assess the ways that LLMs may contribute to housing discrimination, we analyze GPT-4 housing recommendations in response to N = 168,000 prompts for renting and buying in the ten largest majority-minority cities in the US with prompts varying by demographic characteristics like sexuality, race, gender, family status, and source of income, many of which are protected under federal, state, and local fair housing laws. We find evidence of racial steering, default whiteness, and steering of minority homeseekers toward neighborhoods with lower opportunity indices in GPT-4’s housing recommendations to prospective buyers or renters, all of which could have the effect of exacerbating segregation in already segregated cities. Finally, we discuss potential legal implications on how LLMs could be liable under fair housing laws and end with policy recommendations regarding the importance of auditing, understanding, and mitigating risks from AI systems before they are put to use.",
        "authors": [
            "Eric Liu",
            "Wonyoung So",
            "Peko Hosoi",
            "Catherine D'Ignazio"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Equity and Access in Algorithms, Mechanisms, and Optimization",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157628",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "GUI: A Comprehensive Dataset of Global Urban Infrastructure Based on Geospatial Visual Foundation Models",
        "abstract": "The substantial social and financial costs of infrastructure identification impede in-depth analyses of sustainable urban design, especially in developing countries. In this paper, we present a novel framework with interactive web visualization based on geospatial visual foundation models. Leveraging this framework, we examine the urban infrastructure information in 1,178 cities worldwide, covering 93, 088 km2 areas. Cross-validation reveals that the overall accuracy of identified infrastructure achieves 67.0%. It sheds light on the sustainable development of cities and exposes the stark inequity in urban infrastructure provision for vulnerable populations. The identified urban infrastructure dataset of this study are available at https://github.com/tsinghua-fib-lab/GUI, and the interactive web application is at https://tinyurl.com/yz7xbfy3.",
        "authors": [
            "Zhenyu Han",
            "Xin Zhang",
            "Yanxin Xi",
            "Yan Luo",
            "Tong Xia",
            "Yong Li"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 32nd ACM International Conference on Advances in Geographic Information Systems",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157761",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Learning Socio-Temporal Graphs for Multi-Agent Trajectory Prediction",
        "abstract": "In order to predict a pedestrian's trajectory in a crowd accurately, one has to take into account her/his underlying socio-temporal interactions with other pedestrians consistently. Unlike existing work that represents the relevant information separately, partially, or implicitly, we propose a complete representation for it to be fully and explicitly captured and analyzed. In particular, we introduce a Directed Acyclic Graph-based structure, which we term Socio-Temporal Graph (STG), to explicitly capture pair-wise socio-temporal interactions among a group of people across both space and time. Our model is built on a time-varying generative process, whose latent variables determine the structure of the STGs. We design an attention-based model named STGformer that affords an end-to-end pipeline to learn the structure of the STGs for trajectory prediction. Our solution achieves overall state-of-the-art prediction accuracy in two large-scale benchmark datasets. Our analysis shows that a person's past trajectory is critical for predicting another person's future path. Our model learns this relationship with a strong notion of socio-temporal localities. Statistics show that utilizing this information explicitly for prediction yields a noticeable performance gain with respect to the trajectory-only approaches.",
        "authors": [
            "Yuke Li",
            "Lixiong Chen",
            "Guangyi Chen",
            "Ching-Yao Chan",
            "Kun Zhang",
            "Stefano Anzellotti",
            "Donglai Wei"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 5th International Workshop on Human-centric Multimedia Analysis",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157626",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Sulfide Route to Chromium–Nickel–Molybdenum Ferroalloys for Stainless Steel Production",
        "abstract": "New methods of materials separation and metal production utilizing sulfide chemistries may support a paradigm shift in sustainable metallurgy. We leverage sulfidation with elemental sulfur, aluminothermic reduction, and slag refining to obtain a chromium–nickel–molybdenum ferroalloy and stainless steel using a sulfide-based route without direct greenhouse gas emissions. The absence of carbothermic reduction from the mineral, concentrate, and matte feedstocks tried herein indicates that argon-oxygen-decarburization may no longer be necessary to refine stainless steel products.",
        "authors": [
            "Caspar Stinn",
            "Antoine Allanore"
        ],
        "journal_conference_name": "Metallurgical and Materials Transactions B",
        "publisher": "Springer Nature",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157492",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Tango 2: Aligning Diffusion-based Text-to-Audio Generations through Direct Preference Optimization",
        "abstract": "Generative multimodal content is increasingly prevalent in much of the content creation arena, as it has the potential to allow artists and media personnel to create pre-production mockups by quickly bringing their ideas to life. The generation of audio from text prompts is an important aspect of such processes in the music and film industry. Many of the recent diffusion-based text-to-audio models focus on training increasingly sophisticated diffusion models on a large set of datasets of prompt-audio pairs. These models do not explicitly focus on the presence of concepts or events and their temporal ordering in the output audio with respect to the input prompt. Our hypothesis is focusing on how these aspects of audio generation could improve audio generation performance in the presence of limited data. As such, in this work, using an existing text-to-audio model Tango, we synthetically create a preference dataset where each prompt has a winner audio output and some loser audio outputs for the diffusion model to learn from. The loser outputs, in theory, have some concepts from the prompt missing or in an incorrect order. We fine-tune the publicly available Tango text-to-audio model using diffusion-DPO (direct preference optimization) loss on our preference dataset and show that it leads to improved audio output over Tango and AudioLDM2, in terms of both automatic- and manual-evaluation metrics.",
        "authors": [
            "Navonil Majumder",
            "Chia-Yu Hung",
            "Deepanway Ghosal",
            "Wei-Ning Hsu",
            "Rada Mihalcea",
            "Soujanya Poria"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 32nd ACM International Conference on Multimedia",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157614",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "New Tools for Navigating the Highways of Internet Traffic",
        "abstract": "We live in a world where the internet connects everything from global economies to smart homes. Just like how people travel from one place to another by different modes of transport and across different streets, data also travels from one point to another through different containers and takes different routes. Engineers use network analysis to learn more about all this digital traffic.",
        "authors": [
            "Vaneshi Ramdhony"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "The Chicago Council on Science and Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157667",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "FSL-QuickBoost: Minimal-Cost Ensemble for Few-Shot Learning",
        "abstract": "Few-shot learning (FSL) usually trains models on data from one set of classes, but tests them on data from a different set of classes, providing a few labeled support samples of the unseen classes as a reference for the trained model. Due to the lack of target-relevant training data, there is usually high generalization error with respect to the test classes. In this work, we conduct empirical explorations and propose an ensemble method (namely QuickBoost), which is efficient and effective for improving the generalization of FSL. Specifically, QuickBoost includes an alternative-architecture pretrained encoder with a one-vs-all binary classifier (namely FSL-Forest) based on random forest algorithm, and is ensembled with the off-the-shelf FSL models via logit-level averaging. Experiments on three benchmarks demonstrate that our method achieves state-of-the-art performance with good efficiency. Codes are available at https://github.com/WendyBaiYunwei/FSL-QuickBoost.",
        "authors": [
            "Yunwei Bai",
            "Bill Yang Cai",
            "Ying Kiat Tan",
            "Zangwei Zheng",
            "Shiming Chen",
            "Tsuhan Chen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 32nd ACM International Conference on Multimedia",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157613",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Revisiting values in evaluation: exploring the role of values in shaping evaluation practices and their influences on decision-making within English higher education providers",
        "abstract": "Theoretical and empirical contributions to research on evaluation have advanced our understanding of how values influence evaluation practice. Yet rather than understand how values shape evaluation and its use, research on the evaluation of widening participation (WP) programmes delivered by English higher education (HE) providers has focused on methodological deficits. Rather, this study explores the complexity of how national policy, organisational imperatives and the individual values of staff responsible for WP within HE providers influence how evaluation is practised and used to inform decision-making. The results of semi-structured interviews with 17 staff members spanning the organisational hierarchy of three diverse English HE providers highlight conflicts between staff values, job roles and responsibilities and espoused organisational values, and how they can influence symbolic and legitimising evaluation practices. Alternatively, at the individual level staff values support the process and instrumental use of evaluation to inform programme improvements. The findings identify implications for how HE providers can shape their evaluation systems, and how staff choose to enact evaluation within their programme areas.",
        "authors": [
            "Catherine Kelly"
        ],
        "journal_conference_name": "Higher Education",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157514",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Future of Urban Accessibility: The Role of AI",
        "abstract": "We have entered a new era of computing—one where AI permeates every aspect of society from education to healthcare. In this workshop, we examine the emerging role of AI in the design of equitable and accessible cities, transportation systems, and interactive tools for mapping and navigation. We will solicit short papers around key Urban AI + disability themes, including autonomous vehicles, intelligent wheelchairs, assistive human-robotic interaction, assessing and navigating pedestrian pathways, indoor accessibility, and overarching challenges related to ethics, bias, and data privacy and security. We invite both traditional HCI and accessibility researchers as well as scholars and practitioners from other disciplines relevant to this workshop, including disability studies, gerontology, social work, community psychology, and law. Our overarching goal is to identify open challenges, share current work across disciplines, and spur new collaborations related to AI and urban accessibility.",
        "authors": [
            "Jon Froehlich",
            "Chu Li",
            "Maryam Hosseini",
            "Fabio Miranda",
            "Andres Sevtsuk",
            "Yochai Eisenberg"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 26th International ACM SIGACCESS Conference on Computers and Accessibility",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157612",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Skeletal myotubes expressing ALS mutant SOD1 induce pathogenic changes, impair mitochondrial axonal transport, and trigger motoneuron death",
        "abstract": "Amyotrophic lateral sclerosis (ALS) is a fatal neurodegenerative disease characterized by the loss of motoneurons (MNs), and despite progress, there is no effective treatment. A large body of evidence shows that astrocytes expressing ALS-linked mutant proteins cause non-cell autonomous toxicity of MNs. Although MNs innervate muscle fibers and ALS is characterized by the early disruption of the neuromuscular junction (NMJ) and axon degeneration, there are controversies about whether muscle contributes to non-cell-autonomous toxicity to MNs. In this study, we generated primary skeletal myotubes from myoblasts derived from ALS mice expressing human mutant SOD1G93A (termed hereafter mutSOD1). Characterization revealed that mutSOD1 skeletal myotubes display intrinsic phenotypic and functional differences compared to control myotubes generated from non-transgenic (NTg) littermates. Next, we analyzed whether ALS myotubes exert non-cell-autonomous toxicity to MNs. We report that conditioned media from mutSOD1 myotubes (mutSOD1-MCM), but not from control myotubes (NTg-MCM), induced robust death of primary MNs in mixed spinal cord cultures and compartmentalized microfluidic chambers. Our study further revealed that applying mutSOD1-MCM to the MN axonal side in microfluidic devices rapidly reduces mitochondrial axonal transport while increasing Ca2 + transients and reactive oxygen species (i.e., H2O2). These results indicate that soluble factor(s) released by mutSOD1 myotubes cause MN axonopathy that leads to lethal pathogenic changes.",
        "authors": [
            "Pablo Martínez",
            "Mónica Silva",
            "Sebastián Abarzúa",
            "María F. Tevy",
            "Enrique Jaimovich",
            "Martha Constantine-Paton",
            "Fernando J. Bustos",
            "Brigitte van Zundert"
        ],
        "journal_conference_name": "Molecular Medicine",
        "publisher": "BioMed Central",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157459",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "North Atlantic Heat Transport Convergence Derived from a Regional Energy Budget Using Different Ocean Heat Content Estimates",
        "abstract": "This study uses an oceanic energy budget to estimate the ocean heat transport convergence in the North Atlantic during 2005–2018. The horizontal convergence of the ocean heat transport is estimated using ocean heat content tendency primarily derived from satellite altimetry combined with space gravimetry. The net surface energy fluxes are inferred from mass-corrected divergence of atmospheric energy transport and tendency of the ECMWF ERA5 reanalysis combined with top-of-the-atmosphere radiative fluxes from the clouds and the Earth’s radiant energy system project. The indirectly estimated horizontal convergence of the ocean heat transport is integrated between the rapid climate change-meridional overturning circulation and heatflux array (RAPID) section at 26.5°N (operating since 2004) and the overturning in the subpolar north atlantic program (OSNAP) section, situated at 53°–60°N (operating since 2014). This is to validate the ocean heat transport convergence estimate against an independent estimate derived from RAPID and OSNAP in-situ measurements. The mean ocean energy budget of the North Atlantic is closed to within ± 0.25 PW between RAPID and OSNAP sections. The mean oceanic heat transport convergence between these sections is 0.58 ± 0.25 PW, which agrees well with observed section transports. Interannual variability of the inferred oceanic heat transport convergence is also in reasonable agreement with the interannual variability observed at RAPID and OSNAP, with a correlation of 0.54 between annual time series. The correlation increases to 0.67 for biannual time series. Other estimates of the ocean energy budget based on ocean heat content tendency derived from various methods give similar results. Despite a large spread, the correlation is always significant meaning the results are robust against the method to estimate the ocean heat content tendency.",
        "authors": [
            "B. Meyssignac",
            "S. Fourest",
            "Michael Mayer",
            "G. C. Johnson",
            "F. M. Calafat",
            "M. Ablain",
            "T. Boyer",
            "L. Cheng",
            "D. Desbruyères",
            "G. Forget"
        ],
        "journal_conference_name": "Surveys in Geophysics",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157436",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Behaviorally informed digital campaigns and their association with social media engagement and COVID-19 vaccine uptake in Belize",
        "abstract": "Background Increasing vaccination coverage was key to curbing the COVID-19 pandemic globally. However, lack of trust in the vaccine and fear of side effects in regions like the Caribbean resulted in a low uptake despite enough vaccine supply. Methods We conducted two correlational analyses and one experiment between five sequential behaviorally informed Facebook campaigns, social media performance outcomes, and district-level vaccination data. First, we ran multivariate linear regression models to estimate the mean differences between the campaigns in (i) social media performance (“Clicks” and “Engagement”) and (ii) COVID-19 vaccination uptake at the district level. “Clicks” were measured by the number of people who clicked on the respective Facebook advert and visited the official vaccination site. “Engagements” were the number of people interacting with the advert through likes and emojis. Second, we took advantage of the experimental design during one of the campaigns to analyze the differential effect of messages conveying information about the number of people reporting vaccination side effects using words (“Few”/ “Majority) and numbers (“3 out of 100 “) on social media performance. Results The correlational analysis showed that the number of “Clicks” and “Engagement” was similar among campaigns, except for the campaign focusing on vaccines’ effectiveness, which had 14.65 less clicks and 19.52 less engagements per advert (including controls and district-fixed effects) compared to the base “It’s safe” campaign. Vaccination rates were highest at times coinciding with campaigns focusing on vaccination safety and effectiveness. Our experimental results showed that informational messages related to side effects that were framed using words (“Majority did not report discomfort”/ “Few persons reported discomfort”) were better at generating “Clicks” compared to those using numbers (“3 out of 100 reported discomforts”). Conclusions Facebook adverts highlighting vaccine safety had a similar level of social media performance as other campaigns, except for adverts focusing on vaccine efficacy, which performed worse. Communicating side-effect information with words instead of numbers can expand social media interest in low-uptake regions like the Caribbean. Our results serve as preliminary evidence for public health officials to encourage vaccine uptake in high-hesitancy contexts.",
        "authors": [
            "Giuliana Daga",
            "Lajos Kossuth",
            "Cynthia Boruchowicz",
            "Florencia Lopez Boo",
            "Natalia Largaespada Beer"
        ],
        "journal_conference_name": "BMC Global and Public Health",
        "publisher": "BioMed Central",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157458",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Empirical estimation of metal powder bed fusion technological improvement rate",
        "abstract": "This study empirically estimates the technological improvement rate (TIR) of metal powder bed fusion (PBF) technology, widely used in aerospace, automotive, and medical industries. PBF's continuous long-term adoption growth is driven by its ability to enhance manufacturing efficiency in terms of time and raw material use, as well as its capability to produce high-quality, high-strength, complex-shaped parts. Measuring the technological development of PBF is crucial as itis enlarging its application domain and is increasingly considered a viable alternative to traditional manufacturing technologies across a broader range of applications. We resorted to the literature to collect information and assess which technical parameters are most relevant to measure the capabilities of PBF. With those, we established an ideal functional performance metric (FPM) capable of comprehensively assessing PBF's technological performance improvement. Considering all available data sources and PBF machines ever made commercially available, a data set of technical parameters was constructed. This was followed by a data curation process focusing on data availability and reliability. The resultant practical FPM was used to estimate the TIR of PBF technology. By employing regression analysis, we estimate a yearly improvement of 26.8%. This empirical rate comes as a more accurate and reliable substitute to the previously indirectly estimated patent-derived rate of 33.3%. Our findings underscore PBF's capability of keeping pace with its growing significance and wider industrial applications. The results of this study provide a key metric for those in the industry and research, confirming the rapid performance growth and establishing a standard for future industrial uses.",
        "authors": [
            "António Alves de Campos",
            "Bruna Torres Ferreira",
            "Afonso Gonçalves",
            "Marco Leite",
            "Inês Ribeiro",
            "Christopher L. Magee",
            "Elsa Henriques"
        ],
        "journal_conference_name": "Progress in Additive Manufacturing",
        "publisher": "Springer International Publishing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157446",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "How MIT’s Rad Lab rescued D-Day",
        "abstract": "After two British physicists invented a revolutionary gadget, MIT researchers used it to develop the radar\r\ndevices that helped defeat the Nazis.",
        "authors": [
            "Norman Fine"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "MIT Technology Review",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157665",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Flexible Composites with Rare-Earth Element Doped Polycrystalline Particles for Piezoelectric Nanogenerators",
        "abstract": "Energy harvesting plays an important role in advancing personalized wearables by enabling continuous monitoring, enhancing wearable functionality and facilitating sustainable solutions. We aimed to develop a flexible piezoelectric energy harvesting system based on inorganic piezoelectric materials that convert mechanical energy into electricity to power a wide range of mobile and portable electronic devices. There is significant interest in flexible piezoelectric energy harvesting systems that use inorganic piezoelectric materials due to their exceptional physical features and prospective applications. Herein, we successfully demonstrated a flexible piezoelectric nanogenerator (PENG) designed by the co-doped rare-earth element ceramics (RE-PMN-PT) embedded in PVDF and PDMS composite film and attained a significant output performance while avoiding electrical poling process. The impact of dielectric characteristics on the electrical output of nanogenerators was investigated, together with the structure of the composites. The Sm/La-PMN-PT particles effectively amplify both the voltage and current output, showcasing their potential to power portable and wearable devices, as demonstrated by their capacity to illuminate LEDs. The maximal output power of 2 mW was correlated with the high voltage (220 V) and current (90 &micro;A) of Sm/La-PMN-PT/PVDF, which demonstrated that the device has the potential for energy harvesting in biomedical applications.",
        "authors": [
            "Yanzhe Fan",
            "Zihan Jia",
            "Zhuo Zhang",
            "Shengfei Gu",
            "Wenya Du",
            "Dabin Lin"
        ],
        "journal_conference_name": "Micromachines",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157683",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "MEDFuse: Multimodal EHR Data Fusion with Masked Lab-Test Modeling and Large Language Models",
        "abstract": "Electronic health records (EHRs) are multimodal by nature, consisting of structured tabular features like lab tests and unstructured clinical notes. In real-life clinical practice, doctors use complementary multimodal EHR data sources to get a clearer picture of patients' health and support clinical decision-making. However, most EHR predictive models do not reflect these procedures, as they either focus on a single modality or overlook the inter-modality interactions/redundancy. In this work, we propose MEDFuse, a Multimodal EHR Data Fusion framework that incorporates masked lab-test modeling and large language models (LLMs) to effectively integrate structured and unstructured medical data. MEDFuse leverages multimodal embeddings extracted from two sources: LLMs fine-tuned on free clinical text and masked tabular transformers trained on structured lab test results. We design a disentangled transformer module, optimized by a mutual information loss to 1) decouple modality-specific and modality-shared information and 2) extract useful joint representation from the noise and redundancy present in clinical notes. Through comprehensive validation on the public MIMIC-III dataset and the in-house FEMH dataset, MEDFuse demonstrates great potential in advancing clinical predictions, achieving over 90% F1 score in the 10-disease multi-label classification task.",
        "authors": [
            "Phan Nguyen Minh Thao",
            "Cong-Tinh Dao",
            "Chenwei Wu",
            "Jian-Zhe Wang",
            "Shun Liu",
            "Jun-En Ding",
            "David Restrepo",
            "Feng Liu",
            "Fang-Ming Hung",
            "Wen-Chih Peng"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 33rd ACM International Conference on Information and Knowledge Management",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157546",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Tiny Pointers",
        "abstract": "This paper introduces a new data-structural object that we call the tiny pointer. In many applications, traditional logn-bit pointers can be replaced with o(logn)-bit tiny pointers at the cost of only a constant-factor time overhead. We develop a comprehensive theory of tiny pointers, and give optimal constructions for both fixed-size tiny pointers (i.e., settings in which all of the tiny pointers must be the same size) and variable-size tiny pointers (i.e., settings in which the average tiny-pointer size must be small, but some tiny pointers can be larger). If a tiny pointer references an element in an array filled to load factor 1?1/k, then the optimal tiny-pointer size is ?(logloglogn+logk) bits in the fixed-size case, and ?(logk) expected bits in the variable-size case. Our tiny-pointer constructions also require us to revisit several classic problems having to do with balls and bins; these results may be of independent interest.  Using tiny pointers, we revisit five classic data-structure problems: the data-retrieval problem, succinct dynamic binary search trees, space-efficient stable dictionaries, space-efficient dictionaries with variable-size keys, and the internal-memory stash problem. These are all well-studied problems, and in each case tiny pointers allow for us to take a natural space-inefficient solution that uses pointers and make it space-efficient for free.",
        "authors": [
            "Michael Bender",
            "Alex Conway",
            "Martin Farach-Colton",
            "William Kuszmaul",
            "Guido Tagliavini"
        ],
        "journal_conference_name": "ACM Transactions on Algorithms",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157543",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Reproducing Reaction Mechanisms with Machine‐Learning Models Trained on a Large‐Scale Mechanistic Dataset",
        "abstract": "Mechanistic understanding of organic reactions can facilitate reaction development, impurity prediction, and in principle, reaction discovery. While several machine learning models have sought to address the task of predicting reaction products, their extension to predicting reaction mechanisms has been impeded by the lack of a corresponding mechanistic dataset. In this study, we construct such a dataset by imputing intermediates between experimentally reported reactants and products using expert reaction templates and train several machine learning models on the resulting dataset of 5,184,184 elementary steps. We explore the performance and capabilities of these models, focusing on their ability to predict reaction pathways and recapitulate the roles of catalysts and reagents. Additionally, we demonstrate the potential of mechanistic models in predicting impurities, often overlooked by conventional models. We conclude by evaluating the generalizability of mechanistic models to new reaction types, revealing challenges related to dataset diversity, consecutive predictions, and violations of atom conservation.",
        "authors": [
            "Joonyoung F Joung",
            "Mun Hong Fong",
            "Jihye Roh",
            "Zhengkai Tu",
            "John Bradshaw",
            "Connor W Coley"
        ],
        "journal_conference_name": "Angewandte Chemie",
        "publisher": "Wiley",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158162",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Automatic Local Inverse Calculation for Change of Variables",
        "abstract": "Inversion is a fundamental operation that arises frequently in probabilistic inference and computer graphics. For example, inversion is used to decrease variance and to enable differentiation in variational inference (e.g., reparameterization trick) and in differentiable rendering (e.g., to integrate over object boundaries). Existing approaches to inversion limit the class of functions inverted, for example, to affine functions, or require a user-specified inverse. We study when a local inverse—an inverse that is valid in a neighborhood of a point—exists. We provide an algorithm to approximate the local inverse and give the convergence rate of the solver. We present LIN, a system that automatically computes the local inverse of a function using a fixed-point solver. We implement LIN in Python and use it to automatically compute the local inverse of affine, polar, and hyperbolic changes of variables arising in image stylization.",
        "authors": [
            "Elias Rojas Collins"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Companion Proceedings of the 2024 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157627",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "New Generalized Derivatives for Solving Variational Inequalities Using the Nonsmooth Newton Methods",
        "abstract": "Variational inequality (VI) generalizes many mathematical programming problems and has a wide variety of applications. One class of VI solution methods is to reformulate a VI into a normal map nonsmooth equation system, which is then solved using nonsmooth equation-solving techniques. In this article, we propose a first practical approach for furnishing B-subdifferential elements of the normal map, which in turn enables solving the normal map equation system using variants of the B-subdifferential-based nonsmooth Newton method. It is shown that our new method requires less stringent conditions to achieve local convergence than some other established methods, and thus guarantees convergence in certain cases where other methods may fail. We compute a B-subdifferential element using the LD-derivative, which is a recently established generalized derivative concept. In our new approach, an LD-derivative is computed by solving a sequence of strictly convex quadratic programs, which can be terminated early under certain conditions. Numerical examples are provided to illustrate the convergence properties of our new method, based on a proof-of-concept implementation in Python.",
        "authors": [
            "Yingkai Song",
            "Paul I. Barton"
        ],
        "journal_conference_name": "Journal of Optimization Theory and Applications",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157415",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Possession and syntactic categories: An argument from Äiwoo",
        "abstract": "This paper argues that possession is syntactically category-flexible. While it is clear that in many languages possession is mostly grounded in and operates in the nominal extended projection (Szabolcsi 1983; Kayne 1993), I show that this cannot be universal. The empirical part of this article is a case study of Äiwoo, which I argue has an inherently verbal counterpart of English ’s, an abstract transitive verb I label poss. This verb can be used by itself to form clausal possession: ‘I poss this boat’ ≈ ‘this boat is mine.’ Possessed DPs also contain the verb poss: the object of this verb is extracted, forming a relative clause. Informally, ‘my boat’ really is ‘the boati ’ ≈ ‘the boat that is mine.’ Given this, Äiwoo simply lacks true nominal possessives. The theoretical consequence is that possession can be mapped onto different syntactic categories in different languages. This is a welcome result, as it makes the syntax-semantics mapping as flexible as it needs to be: if possession is just a tool to assert that a certain relation holds between two entities, nothing in our theory of grammar predicts that such a notion should only be limited to a specific syntactic category.",
        "authors": [
            "Giovanni Roversi"
        ],
        "journal_conference_name": "Natural Language & Linguistic Theory",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157414",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Integrating Multi-Cancer Early Detection (MCED) Tests with Standard Cancer Screening: System Dynamics Model Development and Feasibility Testing",
        "abstract": "Background Cancer screening plays a critical role in early disease detection and improving outcomes. In Australia, established screening protocols for colorectal, breast and cervical cancer have significantly contributed to timely cancer detection. However, the recent introduction of multi-cancer early detection (MCED) tests arguably can disrupt current screening, yet the extent to which these tests provide additional benefits remains uncertain. We present the development and initial validation of a system dynamics (SD) model that estimates the additional cancer detections and costs associated with MCED tests. Aim This article describes the development of a simulation model built to evaluate the additional patient diagnoses and the economic impact of incorporating MCED testing alongside Australia’s well-established standard of care (SOC) screening programs for colorectal, breast, cervical and lung cancers. The model was designed to estimate the additional number of patients diagnosed at each cancer stage (stage I, II, III, IV, or unknown) and the associated costs. This simulation model allows for the analysis of multiple scenarios under a plausible set of assumptions regarding population-level participation rates. Methods An SD model was developed to represent the existing SOC national cancer screening pathways and to integrate potential clinical pathways that could be introduced by MCED tests. The SD model was built to investigate three scenarios for the use of MCED testing: firstly, to explore the viability of MCED testing as a substitute among individuals who are not opting for SOC screening for any reason; secondly, to implement MCED testing exclusively for individuals ineligible for SOC screening, yet have high-risk characteristics; and thirdly, to employ MCED testing after SOC screening to serve as a triaging/confirmatory tool for individuals receiving inconclusive test results. The three primary scenarios were constructed by varying diagnostic accuracy and uptake rates of MCED tests. Discussion The clinical utility and outcomes of MCED testing for screening and early detection still lack comprehensive evidence. Nonetheless, this simulation model facilitates a thorough analysis of MCED tests within the Australian healthcare context, providing insights into potential additional detections and costs to the healthcare system, which may help prioritise future evidence development. The adaptable yet novel SD model presented herein is anticipated to be of considerable interest to industry, policymakers, consumers and clinicians involved in informing clinical and economic decisions regarding integrating MCED tests as cancer screening and early detection tools. The expected results of applying this SD model will determine whether using MCED testing in conjunction with SOC screening offers any potential benefits, possibly guiding policy decisions and clinical practices towards the adoption of MCED tests.",
        "authors": [
            "Mussab Fagery",
            "Hadi A. Khorshidi",
            "Stephen Q. Wong",
            "Özge Karanfil",
            "Jon Emery",
            "Maarten J. IJzerman"
        ],
        "journal_conference_name": "PharmacoEconomics - Open",
        "publisher": "Springer International Publishing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157420",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Optimizing Transformation-Induced Plasticity to Resist Microvoid Softening",
        "abstract": "Many high-performance steels that are critical for energy-efficient, lightweight designs rely on transformation-induced plasticity (TRIP) to achieve superior combinations of strength and ductility/toughness. Further development of these alloys will require greater optimization of the metastable (retained) austenite phase responsible for TRIP. Considering the complex nature of TRIP and its effects on ductile fracture, an integrated computational materials engineering (ICME) approach to materials optimization is desired. In this work, we report the results of a large series of micromechanical finite element calculations that probe the interaction of TRIP and void-mediated ductile fracture mechanisms. The simulations identify the optimal austenite stability for maximizing the benefit of TRIP across a wide range of stress states. The applied stress triaxiality significantly influences the microvoid growth rate and the computationally determined optimal stability. The simulation results are compared with existing experimental data, demonstrating good agreement.",
        "authors": [
            "Brandon D. Snow",
            "G. B. Olson",
            "D. M. Parks"
        ],
        "journal_conference_name": "Metallurgical and Materials Transactions A",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157417",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Formal contracts mitigate social dilemmas in multi-agent reinforcement learning",
        "abstract": "Multi-agent Reinforcement Learning (MARL) is a powerful tool for training autonomous agents acting independently in a common environment. However, it can lead to sub-optimal behavior when individual incentives and group incentives diverge. Humans are remarkably capable at solving these social dilemmas. It is an open problem in MARL to replicate such cooperative behaviors in selfish agents. In this work, we draw upon the idea of formal contracting from economics to overcome diverging incentives between agents in MARL. We propose an augmentation to a Markov game where agents voluntarily agree to binding transfers of reward, under pre-specified conditions. Our contributions are theoretical and empirical. First, we show that this augmentation makes all subgame-perfect equilibria of all Fully Observable Markov Games exhibit socially optimal behavior, given a sufficiently rich space of contracts. Next, we show that for general contract spaces, and even under partial observability, richer contract spaces lead to higher welfare. Hence, contract space design solves an exploration-exploitation tradeoff, sidestepping incentive issues. We complement our theoretical analysis with experiments. Issues of exploration in the contracting augmentation are mitigated using a training methodology inspired by multi-objective reinforcement learning: Multi-Objective Contract Augmentation Learning. We test our methodology in static, single-move games, as well as dynamic domains that simulate traffic, pollution management, and common pool resource management.",
        "authors": [
            "Andreas Haupt",
            "Phillip Christoffersen",
            "Mehul Damani",
            "Dylan Hadfield-Menell"
        ],
        "journal_conference_name": "Autonomous Agents and Multi-Agent Systems",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157416",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Estimating transit’s land-use multiplier: direct and indirect effects on vehicle miles traveled",
        "abstract": "The significance of public transit in curbing greenhouse gas (GHG) emissions and reducing vehicle miles traveled (VMT) goes beyond its users. Investments in transit infrastructure, coupled with service enhancements and their consequential impacts on urban development (termed as indirect effects), have the potential to foster location efficiency. This concept encompasses the advantageous proximity of vital destinations such as workplaces and retail establishments to the residences that necessitate access. In this context, investments made in public transit systems exhibit a multiplier effect, commonly quantified as the reduction in VMT per each passenger mile of transit usage. While this topic has gained attention over the past few decades, an agreement regarding the size of the multiplier effect has yet to be reached among researchers. This study employs a multilevel structural equation model and leverages a comprehensive database of household travel survey data from 31 diverse regions. By utilizing trip-level data, this study provides results that possess external validity and generalizability, overcoming limitations identified in earlier research. Additionally, this study aims to present a simplified formula that enables transit agencies nationwide to compute their unique multipliers. The findings suggest that regions with extensive transit systems exhibit higher transit multipliers compared to regions with limited transit access. Furthermore, the impact of transit within a community extends well beyond merely the reduction in private vehicle usage by transit passengers. Rather, the alterations in the built environment in transit-served communities lead to substantial VMT savings, surpassing the effects solely attributed to transit passenger usage.",
        "authors": [
            "Sadegh Sabouri",
            "Reid Ewing",
            "Hannaneh A. Kalantari"
        ],
        "journal_conference_name": "Transportation",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157413",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Photoplethysmography Features Correlated with Blood Pressure Changes",
        "abstract": "Blood pressure measurement is a key indicator of vascular health and a routine part of medical examinations. Given the ability of photoplethysmography (PPG) signals to provide insights into the microvascular bed and their compatibility with wearable devices, significant research has focused on using PPG signals for blood pressure estimation. This study aimed to identify specific clinical PPG features that vary with different blood pressure levels. Through a literature review of 297 publications, we selected 16 relevant studies and identified key time-dependent PPG features associated with blood pressure prediction. Our analysis highlighted the second derivative of PPG signals, particularly the 𝑏/𝑎\r\n and 𝑑/𝑎\r\n ratios, as the most frequently reported and significant predictors of systolic blood pressure. Additionally, features from the velocity and acceleration photoplethysmograms were also notable. In total, 29 features were analyzed, revealing novel temporal domain features that show promise for further research and application in blood pressure estimation.",
        "authors": [
            "Mohamed Elgendi",
            "Elisabeth Jost",
            "Aymen Alian",
            "Richard Ribon Fletcher",
            "Hagen Bomberg",
            "Urs Eichenberger",
            "Carlo Menon"
        ],
        "journal_conference_name": "diagnostics",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157434",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Branched Convolutional Neural Network for Forecasting the Occurrence of Hazes in Paris Using Meteorological Maps with Different Characteristic Spatial Scales",
        "abstract": "A convolutional neural network (CNN) has been developed to forecast the occurrence of low-visibility events or hazes in the Paris area. It has been trained and validated using multi-decadal daily regional maps of many meteorological and hydrological variables alongside surface visibility observations. The strategy is to make the machine learn from available historical data to recognize various regional weather and hydrological regimes associated with low-visibility events. To better preserve the characteristic spatial information of input features in training, two branched architectures have recently been developed. These architectures process input features firstly through several branched CNNs with different kernel sizes to better preserve patterns with certain characteristic spatial scales. The outputs from the first part of the network are then processed by the second part, a deep non-branched CNN, to further deliver predictions. The CNNs with new architectures have been trained using data from 1975 to 2019 in a two-class (haze versus non-haze) classification mode as well as a regression mode that directly predicts the value of surface visibility. The predictions of regression have also been used to perform the two-class classification forecast using the same definition in the classification mode. This latter procedure is found to deliver a much better performance in making class-based forecasts than the direct classification machine does, primarily by reducing false alarm predictions. The branched architectures have improved the performance of the networks in the validation and also in an evaluation using the data from 2021 to 2023 that have not been used in the training and validation. Specifically, in the latter evaluation, branched machines captured 70% of the observed low-visibility events during the three-year period at Charles de Gaulle Airport. Among those predicted low-visibility events by the machines, 74% of them are true cases based on observation.",
        "authors": [
            "Chien Wang"
        ],
        "journal_conference_name": "15",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157432",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Enhancing the Antioxidant Activity of Tea (Camellia sinensis) Through Common Herbal Infusions",
        "abstract": "ea is the second most widely consumed beverage globally, after water, and is known for its substantial antioxidant properties, primarily due to its phenolic content. This study quantifies phenolic compounds and assesses antioxidant activity in ten types of tea and selected herbal infusions, individually and in combination. Our findings reveal that free phenolic compounds and their antioxidant activity were twelve times and eight times greater than bound phenolic compounds. Among individual infusions, white tea exhibited the highest antioxidant activity and phenolic content, with 172.51 &micro;mol TE/1000 g and 7.83 mg GAE/1000 g, respectively. In combination, white/linden flower tea showed the highest antioxidant activity (374.44 &micro;mol TE/1000 g), and white/orange tea contained the highest phenolic content (9.24 mg GAE/1000 g). This study identified primarily two phenolic compounds, epigallocatechin gallate and epicatechin gallate, and one alkaloid, caffeine, in tea and herbal combinations. Compared to other combinations, we observed significant variations in catechins and caffeine between white and dark teas. Integrating specific herbal infusions with tea can enhance antioxidant activity up to three-fold compared to tea alone. This research offers valuable insights into optimizing herbal infusions to maximize antioxidant benefits, creating new opportunities to enhance the health benefits of tea-based products.",
        "authors": [
            "Sofia Ortiz-Islas",
            "Claudia A. Espinosa-Leal",
            "Tzitziki González-Rodríguez",
            "Silverio García-Lara"
        ],
        "journal_conference_name": "foods",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157435",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Observation of muonic Dalitz decays of chib mesons and precise spectroscopy of hidden-beauty states",
        "abstract": "The decays of the χb1(1P), χb2(1P), χb1(2P) and χb2(2P) mesons into the Υ(1S)μ+μ− final state are observed with a high significance using proton-proton collision data collected with the LHCb detector and corresponding to an integrated luminosity of 9 fb−1. The newly observed decays together with the Υ(2S) → Υ(1S)π+π− and Υ(3S) → Υ(2S)π+π− decay modes are used for precision measurements of the mass and mass splittings for the hidden-beauty states.",
        "authors": [
            "R. Aaij",
            "A. S. W. Abdelmotteleb",
            "C. Abellan Beteta",
            "F. Abudinén",
            "T. Ackernley",
            "A. A. Adefisoye",
            "B. Adeva",
            "M. Adinolfi",
            "P. Adlarson",
            "C. Agapopoulou",
            "C. A. Aidala",
            "Z. Ajaltouni",
            "S. Akar",
            "K. Akiba",
            "P. Albicocco",
            "J. Albrecht",
            "F. Alessio"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157418",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Modeling the Performance of a Multi-Hop LoRaWAN Linear Sensor Network for Energy-Efficient Pipeline Monitoring Systems",
        "abstract": "In recent years, LoRa technology has emerged as a solution for wide-area coverage IoT applications. Deploying a LoRa single-hop network on applications may be challenging in cases of network deployments that require the installation of linear sensor network topologies covering very large distances over unpopulated areas with limited access to cellular networks and energy grids. In such cases, multi-hop communication may provide better alternative solutions to support these challenges. This research aims to study the deployment of multi-hop linear sensor networks that are energy efficient. The focus will be on assessing the coverage, throughput, and energy consumption benefits that can be achieved and the related tradeoffs that have to be considered when using multi-hop solutions. Since monitoring systems in long-distance infrastructures may benefit from solutions based on multi-hop communication, we consider oil pipeline infrastructures in the Saudi Arabian desert as a case study. An analytical model is considered for estimating the above-stated parameters and evaluating the performance of the multi-hop LoRa WSN (MHWSN) against the single-hop LoRa WSN (SHWSN). In addition, the model is used to study the tradeoffs between throughput and energy consumption in different settings of MHWSNs. Scenarios of oil pipeline monitoring systems in Saudi Arabia are specified for studying the proposed multi-hop system&rsquo;s performance. The obtained results show that when we have a large-scale network, such as an oil pipeline with medium traffic load requirements, multi-hop topologies may be an efficient deployment solution.",
        "authors": [
            "Haneen Alhomyani",
            "Mai Fadel",
            "Nikos Dimitriou",
            "Helen Bakhsh",
            "Ghadah Aldabbagh"
        ],
        "journal_conference_name": "applied sciences",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157433",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A “seat-squatting” strategy via lithium substitution to suppress Fe-migration in Na layered oxide cathodes",
        "abstract": "Na-ion batteries (NIBs) are emerging as a promising alternative to Li-ion batteries (LIBs). To align with sustainability principles, the design of electrode materials must incorporate considerations for abundant and environmentally friendly elements, such as redox-active Fe. Despite its appeal, the enduring challenge of Fe migration in layered cathodes remains inadequately addressed over decades. Here, we propose a “seat-squatting” strategy via Li-substitution to fundamentally suppress Fe migration. Li is strategically introduced to migrate first, occupying available migration sites without inducing structural damage and effectively raising the activation energy for Fe migration. Experimental and theoretical validation using O3-Na0.83Li0.17Fe0.33Mn0.5O2 (NaLFM) demonstrates a robust suppression of irreversible Fe migration. As a result, the NaLFM cathode delivers enhanced structural and electrochemical cycling stability. This work illustrates a compelling strategy to curb irreversible Fe migration in NIBs, offering a pathway for the development of stable and cost-effective layered oxides based on Fe redox centers.",
        "authors": [
            "Yaoshen Niu",
            "Zilin Hu",
            "Huican Mao",
            "Lin Zhou",
            "Liguang Wang",
            "Xiaobing Lou",
            "Bo Zhang",
            "Dongdong Xiao",
            "Yang Yang",
            "Feixiang Ding",
            "Xiaohui Rong",
            "Juping Xu",
            "Wen Yin",
            "Nian Zhang",
            "Zhiwei Li",
            "Yaxiang Lu",
            "Bingwen Hu",
            "Jun Lu",
            "Ju Li",
            "Yong-Sheng Hu"
        ],
        "journal_conference_name": "Energy & Environmental Science",
        "publisher": "Royal Society of Chemistry",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157526",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "UROPOT: study protocol for a randomized, double-blind phase I/II trial for metabolism-based potentiation of antimicrobial prophylaxis in the urological tract",
        "abstract": "Background Urinary tract catheters, including Double-J or ureteral stents, are prone to bacterial colonization forming biofilms and leading to asymptomatic bacteriuria. In the context of asymptomatic bacteriuria, endourological procedures causing mucosa-inducing lesions can lead to severe infections. Antibiotic prophylaxis is warranted, yet its efficacy is limited by biofilm formation on stents. Biofilms promote antibiotic tolerance, the capacity of genetically susceptible bacteria to survive a normally lethal dose of antimicrobial therapy. The UROPOT study evaluates the effectiveness of a first-in-type metabolism-based aminoglycoside potentiation for (i) preventing infectious complications of asymptomatic bacteriuria during mucosa lesion-inducing endourological procedures and (ii) assessing its anti-tolerance efficacy. Methods The UROPOT trial is a phase I/II single-center (Lausanne University Hospital (CHUV), Switzerland) randomized double-blinded trial. Over 2 years, patients with asymptomatic Escherichia coli and/or Klebsiella pneumoniae bacteriuria, undergoing endourological procedures, will be randomly allocated to one of three treatment arms (1:1:1 randomization ratio, 30 patients per group) to evaluate the efficacy of mannitol-potentiated low-dose amikacin compared to established standard treatments (ceftriaxone or amikacin standard dose). Patients will be recruited at the CHUV Urology Outpatient Clinic. The primary outcome is the comparative incidence of postoperative urinary tract infections (assessed at 48 h) between the investigational amikacin/mannitol therapy and standard (ceftriaxone or amikacin) antibiotic prophylaxis, defined by specific systemic symptoms and/or positive blood and/or urine culture. Secondary outcomes include assessing microbiological eradication through anti-biofilm activity, sustained microbiological eradication, and mannitol and antibiotics pharmacokinetics in blood and urine. Safety outcomes will evaluate the incidence of adverse events following amikacin/mannitol therapy and postoperative surgical complications at postoperative day 14. Discussion UROPOT tests a novel antimicrobial strategy based on “metabolic potentiation” for prophylaxis enabling aminoglycoside dose reduction and targeting biofilm activity. The anti-biofilm effect may prove beneficial, particularly in patients who have a permanent stent in situ needing recurrent endourological manipulations strategies in preventing infections and achieving sustained microbiological eradication in pre-stented patients. Trial registration The protocol is approved by the local ethics committee (CER-VD, 2023–01369, protocole 2.0) and the Swiss Agency for Therapeutic Products (Swissmedic, 701,676) and is registered on the NIH’s ClinicalTrials.gov (trial registration number: NCT05761405). Registered on March 07, 2023.",
        "authors": [
            "Kevin Stritt",
            "Beat Roth",
            "Audrey Masnada",
            "Felix Hammann",
            "Damien Jacot",
            "Sonia Domingos-Pereira",
            "François Crettenand",
            "Perrine Bohner",
            "Isabelle Sommer",
            "Emilien Bréat",
            "Julien Sauser",
            "Laurent Derré",
            "Manuel Haschke",
            "James J. Collins",
            "John McKinney"
        ],
        "journal_conference_name": "Trials",
        "publisher": "BioMed Central",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157419",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Observation of the 0 b → J/ψ−K+ decay",
        "abstract": "Abstract Using proton–proton collision data corresponding to an integrated luminosity of 140 fb - 1 collected by the CMS experiment at s = 13 Te V , the Λ b 0 → J / ψ Ξ - K + decay is observed for the first time, with a statistical significance exceeding 5 standard deviations. The relative branching fraction, with respect to the Λ b 0 → ψ ( 2 S ) Λ decay, is measured to be B ( Λ b 0 → J / ψ Ξ - K + ) / B ( Λ b 0 → ψ ( 2 S ) Λ ) = [ 3.38 ± 1.02 ± 0.61 ± 0.03 ] % , where the first uncertainty is statistical, the second is systematic, and the third is related to the uncertainties in B ( ψ ( 2 S ) → J / ψ π + π - ) and B ( Ξ - → Λ π - ) .",
        "authors": [
            "A. Hayrapetyan",
            "A. Tumasyan",
            "W. Adam",
            "J. W. Andrejkovic",
            "T. Bergauer",
            "S. Chatterjee",
            "K. Damanakis",
            "M. Dragicevic",
            "A. E. D. Valle",
            "P. S. Hussain",
            "M. Jeitler",
            "N. Krammer",
            "D. Liko",
            "I. Mikulec",
            "J. Schieck",
            "R. Schöfbeck",
            "D. Schwarz",
            "M. Sonawane",
            "S. Templ",
            "W. Waltenberger"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157412",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Effect of Spacecraft Charging on Performance of Ion Electrospray Propulsion Systems",
        "abstract": "Ion electrospray propulsion systems are known to induce moderate levels of spacecraft charging when operated in\r\na passive dual-polarity neutralization scheme. Here, the relationship between this charging and the performance of\r\nthe electrospray thrusters is experimentally assessed. We characterize a passively-fed ion electrospray thruster in a\r\nsimulated spacecraft charging environment with the ionic liquid propellant EMI-BF4. Performance metrics, including\r\nthrust, specific impulse, and component efficiencies are estimated with the thruster operated at emission currents of\r\n±150 µA for prescribed spacecraft biases between 0 and ±800 V. When the spacecraft and plume are the same polarity,\r\nthrusters exhibit a narrower plume and produce more thrust with increasing spacecraft bias. Conversely, when the\r\nspacecraft and plume are opposite polarities, thrusters show increasingly divergent plumes that are attracted back to the\r\nspacecraft, resulting in less thrust being produced at higher spacecraft biases. The combined thrust output for a dualpolarity pair of thrusters was estimated to decrease by about 36% at a spacecraft bias of 800 V and 25% at a spacecraft\r\nbias of −800 V. These results show that spacecraft charging is a critical consideration for determining the true in-space\r\nperformance of ion electrospray propulsion systems.",
        "authors": [
            "Saba Z. Shaik",
            "Matthew N. Corrado",
            "Paulo C. Lozano"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "International Astronautical Federation",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157325",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Hermes: Boosting the Performance of Machine-Learning-Based Intrusion Detection System through Geometric Feature Learning",
        "abstract": "Anomaly-Based Intrusion Detection Systems (IDSs) have been extensively researched for their ability to detect zero-day attacks. These systems establish a baseline of normal behavior using benign traffic data and flag deviations from this norm as potential threats. They generally experience higher false alarm rates than signature-based IDSs. Unlike image data, where the observed features provide immediate utility, raw network traffic necessitates additional processing for effective detection. It is challenging to learn useful patterns directly from raw traffic data or simple traffic statistics (e.g., connection duration, package inter-arrival time) as the complex relationships are difficult to distinguish. Therefore, some feature engineering becomes imperative to extract and transform raw data into new feature representations that can directly improve the detection capability and reduce the false positive rate. We propose a geometric feature learning method to optimize the feature extraction process. We employ contrastive feature learning to learn a feature space where normal traffic instances reside in a compact cluster. We further utilize H-Score feature learning to maximize the compactness of the cluster representing the normal behavior, enhancing the subsequent anomaly detection performance. Our evaluations using the NSL-KDD and N-BaloT datasets demonstrate that the proposed IDS powered by feature learning can consistently outperform state-of-the-art anomaly-based IDS methods by significantly lowering the false positive rate. Furthermore, we deploy the proposed IDS on a Raspberry Pi 4 and demonstrate its applicability on resource-constrained Internet of Things (IoT) devices, highlighting its versatility for diverse application scenarios.",
        "authors": [
            "Chaoyu Zhang",
            "Shanghao Shi",
            "Ning Wang",
            "Xiangxiang Xu",
            "Shaoyu Li",
            "Lizhong Zheng",
            "Randy Marchany",
            "Mark Gardner",
            "Y. Thomas Hou",
            "Wenjing Lou"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The Twenty-fifth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157548",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Optimal Slicing and Scheduling with Service Guarantees in Multi-Hop Wireless Networks",
        "abstract": "We analyze the problem of scheduling in wireless networks to meet end-to-end service guarantees. Using network slicing to decouple the queueing dynamics between flows, we show that the network's ability to meet hard throughput and deadline requirements is largely influenced by the scheduling policy. We characterize the feasible throughput/deadline region for a flow under a fixed route and set of slices, and find throughput- and deadline-optimal policies for a solitary flow. We formulate the feasibility problem for multiple flows in a general topology, and show its equivalence to finding a bounded-cost cycle on an exponentially large graph, which is un-solvable in polynomial time by the best-known algorithm. Using a novel concept called delay deficit, we develop a sufficient condition for meeting deadlines as a function of inter-scheduling times, and show that regular schedules are optimal for satisfying this condition. Motivated by this, we design a polynomial-time algorithm that returns an (almost) regular schedule, optimized to meet service guarantees for all flows.",
        "authors": [
            "Nicholas Jones",
            "Eytan Modiano"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The Twenty-fifth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157554",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Synthesis of α-methylene-δ-valerolactone and its selective polymerization from a product mixture for concurrent separation and polymer production",
        "abstract": "We report the continuous, gas-phase synthesis of α-methylene-δ-valerolactone (MVL) from δ-valerolactone (DVL) and formaldehyde (FA) over alkaline earth oxide catalysts. MgO, CaO, and BaO supported on silica (∼5 wt%) were active for MVL production (613 K, 0.4 kPa DVL, 1.2 kPa FA, 101 kPa total pressure). CaO and BaO showed 90% and 83% selectivity to MVL at ∼60% DVL conversion, respectively. Decreasing contact times improved MVL selectivity for all three catalysts, achieving near quantitative selectivity at DVL conversions <40% with CaO. Further studies with CaO indicated that increasing the FA partial pressure for a given DVL partial pressure negligibly changed conversion while maintaining high selectivity; however, increasing the reaction temperature generally resulted in lower MVL selectivity. Deactivation and carbon loss were attributed to non-volatile compound formation from series and parallel reactions that consume MVL and DVL and poison the catalyst surface. These side reactions were more pronounced at high temperatures and higher contact times. While slow deactivation poses a challenge, the catalyst could be fully regenerated by calcining at 773 K for 4 h under flowing air. As the product mixture of MVL and DVL is difficult to separate, we developed a selective polymerization strategy to convert either one or both monomers into valuable polymeric materials, thereby achieving efficient separation and concurrent polymer production. Using a model mixture of 30 wt% of MVL in DVL, vinyl-addition polymerization converted MVL to the corresponding vinyl polymer (PMVL)VAP in 98% yield, while DVL was recovered in 96% yield by distillation. Alternatively, ring-opening polymerization of the same mixture resulted in a DVL/MVL copolyester and separatable vinyl homopolymer P(MVL)VAP.",
        "authors": [
            "Alexander A Khechfe",
            "Francesca D Eckstrom",
            "Eswara Rao Chokkapu",
            "Lucas A Baston",
            "Bowei Liu",
            "Eugene Y-X Chen",
            "Yuriy Román-Leshkov"
        ],
        "journal_conference_name": "Green Chemistry",
        "publisher": "Royal Society of Chemistry",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157511",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Intervention-Assisted Online Deep Reinforcement Learning for Stochastic Queuing Network Optimization",
        "abstract": "Deep Reinforcement Learning (DRL) offers a powerful approach to training neural network control policies for stochastic queuing networks (SQN). However, traditional DRL methods rely on offline simulations or static datasets, limiting their real-world application in SQN control. This work proposes Online Deep Reinforcement Learning-based Controls (ODRLC) as an alternative, where an intelligent agent interacts directly with a real environment and learns an optimal control policy from these online interactions. SQNs present a challenge for ODRLC due to the unbounded nature of the queues within the network resulting in an unbounded state-space. An unbounded state-space is particularly challenging for neural network policies as neural networks are notoriously poor at extrapolating to unseen states. To address this challenge, we propose an intervention-assisted framework that leverages strategic interventions from known stable policies to ensure the queue sizes remain bounded. This framework combines the learning power of neural networks with the guaranteed stability of classical control policies for SQNs. We introduce a method to design these intervention-assisted policies to ensure strong stability of the network. Furthermore, we extend foundational DRL theorems for intervention-assisted policies and develop two practical algorithms specifically for ODRLC of SQNs. Finally, we demonstrate through experiments that our proposed algorithms outperform both classical control approaches and prior ODRLC algorithms.",
        "authors": [
            "Jerrod Wigmore",
            "Brooke Shrader",
            "Eytan Modiano"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The Twenty-fifth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157553",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Demonstration of neutron identification in neutrino interactions in the MicroBooNE liquid argon time projection chamber",
        "abstract": "A significant challenge in measurements of neutrino oscillations is reconstructing the incoming neutrino energies. While modern fully-active tracking calorimeters such as liquid argon time projection chambers in principle allow the measurement of all final state particles above some detection threshold, undetected neutrons remain a considerable source of missing energy with little to no data constraining their production rates and kinematics. We present the first demonstration of tagging neutrino-induced neutrons in liquid argon time projection chambers using secondary protons emitted from neutron-argon interactions in the MicroBooNE detector. We describe the method developed to identify neutrino-induced neutrons and demonstrate its performance using neutrons produced in muon-neutrino charged current interactions. The method is validated using a small subset of MicroBooNE’s total dataset. The selection yields a sample with 60 % of selected tracks corresponding to neutron-induced secondary protons. At this purity, the integrated efficiency is 8.4% for neutrons that produce a detectable proton.",
        "authors": [
            "P. Abratenko",
            "O. Alterkait",
            "D. A. Aldana",
            "L. Arellano",
            "J. Asaadi",
            "A. Ashkenazi",
            "S. Balasubramanian",
            "B. Baller",
            "A. Barnard",
            "G. Barr",
            "D. Barrow",
            "J. Barrow",
            "V. Basque",
            "J. Bateman",
            "O. B. Rodrigues",
            "S. Berkman",
            "A. Bhanderi",
            "A. Bhat",
            "M. Bhattacharya",
            "M. Bishai"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157406",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Speed-Modulated Ironing: High-Resolution Shade and Texture Gradients in Single-Material 3D Printing",
        "abstract": "We present Speed-Modulated Ironing, a new fabrication method for programming visual and tactile properties in single-material 3D printing. We use one nozzle to 3D print and a second nozzle to reheat printed areas at varying speeds, controlling the material’s temperature-response. The rapid adjustments of speed allow for fine-grained reheating, enabling high-resolution color and texture variations. We implemented our method in a tool that allows users to assign desired properties to 3D models and creates corresponding 3D printing instructions. We demonstrate our method with three temperature-responsive materials: a foaming filament, a filament with wood fibers, and a filament with cork particles. These filaments respond to temperature by changing color, roughness, transparency, and gloss. Our technical evaluation reveals the capabilities of our method in achieving sufficient resolution and color shade range that allows surface details such as small text, photos, and QR codes on 3D-printed objects. Finally, we provide application examples demonstrating the new design capabilities enabled by Speed-Modulated Ironing.",
        "authors": [
            "Mehmet Ozdemir",
            "Marwa AlAlawi",
            "Mustafa Doga Dogan",
            "Jose Martinez Castro",
            "Stefanie Mueller",
            "Zjenja Doubrovski"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 37th Annual ACM Symposium on User Interface Software and Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157609",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Bluefish: Composing Diagrams with Declarative Relations",
        "abstract": "Diagrams are essential tools for problem-solving and communication as they externalize conceptual structures using spatial relationships. But when picking a diagramming framework, users are faced with a dilemma. They can either use a highly expressive but low-level toolkit, whose API does not match their domain-specific concepts, or select a high-level typology, which offers a recognizable vocabulary but supports a limited range of diagrams. To address this gap, we introduce Bluefish: a diagramming framework inspired by component-based user interface (UI) libraries. Bluefish lets users create diagrams using relations: declarative, composable, and extensible diagram fragments that relax the concept of a UI component. Unlike a component, a relation does not have sole ownership over its children nor does it need to fully specify their layout. To render diagrams, Bluefish extends a traditional tree-based scenegraph to a compound graph that captures both hierarchical and adjacent relationships between nodes. To evaluate our system, we construct a diverse example gallery covering many domains including mathematics, physics, computer science, and even cooking. We show that Bluefish’s relations are effective declarative primitives for diagrams. Bluefish is open source, and we aim to shape it into both a usable tool and a research platform.",
        "authors": [
            "Josh Pollock",
            "Catherine Mei",
            "Grace Huang",
            "Elliot Evans",
            "Daniel Jackson",
            "Arvind Satyanarayan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 37th Annual ACM Symposium on User Interface Software and Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157611",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Augmented Object Intelligence with XR-Objects",
        "abstract": "Seamless integration of physical objects as interactive digital entities remains a challenge for spatial computing. This paper explores Augmented Object Intelligence  (AOI) in the context of XR, an interaction paradigm that aims to blur the lines between digital and physical by equipping real-world objects with the ability to interact as if they were digital, where every object has the potential to serve as a portal to digital functionalities. Our approach utilizes real-time object segmentation and classification, combined with the power of Multimodal Large Language Models (MLLMs), to facilitate these interactions without the need for object pre-registration. We implement the AOI concept in the form of XR-Objects, an open-source prototype system that provides a platform for users to engage with their physical environment in contextually relevant ways using object-based context menus. This system enables analog objects to not only convey information but also to initiate digital actions, such as querying for details or executing tasks. Our contributions are threefold: (1) we define the AOI concept and detail its advantages over traditional AI assistants, (2) detail the XR-Objects  system’s open-source design and implementation, and (3) show its versatility through various use cases and a user study.",
        "authors": [
            "Mustafa Doga Dogan",
            "Eric Gonzalez",
            "Karan Ahuja",
            "Ruofei Du",
            "Andrea Cola?o",
            "Johnny Lee",
            "Mar Gonzalez-Franco",
            "David Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 37th Annual ACM Symposium on User Interface Software and Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157556",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "VizAbility: Enhancing Chart Accessibility with LLM-based Conversational Interaction",
        "abstract": "Traditional accessibility methods like alternative text and data tables typically underrepresent data visualization’s full potential. Keyboard-based chart navigation has emerged as a potential solution, yet efficient data exploration remains challenging. We present VizAbility, a novel system that enriches chart content navigation with conversational interaction, enabling users to use natural language for querying visual data trends. VizAbility adapts to the user’s navigation context for improved response accuracy and facilitates verbal command-based chart navigation. Furthermore, it can address queries for contextual information, designed to address the needs of visually impaired users. We designed a large language model (LLM)-based pipeline to address these user queries, leveraging chart data & encoding, user context, and external web knowledge. We conducted both qualitative and quantitative studies to evaluate VizAbility’s multimodal approach. We discuss further opportunities based on the results, including improved benchmark testing, incorporation of vision models, and integration with visualization workflows.",
        "authors": [
            "Joshua Gorniak",
            "Yoon Kim",
            "Donglai Wei",
            "Nam Wook Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 37th Annual ACM Symposium on User Interface Software and Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157607",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "MouthIO: Fabricating Customizable Oral User Interfaces with Integrated Sensing and Actuation",
        "abstract": "This paper introduces MouthIO, the first customizable intraoral user interface that can be equipped with various sensors and output components. MouthIO consists of an SLA-printed brace that houses a flexible PCB within a bite-proof enclosure positioned between the molar teeth and inner cheeks. Our MouthIO design and fabrication technique enables makers to customize the oral user interfaces in both form and function at low cost. All parts in contact with the oral cavity are made of bio-compatible materials to ensure safety, while the design takes into account both comfort and portability. We demonstrate MouthIO through three application examples ranging from beverage consumption monitoring, health monitoring, to assistive technology. Results from our full-day user study indicate high wearability and social acceptance levels, while our technical evaluation demonstrates the device’s ability to withstand adult bite forces.",
        "authors": [
            "Yijing Jiang",
            "Julia Kleinau",
            "Till Max Eckroth",
            "Eve Hoggan",
            "Stefanie Mueller",
            "Michael Wessely"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 37th Annual ACM Symposium on User Interface Software and Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157608",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "X-Hair: 3D Printing Hair-like Structures with Multi-form, Multi-property and Multi-function",
        "abstract": "In this paper, we present X-Hair, a method that enables 3D-printed hair with various forms, properties, and functions. We developed a two-step suspend printing strategy to fabricate hair-like structures in different forms (e.g. fluff, bristle, barb) by adjusting parameters including Extrusion Length Ratio and Total Length. Moreover, a design tool is also established for users to customize hair-like structures with various properties (e.g. pointy, stiff, soft) on imported 3D models, which virtually shows the results for previewing and generates G-code files for 3D printing. We demonstrate the design space of X-Hair and evaluate the properties of them with different parameters. Through a series of applications with hair-like structures, we validate X-hair’s practical usage of biomimicry, decoration, heat preservation, adhesion, and haptic interaction.",
        "authors": [
            "Guanyun Wang",
            "Junzhe Ji",
            "Yunkai Xu",
            "Lei Ren",
            "Xiaoyang Wu",
            "Chunyuan Zheng",
            "Xiaojing Zhou",
            "Xin Tang",
            "Boyu Feng",
            "Lingyun Sun",
            "Ye Tao",
            "Jiaji Li"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 37th Annual ACM Symposium on User Interface Software and Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157555",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "PortaChrome: A Portable Contact Light Source for Integrated Re-Programmable Multi-Color Textures",
        "abstract": "In this paper, we present PortaChrome, a portable light source that can be attached to everyday objects to reprogram the color and texture of surfaces that come in contact with them. When PortaChrome makes contact with objects previously coated with photochromic dye, the UV and RGB LEDs inside PortaChrome create multi-color textures on the objects. In contrast to prior work, which used projectors for the color-change, PortaChrome has a thin and flexible form factor, which allows the color-change process to be integrated into everyday user interaction. Because of the close distance between the light source and the photochromic object, PortaChrome creates color textures in less than 4 minutes on average, which is 8 times faster than prior work. We demonstrate PortaChrome with four application examples, including data visualizations on textiles and dynamic designs on wearables.",
        "authors": [
            "Yunyi Zhu",
            "Cedric Honnet",
            "Yixiao Kang",
            "Junyi Zhu",
            "Angelina Zheng",
            "Kyle Heinz",
            "Grace Tang",
            "Luca Musk",
            "Michael Wessely",
            "Stefanie Mueller"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 37th Annual ACM Symposium on User Interface Software and Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157610",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "WasteBanned: Supporting Zero Waste Fashion Design Through Linked Edits",
        "abstract": "The commonly used cut-and-sew garment construction process, in which 2D fabric panels are cut from sheets of fabric and assembled into 3D garments, contributes to widespread textile waste in the fashion industry. There is often a significant divide between the design of the garment and the layout of the panels. One opportunity for bridging this gap is the emerging study and practice of zero waste fashion design, which involves creating clothing designs with maximum layout efficiency. Enforcing the strict constraints of zero waste sewing is challenging, as edits to one region of the garment necessarily affect neighboring panels. Based on our formative work to understand this emerging area within fashion design, we present WasteBanned, a tool that combines CAM and CAD to help users prioritize efficient material usage, work within these zero waste constraints, and edit existing zero waste garment patterns. Our user evaluation indicates that our tool helps fashion designers edit zero waste patterns to fit different bodies and add stylistic variation, while creating highly efficient fabric layouts.",
        "authors": [
            "Ruowang Zhang",
            "Stefanie Mueller",
            "Gilbert Bernstein",
            "Adriana Schulz",
            "Mackenzie Leake"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 37th Annual ACM Symposium on User Interface Software and Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157606",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Dynamic event-triggered integrated task and motion planning for process-aware source seeking",
        "abstract": "The process-aware source seeking (PASS) problem in flow fields aims to find an informative trajectory to reach an unknown source location while taking the energy consumption in the flow fields into consideration. Taking advantage of the dynamic flow field partition technique, this paper formulates this problem as a task and motion planning (TAMP) problem and proposes a bi-level hierarchical planning framework to decouple the planning of inter-region transition and inner-region trajectory by introducing inter-region junctions. An integrated strategy is developed to enable efficient upper-level planning by investigating the optimal solution of the lower-level planner. In order to leverage the information acquisition and computational burden, a dynamic event-triggered mechanism is introduced to enable asynchronized estimation, region partitioning and re-plans. The proposed algorithm provides guaranteed convergence of the trajectory, and achieves automatic trade-offs of both exploration-exploitation and accuracy-efficiency. Simulations in a highly complicated and realistic ocean surface flow field validate the merits of the proposed algorithm, which demonstrates a significant reduction in computational burden without compromising planning optimality.",
        "authors": [
            "Yingke Li",
            "Mengxue Hou",
            "Enlu Zhou",
            "Fumin Zhang"
        ],
        "journal_conference_name": "Autonomous Robots",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157398",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Beyond the Holographic Entropy Cone via Cycle Flows",
        "abstract": "Motivated by bit threads, we introduce a new prescription for computing entropy vectors outside the holographic entropy cone. By utilizing cycle flows on directed graphs, we show that the maximum cycle flow associated to any subset of vertices, which corresponds to a subsystem, manifestly obeys purification symmetry. Furthermore, by restricting ourselves to a subclass of directed graphs, we prove that the maximum cycle flow obeys both subadditivity and strong subadditivity, thereby establishing it as a viable candidate for the entropy associated to the subsystem. Finally, we demonstrate how our model generalizes the entropy vectors obtainable via conventional flows in undirected graphs, as well as conjecture that our model similarly generalizes the entropy vectors arising from hypergraphs.",
        "authors": [
            "Temple He",
            "Sergio Hernández-Cuenca",
            "Cynthia Keeler"
        ],
        "journal_conference_name": "Communications in Mathematical Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/159036",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On Generalization Bounds for Neural Networks with Low Rank Layers",
        "abstract": "While previous optimization results have suggested that deep neural networks tend to favour low-rank weight matrices, the implications of this inductive bias on generalization bounds remain under-explored. In this paper, we apply a chain rule for Gaussian complexity (Maurer, 2016a) to analyze how low-rank layers in deep networks can prevent the accumulation of rank and dimensionality factors that typically multiply across layers. This approach yields generalization bounds for rank and spectral norm constrained networks. We compare our results to prior generalization bounds for deep networks, highlighting how deep networks with low-rank layers can achieve better generalization than those with full-rank layers. Additionally, we discuss how this framework provides new perspectives on the generalization capabilities of deep nets exhibiting neural collapse.",
        "authors": [
            "Andrea Pinto",
            "Akshay Rangamani",
            "Tomaso Poggio"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Center for Brains, Minds and Machines (CBMM)",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157263",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Black hole singularity from OPE",
        "abstract": "Eternal asymptotically AdS black holes are dual to thermofield double states in the boundary CFT. It has long been known that black hole singularities have certain signatures in boundary thermal two-point functions related to null geodesics bouncing off the singularities (bouncing geodesics). In this paper we shed light on the manifestations of black hole singularities in the dual CFT. We decompose the boundary CFT correlator of scalar operators using the Operator Product Expansion (OPE) and focus on the contributions from the identity, the stress tensor, and its products. We show that this part of the correlator develops singularities precisely at the points that are connected by bulk bouncing geodesics. Black hole singularities are thus encoded in the analytic behavior of the boundary correlators determined by multiple stress tensor exchanges. Furthermore, we show that in the limit where the conformal dimension of the operators is large, the sum of multi-stress-tensor contributions develops a branch point singularity as predicted by the geodesic analysis. We also argue that the appearance of complexified geodesics, which play an important role in computing the full correlator, is related to the contributions of the double-trace operators in the boundary CFT.",
        "authors": [
            "Nejc Čeplak",
            "Hong Liu",
            "Andrei Parnachev",
            "Samuel Valach"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157400",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "US federal resource allocations are inconsistent with concentrations of energy poverty",
        "abstract": "Recent data from the US Energy Information Administration reveals that nearly one in three households in the United States report experiencing energy poverty, and this number is only expected to rise. Federal assistance programs exist, but allocations across states have been nearly static since 1984, while the distribution of energy poverty is dynamic in location and time. We implement a LASSO-based machine learning approach using sociodemographic and geographical information to estimate energy burden in each US census tract for 2015 and 2020. We then compare the allocation to states from the Low Income Home Energy Assistance Program to an optimized allocation. We allocate funds to the most burdened households, providing them with enough assistance to reduce their energy expenditures so that their household energy burden is equal to a new maximum allowable energy burden. This markedly shifts funds from the northern cold-weather states to the southern warm-weather states.",
        "authors": [
            "Carlos Batlle",
            "Peter Heller",
            "Christopher Knittel",
            "Tim Schittekatte"
        ],
        "journal_conference_name": "Science Advances",
        "publisher": "American Association for the Advancement of Science",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158135",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Mapping Imaging Spectrometer for Europa (MISE)",
        "abstract": "Abstract The Mapping Imaging Spectrometer for Europa (MISE) is an infrared compositional instrument that will fly on NASA’s Europa Clipper mission to the Jupiter system. MISE is designed to meet the Level-1 science requirements related to the mission’s composition science objective to “understand the habitability of Europa’s ocean through composition and chemistry” and to contribute to the geology science and ice shell and ocean objectives, thereby helping Europa Clipper achieve its mission goal to “explore Europa to investigate its habitability.” MISE has a mass of 65 kg and uses an energy per flyby of 75.2 W-h. MISE will detect illumination from 0.8 to 5 μm with 10 nm spectral resolution, a spatial sampling of 25 m per pixel at 100 km altitude, and 300 cross-track pixels, enabling discrimination among the two principal states of water ice on Europa, identification of the main non-ice components of interest: salts, acids, and organics, and detection of trace materials as well as some thermal signatures. Furthermore, the spatial resolution and global coverage that MISE will achieve will be complemented by the higher spectral resolution of some Earth-based assets. MISE, combined with observations collected by the rest of the Europa Clipper payload, will enable significant advances in our understanding of how the large-scale structure of Europa’s surface is shaped by geological processes and inform our understanding of the surface at microscale. This paper describes the planned MISE science investigations, instrument design, concept of operations, and data products.",
        "authors": [
            "Diana L. Blaney",
            "Karl Hibbitts",
            "Serina Diniega",
            "Ashley G. Davies",
            "Roger N. Clark",
            "Robert O. Green",
            "Matthew Hedman",
            "Yves Langevin",
            "Jonathan Lunine",
            "Thomas B. McCord",
            "Scott Murchie",
            "Chris Paranicas",
            "Frank Seelos",
            "Jason M. Soderblom",
            "Morgan L. Cable"
        ],
        "journal_conference_name": "Space Science Reviews",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157393",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Machine Learning Approaches for the Prediction of Postoperative Major Complications in Patients Undergoing Surgery for Bowel Obstruction",
        "abstract": "first_pagesettingsOrder Article Reprints\r\nOpen AccessArticle\r\nMachine Learning Approaches for the Prediction of Postoperative Major Complications in Patients Undergoing Surgery for Bowel Obstruction\r\nby Alessandro D. Mazzotta 1,2ORCID,Elisa Burti 3,Francesco Andrea Causio 4,*ORCID,Alex Orlandi 5,Silvia Martinelli 4,Mattia Longaroni 6,Tiziana Pinciroli 7,Tarek Debs 8,Gianluca Costa 9ORCID,Michelangelo Miccini 10ORCID,Paolo Aurello 3 andNiccolò Petrucciani 3\r\n1\r\nDepartment of Surgery, Vannini General Hospital, Oncological and General Surgery, 00177 Rome, Italy\r\n2\r\nThe BioRobotics Institute, Sant’Anna School of Advanced Studies, 56127 Pisa, Italy\r\n3\r\nDepartment of Medical and Surgical Sciences and Translational Medicine, Division of General and Hepatobiliary Surgery, St. Andrea Hospital, Sapienza University of Rome, 00185 Roma, Italy\r\n4\r\nSection of Hygiene, Department of Life Sciences and Public Health, Università Cattolica del Sacro Cuore, 00168 Rome, Italy\r\n5\r\nEIT Digital Master School, Polytech Nice Sophia, 06410 Biot, France\r\n6\r\nDepartment of Surgery, Santa Maria della Misericordia Hospital, University of Perugia, 06123 Perugia, Italy\r\n7\r\nMIT Professional Education, Massachusetts Institute of Technology, Cambridge, MA 02139, USA\r\n8\r\nDépartement de Chirurgie Digestive, Centre Hospitalier Universitaire de Nice, CHU Nice, 06000 Nice, France\r\n9\r\nDepartment of Life Science, Health, and Health Professions, Link Campus University, 00165 Rome, Italy\r\n10\r\nDepartment of Surgery, Sapienza University of Rome, 00185 Roma, Italy\r\n*\r\nAuthor to whom correspondence should be addressed.\r\nJ. Pers. Med. 2024, 14(10), 1043; https://doi.org/10.3390/jpm14101043\r\nSubmission received: 27 July 2024 / Revised: 13 September 2024 / Accepted: 25 September 2024 / Published: 8 October 2024\r\n(This article belongs to the Special Issue Artificial Intelligence Applied to Clinical Practice)\r\nDownloadkeyboard_arrow_down Browse Figures Review Reports Versions Notes\r\n\r\nAbstract\r\nBackground: Performing emergency surgery for bowel obstruction continues to place a significant strain on the healthcare system. Conventional assessment methods for outcomes in bowel obstruction cases often concentrate on isolated factors, and the evaluation of results for individuals with bowel obstruction remains poorly studied. This study aimed to examine the risk factors associated with major postoperative complications. Methods: We retrospectively analyzed 99 patients undergoing surgery from 2015 to 2022. We divided the patients into two groups: (1) benign-related obstruction (n = 68) and (2) cancer-related obstruction (n = 31). We used logistic regression, KNN, and XGBOOST. We calculated the receiver operating characteristic curve and accuracy of the model. Results: Colon obstructions were more frequent in the cancer group (p = 0.005). Operative time, intestinal resection, and stoma were significantly more frequent in the cancer group. Major complications were at 41% for the cancer group vs. 20% in the benign group (p = 0.03). Uni- and multivariate analysis showed that the significant risk factors for major complications were cancer-related obstruction and CRP. The best model was KNN, with an accuracy of 0.82. Conclusions: Colonic obstruction is associated with tumor-related blockage. Malignant cancer and an increase in C-reactive protein (CRP) are significant risk factors for patients who have undergone emergency surgery due to major complications. KNN could improve the process of counseling and the perioperative management of patients with intestinal obstruction in emergency settings.",
        "authors": [
            "Alessandro D. Mazzotta",
            "Elisa Burti",
            "Francesco Andrea Causio",
            "Alex Orlandi",
            "Silvia Martinelli",
            "Mattia Longaroni",
            "Tiziana Pinciroli",
            "Tarek Debs",
            "Gianluca Costa",
            "Michelangelo Miccini",
            "Paolo Aurello",
            "Niccolò Petrucciani"
        ],
        "journal_conference_name": "Journal of Personalized Medicine",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157431",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Embedding human knowledge in material screening pipeline as filters to identify novel synthesizable inorganic materials",
        "abstract": "How might one embed a chemist's knowledge into an automated materials-discovery pipeline? In generative design for inorganic crystalline materials, generating candidate compounds is no longer a bottleneck – there are now synthetic datasets of millions of compounds. However, weeding out unsynthesizable or difficult to synthesize compounds remains an outstanding challenge. Post-generation “filters” have been proposed as a means of embedding human domain knowledge, either in the form of scientific laws or rules of thumb. Examples include charge neutrality, electronegativity balance, and energy above hull. Some filters are “hard” and some are “soft” — for example, it is difficult to envision creating a stable compound while violating the rule of charge neutrality; however, several compounds break the Hume-Rothery rules. It is therefore natural to wonder: can one compile a comprehensive list of “filters” that embed domain knowledge, adopt a principled approach to classifying them as either non-conditional or conditional “filters,” and envision a software environment to implement combinations of these in a systematic manner? In this commentary we explore such questions, “filters” for screening of novel inorganic compounds for synthesizability.",
        "authors": [
            "Basita Das",
            "Kangyu Ji",
            "Fang Sheng",
            "Kyle M McCall",
            "Tonio Buonassisi"
        ],
        "journal_conference_name": "Faraday Discussions",
        "publisher": "Royal Society of Chemistry",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157460",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Cooperative Advisory Residual Policies for Congestion Mitigation",
        "abstract": "Fleets of autonomous vehicles can mitigate traffic congestion through simple actions, thus improving many socioeconomic factors such as commute time and gas costs. However, these approaches are limited in practice as they assume precise control over autonomous vehicle fleets, incur extensive installation costs for a centralized sensor ecosystem, and also fail to account for uncertainty in driver behavior. To this end, we develop a class of learned residual policies that can be used in cooperative advisory systems and only require the use of a single vehicle with a human driver. Our policies advise drivers to behave in ways that mitigate traffic congestion while accounting for diverse driver behaviors, particularly drivers? reactions to instructions, to provide an improved user experience. To realize such policies, we introduce an improved reward function that explicitly addresses congestion mitigation and driver attitudes to advice. We show that our residual policies can be personalized by conditioning them on an inferred driver trait that is learned in an unsupervised manner with a variational autoencoder. Our policies are trained in simulation with our novel instruction adherence driver model, and evaluated in simulation and through a user study (N=16) to capture the sentiments of human drivers. Our results show that our approaches successfully mitigate congestion while adapting to different driver behaviors, with up to 20% and 40% improvement as measured by a combination metric of speed and deviations in speed across time over baselines in our simulation tests and user study, respectively. Our user study further shows that our policies are human-compatible and personalize to drivers.",
        "authors": [
            "Aamir Hasan",
            "Neeloy Chakraborty",
            "Haonan Chen",
            "Jung-Hoon Cho",
            "Cathy Wu",
            "Katherine Driggs-Campbell"
        ],
        "journal_conference_name": "ACM Journal on Autonomous Transportation Systems",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157542",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Machine learning-guided discovery of gas evolving electrode bubble inactivation",
        "abstract": "The adverse effects of electrochemical bubbles on the performance of gas-evolving electrodes are well known, but studies on the degree of adhered bubble-caused inactivation, and how inactivation changes during bubble evolution are limited. We study electrode inactivation caused by oxygen evolution while using surface engineering to control bubble formation. We find that the inactivation of the entire projected area, as is currently believed, is a poor approximation which leads to non-physical results. Using a machine learning-based image-based bubble detection method to analyze large quantities of experimental data, we show that bubble impacts are small for surface engineered electrodes which promote high bubble projected areas while maintaining low direct bubble contact. We thus propose a simple methodology for more accurately estimating the true extent of bubble inactivation, which is closer to the area which is directly in contact with the bubbles.",
        "authors": [
            "Jack R Lake",
            "Simon Rufer",
            "Jim James",
            "Nathan Pruyne",
            "Aristana Scourtas",
            "Marcus Schwarting",
            "Aadit Ambadkar",
            "Ian Foster",
            "Ben Blaiszik",
            "Kripa K Varanasi"
        ],
        "journal_conference_name": "Nanoscale",
        "publisher": "Royal Society of Chemistry",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157527",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Quantitative and spatially resolved detection of multiplexed microRNA from plant tissue via hybridization to hydrogel-bound DNA probes in nanoliter well arrays",
        "abstract": "Understanding complex regulatory networks in plant systems requires elucidating the roles of various gene regulators under a spatial landscape. MicroRNA are key regulators that impart high information value through their tissue specificity and stability when using expression patterns for evaluating network outcomes. However, current techniques that utilize spatial multiplexing and quantitation of microRNA are limited to primarily mammalian systems. Here, we present a method to spatially resolve and quantify multiple endogenous microRNA in situ using ethanol fixed, paraffin embedded model plant species. This method utilizes target-specific microRNA capture along with universal ligating and labelling, all within functionalized hydrogel posts containing DNA probes in nanoliter well arrays. We demonstrate the platform’s multiplexing capabilities through analyzing three endogenous microRNA in Arabidopsis thaliana rosettes which provide useful answers to fundamental plant growth and development from the unique expression patterns. The spatial tissue technique is also validated using non-spatial small RNA assays to demonstrate the versatility of the well array platform. Our new platform expands the toolkit of spatial omics technologies for plants.",
        "authors": [
            "Jennifer Fang",
            "Patrick S Doyle"
        ],
        "journal_conference_name": "Microsystems & Nanoengineering",
        "publisher": "Springer Science and Business Media LLC",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158254",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Fabrication Strategies for 2D Halide Perovskite Towards Next-Generation Optoelectronic Applications",
        "abstract": "Halide perovskites have emerged as promising materials in high-performance optoelectronics due to their exceptional optoelectrical properties, such as long carrier lifetime and tunable bandgap. Despite the promising capabilities of three-dimensional (3D) halide perovskites in applications like solar cells and light-emitting diodes, their operational stability remains a critical challenge. This review focuses on quasi-two-dimensional (2D) halide perovskites, which offer enhanced stability through their reduced dimensionality. We discuss the unique properties of these materials, including the ability to modify optical and electronic characteristics by altering the organic cations and the layer number in the perovskite structure. Additionally, we review various fabrication techniques, highlighting the shift from traditional low-temperature solution processes to more advanced solid, liquid, and vapor-phase methods, which address the limitations of conventional fabrication and enhance material quality. This comprehensive review aims to provide insights into the development of stable and efficient 2D halide perovskite-based optoelectronic devices, paving the way for their integration into next-generation optoelectronic applications.",
        "authors": [
            "Seong H. Cho",
            "Yonghoon Jung",
            "Yeoun-Woo Jang",
            "Hyemin Kim",
            "Jaehyeon Kim",
            "Changhyun Lim",
            "Ki-Tae Park",
            "Seongheon Kim",
            "Young H. Chu",
            "Taehoon Kim",
            "Jieun Lee",
            "Changhee Lee",
            "Junhyoung Park",
            "Kyung T. Yoon",
            "Dongguen Eom"
        ],
        "journal_conference_name": "International Journal of Precision Engineering and Manufacturing-Green Technology",
        "publisher": "Korean Society for Precision Engineering",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157404",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Unexpected anthropogenic emission decreases explain recent atmospheric mercury concentration declines",
        "abstract": "Anthropogenic activities emit ~2,000 Mg y−1 of the toxic pollutant mercury (Hg) into the atmosphere, leading to long-range transport and deposition to remote ecosystems. Global anthropogenic emission inventories report increases in Northern Hemispheric (NH) Hg emissions during the last three decades, in contradiction with the observed decline in atmospheric Hg concentrations at NH measurement stations. Many factors can obscure the link between anthropogenic emissions and atmospheric Hg concentrations, including trends in the reemissions of previously released anthropogenic (“legacy”) Hg, atmospheric sink variability, and spatial heterogeneity of monitoring data. Here, we assess the observed trends in gaseous elemental mercury (Hg0) in the NH and apply biogeochemical box modeling and chemical transport modeling to understand the trend drivers. Using linear mixed effects modeling of observational data from 51 stations, we find negative Hg0 trends in most NH regions, with an overall trend for 2005 to 2020 of −0.011 ± 0.006 ng m−3 y−1 (±2 SD). In contrast to existing emission inventories, our modeling analysis suggests that annual NH anthropogenic emissions must have declined by at least 140 Mg between the years 2005 and 2020 to be consistent with observed trends. Faster declines in 95th percentile Hg0 values than median values in Europe, North America, and East Asian measurement stations corroborate that the likely cause is a decline in nearby anthropogenic emissions rather than background legacy reemissions. Our results are relevant for evaluating the effectiveness of the Minamata Convention on Mercury, demonstrating that existing emission inventories are incompatible with the observed Hg0 declines.",
        "authors": [
            "Aryeh Feinberg",
            "Noelle E. Selin",
            "Christine F. Braban",
            "Kai-Lan Chang",
            "Danilo Custódio",
            "Daniel A. Jaffe",
            "Katriina Kyllönen",
            "Matthew S. Landis",
            "Sarah R. Leeson",
            "Winston Luke",
            "Koketso M. Molepo",
            "Marijana Murovec",
            "Michelle G. Nerentorp Mastromonaco",
            "Katrine Aspmo Pfaffhuber",
            "Julian Rüdiger",
            "Guey-Rong Sheu",
            "Vincent L. St. Louis"
        ],
        "journal_conference_name": "Proceedings of the National Academy of Sciences of the United States of America",
        "publisher": "Proceedings of the National Academy of Sciences",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157377",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Search for Higgs boson pair production with one associated vector boson in proton-proton collisions at √s = 13 TeV",
        "abstract": "Abstract A search for Higgs boson pair (HH) production in association with a vector boson V (W or Z boson) is presented. The search is based on proton-proton collision data at a center-of-mass energy of 13 TeV, collected with the CMS detector at the LHC, corresponding to an integrated luminosity of 138 fb−1. Both hadronic and leptonic decays of V bosons are used. The leptons considered are electrons, muons, and neutrinos. The HH production is searched for in the b b ¯ b b ¯ decay channel. An observed (expected) upper limit at 95% confidence level of VHH production cross section is set at 294 (124) times the standard model prediction. Constraints are also set on the modifiers of the Higgs boson trilinear self-coupling, kλ, assuming k2V = 1, and vice versa on the coupling of two Higgs bosons with two vector bosons, k2V. The observed (expected) 95% confidence intervals of these coupling modifiers are −37.7 < kλ < 37.2 (−30.1 < kλ < 28.9) and −12.2 < k2V < 13.5 (−7.2 < k2V < 8.9), respectively.",
        "authors": [
            "A. Hayrapetyan",
            "A. Tumasyan",
            "W. Adam",
            "J. W. Andrejkovic",
            "T. Bergauer",
            "S. Chatterjee",
            "K. Damanakis",
            "M. Dragicevic",
            "A. Escalante Del Valle",
            "P. S. Hussain",
            "M. Jeitler",
            "N. Krammer",
            "D. Liko",
            "I. Mikulec",
            "J. Schieck",
            "R. Schöfbeck",
            "D. Schwarz"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157401",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Understanding and training for the impact of large language models and artificial intelligence in healthcare practice: a narrative review",
        "abstract": "Reports of Large Language Models (LLMs) passing board examinations have spurred medical enthusiasm for their clinical integration. Through a narrative review, we reflect upon the skill shifts necessary for clinicians to succeed in an LLM-enabled world, achieving benefits while minimizing risks. We suggest how medical education must evolve to prepare clinicians capable of navigating human-AI systems.",
        "authors": [
            "Liam G. McCoy",
            "Faye Y. Ci Ng",
            "Christopher M. Sauer",
            "Katelyn E. Yap Legaspi",
            "Bhav Jain",
            "Jack Gallifant",
            "Michael McClurkin",
            "Alessandro Hammond",
            "Deirdre Goode",
            "Judy Gichoya",
            "Leo A. Celi"
        ],
        "journal_conference_name": "BMC Medical Education",
        "publisher": "BioMed Central",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157403",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Formation of Representations in Neural Networks",
        "abstract": "Understanding neural representations will help open the black box of neural networks and advance our scientific understanding of modern AI systems. However, how complex, structured, and transferable representations emerge in modern neural networks has remained a mystery. Building on previous results, we propose the Canonical Representation Hypothesis (CRH), which posits a set of six alignment relations to universally govern the formation of representations in most hidden layers of a neural network. Under the CRH, the latent representations (R), weights (W), and neuron gradients (G) become mutually aligned during training. This alignment implies that neural networks naturally learn compact representations, where neurons and weights are invariant to task-irrelevant transformations. We then show that the breaking of CRH leads to the emergence of reciprocal power-law relations between R, W, and G, which we refer to as the Polynomial Alignment Hypothesis (PAH). We present a minimal-assumption theory demonstrating that the balance between gradient noise and regularization is crucial for the emergence the canonical representation. The CRH and PAH lead to an exciting possibility of unifying major key deep learning phenomena, including neural collapse and the neural feature ansatz, in a single framework.",
        "authors": [
            "Liu Ziyin",
            "Isaac Chuang",
            "Tomer Galanti",
            "Tomaso Poggio"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Center for Brains, Minds and Machines (CBMM)",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157132",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Global optimization: a machine learning approach",
        "abstract": "Many approaches for addressing global optimization problems typically rely on relaxations of nonlinear constraints over specific mathematical primitives. This is restricting in applications with constraints that are implicit or consist of more general primitives. Trying to address such limitations, Bertsimas and Ozturk (2023) proposed OCTHaGOn as a way of solving very general global optimization problems by approximating the nonlinear constraints using hyperplane-based decision-trees and then using those trees to construct a unified MIO approximation of the original problem. We provide extensions to this approach, by (i) approximating the original problem using other MIO-representable ML models besides decision trees, such as gradient boosted trees, multi layer perceptrons and suport vector machines (ii) proposing adaptive sampling procedures for more accurate ML-based constraint approximations, (iii) utilizing robust optimization to account for the uncertainty of the sample-dependent training of the ML models, (iv) leveraging a family of relaxations to address the infeasibilities of the final MIO approximation. We then test the enhanced framework in 81 global optimization instances. We show improvements in solution feasibility and optimality in the majority of instances. We also compare against BARON, showing improved optimality gaps and solution times in more than 9 instances.",
        "authors": [
            "Dimitris Bertsimas",
            "Georgios Margaritis"
        ],
        "journal_conference_name": "Journal of Global Optimization",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157394",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Glioblastoma-cortical organoids recapitulate cell state heterogeneity and intercellular transfer",
        "abstract": "Glioblastoma is characterized by heterogeneous malignant cells that are functionally integrated within the neuroglial microenvironment. Here, we model this ecosystem by growing glioblastoma into long-term cultured human cortical organoids that contain the major neuroglial cell types found in the cerebral cortex. Single-cell RNA-seq analysis suggests that, compared to matched gliomasphere models, glioblastoma cortical organoids (GCO) more faithfully recapitulate the diversity and expression programs of malignant cell states found in patient tumors. Additionally, we observe widespread transfer of glioblastoma transcripts and GFP proteins to non-malignant cells in the organoids. Mechanistically, this transfer involves extracellular vesicles and is biased towards defined glioblastoma cell states and astroglia cell types. These results extend previous glioblastoma-organoid modeling efforts and suggest widespread intercellular transfer in the glioblastoma neuroglial microenvironment.",
        "authors": [
            "Vamsi Mangena",
            "Rony Chanoch-Myers",
            "Rafaela Sartore",
            "Bruna Paulsen",
            "Simon Gritsch",
            "Hannah Weisman",
            "Toshiro Hara",
            "Xandra O Breakefield",
            "Koen Breyne",
            "Aviv Regev",
            "Kwanghun Chung",
            "Paola Arlotta",
            "Itay Tirosh",
            "Mario L Suva"
        ],
        "journal_conference_name": "Cancer Discovery",
        "publisher": "American Association for Cancer Research",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157902",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Tagged deep inelastic scattering measurement on deuterium with the LAD experiment",
        "abstract": "The origin of the modification of the quark structure of nucleons in the nuclear medium can be tested with tagged recoil nucleon measurements from deep inelastic scattering off electrons on deuterium. The LAD experiment at the Thomas Jefferson National Laboratory (JLab) will measure the modification of the neutron structure function for high-momentum, highly-virtual neutrons by measuring the spectator recoil protons in coincidence with the scattered electron. An update on the experimental setup and projected results is presented. The experiment will collect data in Fall 2024.",
        "authors": [
            "F. Hauenstein",
            "C. Ayerbe Gayoso",
            "S. Ratliff",
            "H. Szumila-Vance",
            "A. Schmidt",
            "L. Ehinger",
            "O. Hen",
            "D. Higinbotham",
            "I. Korover",
            "T. Kutz",
            "D. Nguyen",
            "E. Piasetzky",
            "L. B. Weinstein"
        ],
        "journal_conference_name": "The European Physical Journal A",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/159166",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "CH4 and CO2 Reductions from Methanol Production Using Municipal Solid Waste Gasification with Hydrogen Enhancement",
        "abstract": "This study evaluates the greenhouse gas (GHG) impacts of converting municipal solid waste (MSW) into methanol, focusing on both landfill methane (CH<sub>4</sub>) emission avoidance and the provision of cleaner liquid fuels with lower carbon intensity. We conduct a life cycle assessment (LCA) to assess potential GHG reductions from MSW gasification to methanol, enhanced with hydrogen produced via natural gas pyrolysis or water electrolysis. Hydrogen enhancement effectively doubles the methanol yield from a given amount of MSW. Special attention is given to hydrogen production through natural gas pyrolysis due to its potential for lower-cost hydrogen and reduced reliance on renewable electricity compared to electrolytic hydrogen. Our analysis uses a case study of methanol production from an oxygen-fired entrained flow gasifier fed with refuse-derived fuel (RDF) simulated in Aspen HYSYS. The LCA incorporates the significant impact of landfill methane avoidance, particularly when considering the 20-year global warming potential (GWP). Based on the LCA, the process has illustrative net GHG emissions of 183 and 709 kgCO<sub>2e</sub>/t MeOH using renewable electricity for electrolytic hydrogen and pyrolytic hydrogen, respectively, for the 100-year GWP. The net GHG emissions using 20-year GWP are &minus;1222 and &minus;434 kgCO<sub>2e</sub>/t MeOH, respectively. Additionally, we analyze the sensitivity of net GHG emissions to varying levels of fugitive methane emissions.",
        "authors": [
            "Mohammad Ostadi",
            "Daniel R. Cohn",
            "Guiyan Zang",
            "Leslie Bromberg"
        ],
        "journal_conference_name": "sustainability",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157320",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploring Potential Application Areas of Artificial Intelligence-Infused System for Engagement Recognition: Insights from Special Education Experts",
        "abstract": "Active engagement where children with autism spectrum disorder (ASD) are involved (e.g., educational and social activities) plays a crucial role in enhancing their cognitive, motor, and social development. This offers opportunities to enhance overall development, including learning abilities, physical coordination, and social interactions. Indirect methods, leveraging sensors and artificial intelligence (AI), have exhibited potential for enhancing engagement predictions but have been primarily focused within specific fields, resulting in a gap that leads to limited generalizability of ASD studies. This gap, due to small ASD sample sizes, presents a significant challenge as the annual ASD population increases, highlighting the need for practical and applicable research solutions, especially for general learning. In this work, we conducted expert interviews to explore the potential application areas of AI-infused systems that provide three levels of engagement status for children with ASD, ranging from \"not engaged and out of control\" to \"highly engaged.\" Interviews with special educators revealed five key application areas for AI-driven engagement recognition: social skills training, stereotyped behavior modification, support for leisure activities, effective tutoring, and independent daily living skills. These findings highlight the potential of adaptive AI interventions in improving educational and daily outcomes, advocating for expanded applications for children with ASD.",
        "authors": [
            "Won Kim",
            "Minwoo Seong",
            "Joseph DelPreto",
            "Wojciech Matusik",
            "Daniela Rus",
            "SeungJun Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Companion of the 2024 ACM International Joint Conference on Pervasive and Ubiquitous Computing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157623",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "9th International Workshop on Mental Health and Well-being: New Research Directions",
        "abstract": "Mental health and well-being influence overall health: suffering from a mental illness can create severe impairment and reduce quality of life. Ubiquitous computing technologies are beginning to play a central role in collecting clinically relevant behavioral and physiological information on mental health that can be used to detect symptoms early-on, deliver preventative interventions, and manage symptoms throughout the course of illness. Despite this potential, designing and translating ubiquitous technologies into mental healthcare is a complex process, and existing technologies have faced numerous challenges towards effective implementation. The goal of this workshop is to bring together researchers, practitioners, and industry professionals to identify, articulate, and address the challenges of designing and implementing ubiquitous computing technologies in mental healthcare. Given these challenges, we are adding a specific call for papers that inspire new research directions, with initial findings that are valuable to the community, but are not fully publishable or finished contributions. Following the success of this workshop for the last eight years, we aim to continue facilitating the UbiComp community in both the conceptualization, translation, and implementation of novel mental health sensing and intervention technologies.",
        "authors": [
            "Daniel Adler",
            "Xuhai Xu",
            "Asif Salekin",
            "Varun Mishra",
            "Hyeokhyen Kwon",
            "Akane Sano",
            "Saeed Abdullah",
            "Jakob Bardram",
            "Yiran Zhao",
            "Manasa Kalanadhabhatta",
            "Han Zhang",
            "Elizabeth Murnane",
            "Tanzeem Choudhury",
            "Mirco Musolesi",
            "Tauhidur Rahman",
            "Zachary King",
            "Rony Krell",
            "Simon D'Alfonso"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Companion of the 2024 ACM International Joint Conference on Pervasive and Ubiquitous Computing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157620",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Intelligent Seat: Tactile Signal-Based 3D Sitting Pose Inference",
        "abstract": "Owing to people spending a large portion of their day sitting while working, commuting, or relaxing, monitoring their sitting posture is crucial for the development of adaptive interventions that respond to the user's pose, state, and behavior. This is because posture is closely linked to actions, health, attention, and engagement levels. The existing systems for posture estimation primarily use computer vision-based measurements or body-attached sensors; however, they are plagued by challenges such as privacy concerns, occlusion issues, and user discomfort. To address these drawbacks, this study proposed a posture-inference system that uses high-density piezoresistive sensors for joint reconstruction. Tactile pressure data were collected from six individuals, each performing seven different postures 20 times. The proposed system achieved an average L2 distance of 20.2 cm in the joint position reconstruction with a posture classification accuracy of 96.3%. Future research will focus on the development of a system capable of providing real-time feedback to help users maintain the correct sitting posture.",
        "authors": [
            "Minwoo Seong",
            "Gwangbin Kim",
            "Jaehee Lee",
            "Joseph DelPreto",
            "Wojciech Matusik",
            "Daniela Rus",
            "SeungJun Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Companion of the 2024 ACM International Joint Conference on Pervasive and Ubiquitous Computing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157621",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "High-speed grinding: from mechanism to machine tool",
        "abstract": "High-speed grinding (HSG) is an advanced technology for precision machining of difficult-to-cut materials in aerospace and other fields, which could solve surface burns, defects and improve surface integrity by increasing the linear speed of the grinding wheel. The advantages of HSG have been preliminarily confirmed and the equipment has been built for experimental research, which can achieve a high grinding speed of more than 300 m/s. However, it is not yet widely used in manufacturing due to the insufficient understanding on material removal mechanism and characteristics of HSG machine tool. To fill this gap, this paper provides a comprehensive overview of HSG technologies. A new direction for adding auxiliary process in HSG is proposed. Firstly, the combined influence law of strain hardening, strain rate intensification, and thermal softening effects on material removal mechanism was revealed, and models of material removal strain rate, grinding force and grinding temperature were summarized. Secondly, the constitutive models under high strain rate boundaries were summarized by considering various properties of material and grinding parameters. Thirdly, the change law of material removal mechanism of HSG was revealed when the thermodynamic boundary conditions changed, by introducing lubrication conditions such as minimum quantity lubrication (MQL), nano-lubricant minimum quantity lubrication (NMQL) and cryogenic air (CA). Finally, the mechanical and dynamic characteristics of the key components of HSG machine tool were summarized, including main body, grinding wheel, spindle and dynamic balance system. Based on the content summarized in this paper, the prospect of HSG is put forward. This study establishes a solid foundation for future developments in the field and points to promising directions for further exploration.",
        "authors": [
            "Yu-Long Wang",
            "Yan-Bin Zhang",
            "Xin Cui",
            "Xiao-Liang Liang",
            "Run-Ze Li",
            "Ruo-Xin Wang",
            "Shubham Sharma",
            "Ming-Zheng Liu",
            "Teng Gao",
            "Zong-Ming Zhou",
            "Xiao-Ming Wang",
            "Yusuf S. Dambatta",
            "Chang-He Li"
        ],
        "journal_conference_name": "Advances in Manufacturing",
        "publisher": "Shanghai University",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157395",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Impact of Acoustic and Optical Phonons on the Anisotropic Heat Conduction in Novel C-Based Superlattices",
        "abstract": "C-based XC binary materials and their (XC)m/(YC)n (X, Y ≡ Si, Ge and Sn) superlattices (SLs) have recently gained considerable interest as valuable alternatives to Si for designing and/or exploiting nanostructured electronic devices (NEDs) in the growing high-power application needs. In commercial NEDs, heat dissipation and thermal management have been and still are crucial issues. The concept of phonon engineering is important for manipulating thermal transport in low-dimensional heterostructures to study their lattice dynamical features. By adopting a realistic rigid-ion-model, we reported results of phonon dispersions ωSLj(k→) of novel short−period (XC)m/(YC)n[001] SLs\r\n, for m, n = 2, 3, 4 by varying phonon wavevectors |k→SL|\r\n along the growth k||\r\n ([001]), and in-plane k⊥\r\n ([100], [010]) directions. The SL phonon dispersions displayed flattening of modes, especially at high-symmetry critical points Γ, Z and M. Miniband formation and anti-crossings in ωSLj(k→)\r\n lead to the reduction in phonon conductivity κz\r\n along the growth direction by an order of magnitude relative to the bulk materials. Due to zone-folding effects, the in-plane phonons in SLs exhibited a strong mixture of XC-like and YC-like low-energy ωTA\r\n, ωLA\r\n modes with the emergence of stop bands at certain |k→SL|\r\n. For thermal transport applications, the results demonstrate modifications in thermal conductivities via changes in group velocities, specific heat, and density of states.",
        "authors": [
            "Devki N. Talwar",
            "Piotr Becla"
        ],
        "journal_conference_name": "materials",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157319",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Proposal of a Framework for Enhancing Teleoperation Experience with Biomechanical Simulation-Based Electrical Muscle Stimulation in Virtual Reality",
        "abstract": "Teleoperation, the remote manual control of robots, is primarily used in high-precision and safety-critical environments such as surgery, space exploration, and deep-sea exploration. Despite being a widely utilized technology, teleoperation relies on human cognitive abilities, leading to significant cognitive load for operators. To address this challenge, we propose a concept of a VR teleoperation haptic system that combines biomechanical simulation and electrical muscle stimulation to provide force feedback in a lightweight, wearable form by mimicking natural force generation without the need for external actuators. Our system is divided into two main components: the physical simulation part, which calculates the joint torques to replicate forces from the manipulator, and the electrical stimulation part, which translates torques into muscle stimulations. Through this integration, we expect our system to bridge the gulf of execution and evaluation, reducing cognitive load and enhancing teleoperation performance. This paper aims to discuss the detailed framework of our system and potential future research directions.",
        "authors": [
            "Seokhyun Hwang",
            "Seongjun Kang",
            "Jeongseok Oh",
            "Jeongju Park",
            "Semoo Shin",
            "Yiyue Luo",
            "Joseph DelPreto",
            "Wojciech Matusik",
            "Daniela Rus",
            "SeungJun Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Companion of the 2024 ACM International Joint Conference on Pervasive and Ubiquitous Computing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157624",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "LegSense: Inducing Walking Sensation in Seated VR by Providing Movement Illusion via Electrical Muscle Stimulation",
        "abstract": "Providing convincing proprioceptive cues is essential for immersive virtual reality (VR) navigation. However, this is challenging for seated users with restricted mobility. To address this gap, this study proposes LegSense, a method designed to induce the walking sensation in VR via electrical muscle stimulation (EMS). This method activates the leg muscle senses in sync with the gait cycle without requiring physical motion to enhance users' immersion. We evaluated the efficacy of LegSense through a user study and confirmed its potential in terms of walking sensation, embodiment, and presence compared to other static conditions (baseline and vibro-tactile). Additionally, participant interviews confirmed that LegSense effectively creates a leg movement illusion, suggesting its potential applications in diverse virtual scenarios to enhance VR experiences for seated users.",
        "authors": [
            "Juwon Um",
            "Eunki Jeon",
            "Yumin Kang",
            "Seongjun Kang",
            "Ahmed Elsharkawy",
            "Joseph DelPreto",
            "Wojciech Matusik",
            "Daniela Rus",
            "SeungJun Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Companion of the 2024 ACM International Joint Conference on Pervasive and Ubiquitous Computing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157622",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Adaptive In-Vehicle Virtual Reality for Reducing Motion Sickness: Manipulating Passenger Posture During Driving Events",
        "abstract": "The rise of autonomous vehicles (AVs) has promoted the adoption of in-vehicle virtual reality (VR) for creating immersive experiences. However, these experiences can trigger motion sickness (MS) due to visual-vestibular mismatches. Traditional techniques, such as visual matching and scene manipulation, address MS but often neglect the impact of body posture changes. This study examines the effects of interactive VR tasks on passenger body posture during MS-inducing events, including turns and vertical displacements. Our findings reveal significant variations in user body postures relative to conditions with event-based designed interactive VR tasks, resulting in a reduction of MS symptoms. Specifically, participants engaged in interactive VR tasks showed improved posture alignment and body stability. These insights offer practical guidelines for developing adaptive VR content that proactively manages posture to alleviate MS, thereby enhancing passenger comfort in in-vehicle VR applications.",
        "authors": [
            "Ahmed Elsharkawy",
            "Aya Ataya",
            "Dohyeon Yeo",
            "Minwoo Seong",
            "Seokhyun Hwang",
            "Joseph DelPreto",
            "Wojciech Matusik",
            "Daniela Rus",
            "SeungJun Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Companion of the 2024 ACM International Joint Conference on Pervasive and Ubiquitous Computing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157625",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Deformed Fréchet law for Wigner and sample covariance matrices with tail in crossover regime",
        "abstract": "Given A n : = 1 n ( a ij ) an n × n symmetric random matrix, with elements above the diagonal given by i.i.d. random variables having mean zero and unit variance. It is known that when lim x → ∞ x 4 P ( | a ij | > x ) = 0 , then fluctuation of the largest eigenvalue of A n follows a Tracy–Widom distribution. When the law of a ij is regularly varying with index α ∈ ( 0 , 4 ) , then the largest eigenvalue has a Fréchet distribution. An intermediate regime is recently uncovered in Diaconu (Ann Probab 51(2):774–804, 2023): when lim x → ∞ x 4 P ( | a ij | > x ) = c ∈ ( 0 , ∞ ) , then the law of the largest eigenvalue converges to a deformed Fréchet distribution. In this work we vastly extend the scope where the latter distribution may arise. We show that the same deformed Fréchet distribution arises (1) for sparse Wigner matrices with an average of n Ω ( 1 ) nonzero entries on each row; (2) for periodically banded Wigner matrices with bandwidth p n = n O ( 1 ) ; and more generally for weighted adjacency matrices of any k n -regular graphs with k n = n Ω ( 1 ) . In all these cases, we further prove that the joint distribution of the finitely many largest eigenvalues of A n converge to a deformed Poisson process, and that eigenvectors of the outlying eigenvalues of A n are localized, implying a mobility edge phenomenon at the spectral edge 2 for Wigner matrices. The sparser case with average degree n o ( 1 ) is also explored. Our technique extends to sample covariance matrices, proving for the first time that its largest eigenvalue still follows a deformed Fréchet distribution, assuming the matrix entries satisfy lim x → ∞ x 4 P ( | a ij | > x ) = c ∈ ( 0 , ∞ ) . The proof utilizes a universality result recently established by Brailovskaya and Van Handel (Universality and sharp matrix concentration inequalities, 2022).",
        "authors": [
            "Yi Han",
            "Yi Han"
        ],
        "journal_conference_name": "Probability Theory and Related Fields",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157265",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Engineering synthetic and recombinant human lysosomal β-glucocerebrosidase for enzyme replacement therapy for Gaucher disease",
        "abstract": "Gaucher Disease (GD) is an autosomal recessive, lysosomal storage disease caused by pathogenic variants in the glucocerebrosidase gene, leading to the loss of β-glucocerebrosidase (GCase) enzymatic activity. Enzyme replacement therapy (ERT) with recombinant GCase is the standard of care in GD patients. Our study investigates the combined use of in silico molecular evolution, synthetic biology and gene therapy approaches to develop a new synthetic recombinant enzyme. We engineered four GCases containing missense mutations in the signal peptide (SP) from four selected mammalian species, and compared them with human GCase without missense mutations in the SP. We investigated transcriptional regulation with CMV and hEF1a promoters alongside a GFP control construct in 293-FT human cells. One hEF1a-driven mutant GCase shows a 5.2-fold higher level of transcription than control GCase. In addition, this mutant exhibits up to a sixfold higher activity compared with the mock-control, and the predicted tertiary structure of this mutant GCase aligns with human GCase. We also evaluated conserved and coevolved residues mapped to functionally important positions. Further studies are needed to assess its functionality in a GD animal model. Altogether, our findings provide in vitro evidence of the potential of this engineered enzyme for improved therapeutic effects for GD.",
        "authors": [
            "Lílian L. S. Figueiredo",
            "Wilson L. Junior",
            "Victor W. da Silva Goncalves",
            "Ester S. Ramos",
            "Vania D’Almeida",
            "Lucas E. B. de Souza",
            "Maristela D. Orellana",
            "Kuruvilla J. Abraham",
            "Flávio Lichtenstein",
            "Lucas Bleicher"
        ],
        "journal_conference_name": "Discover Applied Sciences",
        "publisher": "Springer International Publishing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157396",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Real-time chiral dynamics at finite temperature from quantum simulation",
        "abstract": "In this study, we explore the real-time dynamics of the chiral magnetic effect (CME) at a finite temperature in the (1+1)-dimensional QED, the massive Schwinger model. By introducing a chiral chemical potential μ5 through a quench process, we drive the system out of equilibrium and analyze the induced vector currents and their evolution over time. The Hamiltonian is modified to include the time-dependent chiral chemical potential, thus allowing the investigation of the CME within a quantum computing framework. We employ the quantum imaginary time evolution (QITE) algorithm to study the thermal states, and utilize the Suzuki-Trotter decomposition for the real-time evolution. This study provides insights into the quantum simulation capabilities for modeling the CME and offers a pathway for studying chiral dynamics in low-dimensional quantum field theories.",
        "authors": [
            "Kazuki Ikeda",
            "Zhong-Bo Kang",
            "Dmitri E. Kharzeev",
            "Wenyang Qian",
            "Fanyi Zhao"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157392",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Phenomenological observations of quinone-mediated zinc oxidation in an alkaline environment",
        "abstract": "Redox-mediated electrochemistry is an area of growing interest, particularly in the context of energy storage. The development of such systems requires knowledge of underlying reaction mechanisms, which bear similarities to the processes that underpin corrosion and semiconductor electrochemistry. Herein we discuss an example system, quinone-mediated zinc oxidation in an alkaline environment, using knowledge from the corrosion and semiconductor fields to understand the phenomenological aspects of the reaction.",
        "authors": [
            "Christopher T Mallia",
            "Fikile R Brushett"
        ],
        "journal_conference_name": "Chemical Communications",
        "publisher": "Royal Society of Chemistry",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157525",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Defining Native American",
        "abstract": "This paper explores the question of who is defined as a Native American within the jurisdictions of the United States. Determining individual status can be seen as a two-step process: Is a given individual recognized by a specific tribe as a member? Then, is that specific tribe acknowledged by a relevant governmental unit? Though both seem simple questions, this paper illustrates that the question “Is Person X a Native American?” sometimes can be quite fraught, and manifests what I have described previously as definitional gaps and definitional ruptures. Ultimately, as is typical of regulatory definitions, the choice of definitional criteria to apply is a question of values, interests, and politics. I begin with a description of the varied definitional frameworks at work in determinations of whether a given group of people constitute a recognized tribe, then note how tribes themselves are institutions empowered to define who does or does not count as members through practices of enrollment and disenrollment. I then describe three case studies of definitional phenomena—one as a case of a definitional gap (college professors described as “Pretendians”), the second as a case of definitional rupture (determining Native American eligibility for free tuition within the University of California system), and a third as an illustration of regulatory versus self-definition (U.S. Census practices).",
        "authors": [
            "Edward Schiappa"
        ],
        "journal_conference_name": "Topoi",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157388",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Urban street clusters: unraveling the associations of street characteristics on urban vibrancy dynamics in age, time, and day",
        "abstract": "Understanding urban vibrancy has been considered crucial to promoting human activities and interactions in public open spaces. Recent advancements in urban big data have facilitated the potential to understand and measure vibrancy patterns throughout cities. While streets are considered the center stage of human activity, previous studies have often overlooked their multifaceted nature and their association with urban vibrancy. In this study, we incorporate multi-source big data and combine a set of features that comprehensively describe the scale, function, and topology of street segments in two Seoul districts: Jung-gu and Gangnam-gu. Using these features, we employ a machine learning clustering technique to classify them into five distinct typologies. Then, with street-level aggregated mobile phone tracking data, we investigate whether street typology characteristics are associated with urban vibrancy with respect to age groups, time of day, and day types (weekends/weekdays). The results show varying relationships between street characteristics with age-, time- and day-vibrancy measures by the identified street typology. Further, we contrast the results of the two districts to evaluate urban vibrancy differences in organic and planned urban layouts. This study enables a more nuanced understanding of urban streets to better comprehend their impact on people’s use of street space. The derived novel insights could assist planners and designers to better pinpoint street management solutions for different age- and time-dependent needs based on the complexities in urban vibrancy dynamics.",
        "authors": [
            "Kee M. Jang",
            "Hanew Suh",
            "Fadi G. Haddad",
            "Maoran Sun",
            "Fábio Duarte",
            "Youngchul Kim"
        ],
        "journal_conference_name": "Urban Informatics",
        "publisher": "Springer Nature Singapore",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157397",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "How AI is improving simulations with smarter sampling techniques",
        "abstract": "MIT CSAIL researchers created an AI-powered method for low-discrepancy sampling, which uniformly distributes data points to boost simulation accuracy.",
        "authors": [
            "Rachel Gordon"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "MIT News",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157668",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Comparative Analysis of Serum and Serum-Free Medium Cultured Mesenchymal Stromal Cells for Cartilage Repair",
        "abstract": "Mesenchymal stromal cells (MSCs) are promising candidates for cartilage repair therapy due to their self-renewal, chondrogenic, and immunomodulatory capacities. It is widely recognized that a shift from fetal bovine serum (FBS)-containing medium toward a fully chemically defined serum-free (SF) medium would be necessary for clinical applications of MSCs to eliminate issues such as xeno-contamination and batch-to-batch variation. However, there is a notable gap in the literature regarding the evaluation of the chondrogenic ability of SF-expanded MSCs (SF-MSCs). In this study, we compared the in vivo regeneration effect of FBS-MSCs and SF-MSCs in a rat osteochondral defect model and found poor cartilage repair outcomes for SF-MSCs. Consequently, a comparative analysis of FBS-MSCs and SF-MSCs expanded using two SF media, MesenCult&trade;-ACF (ACF), and Custom StemPro&trade; MSC SFM XenoFree (XF) was conducted in vitro. Our results show that SF-expanded MSCs constitute variations in morphology, surface markers, senescence status, differentiation capacity, and senescence/apoptosis status. Highly proliferative MSCs supported by SF medium do not always correlate to their chondrogenic and cartilage repair ability. Prior determination of the SF medium&rsquo;s ability to support the chondrogenic ability of expanded MSCs is therefore crucial when choosing an SF medium to manufacture MSCs for clinical application in cartilage repair.",
        "authors": [
            "Meiqi Kang",
            "Yanmeng Yang",
            "Haifeng Zhang",
            "Yuan Zhang",
            "Yingnan Wu",
            "Vinitha Denslin",
            "Rashidah Binte Othman",
            "Zheng Yang",
            "Jongyoon Han"
        ],
        "journal_conference_name": "International Journal of Molecular Sciences",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157318",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Gaussian process-based online health monitoring and fault analysis of lithium-ion battery systems from field data",
        "abstract": "Health monitoring, fault analysis, and detection methods are important to operate battery systems safely. We apply Gaussian process resistance models on lithium-iron-phosphate (LFP) battery field data to separate the time-dependent and operating-point-dependent resistances. The dataset contains 28 battery systems returned to the manufacturer for warranty, each with eight cells in series, totaling 224 cells and 133 million data rows. We develop probabilistic fault detection rules using recursive spatiotemporal Gaussian processes. These processes scale linearly with the number of data points, allowing online monitoring. The fault analysis underlines that often, only a single cell shows abnormal behavior or a knee point, consistent with weakest-link failure for cells connected in series, amplified by local resistive heating. The results further the understanding of how battery packs degrade and fail in the field and demonstrate the potential of online monitoring. We open source the code and publish the dataset with this article.",
        "authors": [
            "Joachim Schaeffer",
            "Eric Lenz",
            "Duncan Gulla",
            "Martin Z Bazant",
            "Richard D Braatz",
            "Rolf Findeisen"
        ],
        "journal_conference_name": "Cell Reports Physical Science",
        "publisher": "Elsevier BV",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157659",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Clio: Real-Time Task-Driven Open-Set 3D Scene Graphs",
        "abstract": "",
        "authors": [
            "Dominic Maggio",
            "Yun Chang",
            "Nathan Hughes",
            "Matthew Trang",
            "Dan Griffith",
            "Carlyn Dougherty",
            "Eric Cristofalo",
            "Lukas Schmid",
            "Luca Carlone"
        ],
        "journal_conference_name": "IEEE Robotics and Automation Letters",
        "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157072",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Incorporating Energy Storage in the Design  of an All-electric Naval Vessel",
        "abstract": "Incorporation of energy storage directly into the\r\ndistribution system of a Navy ship can enable new dynamic highpower loads and improve overall energy efficiency. This paper\r\ninvestigates the integration of energy storage onboard an allelectric destroyer by designing a solution for an advanced\r\ncombination of loads and establishing a procedure for\r\nincorporating energy storage directly into the distribution system\r\ndesign. A case study is examined in which a battery is sized for a\r\ndefined load and integrated into a ship design with associated\r\nperipherals. The selected battery is simulated to verify expected\r\nperformance. The study investigates a battery-based system that\r\nprovides peak shaving to assist the traditional generation system\r\nin supplying pulsed and stochastic loads, while also enabling\r\nsingle-generator operations. Such an energy storage system\r\nprovides notable benefits in terms of fuel consumption during\r\nnormal operations while also guaranteeing a stable and continuous\r\npower supply to the mission loads during battle scenarios.",
        "authors": [
            "Andrea Alessia Tavagnutti",
            "Hayden Atchison",
            "Julie Chalfant",
            "Chryssostomos Chryssostomidis",
            "David Wetz",
            "Giorgio Sulligoi"
        ],
        "journal_conference_name": "IEEE Transactions on Transportation Electrification",
        "publisher": "Institute of Electrical and Electronics Engineers",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156678",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Anomaly Detection in Fractal Time Series with LSTM Autoencoders",
        "abstract": "This study explores the application of neural networks for anomaly detection in time series data exhibiting fractal properties, with a particular focus on changes in the Hurst exponent. The objective is to investigate whether changes in fractal properties can be identified by transitioning from the analysis of the original time series to the analysis of the sequence of Hurst exponent estimates. To this end, we employ an LSTM autoencoder neural network, demonstrating its effectiveness in detecting anomalies within synthetic fractal time series and real EEG signals by identifying deviations in the sequence of estimates. Whittle&rsquo;s method was utilized for the precise estimation of the Hurst exponent, thereby enhancing the model&rsquo;s ability to differentiate between normal and anomalous data. The findings underscore the potential of machine learning techniques for robust anomaly detection in complex datasets.",
        "authors": [
            "Lyudmyla Kirichenko",
            "Yulia Koval",
            "Sergiy Yakovlev",
            "Dmytro Chumachenko"
        ],
        "journal_conference_name": "mathematics",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157317",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Semantic Reader Project: Augmenting Scholarly Documents through AI-Powered Interactive Reading Interfaces",
        "abstract": "Scholarly publications are key to the transfer of knowledge from scholars to others. However, research papers are information-dense, and as the volume of the scientific literature grows, the greater the need for new technology to support scholars. In contrast to the process of finding papers, which has been transformed by Internet technology, the experience of reading research papers has changed little in decades. For instance, the PDF format for sharing papers remains widely used due to its portability but has significant downsides, inter alia, static content and poor accessibility for low-vision readers. This paper explores the question \"Can recent advances in AI and HCI power intelligent, interactive, and accessible reading interfaces, even for legacy PDFs?\" We describe the Semantic Reader Project, a collaborative effort across multiple institutions to explore automatic creation of dynamic reading interfaces for research papers. Through this project, we've developed a collection of novel reading interfaces and evaluated them with study participants and real-world users to show improved reading experiences for scholars. We've also released a production research paper reading interface that will continuously incorporate novel features from our research as they mature. We structure this paper around five key opportunities for AI assistance in scholarly reading---discovery, efficiency, comprehension, synthesis, and accessibility---and present an overview of our progress and discuss remaining open challenges.",
        "authors": [
            "Kyle Lo",
            "Joseph Chang",
            "Andrew Head",
            "Jonathan Bragg",
            "Amy Zhang",
            "Cassidy Trier",
            "Chloe Anastasiades",
            "Tal August",
            "Russell Authur",
            "Danielle Bragg",
            "Erin Bransom",
            "Isabel Cachola",
            "Stefan Candra",
            "Yoganand Chandrasekhar",
            "Yen-Sung Chen",
            "Evie Cheng",
            "Yvonne Chou",
            "Doug Downey",
            "Rob Evans",
            "Raymond Fok"
        ],
        "journal_conference_name": "Communications of the ACM",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157322",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Brief History of Blockchain Interoperability",
        "abstract": "Blockchain interoperability conflates the need for distributed systems to communicate with third-party systems without a canonical chain or orchestration layer. As there is \"no chain to rule them all\" (for performance, privacy, and market forces), these distributed systems rely on exchanging data and value across network boundaries. Interconnected systems achieve a higher value than the sum of their parts, similar to how the Internet emerged as a set of isolated Local Area Networks  (LANs) - and, by force of surprising synergies, such networks fundamentally transformed society forever. Concurrently, in the last decade, we have witnessed the astonishing development of blockchain technologies, which seem more connected than ever: via bridges [13 ], oracles [ 25], and other interoperability mechanisms [ 8, 27, 45]. These recent developments have, slowly but steadily,  contributed to the improvement of the scalability of blockchain networks, as well as providing new functionality and use cases [36], but there is still a long way to go until mass adoption. In this paper, we will dive into the rabbit hole of blockchain interoperability and explain why it is needed,  what has work been done in the last decade (the past), how it is currently deployed and used in practice (the present), and likely paths of development (the future).",
        "authors": [
            "Rafael Belchior",
            "Jan S??enguth",
            "Qi Feng",
            "Thomas Hardjono",
            "Andr? Vasconcelos",
            "Miguel Correia"
        ],
        "journal_conference_name": "Communications of the ACM",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157321",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Information FOMO: The Unhealthy Fear of Missing Out on Information&mdash;A Method for Removing Misleading Data for Healthier Models",
        "abstract": "Misleading or unnecessary data can have out-sized impacts on the health or accuracy of Machine Learning (ML) models. We present a Bayesian sequential selection method, akin to Bayesian experimental design, that identifies critically important information within a dataset while ignoring data that are either misleading or bring unnecessary complexity to the surrogate model of choice. Our method improves sample-wise error convergence and eliminates instances where more data lead to worse performance and instabilities of the surrogate model, often termed sample-wise &ldquo;double descent&rdquo;. We find these instabilities are a result of the complexity of the underlying map and are linked to extreme events and heavy tails. Our approach has two key features. First, the selection algorithm dynamically couples the chosen model and data. Data is chosen based on its merits towards improving the selected model, rather than being compared strictly against other data. Second, a natural convergence of the method removes the need for dividing the data into training, testing, and validation sets. Instead, the selection metric inherently assesses testing and validation error through global statistics of the model. This ensures that key information is never wasted in testing or validation. The method is applied using both Gaussian process regression and deep neural network surrogate models.",
        "authors": [
            "Ethan Pickering",
            "Themistoklis P. Sapsis"
        ],
        "journal_conference_name": "entropy",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157430",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On the Complexity of Neural Computation in Superposition",
        "abstract": "Recent advances in the understanding of neural networks suggest that superposition, the ability of a single neuron to represent multiple features simultaneously, is a key mechanism underlying the computational efficiency of large-scale networks. This paper explores the theoretical foundations of computing in superposition, focusing on explicit, provably correct algorithms and their efficiency.\r\n\r\nWe present the first lower bounds showing that for a broad class of problems, including permutations and pairwise logical operations, a neural net- work computing in superposition requires at least Ω(m′ log m′) parameters and Ω(√(m′ log m′)) neurons, where m′ is the number of output features being computed. This implies that any “lottery ticket” sparse sub-network must have at least Ω(m′ log m′ ) parameters no matter what the initial dense network size. Conversely, we show a nearly tight upper bound: logical operations like pair- wise AND can be computed using O(√(m′) log m′) neurons and O(m′ log^2 m′) parameters. There is thus an exponential gap between computing in superposition, the subject of this work, and representing features in superposition, which can require as little as O(log m′) neurons based on the Johnson-Lindenstrauss Lemma.\r\n\r\nOur hope is that our results open a path for using complexity theoretic techniques in neural network interpretability research.",
        "authors": [
            "Micah Adler",
            "Nir Shavit"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157073",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Bilateral Trade Welfare Impacts of India’s Export Ban of Non-Basmati Rice Using the Global Partial Equilibrium Simulation Model (GSIM)",
        "abstract": "India, the world&rsquo;s leading rice exporter, banned the export of non-Basmati white rice, accounting for 25% of its total exports (or 10% of the global rice trade). The ban aims to ensure availability to domestic Indian consumers and reduce domestic market prices, impacting global rice market accessibility, consumers, and producers across twelve regions. The study utilized the global simulation model (GSIM) to analyze the effects of trade restrictions on industries. The model uses national product differentiation to assess trade policy changes at global, regional, or national scales. It examined importer and exporter effects on trade values, tariff revenues, exporter surplus, and importer surplus. It found that India&rsquo;s Voluntary Export Restraint (VER) ban on non-Basmati rice resulted in a higher local price and a negative global net welfare impact of USD 1.7 billion. The losses decreased to USD 1.4 billion when importing countries responded by reducing rice import tariffs by 25% and USD 1.1 billion when importing countries reduced tariffs by 75%. Sub-Saharan Africa, the Middle East, North Africa, and the Gulf Cooperation Council regions were most affected. The study also found minimal impact on consumer surplus in India due to inelastic rice demand.",
        "authors": [
            "Eihab Fathelrahman",
            "Raeda Osman",
            "Dana Loyd Keske Hoag",
            "Gregory N. Sixt",
            "Kenneth Strzepek"
        ],
        "journal_conference_name": "foods",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157316",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On the Power of Decision Trees in Auto-Regressive Language Modeling",
        "abstract": "Originally proposed for handling time series data, Auto-regressive Decision Trees (ARDTs) have not yet been explored for language modeling. This paper delves into both the theoretical and practical applications of ARDTs in this new context. We theoretically demonstrate that ARDTs can compute complex functions, such as simulating automata, Turing machines, and sparse circuits, by leveraging \"chain-of-thought\" computations. Our analysis provides bounds on the size, depth, and computational efficiency of ARDTs, highlighting their surprising computational power. Empirically, we train ARDTs on simple language generation tasks, showing that they can learn to generate coherent and grammatically correct text on par with a smaller Transformer model. Additionally, we show that ARDTs can be used on top of transformer representations to solve complex reasoning tasks. This research reveals the unique computational abilities of ARDTs, aiming to broaden the architectural diversity in language model development.",
        "authors": [
            "Yulu Gan",
            "Tomer Galanti",
            "Tomaso Poggio",
            "Eran Malach"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Center for Brains, Minds and Machines (CBMM)",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157074",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design and Evaluation of a Precision Irrigation Tool’s Human–Machine Interaction to Bring Water- and Energy-Efficient Irrigation to Resource-Constrained Farmers",
        "abstract": "As freshwater supplies decrease, adopting sustainable practices like water- and energy-efficient irrigation is crucial, particularly in resource-constrained regions. Here, farmers often cannot purchase precision irrigation equipment, which achieves high water and energy efficiencies via full automation. Currently, no irrigation methods exist that combine automatic scheduling of events with manual operation of valves, familiar hardware on low-income farms. This work synthesizes functional requirements for a tool that could address efficiency needs while integrating into current manual practices. Then, a design concept for an automatic scheduling and manual operation (AS-MO) human&ndash;machine interaction (HMI) that meets these requirements is proposed. Two design stages of the AS-MO HMI were evaluated by farmers and market stakeholders in three countries. Results show that farmers in Kenya and Jordan valued the proposed AS-MO HMI because they could increase efficiency on their farms without the cost or complexity of automatic valves. In Morocco, a possible market was found, but a majority of participants preferred full automation. Interviewees provided feedback on how to improve the tool&rsquo;s design in future iterations. If adopted at scale, the proposed AS-MO tool could increase efficiency on farms that otherwise cannot afford current precision irrigation technology, improving sustainable agriculture worldwide.",
        "authors": [
            "Georgia D. Van de Zande",
            "Fiona Grant",
            "Carolyn Sheline",
            "Susan Amrose",
            "Jeffery Costello",
            "Aditya Ghodgaonkar",
            "Amos G. Winter V"
        ],
        "journal_conference_name": "sustainability",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157314",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Measurement of the background in the CMS muon detector in 𝑝𝑝 -collisions at √𝑠 =13  TeV",
        "abstract": "The CMS detector, including its muon system, has been operating at the CERN LHC in increasingly challenging conditions for about 15 years. The muon detector was designed to provide excellent triggering and track reconstruction for muons produced in proton–proton collisons at an instantaneous luminosity (\r\n) of 1×1034\r\n cm−2\r\ns−1\r\n. During the Run 2 data-taking period (2015–2018), the LHC achieved an instantaneous luminosity of twice its design value, resulting in larger background rates and making the efficient detection of muons more difficult. While some backgrounds result from natural radioactivity, cosmic rays, and interactions of the circulating protons with residual gas in the beam pipe, the dominant source of background hits in the muon system arises from proton–proton interactions themselves. Charged hadrons leaving the calorimeters produce energy deposits in the muon chambers. In addition, high-energy particles interacting in the hadron calorimeter and forward shielding elements generate thermal neutrons, which leak out of the calorimeter and shielding structures, filling the CMS cavern. We describe the method used to measure the background rates in the various muon subsystems. These rates, in conjunction with simulations, can be used to estimate the expected backgrounds in the High-Luminosity LHC. This machine will run for at least 10 years starting in 2029 reaching an instantaneous luminosity of =5×1034cm-2s-1\r\n and increasing ultimately to =7.5×1034cm-2s-1\r\n. These background estimates have been a key ingredient for the planning and design of the muon detector upgrade.",
        "authors": [
            "M. Tytgat",
            "A. Muhammad",
            "G. De Lentdecker",
            "J. Jaramillo",
            "L. Moureaux",
            "L. Pétré",
            "Y. Yang",
            "C. Rendón",
            "G. Gokbulut",
            "Y. Hong",
            "A. Samalan",
            "G. A. Alves",
            "F. Marujo da Silva",
            "E. Alves Coelho",
            "M. Barroso Ferreira Filho",
            "E. M. Da Costa",
            "D. De Jesus Damiao",
            "B. C. Ferreira",
            "S. Fonseca De Souza",
            "K. Mota Amarilo"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157267",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A hypergraph model shows the carbon reduction potential of effective space use in housing",
        "abstract": "Humans spend over 90% of their time in buildings, which account for 40% of anthropogenic greenhouse gas emissions and are a leading driver of climate change. Incentivizing more sustainable construction, building codes are used to enforce indoor comfort standards and minimum energy efficiency requirements. However, they currently only reward measures such as equipment or envelope upgrades and disregard the actual spatial configuration and usage. Using a new hypergraph model that encodes building floorplan organization and facilitates automatic geometry creation, we demonstrate that space efficiency outperforms envelope upgrades in terms of operational carbon emissions in 72%, 61% and 33% of surveyed buildings in Zurich, New York, and Singapore. Using automatically generated floorplans in a case study in Zurich further increased access to daylight by up to 24%, revealing that auto-generated floorplans have the potential to improve the quality of residential spaces in terms of environmental performance and access to daylight.",
        "authors": [
            "Ramon Elias Weber",
            "Caitlin Mueller",
            "Christoph Reinhart"
        ],
        "journal_conference_name": "Nature Communications",
        "publisher": "Springer Science and Business Media LLC",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158149",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Unraveling the Dynamics of Mental and Visuospatial Workload in Virtual Reality Environments",
        "abstract": "Mental workload, visuospatial processes and autonomic nervous system (ANS) activity are highly intertwined phenomena crucial for achieving optimal performance and improved mental health. Virtual reality (VR) serves as an effective tool for creating variety of controlled environments to better probe these features. This study investigates the relationship between mental and visuospatial workload, physiological arousal, and performance during a high-demand task in a VR environment. We utilized a modified version of the popular computer game TETRIS as the task, involving 25 participants, and employed a physiological computing VR headset that simultaneously records multimodal physiological data. Our findings indicate a broadband increase in EEG power just prior to a helper event, followed by a spike of visuospatial engagement (parietal alpha and beta 0-1-3 s) occurring concurrently with a decrease in mental workload (frontal theta 2&ndash;4 s), and subsequent decreases in visuospatial engagement (parietal theta at 14 s) and physiological arousal (HRV at 20 s). Regression analysis indicated that the subjective relief and helpfulness of the helper intervention was primarily driven by a decrease in physiological arousal and an increase in visuospatial engagement. These findings highlight the importance of multimodal physiological recording in rich environments, such as real world scenarios and VR, to understand the interplay between the various physiological responses involved in mental and visuospatial workload.",
        "authors": [
            "Guillermo Bernal",
            "Hahrin Jung",
            "İsmail Emir Yassı",
            "Nelson Hidalgo",
            "Yodahe Alemu",
            "Tyler Barnes-Diana",
            "Pattie Maes"
        ],
        "journal_conference_name": "Computers",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157422",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Highly Linear Ultra-Low-Area-and-Power CMOS Voltage-Controlled Oscillator for Autonomous Microsystems",
        "abstract": "Voltage-controlled oscillators (VCOs) can be an excellent means of converting a magnitude into a readable value. However, their design becomes a real challenge for power-and-area-constrained applications, especially when a linear response is required. This paper presents a VCO for smart dust systems fabricated by 65 nm technology. It is designed to minimize leakage, limit high peak currents and provide an output whose frequency variation is linear with the input voltage, while allowing rail-to-rail input range swing. The oscillator occupies 592 μm2\r\n, operates in a frequency range from 43 to 53 Hz and consumes a maximum average power of 210 pW at a supply voltage of 1 V and 4 pW at 0.3 V. In addition, the proposed VCO exhibits a quasi-linear response of frequency vs. supply voltage and temperature, allowing easy temperature compensation with complementary to absolute temperature (CTAT) voltage.",
        "authors": [
            "Javier de Mena Pacheco",
            "Tomas Palacios",
            "Marek Hempel",
            "Marisa Lopez Vallejo"
        ],
        "journal_conference_name": "micromachines",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157429",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Mixed-Methods Research Design to Advance Inclusive and Equitable Teaching",
        "abstract": "We designed this project to advance inclusive and equitable teaching by leveraging data to motivate, inform, and tailor teaching development initiatives to the varied needs and resources of academic departments. We developed an innovative framework and mixed methods research design to systematically assess inclusive and equitable teaching at the student, course, department, and institution levels. In the context of a decentralized institution, we partnered with academic departments to collect data about their current practices, existing resources, and needs for advancing inclusive and equitable teaching through a student survey, analysis of course syllabi, and interviews with instructors. We shared and discussed results with partners in academic departments to support and inform departmental change initiatives. We highlight how synthesizing findings across multiple levels of analysis using a mixed methods design provides a new perspective on the perennial issue of the uptake of inclusive and equitable teaching practices in higher education. We discuss lessons learned and future directions with the hope that the framework and/or the research methodology can be a template for other researchers or educational developers to support implementation and sustainability of inclusive and equitable teaching practices.",
        "authors": [
            "Raechel N. Soicher",
            "Amanda R. Baker",
            "Ruthann C. Thomas"
        ],
        "journal_conference_name": "Innovative Higher Education",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157382",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Molecular Dynamic Simulations Reveal that Water-Soluble QTY-Variants of Glutamate Transporters EAA1, EAA2 and EAA3 Retain the Conformational Characteristics of Native Transporters",
        "abstract": "Objective\r\n              Glutamate transporters play a crucial role in neurotransmitter homeostasis, but studying their structure and function is challenging due to their membrane-bound nature. This study aims to investigate whether water-soluble QTY-variants of glutamate transporters EAA1, EAA2 and EAA3 retain the conformational characteristics and dynamics of native membrane-bound transporters.\r\n            \r\n            \r\n              Methods\r\n              Molecular dynamics simulations and comparative genomics were used to analyze the structural dynamics of both native transporters and their QTY-variants. Native transporters were simulated in lipid bilayers, while QTY-variants were simulated in aqueous solution. Lipid distortions, relative solvent accessibilities, and conformational changes were examined. Evolutionary conservation profiles were correlated with structural dynamics. Statistical analyses included multivariate analysis to account for confounding variables.\r\n            \r\n            \r\n              Results\r\n              QTY-variants exhibited similar residue-wise conformational dynamics to their native counterparts, with correlation coefficients of 0.73 and 0.56 for EAA1 and EAA3, respectively (p < 0.001). Hydrophobic interactions of native helices correlated with water interactions of QTY- helices (rs = 0.4753, p < 0.001 for EAA1). QTY-variants underwent conformational changes resembling the outward-to-inward transition of native transporters.\r\n            \r\n            \r\n              Conclusions\r\n              Water-soluble QTY-variants retain key structural properties of native glutamate transporters and mimic aspects of native lipid interactions, including conformational flexibility. This research provides valuable insights into the conformational changes and molecular mechanisms of glutamate transport, potentially offering a new approach for studying membrane protein dynamics and drug interactions.",
        "authors": [
            "Alper Karagöl",
            "Taner Karagöl",
            "Shuguang Zhang"
        ],
        "journal_conference_name": "Pharmaceutical Research",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157376",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Search for a resonance decaying to a W boson and a photon in proton-proton collisions at  √s = 13 TeV using leptonic W boson decays",
        "abstract": "A search for a new charged particle X with mass between 0.3 and 2.0 TeV decaying to a W boson and a photon is presented, using proton-proton collision data at a center-of-mass energy of 13 TeV, collected by the CMS experiment and corresponding to an integrated luminosity of 138 fb−1. Particle X has electric charge ±1 and is assumed to have spin 0. The search is performed using the electron and muon decays of the W boson. No significant excess above the predicted background is observed. The upper limit at 95% confidence level on the product of the production cross section of the X and its branching fraction to a W boson and a photon is found to be 94 (137) fb for a 0.3 TeV resonance and 0.75 (0.81) fb for a 2.0 TeV resonance, for an X width-to-mass ratio of 0.01% (5%). This search presents the most stringent constraints to date on the existence of such resonances across the probed mass range. A statistical combination with an earlier study based on the hadronic decay mode of the W boson is also performed, and the upper limit at 95% confidence level for a 2.0 TeV resonance is reduced to 0.50 (0.63) fb for an X width-to-mass ratio of 0.01% (5%).",
        "authors": [
            "A. Hayrapetyan",
            "A. Tumasyan",
            "W. Adam",
            "J. W. Andrejkovic",
            "T. Bergauer",
            "S. Chatterjee",
            "K. Damanakis",
            "M. Dragicevic",
            "P. S. Hussain",
            "M. Jeitler",
            "N. Krammer",
            "A. Li",
            "D. Liko",
            "I. Mikulec",
            "J. Schieck",
            "R. Schöfbeck"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157391",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Predicting Outcomes of Preterm Neonates Post Intraventricular Hemorrhage",
        "abstract": "Intraventricular hemorrhage (IVH) in preterm neonates presents a high risk for developing posthemorrhagic ventricular dilatation (PHVD), a severe complication that can impact survival and long-term outcomes. Early detection of PHVD before clinical onset is crucial for optimizing therapeutic interventions and providing accurate parental counseling. This study explores the potential of explainable machine learning models based on targeted liquid biopsy proteomics data to predict outcomes in preterm neonates with IVH. In recent years, research has focused on leveraging advanced proteomic technologies and machine learning to improve prediction of neonatal complications, particularly in relation to neurological outcomes. Machine learning (ML) approaches, combined with proteomics, offer a powerful tool to identify biomarkers and predict patient-specific risks. However, challenges remain in integrating large-scale, multiomic datasets and translating these findings into actionable clinical tools. Identifying reliable, disease-specific biomarkers and developing explainable ML models that clinicians can trust and understand are key barriers to widespread clinical adoption. In this prospective longitudinal cohort study, we analyzed 1109 liquid biopsy samples from 99 preterm neonates with IVH, collected at up to six timepoints over 13 years. Various explainable ML techniques&mdash;including statistical, regularization, deep learning, decision trees, and Bayesian methods&mdash;were employed to predict PHVD development and survival and to discover disease-specific protein biomarkers. Targeted proteomic analyses were conducted using serum and urine samples through a proximity extension assay capable of detecting low-concentration proteins in complex biofluids. The study identified 41 significant independent protein markers in the 1600 calculated ML models that surpassed our rigorous threshold (AUC-ROC of &ge;0.7, sensitivity &ge; 0.6, and selectivity &ge; 0.6), alongside gestational age at birth, as predictive of PHVD development and survival. Both known biomarkers, such as neurofilament light chain (NEFL), and novel biomarkers were revealed. These findings underscore the potential of targeted proteomics combined with ML to enhance clinical decision-making and parental counseling, though further validation is required before clinical implementation.",
        "authors": [
            "Gabriel A. Vignolle",
            "Priska Bauerstätter",
            "Silvia Schönthaler",
            "Christa Nöhammer",
            "Monika Olischar",
            "Angelika Berger",
            "Gregor Kasprian",
            "Georg Langs",
            "Klemens Vierlinger",
            "Katharina Goeral"
        ],
        "journal_conference_name": "International Journal of Molecular Sciences",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157315",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Fracturing Processes in Specimens with Internal vs. Throughgoing Flaws: An Experimental Study Using 3D Printed Materials",
        "abstract": "The fracturing behavior and associated mechanical characterization of rocks are important for many applications in the fields of civil, mining, geothermal, and petroleum engineering. Laboratory testing of rocks plays a major role in understanding the underlying processes that occur on the larger scale and for predicting rock behavior. Fracturing research requires well-defined and consistent boundary conditions. Consequently, the testing design and setup can greatly influence the results. In this study, a comprehensive experimental program using an artificial material was carried out to systematically evaluate the effects of different parameters in rock testing under uniaxial compression. The parameters include compression platen type, specimen centering, loading control method, boundary constraints, and flaw parameters. The results show that these testing conditions have a significant effect on the mechanical behavior of rocks. Using a fixed compression platen helped reduce bulging of the material. Centering of the specimen played a critical role to avoid buckling and unequal distribution of stress. Slower displacement rates can control the energy being released once failure occurs to prevent the specimen from exploding. Also, the frictional end effects were investigated by comparing friction-reduced and non-friction-reduced end conditions. Very importantly, the study also identified variations in crack initiation and propagation between specimens with internal flaws and specimens with throughgoing flaws. This investigation showed that wing cracks appeared in specimens with throughgoing flaws, while wing cracks with petal cracks were associated with the internal flaws. It also showed that the mechanical properties are influenced by the inclination of the flaws and established that specimens with internal flaws generally exhibit higher strength compared to specimens with throughgoing flaws. The systematic analysis presented in this work sheds light on important considerations that need to be taken into account when conducting fracture research and adds knowledge to the fundamental understanding of how fractures occur in nature.",
        "authors": [
            "Majed Almubarak",
            "John T. Germaine",
            "Herbert H. Einstein"
        ],
        "journal_conference_name": "Rock Mechanics and Rock Engineering",
        "publisher": "Springer Vienna",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157374",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "New horizon symmetries, hydrodynamics, and quantum chaos",
        "abstract": "We generalize the formulation of horizon symmetries presented in previous literature to include diffeomorphisms that can shift the location of the horizon. In the context of the AdS/CFT duality, we show that horizon symmetries can be interpreted on the boundary as emergent low-energy gauge symmetries. In particular, we identify a new class of horizon symmetries that extend the so-called shift symmetry, which was previously postulated for effective field theories of maximally chaotic systems. Additionally, we comment on the connections of horizon symmetries with bulk calculations of out-of-time-ordered correlation functions and the phenomenon of pole-skipping.",
        "authors": [
            "Maria Knysh",
            "Hong Liu",
            "Natalia Pinzani-Fokeeva"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157387",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Fifteen Lincoln Laboratory Technologies Receive 2024 R&D 100 Awards",
        "abstract": "The innovations map the ocean floor and the brain, prevent heat stroke and cognitive injury, expand AI processing and quantum system capabilities, and introduce new fabrication approaches.",
        "authors": [
            "Kylie Foy"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "MIT News",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157669",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Mesoscale Brain Mapping: Bridging Scales and Modalities in Neuroimaging – A Symposium Review",
        "abstract": "Advances in the spatiotemporal resolution and field-of-view of neuroimaging tools are driving mesoscale studies for translational neuroscience. On October 10, 2023, the Center for Mesoscale Mapping (CMM) at the Massachusetts General Hospital (MGH) Athinoula A. Martinos Center for Biomedical Imaging and the Massachusetts Institute of Technology (MIT) Health Sciences Technology based Neuroimaging Training Program (NTP) hosted a symposium exploring the state-of-the-art in this rapidly growing area of research. “Mesoscale Brain Mapping: Bridging Scales and Modalities in Neuroimaging” brought together researchers who use a broad range of imaging techniques to study brain structure and function at the convergence of the microscopic and macroscopic scales. The day-long event centered on areas in which the CMM has established expertise, including the development of emerging technologies and their application to clinical translational needs and basic neuroscience questions. The in-person symposium welcomed more than 150 attendees, including 57 faculty members, 61 postdoctoral fellows, 35 students, and four industry professionals, who represented institutions at the local, regional, and international levels. The symposium also served the training goals of both the CMM and the NTP. The event content, organization, and format were planned collaboratively by the faculty and trainees. Many CMM faculty presented or participated in a panel discussion, thus contributing to the dissemination of both the technologies they have developed under the auspices of the CMM and the findings they have obtained using those technologies. NTP trainees who benefited from the symposium included those who helped to organize the symposium and/or presented posters and gave “flash” oral presentations. In addition to gaining experience from presenting their work, they had opportunities throughout the day to engage in one-on-one discussions with visiting scientists and other faculty, potentially opening the door to future collaborations. The symposium presentations provided a deep exploration of the many technological advances enabling progress in structural and functional mesoscale brain imaging. Finally, students worked closely with the presenting faculty to develop this report summarizing the content of the symposium and putting it in the broader context of the current state of the field to share with the scientific community. We note that the references cited here include conference abstracts corresponding to the symposium poster presentations.",
        "authors": [
            "Joshua K. Marchant",
            "Natalie G. Ferris",
            "Diana Grass",
            "Magdelena S. Allen",
            "Vivek Gopalakrishnan",
            "Mark Olchanyi",
            "Devang Sehgal",
            "Maxina Sheft",
            "Amelia Strom",
            "Berkin Bilgic",
            "Brian Edlow",
            "Elizabeth M. C. Hillman",
            "Meher R. Juttukonda",
            "Laura Lewis"
        ],
        "journal_conference_name": "Neuroinformatics",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157383",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Leveraging natural language processing to curate the tmCAT, tmPHOTO, tmBIO, and tmSCO datasets of functional transition metal complexes",
        "abstract": "The breadth of transition metal chemical space covered by databases such as the Cambridge Structural Database and the derived computational database tmQM is not conducive to application-specific modeling and the development of structure–property relationships. Here, we employ both supervised and unsupervised natural language processing (NLP) techniques to link experimentally synthesized compounds in the tmQM database to their respective applications. Leveraging NLP models, we curate four distinct datasets: tmCAT for catalysis, tmPHOTO for photophysical activity, tmBIO for biological relevance, and tmSCO for magnetism. Analyzing the chemical substructures within each dataset reveals common chemical motifs in each of the designated applications. We then use these common chemical structures to augment our initial datasets for each application, yielding a total of 21 631 compounds in tmCAT, 4599 in tmPHOTO, 2782 in tmBIO, and 983 in tmSCO. These datasets are expected to accelerate the more targeted computational screening and development of refined structure–property relationships with machine learning.",
        "authors": [
            "Ilia Kevlishvili",
            "Roland G St. Michel",
            "Aaron G Garrison",
            "Jacob W Toney",
            "Husain Adamji",
            "Haojun Jia",
            "Yuriy Román-Leshkov",
            "Heather J Kulik"
        ],
        "journal_conference_name": "Faraday Discussions",
        "publisher": "Royal Society of Chemistry",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157447",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Shuffle algebras for quivers as quantum groups",
        "abstract": "We define a quantum loop group U Q + associated to an arbitrary quiver Q = ( I , E ) and maximal set of deformation parameters, with generators indexed by I × Z and some explicit quadratic and cubic relations. We prove that U Q + is isomorphic to the (generic, small) shuffle algebra associated to the quiver Q and hence, by Neguț (Shuffle algebras for quivers and wheel conditions. arXiv:2102.11269 ), to the localized K-theoretic Hall algebra of Q. For the quiver with one vertex and g loops, this yields a presentation of the spherical Hall algebra of a (generic) smooth projective curve of genus g [invoking the results of Schiffmann and Vasserot (Math Ann 353(4):1399–1451, 2012)]. We extend the above results to the case of non-generic parameters satisfying a certain natural metric condition. As an application, we obtain a description by generators and relations of the subalgebra generated by absolutely cuspidal eigenforms of the Hall algebra of an arbitrary smooth projective curve [(invoking the results of Kapranov et al. (Sel Math (NS) 23(1):117–177, 2017)].",
        "authors": [
            "Andrei Neguț",
            "Francesco Sala",
            "Olivier Schiffmann"
        ],
        "journal_conference_name": "Mathematische Annalen",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/159017",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Multi-Objective Framework for Balancing Fairness and Accuracy in Debiasing Machine Learning Models",
        "abstract": "Machine learning algorithms significantly impact decision-making in high-stakes domains, necessitating a balance between fairness and accuracy. This study introduces an in-processing, multi-objective framework that leverages the Reject Option Classification (ROC) algorithm to simultaneously optimize fairness and accuracy while safeguarding protected attributes such as age and gender. Our approach seeks a multi-objective optimization solution that balances accuracy, group fairness loss, and individual fairness loss. The framework integrates fairness objectives without relying on a weighted summation method, instead focusing on directly optimizing the trade-offs. Empirical evaluations on publicly available datasets, including German Credit, Adult Income, and COMPAS, reveal several significant findings: the ROC-based approach demonstrates superior performance, achieving an accuracy of 94.29%, an individual fairness loss of 0.04, and a group fairness loss of 0.06 on the German Credit dataset. These results underscore the effectiveness of our framework, particularly the ROC component, in enhancing both the fairness and performance of machine learning models.",
        "authors": [
            "Rashmi Nagpal",
            "Ariba Khan",
            "Mihir Borkar",
            "Amar Gupta"
        ],
        "journal_conference_name": "machine learning & knowledge extraction",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157313",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Petrogenesis of the Deccan high-Mg basalts and picrites",
        "abstract": "Tholeiitic basalts and picrites from the Deccan Traps were used to constrain the pressure and temperature conditions of mantle melting for their origin. Clinopyroxene thermobarometry indicates that all Deccan tholeiites crystallized at low pressures in the upper crust (< 6 kbar/1047–1221 °C). In comparison, the Deccan alkalic rocks crystallized at pressures up to ~ 12.7 kbar. Rare samples of the tholeiites plot on their low-pressure olivine-plagioclase-clinopyroxene (Ol-Pl-Cpx) cotectic boundaries or olivine control lines in phase diagrams. These samples represent unmodified magmatic liquids. Primary magmas of the basalts that plot on their cotectic boundaries were modeled through reverse fractionation by incrementally adding equilibrium Ol + Pl + Cpx, Ol + Pl and Ol ± spinel, until the liquid was multiply saturated with lherzolite at a high pressure. The high-Mg basalts are contaminated with continental crust. Hence, a crustal partial melt was simultaneously subtracted according to energy constraints at each reverse fractionation step for these samples. The results show that the high-Mg basalts are 41–53% fractionated and 1–6% contaminated, and the low-Mg basalts are 63–67% fractionated. Their primary magmas were last equilibrated with spinel lherzolite at 10–13 kbar/1289–1333 °C. A picrite and two very high-Mg basalts plot on their olivine control lines. So, their primary magmas were calculated by adding only equilibrium olivine. These samples are 9–25% fractionated, and their primary magmas were last equilibrated with garnet lherzolite at 25–36 kbar/1452–1531 °C. The estimated mantle potential temperatures are 1400–1500 °C for the Deccan tholeiites, consistent with their origin from a mantle plume.",
        "authors": [
            "Nilanjan Chatterjee"
        ],
        "journal_conference_name": "Contributions to Mineralogy and Petrology",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156925",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Learning design for short-duration e-textile workshops: outcomes on knowledge and skills",
        "abstract": "E-textiles provide an interesting field of research as they “blend traditional craft with modern science” (Peppler, 2016) and help learners “broaden their own perceptions of computing” (Searle et al., 2016). Despite the promising findings by primarily long-term interventions structured around e-textiles, educational curriculum reform has been slow to materialize. Educators who embrace a STEAM philosophy are more likely to endorse short workshops, integrating them in existing courses or initiatives; this could serve as a steppingstone for longer interventions and bottom-up curriculum reform. This study examines whether shorter e-textile workshops (lasting four hours) can result in significant gains in understanding. We present an investigation of e-textiles with 22 young children who have no prior experience with e-textiles or working with microprocessors. We present details of our learning design, as well as findings related to circuitry knowledge and computational making skills. We find that the children advanced their circuitry knowledge and practice a range of computational making skills. We further document a series of emerging challenges, including the children’s unwillingness to engage or lack of adeptness with software, a tension between aesthetics and construction, creativity limited by samples of previous e-textile projects, and the difficulty in grasping the materiality of e-textiles. We propose that some direct instruction and facilitation is not incompatible with the making ethos; the approach can help address these challenges, allowing young children to benefit from their participation in short-duration e-textile workshops.",
        "authors": [
            "Andri Ioannou",
            "Ourania Miliou",
            "Yiannis Georgiou",
            "Stella Timotheou",
            "Louise Barkhuus",
            "Jennifer Rode"
        ],
        "journal_conference_name": "Education Tech Research Dev",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156926",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Self-Supervised Learning across the Spectrum",
        "abstract": "Satellite image time series (SITS) segmentation is crucial for many applications, like environmental monitoring, land cover mapping, and agricultural crop type classification. However, training models for SITS segmentation remains a challenging task due to the lack of abundant training data, which requires fine-grained annotation. We propose S4, a new self-supervised pretraining approach that significantly reduces the requirement for labeled training data by utilizing two key insights of satellite imagery: (a) Satellites capture images in different parts of the spectrum, such as radio frequencies and visible frequencies. (b) Satellite imagery is geo-registered, allowing for fine-grained spatial alignment. We use these insights to formulate pretraining tasks in S4. To the best of our knowledge, S4 is the <i><b>first</b></i> multimodal and temporal approach for SITS segmentation. S4&rsquo;s novelty stems from leveraging multiple properties required for SITS self-supervision: (1) multiple modalities, (2) temporal information, and (3) pixel-level feature extraction. We also curate m2s2-SITS, a large-scale dataset of unlabeled, spatially aligned, multimodal, and geographic-specific SITS that serves as representative pretraining data for S4. Finally, we evaluate S4 on multiple SITS segmentation datasets and demonstrate its efficacy against competing baselines while using limited labeled data. Through a series of extensive comparisons and ablation studies, we demonstrate S4&rsquo;s ability as an effective feature extractor for downstream semantic segmentation.",
        "authors": [
            "Jayanth Shenoy",
            "Xingjian Davis Zhang",
            "Bill Tao",
            "Shlok Mehrotra",
            "Rem Yang",
            "Han Zhao",
            "Deepak Vasisht"
        ],
        "journal_conference_name": "remote sensing",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157312",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The effect of targeting both quantitative and qualitative objectives in generative design tools on the design outcomes",
        "abstract": "Current generative design tools backed by artificial intelligence (AI) primarily allow for quantitative inputs while qualitative aspects of a design, in particular aesthetics, have been shown to be considered indirectly by designers. To explore this further, controlled lab experiments were conducted to understand how designers incorporate quantitative and qualitative objectives while using generative design tools and how their behavior may affect design performance. Thirty-four participants completed a design task with quantitative and qualitative objectives with and without generative design tools. The outcomes produced using generative design tools displayed a greater aesthetic diversity and expanded a larger portion of the objective space compared to those without using a generative design tool. Participants also expressed the ability to focus on the qualitative objectives by delegating the quantitative objective to the generative design tool. This showcases the potential for high-performing generative design tools to assist human designers by alleviating part of their cognitive load when balancing multiple objectives, giving them the bandwidth to focus on other objectives not fully incorporated by the tool. In this way, leveraging the expertise of both the human designer and the generative design tool can allow for greater consideration of various objectives throughout the design process.",
        "authors": [
            "Jana I. Saadi",
            "Leah Chong",
            "Maria C. Yang"
        ],
        "journal_conference_name": "Research in Engineering Design",
        "publisher": "Springer London",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156924",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Distance Between Us: Exploring the COVID-19 Intensive Care Unit Through the Soundscape of Biometric Monitoring",
        "abstract": "Audio technology rested at the heart of the COVID-19 pandemic. Though heart rate monitors, ventilators, and pulse oximeters are not traditionally thought of as tools for musical expression, their sounds hold a resonance that has not been fully explored, given the spaces these biometric monitoring devices held in the zeitgeist. In my piece “The Distance Between Us”, I investigate the role of the sonic output of biometric measurement as a source of input data for artistic expression on its own. I present a score grounded in the auditory environment of a COVID-19 intensive care unit (ICU), with graphic notation inspired by the visual output of heart rate monitors and pulse oximeters, that directs the dynamics of the performers as they improvise using steady-state and alert tones of monitoring devices to explore the soundscape of the ICU. “The Distance Between Us” becomes a representation of the profound connection between soundscape, score, and the emotional and physical toll of the pandemic. I include analyses of performer and audience member reflections that suggest the importance of considering the contextual relationships that we build with auditory stimuli when composing and performing pieces grounded in emotionally-charged subject matter.",
        "authors": [
            "Kimaya Lecamwasam"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Audio Mostly 2024 - Explorations in Sonic Cultures",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157370",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Transverse polarization measurement of Λ hyperons in pNe collisions at  √sNN = 68.4 GeV with the LHCb detector",
        "abstract": "A measurement of the transverse polarization of the Λ andΛ hyperons in pNe\r\nfxed-target collisions at √\r\nsNN = 68.4 GeV is presented using data collected by the LHCb\r\ndetector. The polarization is studied using the decay Λ → pπ− together with its charge\r\nconjugated process, the integrated values measured are\r\nPΛ = 0.029 ± 0.019 (stat) ± 0.012 (syst),\r\nPΛ = 0.003 ± 0.023 (stat) ± 0.014 (syst).\r\nFurthermore, the results are shown as a function of the Feynman x variable, transverse\r\nmomentum, pseudorapidity and rapidity of the hyperons, and are compared with previous\r\nmeasurements.",
        "authors": [
            "R. Aaij",
            "A. S. W. Abdelmotteleb",
            "C. Abellan Beteta",
            "F. Abudinén",
            "T. Ackernley",
            "A. A. Adefisoye",
            "B. Adeva",
            "M. Adinolfi",
            "P. Adlarson",
            "C. Agapopoulou",
            "C. A. Aidala",
            "Z. Ajaltouni",
            "S. Akar",
            "K. Akiba",
            "P. Albicocco",
            "J. Albrecht",
            "F. Alessio",
            "M. Alexander"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157385",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Metabolic modulation to improve MSC expansion and therapeutic potential for articular cartilage repair",
        "abstract": "Background\r\n                Articular cartilage degeneration can result from injury, age, or arthritis, causing significant joint pain and disability without surgical intervention. Currently, the only FDA cell-based therapy for articular cartilage injury is Autologous Chondrocyte Implantation (ACI); however, this procedure is costly, time-intensive, and requires multiple treatments. Mesenchymal stromal cells (MSCs) are an attractive alternative autologous therapy due to their availability and ability to robustly differentiate into chondrocytes for transplantation with good safety profiles. However, treatment outcomes are variable due to donor-to-donor variability as well as intrapopulation heterogeneity and unstandardized MSC manufacturing protocols. Process improvements that reduce cell heterogeneity while increasing donor cell numbers with improved chondrogenic potential during expansion culture are needed to realize the full potential of MSC therapy.\r\n              \r\n              \r\n                Methods\r\n                In this study, we investigated the potential of MSC metabolic modulation during expansion to enhance their chondrogenic commitment by varying the nutrient composition, including glucose, pyruvate, glutamine, and ascorbic acid in culture media. We tested the effect of metabolic modulation in short-term (one passage) and long-term (up to seven passages). We measured metabolic state, cell size, population doubling time, and senescence and employed novel tools including micro-magnetic resonance relaxometry (µMRR) relaxation time (T2) to characterize the effects of AA on improved MSC expansion and chondrogenic potential.\r\n              \r\n              \r\n                Results\r\n                Our data show that the addition of 1 mM L-ascorbic acid-2-phosphate (AA) to cultures for one passage during MSC expansion prior to initiation of differentiation improves chondrogenic differentiation. We further demonstrate that AA treatment reduced the proportion of senescent cells and cell heterogeneity also allowing for long-term expansion that led to a > 300-fold increase in yield of MSCs with enhanced chondrogenic potential compared to untreated cells. AA-treated MSCs with improved chondrogenic potential showed a robust shift in metabolic profile to OXPHOS and higher µMRR T2 values, identifying critical quality attributes that could be implemented in MSC manufacturing for articular cartilage repair.\r\n              \r\n              \r\n                Conclusions\r\n                Our results suggest an improved MSC manufacturing process that can enhance chondrogenic potential by targeting MSC metabolism and integrating process analytic tools during expansion.",
        "authors": [
            "Ching A. Tee",
            "Daniel N. Roxby",
            "Rashidah Othman",
            "Vinitha Denslin",
            "Kiseer S. Bhat",
            "Zheng Yang",
            "Jongyoon Han",
            "Lisa Tucker-Kellogg",
            "Laurie A. Boyer"
        ],
        "journal_conference_name": "Stem Cell Research & Therapy",
        "publisher": "BioMed Central",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156927",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Additive manufacturing of interlocking glass masonry units",
        "abstract": "In comparison to traditional glass casting, glass additive manufacturing (AM) presents an opportunity to increase design flexibility and reduce tooling costs for the production of highly variable geometries. While the latter has been extensively explored for masonry units, there is minimal research on the former for its viability to produce structural building components. This paper encompasses design, manufacturing, and experimental testing to assess the feasibility of using glass AM to produce interlocking masonry units for the construction industry. The glass 3D printer employed in this study is capable of printing a maximum volume of 32.5 ×\r\n 32.5 ×\r\n 38 cm–suitable for producing full-size masonry units. As part of this work, we discuss how to adapt design guidelines for glass AM to produce interlocking units. To evaluate fabrication ease and structural performance, three fabrication methods, Fully Hollow, Print-Cast, and Fully Printed, are compared. To compare the accuracy, repeatability, and structural capacity of each masonry unit, geometric analysis, surface roughness, and mechanical testing is conducted. Results varied by fabrication method, with average strength ranging from 3.64−\r\n42.3 MPa for initial fracture and 64.0–118 MPa for ultimate strength. Accuracy in print dimensions was less than 1 mm with a standard deviation of 0.14–1.6 mm. Results demonstrated that Fully Hollow masonry units provide a more immediate path to implementation, while Fully Printed units have the potential to provide an entirely glass, transparent, and circular building component fabrication method.",
        "authors": [
            "Daniel Massimino",
            "Ethan Townsend",
            "Charlotte Folinus",
            "Michael Stern",
            "Kaitlyn Becker"
        ],
        "journal_conference_name": "Glass Structures & Engineering",
        "publisher": "Springer International Publishing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156928",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Another Myth of Persistence?",
        "abstract": "Gender dysphoria is “the aversion to some or all of those physical characteristics or social roles that connote one’s own biological sex” (Schneider et al., 2009, p. 28). The onset of gender dysphoria may be in early childhood or “around puberty or even much later in life” (American Psychiatric and Association, 2022, p. 517). This Letter concerns childhood-onset gender dysphoria; not gender dysphoria that first manifests in adolescence or adulthood (Zucker et al., 2016). The reported new presentation of “rapid-onset gender dysphoria” (Diaz & Bailey, 2023; Littman, 2018), mostly affecting adolescent natal females, is also not relevant.",
        "authors": [
            "Alex Byrne"
        ],
        "journal_conference_name": "Archives of Sexual Behavior",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157399",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Explaining the spread in measurement of PDMS elastic properties: influence of test method and curing protocol",
        "abstract": "Accuracy in the measurement of mechanical properties is essential for precision engineering and for the interrogation of composition–property relationships. Conventional methods of mechanical testing, such as uniaxial tension, compression, and nanoindentation, provide highly repeatable and reliable results for stiff materials, for which they were originally developed. However, when applied to the characterization of soft and biological materials, the same cannot be said, and the spread of reported properties of similar materials is vast. Polydimethylsiloxane (PDMS), commonly obtained from Dow as SYLGARD 184, is a ubiquitous such material, which has been integral to the rapid development of biocompatible microfluidic devices and flexible electronics in recent decades. However, reported shear moduli of this material range over 2 orders of magnitude for similar chemical compositions. Taking advantage of the increased mechanical scrutiny afforded to SYLGARD 184 in recent years, we combine both published and new experimental data obtained using 9 mechanical test methods. A statistical analysis then elucidates the significant bias induced by the test method itself, and distinguishes this bias from the influence of curing protocols on the mechanical properties. The goal of this work is thus two-fold: (i) it provides a quantitative understanding of the different factors that influence reported properties of this particular material, and (ii) it serves as a cautionary tale. As researchers in the field of mechanics strive to quantify the properties of increasingly complex soft and biological materials, converging on a standardized measurement of PDMS is a necessary first step.",
        "authors": [
            "Hannah Varner",
            "Tal Cohen"
        ],
        "journal_conference_name": "Soft Matter",
        "publisher": "Royal Society of Chemistry",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157524",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Active Learning Pedagogies in High School and Undergraduate STEM Education",
        "abstract": "Active learning (AL) typically involves (1) students applying knowledge and higher-order thinking (2) individually and in groups to (3) problems, cases, scenarios, or questions while (4) reflecting on their learning [1]. AL approaches, including but not limited to project-, problem-, inquiry-, and case-based learning, have been shown to help foster STEM student engagement, performance, interpersonal skills, and higher-order thinking [2,3]. However, the implementation of AL remains challenging for schools and higher education institutions in many contexts, requiring resources, pedagogical expertise, and student buy-in.",
        "authors": [
            "Rea Lavi",
            "Lykke Brogaard Bertel"
        ],
        "journal_conference_name": "education sciences",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157311",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "CRISPR-enabled point-of-care genotyping for APOL1 genetic risk assessment",
        "abstract": "Detecting genetic variants enables risk factor identification, disease screening, and initiation of preventative therapeutics. However, current methods, relying on hybridization or sequencing, are unsuitable for point-of-care settings. In contrast, CRISPR-based-diagnostics offer high sensitivity and specificity for point-of-care applications. While these methods have predominantly been used for pathogen sensing, their utilization for genotyping is limited. Here, we report a multiplexed CRISPR-based genotyping assay using LwaCas13a, PsmCas13b, and LbaCas12a, enabling the simultaneous detection of six genotypes. We applied this assay to identify genetic variants in the APOL1 gene prevalent among African Americans, which are associated with an 8–30-fold increase in the risk of developing kidney disease. Machine learning facilitated robust analysis across a multicenter clinical cohort of more than 100 patients, accurately identifying their genotypes. In addition, we optimized the readout using a multi-analyte lateral-flow assay demonstrating the ability for simplified genotype determination of clinical samples. Our CRISPR-based genotyping assay enables cost-effective point-of-care genetic variant detection due to its simplicity, versatility, and fast readout.",
        "authors": [
            "Robert Greensmith",
            "Isadora T. Lape",
            "Cristian V. Riella",
            "Alexander J. Schubert",
            "Jakob J. Metzger",
            "Anand S. Dighe",
            "Xiao Tan",
            "Bernhard Hemmer",
            "Josefine Rau",
            "Sarah Wendlinger",
            "Nora Diederich",
            "Anja Schütz",
            "Leonardo V. Riella"
        ],
        "journal_conference_name": "EMBO Molecular Medicine",
        "publisher": "Nature Publishing Group UK",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156882",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The transverse energy-energy correlator at next-to-next-to-next-to-leading logarithm",
        "abstract": "We present an operator based factorization formula for the transverse energy-energy correlator in the back-to-back (dijet) region, and uncover its remarkable perturbative simplicity and relation to transverse momentum dynamics. This simplicity enables us to achieve next-to-next-to-next-to leading logarithmic (N3LL) accuracy for a hadron collider dijet event shape for the first time. Our factorization formula applies to W/Z/γ + jet, and dijet production, providing a natural generalization of transverse momentum observables to one- and two-jet final states. This provides a laboratory for precision studies of QCD and transverse momentum dynamics at hadron colliders, as well as an opportunity for understanding factorization and its violation in a perturbatively well controlled setting.",
        "authors": [
            "Anjie Gao",
            "Hai T. Li",
            "Ian Moult",
            "Hua X. Zhu"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157386",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Parisi Formula for Balanced Potts Spin Glass",
        "abstract": "The Potts spin glass is a generalization of the Sherrington–Kirkpatrick (SK) model that allows for spins to take more than two values. Based on a novel synchronization mechanism, Panchenko (Ann Probab 46(2):829–864, 2018) showed that the limiting free energy is given by a Parisi-type variational formula. The functional order parameter in this formula is a probability measure on a monotone path in the space of positive-semidefinite matrices. By comparison, the order parameter for the SK model is much simpler: a probability measure on the unit interval. Nevertheless, a longstanding prediction by Elderfield and Sherrington (J Phys C Solid State Phys 16(15):L497–L503, 1983) is that the order parameter for the Potts spin glass can be reduced to that of the SK model. We prove this prediction for the balanced Potts spin glass, where the model is constrained so that the fraction of spins taking each value is asymptotically the same. It is generally believed that the limiting free energy of the balanced model is the same as that of the unconstrained model, in which case our results reduce the functional order parameter of Panchenko’s variational formula to probability measures on the unit interval. The intuitive reason—for both this belief and the Elderfield–Sherrington prediction—is that no spin value is a priori preferred over another, and the order parameter should reflect this inherent symmetry. This paper rigorously demonstrates how symmetry, when combined with synchronization, acts as the desired reduction mechanism. Our proof requires that we introduce a generalized Potts spin glass model with mixed higher-order interactions, which is interesting it its own right. We prove that the Parisi formula for this model is differentiable with respect to inverse temperatures. This is a key ingredient for guaranteeing the Ghirlanda–Guerra identities without perturbation, which then allow us to exploit symmetry and synchronization simultaneously.",
        "authors": [
            "Erik Bates",
            "Youngtak Sohn"
        ],
        "journal_conference_name": "Communications in Mathematical Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/159035",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Can we achieve atmospheric chemical environments in the laboratory? An integrated model-measurement approach to chamber SOA studies",
        "abstract": "Secondary organic aerosol (SOA), atmospheric particulate matter formed from low-volatility products of volatile organic compound (VOC) oxidation, affects both air quality and climate. Current 3D models, however, cannot reproduce the observed variability in atmospheric organic aerosol. Because many SOA model descriptions are derived from environmental chamber experiments, our ability to represent atmospheric conditions in chambers directly affects our ability to assess the air quality and climate impacts of SOA. Here, we develop an approach that leverages global modeling and detailed mechanisms to design chamber experiments that mimic the atmospheric chemistry of organic peroxy radicals (RO2), a key intermediate in VOC oxidation. Drawing on decades of laboratory experiments, we develop a framework for quantitatively describing RO2 chemistry and show that no previous experimental approaches to studying SOA formation have accessed the relevant atmospheric RO2 fate distribution. We show proof-of-concept experiments that demonstrate how SOA experiments can access a range of atmospheric chemical environments and propose several directions for future studies.",
        "authors": [
            "Hannah S Kenagy",
            "Colette L Heald",
            "Nadia Tahsini",
            "Matthew B Goss",
            "Jesse H Kroll"
        ],
        "journal_conference_name": "Science Advances",
        "publisher": "American Association for the Advancement of Science",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158122",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Crack densification in drying colloidal suspensions",
        "abstract": "As sessile drops of aqueous colloidal suspensions dry, a close-packed particle deposit forms that grows from the edge of the drop toward the center. To compensate for evaporation over the solid’s surface, water flows radially through the deposit, generating a negative pore pressure in the deposit associated with tensile drying stresses that induce the formation of cracks. As these stresses increase during drying, existing cracks propagate and additional cracks form, until the crack density eventually saturates. We rationalize the dynamics of crack propagation and crack densification with a local energy balance between the elastic energy released by the crack, the energetic cost of fracture, and the elastic energy released by previously formed cracks. We show that the final spacing between radial cracks is proportional to the local thickness of the deposit, while the aspect ratio of the crack segments depends on the shape of the deposit.",
        "authors": [
            "Paul Lilin",
            "Mario Ibrahim",
            "Irmgard Bischofberger"
        ],
        "journal_conference_name": "Science Advances",
        "publisher": "American Association for the Advancement of Science",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158098",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Evolving code with a large language model",
        "abstract": "Algorithms that use Large Language Models (LLMs) to evolve code arrived on the Genetic Programming (GP) scene very recently. We present LLM_GP, a general LLM-based evolutionary algorithm designed to evolve code. Like GP, it uses evolutionary operators, but its designs and implementations of those operators significantly differ from GP’s because they enlist an LLM, using prompting and the LLM’s pre-trained pattern matching and sequence completion capability. We also present a demonstration-level variant of LLM_GP and share its code. By presentations that range from formal to hands-on, we cover design and LLM-usage considerations as well as the scientific challenges that arise when using an LLM for genetic programming.",
        "authors": [
            "Erik Hemberg",
            "Stephen Moskal",
            "Una-May O’Reilly"
        ],
        "journal_conference_name": "Genetic Programming and Evolvable Machines",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156878",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Quantifying the internal and external drivers of Southeast Asian rainfall extremes on decadal timescales",
        "abstract": "Rainfall over mainland Southeast Asia experiences variability on seasonal to decadal timescales in response to a multitude of climate phenomena. Historical records and paleoclimate archives that span the last millennium reveal extreme multi-year rainfall variations that significantly affected the societies of mainland Southeast Asia. Here we utilize the Community Earth System Model Last Millennium Ensemble (CESM-LME) to quantify the contributions of internal and external drivers to decadal-scale rainfall extremes in the Southeast Asia region. We find that internal variability was dominant in driving both Southeast Asian drought and pluvial extremes on decadal timescales although external forcing impacts are also detectable. Specifically, rainfall extremes are more sensitive to Pacific Ocean internal variability than the state of the Indian Ocean. This discrepancy is greater for droughts than pluvials which we suggest is attributable to external forcing impacts that counteract the forced Indian Ocean teleconnections to Southeast Asia. Volcanic aerosols, the most effective radiative forcing during the last millennium, contributed to both the Ming Dynasty Drought (1637–1643) and the Strange Parallels Drought (1756–1768). From the Medieval Climate Anomaly to the Little Ice Age, we observe a shift in Indo-Pacific teleconnection strength to Southeast Asia consistent with enhanced volcanism during the latter interval. This work not only highlights asymmetries in the drivers of rainfall extremes but also presents a framework for quantifying multivariate drivers of decadal-scale variability and hydroclimatic extremes.",
        "authors": [
            "Shouyi Wang",
            "Caroline C. Ummenhofer",
            "Sujata A. Murty",
            "Hung T. T. Nguyen",
            "Brendan M. Buckley"
        ],
        "journal_conference_name": "Climate Dynamics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156874",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Performance of a Modular Ton-Scale Pixel-Readout Liquid Argon Time Projection Chamber",
        "abstract": "The Module-0 Demonstrator is a single-phase 600 kg liquid argon time projection chamber operated as a prototype for the DUNE liquid argon near detector. Based on the ArgonCube design concept, Module-0 features a novel 80k-channel pixelated charge readout and advanced high-coverage photon detection system. In this paper, we present an analysis of an eight-day data set consisting of 25 million cosmic ray events collected in the spring of 2021. We use this sample to demonstrate the imaging performance of the charge and light readout systems as well as the signal correlations between the two. We also report argon purity and detector uniformity measurements and provide comparisons to detector simulations.",
        "authors": [
            "A. Abed Abud",
            "B. Abi",
            "R. Acciarri",
            "M. A. Acero",
            "M. R. Adames",
            "G. Adamov",
            "M. Adamowski",
            "D. Adams",
            "M. Adinolfi",
            "C. Adriano",
            "A. Aduszkiewicz",
            "J. Aguilar",
            "B. Aimard",
            "F. Akbar",
            "K. Allison",
            "S. Alonso Monsalve",
            "M. Alrashed",
            "A. Alton",
            "R. Alvarez",
            "T. Alves"
        ],
        "journal_conference_name": "instruments",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157310",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Ductile-to-brittle transition and yielding in soft amorphous materials: perspectives and open questions",
        "abstract": "Soft amorphous materials are viscoelastic solids ubiquitously found around us, from clays and cementitious pastes to emulsions and physical gels encountered in food or biomedical engineering. Under an external deformation, these materials undergo a noteworthy transition from a solid to a liquid state that reshapes the material microstructure. This yielding transition was the main theme of a workshop held from January 9 to 13, 2023 at the Lorentz Center in Leiden. The manuscript presented here offers a critical perspective on the subject, synthesizing insights from the various brainstorming sessions and informal discussions that unfolded during this week of vibrant exchange of ideas. The result of these exchanges takes the form of a series of open questions that represent outstanding experimental, numerical, and theoretical challenges to be tackled in the near future.",
        "authors": [
            "Unknown author"
        ],
        "journal_conference_name": "Soft Matter",
        "publisher": "Royal Society of Chemistry",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157449",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Extracting structured data from organic synthesis procedures using a fine-tuned large language model",
        "abstract": "The popularity of data-driven approaches and machine learning (ML) techniques in the field of organic chemistry and its various subfields has increased the value of structured reaction data. Most data in chemistry is represented by unstructured text, and despite the vastness of the organic chemistry literature (papers, patents), manual conversion from unstructured text to structured data remains a largely manual endeavor. Software tools for this task would facilitate downstream applications such as reaction prediction and condition recommendation. In this study, we fine-tune a large language model (LLM) to extract reaction information from organic synthesis procedure text into structured data following the Open Reaction Database (ORD) schema, a comprehensive data structure designed for organic reactions. The fine-tuned model produces syntactically correct ORD records with an average accuracy of 91.25% for ORD “messages” (e.g., full compound, workups, or condition definitions) and 92.25% for individual data fields (e.g., compound identifiers, mass quantities), with the ability to recognize compound-referencing tokens and to infer reaction roles. We investigate its failure modes and evaluate performance on specific subtasks such as reaction role classification.",
        "authors": [
            "Qianxiang Ai",
            "Fanwang Meng",
            "Jiale Shi",
            "Brenden Pelkie",
            "Connor W Coley"
        ],
        "journal_conference_name": "Digital Discovery",
        "publisher": "Royal Society of Chemistry",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157469",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Shear annealing of a self-interacting sheet",
        "abstract": "2D materials such as graphene, graphene oxide, transition metal dichalcogenides, and 2D polymers have unique properties which allow them to be used in many applications from electronics to energy to biotechnology. Producing and applying these materials often involves solution processing. Previous computational studies have observed 2D sheets in shear and extensional flows, but have focused on steady flows, even though the dynamics of these materials might exhibit hysteresis. In this work, we study 2D sheets with short-ranged attractive interactions under time-varying shear. We show that, even with relatively simple protocols, the properties of sheet suspensions can be tuned.",
        "authors": [
            "William T Funkenbusch",
            "Kevin S Silmore",
            "Patrick S Doyle"
        ],
        "journal_conference_name": "Soft Matter",
        "publisher": "Royal Society of Chemistry",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157509",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The impacts of stockout cost on a stochastic production-inventory system in minimizing total cost conditional value-at-risk",
        "abstract": "Conditional value-at-risk (CVaR) is a metric for downside risks and increasingly used in supply chain management. Under this metric, we build a single-item production-inventory model with stochastic demand. The production capacity, too, is stochastic due to random unit processing time. The total cost includes stockout, inventory holding, and backordering costs and accumulates. After presenting convexity properties of the total cost CVaR within a finite time horizon, we propose a two-moment closed-form normal approximation of it using Markov reward chain theory. The approximated total cost CVaR is exact with respect to the length of the horizon asymptotically. The desirable base-stock levels obtained perform satisfactorily for industry settings. In an extension, we have also benchmarked the proposed approximation with alternative formulations and show that the proposed approximation works satisfactorily. With a numerical simulation experiment, we outline observations and findings that characterize how relevant economic factors could impact the approximately optimal base-stock levels.",
        "authors": [
            "Bingfeng Bai",
            "Bo Li",
            "Xingzhi Jia"
        ],
        "journal_conference_name": "Flexible Services and Manufacturing Journal",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156876",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A cluster of results on amplituhedron tiles",
        "abstract": "The amplituhedron is a mathematical object which was introduced to provide a geometric origin of scattering amplitudes in =4\r\n super Yang–Mills theory. It generalizes cyclic polytopes and the positive Grassmannian and has a very rich combinatorics with connections to cluster algebras. In this article, we provide a series of results about tiles and tilings of the 𝑚=4\r\n amplituhedron. Firstly, we provide a full characterization of facets of BCFW tiles in terms of cluster variables for  Gr4,𝑛. Secondly, we exhibit a tiling of the 𝑚=4 amplituhedron which involves a tile which does not come from the BCFW recurrence—the spurion tile, which also satisfies all cluster properties. Finally, strengthening the connection with cluster algebras, we show that each standard BCFW tile is the positive part of a cluster variety, which allows us to compute the canonica",
        "authors": [
            "Chaim Even-Zohar",
            "Tsviqa Lakrec",
            "Matteo Parisi",
            "Melissa Sherman-Bennett",
            "Ran Tessler",
            "Lauren Williams"
        ],
        "journal_conference_name": "Letters in Mathematical Physics",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156877",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Electrostatic microfiltration (EM) enriches and recovers viable microorganisms at low-abundance in large-volume samples and enhances downstream detection",
        "abstract": "Rapid and sensitive detection of pathogens in various samples is crucial for disease diagnosis, environmental surveillance, as well as food and water safety monitoring. However, the low abundance of pathogens (<10 CFU) in large volume (1 mL−1 L) samples containing vast backgrounds critically limits the sensitivity of even the most advanced techniques, such as digital PCR. Therefore, there is a critical need for sample preparation that can enrich low-abundance pathogens from complex and large-volume samples. This study develops an efficient electrostatic microfiltration (EM)-based sample preparation technique capable of processing ultra-large-volume (≥500 mL) samples at high throughput (≥10 mL min−1). This approach achieves a significant enrichment (>8000×) of extremely-low-abundance pathogens (down to level of 0.02 CFU mL−1, i.e., 10 CFU in 500 mL). Furthermore, EM-enabled sample preparation facilitates digital amplification techniques sensitively detecting broad pathogens, including bacteria, fungi, and viruses from various samples, in a rapid (≤3 h) sample-to-result workflow. Notably, the operational ease, portability, and compatibility/integrability with various downstream detection platforms highlight its great potential for widespread applications across diverse settings.",
        "authors": [
            "Yaoping Liu",
            "Joshua J Raymond",
            "Xiaolin Wu",
            "Patrina Wei Lin Chua",
            "Sharon Yan Han Ling",
            "Chia Ching Chan",
            "Cheryl Chan",
            "Joanne Xin Yi Loh",
            "Melody Xing Yen Song",
            "Matilda Yu Yan Ong",
            "Peiying Ho",
            "Megan E Mcbee",
            "Stacy L Springs",
            "Hanry Yu",
            "Jongyoon Han"
        ],
        "journal_conference_name": "Lab on a Chip",
        "publisher": "Royal Society of Chemistry",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157512",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Analyticity and the Unruh effect: a study of local modular flow",
        "abstract": "The Unruh effect can be formulated as the statement that the Minkowski vacuum in a Rindler wedge has a boost as its modular flow. In recent years, other examples of states with geometrically local modular flow have played important roles in understanding energy and entropy in quantum field theory and quantum gravity. Here I initiate a general study of the settings in which geometric modular flow can arise, showing (i) that any geometric modular flow must be a conformal symmetry of the background spacetime, and (ii) that in a well behaved class of “weakly analytic” states, geometric modular flow must be future-directed. I further argue that if a geometric transformation is conformal but not isometric, then it can only be realized as modular flow in a conformal field theory. Finally, I discuss a few settings in which converse results can be shown — i.e., settings in which a state can be constructed whose modular flow reproduces a given vector field.",
        "authors": [
            "Jonathan Sorce"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156880",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Perovskite nanocomposites: synthesis, properties, and applications from renewable energy to optoelectronics",
        "abstract": "The oxide and halide perovskite materials with a ABX3 structure exhibit a number of excellent properties, including a high dielectric constant, electrochemical properties, a wide band gap, and a large absorption coefficient. These properties have led to a range of applications, including renewable energy and optoelectronics, where high-performance catalysts are needed. However, it is difficult for a single structure of perovskite alone to simultaneously fulfill the diverse needs of multiple applications, such as high performance and good stability at the same time. Consequently, perovskite nanocomposites have been developed to address the current limitations and enhance their functionality by combining perovskite with two or more materials to create complementary materials. This review paper categorizes perovskite nanocomposites according to their structural composition and outlines their synthesis methodologies, as well as their applications in various fields. These include fuel cells, electrochemical water splitting, CO2 mitigation, supercapacitors, and optoelectronic devices. Additionally, the review presents a summary of their research status, practical challenges, and future prospects in the fields of renewable energy and electronics.",
        "authors": [
            "Yunseok Choi",
            "Sangmoon Han",
            "Bo-In Park",
            "Zhihao Xu",
            "Qingge Huang",
            "Sanggeun Bae",
            "Justin S. Kim",
            "Sun O. Kim",
            "Yuan Meng",
            "Seung‐Il Kim",
            "Ji‐Yun Moon",
            "Ilpyo Roh",
            "Ji-Won Park",
            "Sang‑Hoon Bae"
        ],
        "journal_conference_name": "Nano Convergence",
        "publisher": "Springer Nature Singapore",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156881",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "TimelyTale: A Multimodal Dataset Approach to Assessing Passengers' Explanation Demands in Highly Automated Vehicles",
        "abstract": "Explanations in automated vehicles enhance passengers' understanding of vehicle decision-making, mitigating negative experiences by increasing their sense of control. These explanations help maintain situation awareness, even when passengers are not actively driving, and calibrate trust to match vehicle capabilities, enabling safe engagement in non-driving related tasks. While design studies emphasize timing as a crucial factor affecting trust, machine learning practices for explanation generation primarily focus on content rather than delivery timing. This discrepancy could lead to mistimed explanations, causing misunderstandings or unnecessary interruptions. This gap is partly due to a lack of datasets capturing passengers' real-world demands and experiences with in-vehicle explanations. We introduce TimelyTale, an approach that records passengers' demands for explanations in automated vehicles. The dataset includes environmental, driving-related, and passenger-specific sensor data for context-aware explanations. Our machine learning analysis identifies proprioceptive and physiological data as key features for predicting passengers' explanation demands, suggesting their potential for generating timely, context-aware explanations. The TimelyTale dataset is available at https://doi.org/10.7910/DVN/CQ8UB0.",
        "authors": [
            "Gwangbin Kim",
            "Seokhyun Hwang",
            "Minwoo Seong",
            "Dohyeon Yeo",
            "Daniela Rus",
            "SeungJun Kim"
        ],
        "journal_conference_name": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157371",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Stem Life: A Framework for Understanding the Prebiotic-Biotic Transition",
        "abstract": "Abiogenesis is frequently envisioned as a linear, ladder-like progression of increasingly complex chemical systems, eventually leading to the ancestors of extant cellular life. This “pre-cladistics” view is in stark contrast to the well-accepted principles of organismal evolutionary biology, as informed by paleontology and phylogenetics. Applying this perspective to origins, I explore the paradigm of “Stem Life,” which embeds abiogenesis within a broader continuity of diversification and extinction of both hereditary lineages and chemical systems. In this new paradigm, extant life’s ancestral lineage emerged alongside and was dependent upon many other complex prebiotic chemical systems, as part of a diverse and fecund prebiosphere. Drawing from several natural history analogies, I show how this shift in perspective enriches our understanding of Origins and directly informs debates on defining Life, the emergence of the Last Universal Common Ancestor (LUCA), and the implications of prebiotic chemical experiments.",
        "authors": [
            "Gregory P. Fournier"
        ],
        "journal_conference_name": "Journal of Molecular Evolution",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156873",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Environmental identity and perceived salience of policy issues in coastal communities: a moderated-mediation analysis",
        "abstract": "Risk perception influences the perceived salience of various policy issues. In this study, we examine the pathways through which environmental identity influences the perceived salience of two kinds of policy issues—climate change (climate mitigation and climate adaptation) and development (economic growth and infrastructure). Based on a dataset of 503 respondents from coastal communities along the east coast of the United States, our findings indicate that environmental identity is associated with a greater perceived salience of climate mitigation, and that this relationship is mediated by hydrometeorological disaster risk perception. While we found no significant total effect of environmental identity on the perceived salience of climate adaptation, perceived salience of infrastructure development, and perceived salience of economic growth, hydrometeorological disaster risk perception was found to fully mediate all three relationships. Also, the mediated relationships were found to be significantly moderated by gender identity, but not by age (except for the perceived salience of infrastructure development). The study highlights the pivotal role of hydrometeorological risk perception in modifying the perceived importance of different policy issues among environmentalists and has implications for policy and planning in coastal regions.",
        "authors": [
            "Pallavi R. George",
            "Vishal Gupta"
        ],
        "journal_conference_name": "Policy Sciences",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156699",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Wearable bio-adhesive metal detector array (BioMDA) for spinal implants",
        "abstract": "Dynamic tracking of spinal instrumentation could facilitate real-time evaluation of hardware integrity and in so doing alert patients/clinicians of potential failure(s). Critically, no method yet exists to continually monitor the integrity of spinal hardware and by proxy the process of spinal arthrodesis; as such hardware failures are often not appreciated until clinical symptoms manifest. Accordingly, herein, we report on the development and engineering of a bio-adhesive metal detector array (BioMDA), a potential wearable solution for real-time, non-invasive positional analyses of osseous implants within the spine. The electromagnetic coupling mechanism and intimate interfacial adhesion enable the precise sensing of the metallic implants position without the use of radiation. The customized decoupling models developed facilitate the precise determination of the horizontal and vertical positions of the implants with incredible levels of accuracy (e.g., <0.5 mm). These data support the potential use of BioMDA in real-time/dynamic postoperative monitoring of spinal implants.",
        "authors": [
            "Unknown author"
        ],
        "journal_conference_name": "Nature Communications",
        "publisher": "Springer Science and Business Media LLC",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158147",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Quantification of Historical Riverbank Erosion and Population Displacement Using Satellite Earth Observations and Gridded Population Data",
        "abstract": "Riverbank erosion in Bangladesh is a significant hazard, recurring annually and causing loss of homes, land, and livelihoods. Each year, thousands of people are displaced as a result. Given the urgency of mitigating extreme erosion and preventing further displacement, it is imperative to accurately quantify the magnitude and severity of this phenomenon. With an aim to assess the spatiotemporal changes in riverbank erosion and its impact on population displacement, this study used multi-temporal Landsat imagery from 1990 to 2020. To evaluate the impact of riverbank movement on population displacement, this study utilized gridded population data. The analysis revealed that the region has experienced extreme erosion over the past three decades, with the central region exhibiting the highest erosion rates (-128.5 m/year). More than 50% of transects are experiencing high erosion rates (> 50 m/year). The analysis also revealed that over three decades, more than 11% of transects experience continuous erosion, with the central region being the most affected (44%). Additionally, findings indicate that thousands of individuals have been displaced due to severe erosion. The insights gained from this study will help policymakers in formulating effective mitigation and adaptation strategies tailored to the unique challenges of this region.",
        "authors": [
            "Md S. Islam",
            "Juthi R. Mitra"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Springer International Publishing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156702",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Quantifying concentration distributions in redox flow batteries with neutron radiography",
        "abstract": "The continued advancement of electrochemical technologies requires an increasingly detailed understanding of the microscopic processes that control their performance, inspiring the development of new multi-modal diagnostic techniques. Here, we introduce a neutron imaging approach to enable the quantification of spatial and temporal variations in species concentrations within an operating redox flow cell. Specifically, we leverage the high attenuation of redox-active organic materials (high hydrogen content) and supporting electrolytes (boron-containing) in solution and perform subtractive neutron imaging of active species and supporting electrolyte. To resolve the concentration profiles across the electrodes, we employ an in-plane imaging configuration and correlate the concentration profiles to cell performance with polarization experiments under different operating conditions. Finally, we use time-of-flight neutron imaging to deconvolute concentrations of active species and supporting electrolyte during operation. Using this approach, we evaluate the influence of cell polarity, voltage bias and flow rate on the concentration distribution within the flow cell and correlate these with the macroscopic performance, thus obtaining an unprecedented level of insight into reactive mass transport. Ultimately, this diagnostic technique can be applied to a range of (electro)chemical technologies and may accelerate the development of new materials and reactor designs.</jats:p>",
        "authors": [
            "Rémy Richard Jacquemond",
            "Maxime van der Heijden",
            "Emre Burak Boz",
            "Eric Ricardo Carreón Ruiz",
            "Katharine Virginia Greco",
            "Jeffrey Adam Kowalski",
            "Vanesa Muñoz Perales",
            "Fikile Richard Brushett",
            "Kitty Nijmeijer",
            "Pierre Boillat",
            "Antoni Forner-Cuenca"
        ],
        "journal_conference_name": "Nature Communications",
        "publisher": "Springer Science and Business Media LLC",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157754",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Comprehensive analysis of local and nonlocal amplitudes in the B0 → K*0μ+μ− decay",
        "abstract": "A comprehensive study of the local and nonlocal amplitudes contributing to the decay B0 → K*0(→ K+π−)μ+μ− is performed by analysing the phase-space distribution of the decay products. The analysis is based on pp collision data corresponding to an integrated luminosity of 8.4 fb−1 collected by the LHCb experiment. This measurement employs for the first time a model of both one-particle and two-particle nonlocal amplitudes, and utilises the complete dimuon mass spectrum without any veto regions around the narrow charmonium resonances. In this way it is possible to explicitly isolate the local and nonlocal contributions and capture the interference between them. The results show that interference with nonlocal contributions, although larger than predicted, only has a minor impact on the Wilson Coefficients determined from the fit to the data. For the local contributions, the Wilson Coefficient 9\r\n, responsible for vector dimuon currents, exhibits a 2.1σ deviation from the Standard Model expectation. The Wilson Coefficients 10\r\n, ′9\r\n and ′10\r\n are all in better agreement than 9\r\n with the Standard Model and the global significance is at the level of 1.5σ. The model used also accounts for nonlocal contributions from B0 → K*0[τ+τ − → μ+μ−] rescattering, resulting in the first direct measurement of the bsττ vector effective-coupling 9𝜏\r\n.",
        "authors": [
            "R. Aaij",
            "A. S. W. Abdelmotteleb",
            "C. Abellan Beteta",
            "F. Abudinén",
            "T. Ackernley",
            "A. A. Adefisoye",
            "B. Adeva",
            "M. Adinolfi",
            "P. Adlarson",
            "C. Agapopoulou",
            "C. A. Aidala",
            "Z. Ajaltouni",
            "S. Akar",
            "K. Akiba",
            "P. Albicocco",
            "J. Albrecht"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156879",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Portable Acceleration of CMS Computing Workflows with Coprocessors as a Service",
        "abstract": "Computing demands for large scientific experiments, such as the CMS experiment at the CERN LHC, will increase dramatically in the next decades. To complement the future performance increases of software running on central processing units (CPUs), explorations of coprocessor usage in data processing hold great potential and interest. Coprocessors are a class of computer processors that supplement CPUs, often improving the execution of certain functions due to architectural design choices. We explore the approach of Services for Optimized Network Inference on Coprocessors (SONIC) and study the deployment of this as-a-service approach in large-scale data processing. In the studies, we take a data processing workflow of the CMS experiment and run the main workflow on CPUs, while offloading several machine learning (ML) inference tasks onto either remote or local coprocessors, specifically graphics processing units (GPUs). With experiments performed at Google Cloud, the Purdue Tier-2 computing center, and combinations of the two, we demonstrate the acceleration of these ML algorithms individually on coprocessors and the corresponding throughput improvement for the entire workflow. This approach can be easily generalized to different types of coprocessors and deployed on local CPUs without decreasing the throughput performance. We emphasize that the SONIC approach enables high coprocessor usage and enables the portability to run workflows on different types of coprocessors.",
        "authors": [
            "A. Hayrapetyan",
            "A. Tumasyan",
            "W. Adam",
            "J. W. Andrejkovic",
            "T. Bergauer",
            "S. Chatterjee",
            "K. Damanakis",
            "M. Dragicevic",
            "P. S. Hussain",
            "M. Jeitler",
            "N. Krammer",
            "A. Li",
            "D. Liko",
            "I. Mikulec",
            "J. Schieck",
            "R. Schöfbeck",
            "D. Schwarz"
        ],
        "journal_conference_name": "Computing and Software for Big Science",
        "publisher": "Springer International Publishing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156703",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "What we mean when we say semantic: Toward a multidisciplinary semantic glossary",
        "abstract": "Tulving characterized semantic memory as a vast repository of meaning that underlies language and many other cognitive processes. This perspective on lexical and conceptual knowledge galvanized a new era of research undertaken by numerous fields, each with their own idiosyncratic methods and terminology. For example, “concept” has different meanings in philosophy, linguistics, and psychology. As such, many fundamental constructs used to delineate semantic theories remain underspecified and/or opaque. Weak construct specificity is among the leading causes of the replication crisis now facing psychology and related fields. Term ambiguity hinders cross-disciplinary communication, falsifiability, and incremental theory-building. Numerous cognitive subdisciplines (e.g., vision, affective neuroscience) have recently addressed these limitations via the development of consensus-based guidelines and definitions. The project to follow represents our effort to produce a multidisciplinary semantic glossary consisting of succinct definitions, background, principled dissenting views, ratings of agreement, and subjective confidence for 17 target constructs (e.g., abstractness, abstraction, concreteness, concept, embodied cognition, event semantics, lexical-semantic, modality, representation, semantic control, semantic feature, simulation, semantic distance, semantic dimension). We discuss potential benefits and pitfalls (e.g., implicit bias, prescriptiveness) of these efforts to specify a common nomenclature that other researchers might index in specifying their own theoretical perspectives (e.g., They said X, but I mean Y).",
        "authors": [
            "Jamie Reilly",
            "Cory Shain",
            "Valentina Borghesani",
            "Philipp Kuhnke",
            "Gabriella Vigliocco",
            "Jonathan E. Peelle",
            "Bradford Z. Mahon",
            "Laurel J. Buxbaum",
            "Asifa Majid",
            "Marc Brysbaert",
            "Anna M. Borghi",
            "Simon De Deyne",
            "Guy Dove",
            "Liuba Papeo"
        ],
        "journal_conference_name": "Psychonomic Bulletin & Review",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156701",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Implications for spatial non-stationarity and the neighborhood effect averaging problem (NEAP) in green inequality research: evidence from three states in the USA",
        "abstract": "Recent studies on green space exposure have argued that overlooking human mobility could lead to erroneous exposure estimates and their associated inequality. However, these studies are limited as they focused on single cities and did not investigate multiple cities, which could exhibit variations in people’s mobility patterns and the spatial distribution of green spaces. Moreover, previous studies focused mainly on large-sized cities while overlooking other areas, such as small-sized cities and rural neighborhoods. In other words, it remains unclear the potential spatial non-stationarity issues in estimating green space exposure inequality. To fill these significant research gaps, we utilized commute data of 31,862 people from Virginia, West Virginia, and Kentucky. The deep learning technique was used to extract green spaces from street-view images to estimate people’s home-based and mobility-based green exposure levels. The results showed that the overall inequality in exposure levels reduced when people’s mobility was considered compared to the inequality based on home-based exposure levels, implying the neighborhood effect averaging problem (NEAP). Correlation coefficients between individual exposure levels and their social vulnerability indices demonstrated mixed and complex patterns regarding neighborhood type and size, demonstrating the presence of spatial non-stationarity. Our results underscore the crucial role of mobility in exposure assessments and the spatial non-stationarity issue when evaluating exposure inequalities. The results imply that local-specific studies are urgently needed to develop local policies to alleviate inequality in exposure precisely.",
        "authors": [
            "Sophiya Gyanwali",
            "Shashank Karki",
            "Kee M. Jang",
            "Tom Crawford",
            "Mengxi Zhang",
            "Junghwan Kim"
        ],
        "journal_conference_name": "Journal of Geographical Systems",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156698",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Automatic hyperparameter tuning of topology optimization algorithms using surrogate optimization",
        "abstract": "This paper presents a new approach that automates the tuning process in topology optimization of parameters that are traditionally defined by the user. The new method draws inspiration from hyperparameter optimization in machine learning. A new design problem is formulated where the topology optimization hyperparameters are defined as design variables and the problem is solved by surrogate optimization. The new design problem is nested, such that a topology optimization problem is solved as an inner problem. To encourage the identification of high-performing solutions while limiting the computational resource requirements, the outer objective function is defined as the original objective combined with penalization for intermediate densities and deviations from the prescribed material consumption. The contribution is demonstrated on density-based topology optimization with various hyperparameters and objectives, including compliance minimization, compliant mechanism design, and buckling load factor maximization. Consistent performance is observed across all tested examples. For a simple two hyperparameter case, the new framework is shown to reduce amount of times a topology optimization algorithm is executed by 90% without notably sacrificing the objective compared to a rigorous manual grid search.",
        "authors": [
            "Dat Ha",
            "Josephine Carstensen"
        ],
        "journal_conference_name": "Structural and Multidisciplinary Optimization",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156697",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Strong interaction physics at the luminosity frontier with 22 GeV electrons at Jefferson Lab",
        "abstract": "The purpose of this document is to outline the developing scientific case for pursuing an energy upgrade to 22 GeV of the\r\nContinuous Electron Beam Accelerator Facility (CEBAF) at\r\nthe Thomas Jefferson National Accelerator Facility (TJNAF,\r\nor JLab). This document was developed with input from a series of workshops held in the period between March 2022\r\nand April 2023 that were organized by the JLab user community and staff with guidance from JLab management (see\r\nSect. 10). The scientific case for the 22 GeV energy upgrade\r\nleverages existing or already planned Hall equipment and\r\nworld-wide uniqueness of CEBAF high-luminosity operations.\r\nCEBAF delivers the world’s highest intensity and highest precision multi-GeV electron beams and has been do so\r\nfor more than 25 years. In Fall 2017, with the completion\r\nof the 12 GeV upgrade and the start of the 12 GeV science\r\nprogram, a new era at the Laboratory began. The 12 GeV\r\nera is now well underway, with many important experimental results already published, and an exciting portfolio Program Advisory Committee approved experiments planned\r\nfor at least the next 8–10 years [1]. At the same time, the\r\nCEBAF community is looking toward its future and the science that could be obtained through a future cost-effective\r\nupgrade to 22 GeV. The great potential to upgrade CEBAF to\r\nhigher energies opens a rich and unique experimental nuclear\r\nphysics program that combines illustrious history with an\r\nexciting future, extending the life of the facility well into the\r\n2030s and beyond.\r\nJLab at 22 GeV will provide unique, world-leading science with high-precision, high-luminosity experiments elucidating the properties of quantum chromodynamics (QCD) in\r\nthe valence regime (x ≥ 0.1). JLab at 22 GeV also enables\r\nresearchers to probe the transition to a region of sea dominance, with access to hadrons of larger mass and different structures. With a fixed-target program at the “luminosity frontier”, large acceptance detection systems, as well as\r\nhigh-precision spectrometers, CEBAF will continue to offer\r\nunique opportunities to shed light on the nature of QCD and\r\nthe emergence of hadron structure for decades to come. In\r\nfact, CEBAF today, and with an energy upgrade, will continue to operate with several orders of magnitude higher\r\nluminosity than what is planned at the Electron-Ion Collider\r\n(EIC). CEBAF’s current and envisioned capabilities enable\r\nexciting scientific opportunities that complement the EIC\r\noperational reach, thus giving scientists the full suite of tools\r\nnecessary to comprehensively understand how QCD builds\r\nhadronic matter.\r\nThe physics program laid out in this document spans a\r\nbroad range of exciting initiatives that focus on a common\r\ntheme, namely, investigations that explore different facets of\r\nthe nonperturbative dynamics that manifest in hadron structure and probe the richness of these strongly interacting systems. The central themes of this program are reviewed in\r\nSect. 2 - Introduction. The main components of the research\r\nprogram are highlighted in Sects. 3 through 8, followed by\r\nSect. 9, which provides a brief overview of the 22 GeV\r\nCEBAF energy-doubling concept. These sections outline the\r\nkey measurements in different areas of experimental studies\r\npossible at a 22 GeV CEBAF accelerator in the existing JLab\r\nexperimental end stations. They provide details on the key\r\nphysics outcomes and unique aspects of the programs not\r\npossible at other existing or planned facilities.\r\nThe 22 GeV physics program is being developed following three main principles: (a) identify the flagship measurements that can be done only with 22 GeV and their science impacts (Uniqueness); (b) identify the flagship measurements with 22 GeV that can extend and improve the 12 GeV\r\nmeasurements, helping the physics interpretation through\r\nmultidimensional bins in extended kinematics (Enrichment);\r\n(c) identify the measurements with 22 GeV that can set the\r\nbridge between JLab12 and EIC (Complementarity). Even if\r\na sharp separation among these three categories sometimes\r\nis difficult to maintain, we highlight the main points in the\r\nfollowing.",
        "authors": [
            "A. Accardi",
            "P. Achenbach",
            "D. Adhikari",
            "A. Afanasev",
            "C. S. Akondi",
            "N. Akopov",
            "M. Albaladejo",
            "H. Albataineh",
            "M. Albrecht",
            "B. Almeida-Zamora",
            "M. Amaryan",
            "D. Androić",
            "W. Armstrong",
            "D. S. Armstrong",
            "M. Arratia",
            "J. Arrington",
            "A. Asaturyan",
            "A. Austregesilo",
            "H. Avakian",
            "T. Averett",
            "C. A. Gayoso"
        ],
        "journal_conference_name": "The European Physical Journal A",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/159165",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Round-Trip Time Ranging to Wi-Fi Access Points Beats GNSS Localization",
        "abstract": "Wi-Fi round-trip time (RTT) ranging has proven successful in indoor localization. Here, it is shown to be useful outdoors as well&mdash;and more accurate than smartphone code-based GNSS when used near buildings with Wi-Fi access points (APs). A Bayesian grid with observation and transition models is used to update a probability distribution of the position of the user equipment (UE). The expected value (or the mode) of this probability distribution provides an estimate of the UE location. Localization of the UE using RTT ranging depends on knowing the locations of the Wi-Fi APs. Determining these positions from floor plans can be time-consuming, particularly when the APs may not be accessible (as is often the case in order to prevent unauthorized access to the network). An alternative is to invert the Bayesian grid method for locating the UE&mdash;which uses distance measurements from the UE to several APs with known position. In the inverted method we instead locate the AP using distance measurements from several known positions of the UE. In localization using RTT, at any given time, a decision has to be made as to which APs to range to, given that there is a cost associated with each &ldquo;range probe&rdquo; and that some APs may not respond. This can be problematic when the APs are not uniformly distributed. Without a suitable ranging strategy, one can enter a dead-end state where there is no response from any of the APs currently being ranged to. This is a particular concern when there are local clusters of APs that may &ldquo;capture&rdquo; the attention of the RTT app. To avoid this, a strategy is developed here that takes into account distance, signal strength, time since last &ldquo;seen&rdquo;, and the distribution of the directions to APs from the UE&mdash;plus a random contribution. We demonstrate the method in a situation where there are no line-of-sight (LOS) connections and where the APs are inaccessible. The localization accuracy achieved exceeds that of the smartphone code-based GNSS.",
        "authors": [
            "Berthold K. P. Horn"
        ],
        "journal_conference_name": "Applied Sciences",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156737",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The political and legal landscape of space debris mitigation in emerging space nations",
        "abstract": "The issue of space debris and its impact on space sustainability is a growing concern that requires collective action from all nations. Over the past decade, the number of spacefaring nations has increased, as evidenced by the number of satellites launched by emerging space nations and by an increase in the number of applications for United Nations Committee on the Peaceful Uses of Outer Space (UN COPUOS) membership from emerging member states. More recently, there has been an increase in emerging space nations stating their commitment to join the COPUOS Long-term Sustainability (LTS) 2.0 Working Group, as well as nations who have opted to join as signatories to initiatives such as “Net Zero Space” (e.g., Azercosmos, EgSA, GISTDA), and the Artemis Accords (e.g., Nigeria, Rwanda, and Angola). These initiatives share a common goal of promoting the sustainable and responsible use of space to ensure the long-term sustainability of space activities, including: 1) the recognition of the need for sustainable practices; 2) the importance of promoting cooperation in long-term sustainability between all nations; 3) the support of international guidelines and best practices; and 4) the recognition of the increasing role and contribution of emerging space nations.\r\n\r\nGiven the rapid diversification of the space sector, and in accordance with Part C International Cooperation, Capacity-Building and Awareness of the 2019 COPUOS Long Term Sustainability guidelines, many emerging nations continue to face challenges in implementing space debris mitigation and removal measures. The aim of this paper is threefold: 1) showcase examples of emerging space nations who are actively supporting the sustained use of space at a national, regional, and international level, which includes complying with existing binding requirements concerning space debris within national laws; 2) discuss how the Space Sustainability Rating (SSR) provides opportunities for emerging space nations to progress in their efforts to participate in seeking space sustainability; and 3) provide an analysis using the SSR for several missions launched by emerging space nations including recommended steps for increased sustainability in both the design phase and during operations. The study aims to identify potential challenges and opportunities in the adoption of the SSR by emerging space nations, and dispel the perception that sustainable design, operations, and implementation of the LTS guidelines is a barrier for emerging space nations. The selection of nations chosen for the analysis of this paper aims to ensure a representative sample of diverse space market sizes and maturity, with particular consideration given to geographic diversity.",
        "authors": [
            "Jacqueline H. Smith",
            "Minoo Rathnasabapathy",
            "Danielle Wood"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Journal of Space Safety Engineering",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156705",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Fast Charging of Lithium-Ion Batteries While Accounting for Degradation and Cell-to-Cell Variability",
        "abstract": "Safety and maintaining high performance are key considerations during the operation of lithium-ion batteries. Battery degradation, in particular lithium plating and loss of active material, is often accelerated by fast charging. This study explores a strategy for the design of fast charging protocols that takes into account the influence of the variability between battery cells on factors that can impact degradation. We employ a non-intrusive polynomial chaos expansion to identify the key parameters for each degradation condition. We explore the reduction of battery degradation by adjusting constraints such as the maximum C-rate and voltage. Tight control of the key adjustable parameters contributes significantly to reducing the confidence interval of the degradation factors, allowing reduced charging time with minimal degradation. The application of our approach to two state-dependent fast charging protocols for a LiC6/LiCoO2 battery indicates the value in explicitly accounting for uncertainties when designing charging protocols that minimize degradation.",
        "authors": [
            "Minsu Kim",
            "Joachim Schaeffer",
            "Marc D Berliner",
            "Berta Pedret Sagnier",
            "Martin Z Bazant",
            "Rolf Findeisen",
            "Richard D Braatz"
        ],
        "journal_conference_name": "Journal of The Electrochemical Society",
        "publisher": "The Electrochemical Society",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157407",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "FESTIM V&V book",
        "abstract": "This book aims to provide a comprehensive overview of the verification and validation processes conducted for the FESTIM code. Through a series of verification and validation cases, we aim to demonstrate the accuracy, reliability, and applicability of FESTIM for simulating hydrogen transport phenomena.",
        "authors": [
            "Remi Delaporte-Mathurin",
            "Jair Santana",
            "Vladimir Kulagin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156690.3",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Modeling rational agents with limited capability",
        "abstract": "In many scenarios, players exhibit inherent limitations in various aspects of their capability to generate maximally rational play in strategic games. Modeling such capability limitations and elucidating their implications will advance our understanding of the strategic interactions among players. In this thesis, I study two novel settings where players have limited capabilities. I formalize a hierarchy of capabilities and study related equilibrium concepts, computational complexity, solution algorithms, and the impact of varying capabilities on game outcomes.\r\n\r\nThe first limited-capability setting is limited-perception games. I focus on a class of oneshot limited-perception games. Such games extend simultaneous-move normal-form games by presenting each player with an individualized perception of the true game. Players’ payoffs are determined by the true game hidden from players. The accuracy of a player’s perception is determined by the player’s capability level, with a higher level corresponding to a more accurate perception. I study both capability-oblivious and capability-aware players. A capability-oblivious player does not know they have limited perception and therefore plays the optimal strategy of their perceived game. I present payoff bounds and other predictable behavior of capability-oblivious players in a special class of limited-perception games. A capability-aware player reasons with the set of possible true payoff functions and other players’ perceptions and incentives to maximize their own objective (e.g., the worst-case payoff) based on their limited perception. I present novel formalizations of simultaneousmove equilibria and show the hardness of equilibrium solving. I further present positive results that (i) an approximate equilibrium has a compact, tractable representation; and (ii) a few classes of zero-sum games can be efficiently solved.\r\n\r\nThe aforementioned efficiently solvable zero-sum games are reduced to solving nonsmooth convex programs. To this end, I present the Trust Region Adversarial Functional Subdifferential (TRAFS) algorithm for constrained optimization of unstructured nonsmooth convex Lipschitz functions. Unlike previous methods that assume a subgradient oracle model, I propose the functional subdifferential, defined as a set of subgradients that simultaneously captures sufficient local information for effective minimization while being easy to compute for a wide range of functions. Intriguingly, the TRAFS design also incorporates game-theoretical thinking. In each iteration, TRAFS solves a zero-sum game between the optimizer and a local approximation of the objective function to guarantee progress. The optimizer has access to step vectors in a local ℓ2 -bounded trust region; the local approximation uses the functional subdifferential. TRAFS finds an approximate solution with an absolute error up to ϵ in O(1/ϵ) or O(\\sqrt{1/ϵ}) 1/ϵ iterations depending on whether the objective function is strongly convex, improving the previously best-known bounds of O((1/ϵ)^2) and O(1/ϵ) in these settings. TRAFS makes faster progress if the functional subdifferential satisfies a locally quadratic property; as a corollary, TRAFS achieves linear convergence (i.e., O(log 1/ϵ)) for strongly convex smooth functions. In the numerical experiments, TRAFS solves twice as many problems compared to the second-best method and is on average 39.1x faster on problems solved by both methods.\r\n\r\nThe second limited-capability setting is limited-strategy games where a player’s capability limits the strategies available to them. I work with a formalization where a player’s strategy space is defined as programs in a Domain-Specific Language (DSL). A player’s capability limits the size of programs available to that player. I focus on characterizing the impact of player capability on game outcomes. I study a new game model called McDncDa derived from network congestion games. I show that it is computationally hard to determine whether an McDncDa instance is capability-positive (i.e., whether increasing a player’s capability level leads to a better payoff). I then study a parameterized special class of McDncDa called MGMG. I show that MGMG is always capability-positive, and it is socially capabilitypositive (i.e., the sum of all players’ payoffs always gets better if every player’s capability level is increased by one) if some resources in the game have increasing returns to scale despite the existence of multiple equilibria.",
        "authors": [
            "Kai Jia"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158493",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Power Efficient Analog Front End for Continuous\r\nUltrasound Imaging of the Bladder",
        "abstract": "Continuous bladder monitoring is important for the monitoring of bedridden patients. One method to continuously monitor the bladder is to capture ultrasound images and use machine learning processing to measure the bladder volume from these images. Circuits for implementing these functions can be integrated onto a wearable device, and each of these functions can be integrated onto a single chip. In this thesis, we analyze ultrasound imaging in the context of the bladder to come up with algorithms and hardware to perform continuous bladder monitoring. We first assemble a discrete setup which can form ultrasound images. Using this setup, we describe a new algorithm for generating an ultrasound image by to power gate the hardware during the imaging process to save additional power when capturing the image. We combine these concepts into a single Analog Front End (AFE) chip that can capture images in a power efficient manner.",
        "authors": [
            "Mohith Manohara"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158517",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Light-induced States and Phase Transitions in Quantum Materials investigated by Photoemission Spectroscopy and Epitaxial Synthesis",
        "abstract": "In condensed matter physics, a field of the study on phases of matter and their transitions, light-induced states and phase transitions have attracted significant attention due to their importance in both fundamental research and applications. This thesis will specifically delve into three compelling studies: (1) Floquet-Bloch states, photon-dressed Bloch states, were investigated in graphene. These states are generated by a time-periodic potential of light, closely related to the topic of Floquet engineering. (2) A light-induced insulator-to-metal transition was observed in Sr₂IrO₄, providing valuable insights into the fundamental characteristics of its ground states. (3) A light-induced topological phase transition (from a Z₂ topological insulator to a trivial insulator) was investigated in Bi-doped (Pb,Sn)Se thin-films. For these studies, we employed time- and angle-resolved photoemission spectroscopy (trARPES) and molecular beam epitaxy (MBE). Through in-depth investigation into these phenomena, this thesis seeks to contribute to the broader understanding of light-matter interactions in quantum materials.",
        "authors": [
            "Dongsung Choi"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158272",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Programmable Interactions between Optical Fields and Atom-like Systems in Integrated Circuits",
        "abstract": "Photons can interact with a wide variety of quantum systems and their ability to more easily preserve their coherence makes them ideal candidates for transmitting information between remote quantum information processors. Photonic integrated circuits (PICs), which can be manufactured with modern semiconductor fabrication, provide a platform in which such interactions can occur at scale. Implementing integrated devices enabling these interactions within programmable and scalable settings while preserving a sufficient amount of strength continues to be a general goal in quantum photonics. Here, we implement device designs and architectures that improve current limits on the programmability and scalability of three types of optical interactions. More specifically, we explore the use of programmable multimode interference as a means for unitary transformations onto a set of optical spatial modes, optical resonators for high-extinction coherent modulators driven by RF signals, and large-scale silicon photonics for interacting with hybrid integrated quantum dot emitters.",
        "authors": [
            "Hugo Larocque"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158479",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Contact Free Monitoring of Cell Density in a Bioreactor with Magnetic Resonance Relaxometry",
        "abstract": "Frequent, low-latency measurements of bioreactor culture growth are critical for achieving maximum culture efficiency and productivity. Typical cell density and viability measurements are made by removing a sample from the culture, but this approach is both slow and unsuitable for small culture volumes that cannot support frequent destructive sampling. In this work, magnetic resonance relaxometry measurements taken through the walls of the bioreactor tubing are used to monitor the cell density in near real-time. Using intracellular iron as the marker, the system detects variations in cell density in minutes, enabling rapid intervention to save the culture that would be impossible with the once-daily measurements taken by a traditional sampling-based culture analysis system. Given the biochemical importance of intracellular iron, these measurements have the potential to provide phenotypic information on cells without disrupting the bioreactor culture.",
        "authors": [
            "Hans Gaensbauer"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158512",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Optical Characterisation of Strain and Defects in 2D Photonic Materials",
        "abstract": "Strain and defect engineering have shown to be powerful tools in modifying optoelectronic properties of semiconductors. This thesis aims to advance the fundamental understanding of electronic and optical properties in material systems with broken inversion symmetries and to use this understanding to engineer in-situ, localized strain fields for tailoring photonic responses at the nanoscale. We will address the fundamental question: How can we characterize the effect of strain and defects in two-dimensional photonic materials? To this end, we open with a review of current strategies in strain engineering, its fundamental consequences on electronic, optical, and magnetic properties, and the state-of-the-art applications of this technology in achieving band-gap-engineered straintronic devices. Touching on the advent of strain engineering for flexoelectricity - a spontaneous material polarization produced by a strain gradient that lifts the inversion symmetry, which can enable a bulk photogalvanic effect, we posit the aspect of meta-valent bonding in materials having a key role in this, by showing that the majority of prime material candidates known to have exhibit large photogalvanic response exhibit this characteristic. The rest of the thesis focuses on characterizing layered metal thio(seleno)phosphates, a family of materials known for their magnetic, electronic, and nonlinear optical properties. We show how the optical properties of these materials can be modulated via different means of defects and strain. These photoactive materials can be pivotal to a future comprising of strain-engineered flexoelectric devices, which take advantage of the bulk photogalvanic effect, to develop a new family of practical, deployable, self-powered, and low-cost photodetectors, and integrated arrays with limits-breaking performance in the UV-to-LWIR spectral bands.",
        "authors": [
            "Abhishek Mukherjee"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158473",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Parsimonious Principles of Deep Neural Networks",
        "abstract": "At the core of human intelligence lies an insatiable drive to uncover the simple underlying principles that govern the world’s complexities. This quest for parsimony is not unique to biological cognition but also seems to be a fundamental characteristic of artificial intelligence systems. In this thesis, we explore the intrinsic simplicity bias exhibited by deep neural networks — the powerhouse of modern AI. By analyzing the effective rank of the learned representation kernels, we unveil the observation that these models have an inherent preference for learning parsimonious relationships in the data. We provide further experimental results to support the hypothesis that simplicity bias is a good inductive bias for finding generalizing solutions. Building upon this finding, we present the Platonic Representation Hypothesis — the idea that as AI systems continue to grow in capability, they will converge toward not only simple representational kernels but also a common one. This phenomenon is evidenced by the increasing similarity of models across domains, suggesting the existence of a Platonic “ideal” way to represent the world. However, this path to the Platonic representation necessitates scaling up AI models, which poses significant challenges regarding computational demand. To address this obstacle, we conclude the thesis by proposing a framework for training a model with parallel low-rank updates to effectively reach this convergent endpoint.",
        "authors": [
            "Minyoung Huh"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158482",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Quantum free games",
        "abstract": "The complexity of free games with two or more classical players was essentially settled by Aaronson, Impagliazzo, and Moshkovitz [AIM14]. In the quantum world, there are two complexity classes that can be considered quantum analogues of classical free games: (1) AM⇤, the multiprover interactive proof class corresponding to free games with entangled players, and, somewhat less obviously, (2) BellQMA(2), the class of quantum Merlin-Arthur proof systems with two unentangled Merlins, whose proof states are separately measured by Arthur. In this work, we make significant progress towards a tight characterization of both of these classes. \r\n1. We show a BellQMA(2) protocol for 3SAT on n variables, where the total amount of communication is Õ(√n). This answers an open question of Chen and Drucker [CD10] and also shows, conditional on ETH, that the algorithm of Brandao, Christandl and Yard [BCY10] for optimizing ˜ over separable states is tight up to logarithmic factors. \r\n2. We show that AM*[ⁿprovers = 2, q = O(1), a = poly log(n)] = RE, i.e. that free entangled games with constant-sized questions are as powerful as general entangled games. (In contrast, [AIM14] shows that classical free games are much weaker than general classical games.) We show this using a question “hyper-compression” theorem that iteratively applies the introspection technique of Ji et al. [JNV⁺20]. Our result is a significant improvement over the headline result of Ji et al., whose MIP⇤ protocol for the halting problem has poly(n)-sized questions and answers. \r\n3. By the same techniques, we obtain a zero-gap AM* protocol for a P2 complete language with constant-size questions and almost logarithmically (O(log n · log* n)) large answers, improving on the headline result of Mousavi, Nezhadi and Yuen [MNY21]. \r\n4. Using a connection to the nonuniform complexity of the halting problem we show that any MIP* protocol for RE requires W(log n) bits of communication. It follows that our results in item 3 are optimal up to an O(log* n) factor, and that the gapless compression theorems of [MNY21] are asymptotically optimal. We conjecture that these bounds can be saturated in the gapped case as well.",
        "authors": [
            "Tina Zhang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158516",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design and Evaluation of a Powered Series-Elastic Cycloidal Ankle (CyAn) Prosthesis",
        "abstract": "The prevalence of major lower limb loss in the United States is projected to increase significantly due to rising rates of diabetes and obesity, highlighting an urgent need for advanced prosthetic solutions [1]. Individuals with lower limb amputations often face increased energy expenditure and secondary musculoskeletal conditions as a result of using conventional prosthetic devices [2]. These challenges underscore the necessity for innovative prosthetic designs that can enhance user mobility and comfort. A promising prosthesis solution are powered ankle-foot prostheses, which have the potential to provide biologically accurate push-off power, thereby offering significant benefits such as improved walking economy, increased mobility, and reduced impact forces on the user’s residual limb. However, existing powered prostheses often lack customization and fail to adequately meet the diverse and specific needs of individual users, which can limit their effectiveness and adoption. This thesis introduces a personalized, optimized, low-profile powered ankle-foot prosthesis, known as the Cycloidal Ankle (CyAn), designed to achieve biological ranges of motion and torque during level-ground walking. The CyAn employs a cycloidal drive transmission and a series carbon fiber spring to mimic tendon-like compliance, which enhances energy storage and return while maintaining a low build height to accommodate a broader range of users. The prosthesis device is capable of 25◦ of dorsiflexion and 41◦ of plantarflexion, and is capable of outputting at least 130 Nm of torque during walking, corresponding to biological ankle torque during level ground walking at 1.5 m/s for a 50th percentile male [3]. The CyAn prosthesis uses of a cycloidal drive transmission coupled with a series carbon fiber spring. This combination replicates tendon-like compliance and allows for a reduced build height without compromising the prosthesis’s range of motion or mechanical performance. The development of the CyAn prosthesis involved a comprehensive mechanical and mechatronic design process, encompassing modeling, optimization of electrical energy consumption, component selection, and benchtop and clinical evaluation. This thesis describes the detailed design and analysis of the CyAn prosthesis, including a parametric model for predicting device performance, fatigue life calculations, and mechanical integrity assessments of device components. Benchtop testing results confirm that the device successfully achieves the targeted performance metrics, demonstrating its capability to replicate natural gait mechanics. The clinical validation study was conducted with 3 participants with unilateral transtibial amputation at 3 different walking conditions: level ground at 1.5 m/s, uphill (+10◦ slope) at 0.8 m/s, and downhill (-10◦ slope) at 1.2 m/s. During the experiment, the subjects walked on an instrumented treadmill to regulate the walking speed while force and motion data were recorded. The results of these tests demonstrate the prosthesis design’s capability to replicate natural gait mechanics and kinetics, as well as insights into further improvements and adaptations. This thesis comprehensively details the mechanical and mechatronic design processes, encompassing modeling, optimization, component selection, and empirical evaluation of the CyAn prosthesis. This thesis presents the first of its kind rotary powered ankle-foot prosthesis, utilizing a cycloidal drive mechanism and a custom series carbon fiber spring. Compared to existing powered devices, the CyAn offers a lower device mass and increased biomimetic functionality, making it a cost-effective solution for improving mobility and quality of life for transtibial amputees. This research establishes a framework for developing customized prosthetic solutions that address the unique needs of individual users, with significant clinical results demonstrating the potential of the CyAn to improve health outcomes by normalizing biomechanics, increasing energy efficiency, and reducing adverse limb loading.",
        "authors": [
            "Lucy W. Du"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158319",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Towards a Unified Framework for Visual Recognition and Generation via Masked Generative Modeling",
        "abstract": "Recognition and generation are two key tasks in computer vision. However, recognition and generative models are typically trained independently, which ignores the complementary nature of the two tasks. In this thesis, we present a unified framework for visual data recognition and generation via masked generative modeling, and further demonstrate its superior power to address challenges across various applications. We will begin with MAGE, a novel framework that unifies image generation and recognition while achieving state-ofthe-art performance on both tasks. We then extend it into vision-language multi-modal training through ITIT, which utilizes unpaired image and text data to train models capable of high-quality, bidirectional image-text generation – the recognition power enables accurate image-to-text captioning, while the generation power enables realistic text-to-image generation. Moreover, inspired by the synergy between image generation and recognition observed in MAGE, we introduce RCG, a framework that enhances the quality of unconditional image generation to the same level of class-conditional generation, by using representations learned in a self-supervised manner to guide the generative process. Lastly, we introduce Reparo to address the challenge of packet loss in video conferencing with the help of masked generative modeling, enabling the reconstruction of lost video data without traditional error correction methods. This ensures high-quality communication even under conditions of substantial data loss. These works demonstrate the power of the proposed unified framework, to not only push forward the state-of-the-art in individual downstream applications but also to provide robust, versatile solutions adaptable to a wide range of real-world problems in computer vision and beyond.",
        "authors": [
            "Tianhong Li"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158500",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "New tools for Bayesian optimal experimental design and kernel-based generative modeling",
        "abstract": "This thesis develops new computational approaches for two canonical problems in statistics and machine learning: optimal experimental design and generative modeling.\r\nOptimal experimental design (OED) is important to model development for science and engineering applications and beyond, especially when only a small number of observations can be taken or experiments performed, due to resource limitations. In the Bayesian setting, a useful criterion for the importance of candidate experiments is the expected information gain (EIG) from prior to posterior, or equivalently, the mutual information (MI) between candidate observations and the parameters of interest. Yet estimating EIG for a given design can be quite challenging in nonlinear/non-Gaussian models, and for high-dimensional parameters and observations. \r\n\r\nIn the first part of the thesis, we introduce new methods for estimating EIG based on transportation of measure. Specifically, we use marginal and conditional density estimates, obtained with semi-parametric transport models, in a Monte Carlo estimator. The density estimates are obtained by solving convex optimization problems. This framework is also compatible with implicit models, where one can simulate from the likelihood or prior but the associated density functions are unknown. We identify the optimal scaling of sample sizes between the \"inner\" density estimation steps and the \"outer\" EIG estimation, and demonstrate the efficiency of these choices numerically. If the dimensions of the parameters or observations are high, however, direct density estimation becomes intractable. Here, we use gradient-based information bounds, obtained via log-Sobolev inequalities, to identify optimal projections of the parameters and observations, and then apply our transport-based EIG estimation scheme. \r\n\r\nWe next study the problem of cardinality-constrained observation selection to maximize MI in non-Gaussian settings, i.e., choosing the most informative subset of k observations from a candidate pool of size n > k. Finding the exact solution is to this combinatorial optimization problem is computationally costly, so we resort to greedy approaches based on computationally inexpensive lower bounds for MI. Here we again use log-Sobolev inequalities to construct such lower bounds for certain classes of non-Gaussian distributions, and exploit these lower bounds within the combinatorial problems. We demonstrate that our method outperforms random selection strategies and Gaussian approximations in many settings, including challenging nonlinear design problems with non-additive noise.\r\n\r\nIn the second part of the thesis, we turn our attention to generative modeling, which can be understood as the problem of drawing new samples from an unknown distribution, from which a fixed sample is available. Our approaches employ kernel-type algorithms based on diffusion maps.\r\nFirst, we propose an interacting particle system for generative modeling, based on diffusion maps and Laplacian-adjusted Wasserstein gradient descent (LAWGD). Diffusion maps are used to approximate the generator of the corresponding Langevin diffusion process from samples, and hence to learn the underlying data-generating manifold. LAWGD enables efficient sampling from the target distribution given the generator of the Langevin diffusion process, which we construct here via a spectral approximation via kernels, computed with diffusion maps. Our method requires no offline training and minimal tuning, and can outperform other approaches on data sets of moderate dimension.\r\n\r\nSecond, we propose a generative model combining diffusion maps and Langevin dynamics. Diffusion maps are used to approximate the drift term from the available training samples, which is then implemented in a discrete-time Langevin sampler to generate new samples. By setting the kernel bandwidth to match the time step size used in the unadjusted Langevin algorithm, our method effectively circumvents any stability issues typically associated with time-stepping stiff stochastic differential equations. We demonstrate the performance of our proposed scheme through experiments on synthetic datasets of increasing dimension, and on a conditional sampling problem arising in stochastic subgrid-scale parametrization of a dynamical system.",
        "authors": [
            "Fengyi Li"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158795",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Designing Sparse Representations for Efficient Planning\r\nin Uncertain Environments",
        "abstract": "We would like to enable robots to navigate efficiently in large, outdoor environments, where the traversabilities of many regions are unknown prior to planning. If we reason about the uncertainty in the environment instead of assuming that all unknown space is free to move through, we can generate policies that result in, on average, more efficient navigation. However, designing models that enable intelligent and efficient reasoning about environmental uncertainty is challenging. We would like our model to capture the underlying navigation problem and accurately represent the relevant uncertainty, yet remain as sparse as possible, so that planning remains tractable. Higher model expressiveness improves plan quality but reduces computational efficiency in planning, whereas higher model sparsity improves efficiency at the cost of plan quality. Balancing model expressiveness and model sparsity, thus, is crucial for generating high quality plans efficiently. In this thesis, we describe several useful models for planning under uncertainty and justify our decision to use weighted stochastic graphs with probabilistically traversable edges. We then present a novel method of efficiently generating sparse stochastic graphs given coarse information derived from overhead images of our environments. We test our approach in several simulated environments, demonstrating that our graphs effectively trade off between plan quality and planning efficiency for uncertainty-aware agents navigating in the graph. We then deploy our algorithms in a real-world environment on real-world hardware for single-agent and multi-agent teams. We discuss the challenges associated with using our approach in the field and the implications of our model assumptions not matching the real world. Finally, we present preliminary results for adding cost uncertainty to our graph-based representation.",
        "authors": [
            "Yasmin Veys"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158518",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Guiding Navigation of Unknown Environments with Distant Visual Cues",
        "abstract": "While navigating unknown environments, robots rely primarily on proximate features for guidance in decision making such as depth information from lidar or stereo to build a costmap, or local semantic information from images. The limited range over which these features can be used can result in poor robot behavior when assumptions made by motion planning about the cost of the map beyond the range of proximate features misguide the robot. Integrating “far-field” image features that originate beyond these proximate features into the mapping pipeline has the promise of enabling more intelligent and aware navigation through unknown terrain. To navigate with far-field features, key challenges must be overcome. As far-field features are typically too distant to localize precisely they are difficult to place in a map. Additionally, the large distance between the robot and these features makes connecting these features to their navigation implications more challenging. In this thesis we propose FITAM, an approach that learns to use far-field features to predict navigation costs to guide navigation through unknown environments from previous experience in a self-supervised manner. Unlike previous work, our approach does not rely on flat ground plane assumptions or range sensors to localize observations. We demonstrate the benefits of our approach through simulated trials and real-world deployment on a Clearpath Robotics Warthog navigating through a forest environment.",
        "authors": [
            "Ethan Kendall Fahnestock"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158514",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Building Strategic AI Agents for Human-centric Multi-agent Systems",
        "abstract": "This thesis addresses the challenge of developing strategic AI agents capable of effective decision-making and communication in human-centric multi-agent systems. While significant progress has been made in AI for strategic decision-making, creating agents that can seamlessly interact with humans in multi-agentic settings remains a challenge. This research explores the limitations of current approaches, such as self-play reinforcement learning (RL) and imitation learning (IL), and proposes novel methods to overcome these constraints. Modeling human-like communication and decision making is a crucial first step toward building effective strategic agents. The initial part of the thesis addresses this through two approaches. We start by developing a regret minimization algorithm for modeling actions of strong and human-like agents called piKL, which incorporates a cost term proportional to the KL divergence between a search policy and a humanimitation learned policy. This approach improves reward while keeping behavior close to a human-imitation learned policy, producing agents that predict human actions accurately while improving performance in the benchmark game of no-press Diplomacy. Then, we develop a procedure for modeling populations of agents that communicate with humans using natural language. Our sample-efficient multitask training scheme for latent language policies (LLPs) improves the reward obtained by these policies while preserving the semantics of language in a complex real-time strategy game. Building on these foundations, the second part of the thesis focuses on building strategic agents for human-centric multi-agent domains. The research introduces the DiL-piKL planning algorithm and its extension, RL-DiL-piKL, which regularize self-play reinforcement learning and search towards a human imitation-learned policy. These algorithms enable the training of Diplodocus, an agent achieving expert human-level performance in no-press Diplomacy. A significant milestone is reached with Cicero, the first AI agent to achieve human-level performance in full-press Diplomacy, integrating a language model (LM) with planning and reinforcement learning algorithms based on piKL. The final part of the thesis revisits language generation tasks, applying piKL to model pragmatic communication and improving LM truthfulness. It presents Regularized Conventions (ReCo), a model of pragmatic language understanding that outperforms existing best response and rational speech act models across several datasets. Furthermore, a novel approach to LM decoding is introduced, casting it as a regularized imperfect-information sequential signaling game. This results in the equilibrium-ranking algorithm, which consistently improves performance over existing language model decoding procedures.",
        "authors": [
            "Athul Paul Jacob"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158481",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Characterizing the Epistemic Uncertainty of Predictive Action Models and Sampling-Based Motion Planners for Robotic Manipulation",
        "abstract": "We derive methods to represent the epistemic uncertainty of models used in long-horizon robot planning problems in autonomous manipulation. We develop a representation of epistemic uncertainty for two types of models: uncertainty over the physical parameters of a model that predicts the observed outcome of a manipulation action and uncertainty over a geometric graph built by a sampling-based motion planner as a representation of the configuration space to answer a motion planning query. We propose a simple planning system that integrates these uncertainty characterizations to reason about the informational value of executing a manipulation action or allocating a number of samples to a sampling-based motion planner.",
        "authors": [
            "Seiji A. Shaw"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158490",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Computational and Statistical Detection of High-Dimensional Latent Space Structure in Random Networks",
        "abstract": "A probabilistic latent space graph PLSG (n, Ω, D, σ) is parametrized by its number of vertices n, a\r\nprobability distribution D over some latent space Omega,  and a connection function [mathematical function] such that [mathematical formula] almost surely with respect to D. To sample from [mathematical notations], first for each node [mathematical formula] an independent latent (feature) vector x_i is drawn from Omega according to D. Then, for each pair of vertices i and j an edge is drawn independently with probability sigma(x_i,x_j).$ Interest in settings of high-dimensional latent spaces $\\Omega$ has surged in recent years due to the rise of high-dimensional data and powerful compute.\r\n\r\nThe features x₁, x₂, . . . , xₙ are oftentimes hidden due to privacy considerations or absence of measurement. This gives rise to many challenging statistical tasks. A prerequisite for nearly any more sophisticated inference and estimation task is the following simple hypothesis testing question. When can we even test for the presence of high-dimensional latent space structure? When is there a computationally efficient test and what could this computationally efficient test be? We address the following aspects of these questions in the thesis.\r\n\r\nChapter 2: We focus on the canonical geometric setting when latent vectors are distributed uniformly over the sphere [mathematical formula] where Tₚ is such that expected graph density is p. A conjecture that has witnessed continuous interest and progress in the past 15 years is that the information-theoretically optimal test for detecting the spherical random geometric graph is the signed triangle count. We contribute to the existing literature by confirming that the signed triangle count is computationally optimal among low-degree polynomial tests. Our main technical ingredient is a strategy for bounding Fourier coefficients of random geometric graphs based on a representation of spherical random geometric graphs as Erdős-Rényi with few planted edges. This part of the thesis is based on [BB24b].\r\n\r\nChapter 3: The conjectured optimality of the signed triangle count and the relavance of triangle-based statistics to the axiomatic triangle inequality of metric spaces have led to the conventional wisdom that triangle-based statistics are optimal in monotone random geometric graphs. We break this intuition by showing that in the case of a sup-norm geometry over the torus, the signed 4-cycle count is strictly stronger than the signed triangle count and is, furthermore, optimal among low-degree tests. Our main technical contribution is a novel strategy for bounding Fourier coefficients of random geometric graphs mimicking the cluster-expansion formula from statistical physics. This part of the thesis is based on [BB24a].\r\n\r\nChapter 4: While random geometric graphs over the sphere with Euclidean geometry and the torus with sup-norm geometry are interesting mathematically, they are perhaps too simplistic to describe real-world networks. Hence, one should ask to what extent the results and techniques used for these models generalize to other probabilistic latent space graphs. We introduce a new family of probabilistic latent space graphs which we call random algebraic graphs. In random algebraic graphs, Omega is an algebraic group and sigma is compatible with the group structure. This family captures the aforementioned random geometric graphs as well as instances of the stochastic block model and random subgraphs of Cayley graphs. We have two sets of results. First, we develop a general criterion based solely on the magnitudes of Fourier coefficients of sigma for the statistical hardness of detecting a random algebraic graph when the underlying group is the Boolean hypercube. We use this result to provide a uniform approach to many previously known results in the literature, but also highlight that certain structural properties of the connection function such as non-trivial symmetries and non-monotonicity yield novel behavior. Second, we exhibit a universal behavior for the impossibility of detecting a random algebraic graph based solely on the group size but not on the group structure. The result can be equivalently phrased in terms of the local structure of typical Cayley graphs. This part of the thesis is based on [BB23].",
        "authors": [
            "Kiril Bangachev"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158510",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Maliciously Secure Computation, Theory and Practice",
        "abstract": "Data analytics fuels countless innovations and reveals unparalleled insights, and these benefits only grow the more data is amassed. This has resulted in the size of datasets and the compute needed to manage them becoming too resource-intensive for even large companies to handle alone, fueling the rise of cloud computing and outsourced data management. A central problem with this outsourcing is security. How can parties ensure that an untrusted cloud is accurately running the prescribed protocol? More generally, how can two parties collaborate to run a computation over joint inputs, where both inputs remain private while still delivering the correct output? This thesis focuses on answering these questions by constructing secure computation protocols with low communication & computation overhead. The protocols in this thesis include several concretely efficient constructions of private information retrieval, a functional commitment scheme for all functions, and a general two-party secure computation scheme that comes within polylogarithmic factors of the optimal communication and computation complexity. In addition to their efficiency, all protocols presented in this thesis guarantee protection against worst-case, malicious adversaries.",
        "authors": [
            "Leo de Castro"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158496",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Multi-Proxy Records of Climate and Carbon Cycle Perturbations in the Paleozoic: Integrating Isotope Geochemistry and Sedimentology",
        "abstract": "Carbonate rocks are a valuable archive of past environmental conditions. To glean robust information from this archive, we must understand how carbonate sediments form, ensure our analytical techniques are optimized, and consider how inherently local deposition of sediments can communicate information about global changes in climate. Chapter 1 proposes a new conceptual model for the formation of ooids that suggests that these small carbonate grains could form while buried in the shallow sediment pile during certain intervals of Earth history. Chapters 2 and 3 calibrate the clumped isotope paleothermometer for calcite, dolomite, and apatite, resolving significant discrepancies in calculated paleotemperatures. Chapter 4 applies clumped isotope thermometry to Early Mississippian strata and demonstrates a ~5ºC global cooling and substantial ice volume expansion coincident with a major perturbation to the global carbon cycle. Chapter 5 examines the extent to which diagenesis and facies- and phase-specific effects drive a major Early Mississippian carbon isotope excursion. In aggregate, this thesis outlines a roadmap for assessing changes to climate and the carbon cycle for carbonate rocks in the Paleozoic.",
        "authors": [
            "Noah Anderson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157970",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Superparamagnetic Tunnel Junctions for Reliable True Randomness and Efficient Probabilistic Machine Learning",
        "abstract": "Physical devices exhibiting stochastic functions with low energy consumption and high device density have the potential to enable complex probability-based computing algorithms, accelerate machine learning tasks, and enhance hardware security. Recently, superparamagnetic tunnel junctions (sMTJs) have been widely explored for such purposes, leading to the development of limited-scale sMTJ-based systems. Existing sMTJs face significant scalability and reliability issues, however, because their intrinsically low energy barrier and correspondingly small device area result in high sensitivity to external perturbations, as well as large variations from device to device. Here, we present an experimental demonstration of three-terminal sMTJs as reliable and potentially scalable sources of true randomness in the field-free regime. By leveraging dual-current controllability and incorporating feedback, we stabilize the switching operation of superparamagnets and reach cryptographic-quality random bitstreams. The realization of controllable and robust true random sMTJs underpin a general hardware platform for computing schemes exploiting the stochasticity in the physical world, as demonstrated by the generative artificial intelligence example in our experiment. Furthermore, we experimentally demonstrate a novel method of utilizing sMTJs as stochastic analog-to-digital converters (sADCs) in a crossbar array architecture for neural network acceleration, showing performance comparable to software implementations. This work highlights the potential of sMTJs to revolutionize energy-efficient computing and provides a foundation for future advancements in probabilistic computing and hardware security.",
        "authors": [
            "Dooyong Koh"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158486",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Leveraging Mechanics for Multi-step Robotic Manipulation Planning",
        "abstract": "This thesis focuses on enabling robots to robustly perform complex, multi-step manipulation tasks, like chopping vegetables or wielding a wrench. Completing such tasks requires a robot to plan and execute long sequences of actions, where each action involves many connected, discrete and continuous choices that are critically impacted by constraints relating to force, motion and contact. To tackle this, this thesis contributes models and algorithms that exploit the physics and geometry of the world in order to address the dual challenges of long-horizon decision-making and acting under uncertainty. We apply this in the context of three domains: in-hand manipulation, forceful manipulation and briefly-dynamic manipulation.\r\n\r\nFirst, to reorient a grasped object, we develop a sampling-based motion planner to generate sequences of pushes that slide the object in-hand. We derive an abstraction for pushing to enable the planner to reason about frictional constraints. Second, we focus on forceful manipulation tasks, such as opening a childproof medicine bottle or twisting a nut on a bolt, where the robot's planning choices are impacted by the need to exert force. We define constraints that explicitly consider torque and frictional limits and integrate these into an existing task and motion planning framework. We leverage cost-sensitive planning to enable the robot to generate plans that are robust to uncertainty in the physical parameters. Finally, we frame planning with dynamic actions, like shoveling or toppling, as requiring the robot to reason about both action uncertainty and potential dead ends. We learn a simple action model and formulate a sample-based manipulation planner that guards against dead ends in the face of uncertainty. Throughout this thesis, we validate the practical applicability of our model-based approaches by evaluating them on real robots.",
        "authors": [
            "Rachel Holladay"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158511",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Magnetic Weyl Semimetals for Spintronic Applications",
        "abstract": "Magnetic Weyl semimetals are a category of topological materials that hold promise for spintronic applications due to their unconventional transport properties, which arise from both bulk and surface topological states, as well as the rich interplay between band topology and magnetism. Among the family of semimetallic materials, the antiferromagnetic Weyl semimetals Mn₃X (X=Sn, Ge, etc.) and the ferromagnetic Weyl semimetal Co₂MnGa have attracted significant interest. So far, despite extensive theoretical and experimental investigations, the magnetic dynamics of Mn₃X and the spin-polarized tunneling in Co₂MnGa based spintronic devices remain not fully explored.\r\n\r\nIn this thesis, I establish a theoretical framework to describe the low energy dynamics of strained Mn₃X. Using perturbation theory, I identify three distinct dynamic modes and derive a Landau-Lifshitz-Gilbert (LLG)-like equation to describe uniform mode dynamics. I also analyze the excitation of dissipative spin waves and the spin superfluidity state in Mn₃X by extending the model to include spatial inhomogeneity. The analytical results are validated against numerical simulations based on fully coupled LLG equations, where good agreement is achieved. In addition, I study fully epitaxial magnetic tunnel junctions (MTJs) composed of Co₂MnGa. By growing Co₂MnGa/MgO/Co₂MnGa stacks under different conditions, I develop a series of MTJs with varying degrees of chemical ordering in the Weyl semimetal electrodes and compare their tunneling magnetoresistance (TMR). I find that the TMR is enhanced with the improvement of the chemical ordering in Co₂MnGa. Our results reveal the relationship between the spin tunneling in MTJs and the chemical order of Co₂MnGa electrodes, offering insights into further enhancing TMR through Weyl semimetal engineering.",
        "authors": [
            "Zhiping He"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158487",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Language Evolution for Parallel and Scientific Computing",
        "abstract": "Scientists, working on the biggest problems facing humanity today, write and run largescale computer simulations. It has been a decades’ long dream of both scientists and programming language designers to make the development for and usage of high-performance computing easier. Many attempts have failed, perhaps because this is a hard problem, perhaps because the social motivation and the required steps to achieve success have not come together, and perhaps solutions to date only solve part of the problem in essence never fully solving the problem. This thesis proposes that there is a combination of features necessary to form a solution. Starting from a bedrock that combines performance with high-level abstractions in a single language. The language needs to enable composable abstractions, or we are doomed to keep developing the single-shot applications of the past. These abstractions should enable code reuse for different forms of compute architectures, to allow users to keep up with the fluid landscape of accelerators. These abstractions should enable code reuse for different mathematical objects such as dense, sparse and structured matrices. These abstractions should enable code reuse for differentiable programming, to enable integration of techniques like sensitivity analysis and scientific machine learning. With the right methodology, these abstractions can compose with each other and specialize to the domain. I will demonstrate that the combination of high-level array-based abstractions and a lowlevel performance portable kernel programming framework form a potent combination for large-scale scientific computing. I will show its efficacy using real-world scientific codes. Furthermore, I will introduce a differentiable programming framework built on top of a general automatic differentiation engine operating on compiler level. The automatic differentiation framework outperforms state-of-the-art, is capable of synthesizing gradient functions from GPU kernels, and can differentiate a wide variety of parallel constructs. As the infrastructure supporting this language needs to be more sophisticated than those of yesteryear, new problems arise. This thesis solves some of these problems and demonstrates their solution on a fluid dynamics code used in climate modelling as one of many imaginable applications.",
        "authors": [
            "Valentin Churavy"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158502",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Low-cost Agents with Language Perception and Dynamic Inference",
        "abstract": "Designing efficient artificial intelligence agents presents significant challenges, particularly in terms of learning and inference costs. Traditional agents often suffer from high learning expenses due to their limited ability to generalize across diverse tasks and environments. Recent advances in large language models (LLMs) have shown strong generalization capabilities by leveraging high-level abstractions of the world through language. In this thesis, we propose leveraging language as a perceptual representation to enable LLM-based agents to perform vision-language navigation tasks with reduced data collection costs. We demonstrate that language not only facilitates the generation of efficient synthetic data but also serves as a bridge to minimize domain gaps between different environments. However, transformer-based agents are burdened with high inference costs, especially when handling long-horizon visual content. To mitigate this, we introduce two strategies: (1) reducing visual input redundancy through dynamic token selection, and (2) accelerating model inference using a memory-efficient Mixture of Experts (MoE) architecture. Together, these approaches offer a robust framework for enhancing both learning and inference efficiency in LLM agents.",
        "authors": [
            "Bowen Pan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158499",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Structured Handwritten Input for Dementia Classification",
        "abstract": "We explore the use of deep learning to score the Digit Symbol Substitution Test (DSST), a paper-and-pencil behavioral test useful in diagnosing Alzheimer’s. We train a model to classify Alzheimer’s based on the subject’s responses to any one of the 108 queries in the test. We then combine predictions across the test to produce a new classifier that is considerably stronger. We also make an extensive search of architectures and optimization techniques that have proved useful in other settings. The ultimate result is a very strong classifier, with AUC for a response to a single question of 86% and for an overall patient of 97.25%.",
        "authors": [
            "Gerardo Flores"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158498",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Making Sense of Training Large AI Models",
        "abstract": "Today, one of the most impressive applications of optimization is the training of large AI models. But currently such models are trained with ad-hoc heuristics at a very large computational cost, mainly due to lack of understanding of their working mechanisms. In this thesis, we conduct a systematic study of large-model optimization, crucially informed by practical applications. The first part investigates two interesting phenomena regarding optimization of Transformer-based models, one of the most popular architectures for language modeling. We investigate how training Transformer-based models can lead to remarkable properties such as in-context learning, and we further discuss the main challenges associated with Transformer training. The second part of this thesis focuses on understanding the Adam optimizer, one of the most popular algorithms for training large models. We offer a new view on Adam based on an online learning perspective that elucidates the importance of Adam’s algorithmic components. Building on this new perspective, we also prove that Adam achieves the optimal convergence rate in various non-convex optimization settings, both smooth and non-smooth settings. The third part of this thesis focuses on the unstable convergence phenomenon in training large models. We identify its main characteristics from first principles, and discuss its causes and implications for learning. We then discuss its connection to popular flat minima optimization algorithms, and initiate a formal study of them by defining a formal notion of flat minima, and analyzing the complexities of finding them.",
        "authors": [
            "Kwangjun Ahn"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158484",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Burst Imaging with Learned Continuous Kernels",
        "abstract": "Burst imaging is a technique that consists of taking multiple images in quick succession and merging them into one output image. By aligning and combining data from multiple frames, we can increase resolution, attenuate noise, reduce motion blur and expand the dynamic range to obtain a higher quality image. In this thesis, we propose a method that learns continuous kernels to process and merge burst frames. We show that the learned kernels adapt to local image information and take advantage of sub-pixel sample location information to demosaic, denoise and merge the burst into a high quality output.",
        "authors": [
            "Camille Biscarrat"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158488",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Condensed Buck-Boost Switched Capacitor Converter for\r\nEfficient Voltage Distribution in Electrified Aircraft",
        "abstract": "Switched capacitor converters are a category of power electronic converters that harness the significantly improved energy density of capacitors as opposed to that of their conventional, inductor-based counterparts to reap benefits in terms of efficiency, size, and utilization. This work presents the analysis, design, construction, and evaluation of one such converter, inspired by the flying capacitor multilevel topology and referred to as a condensed buck-boost converter. This converter is designed and built for an application as the interface between the battery voltage and DC bus on partially electrified aircraft, where the advantages of its ability to step up/down voltage in an efficient and lightweight fashion can be fully realized. In order to be implemented in hardware for the first time, this work utilize new monolithic, bidirectional GaN FETs, whose reverse voltage blocking capabilities open new possibilities for a converter design that wastes less power and occupies less board area. This converter is compared with others that perform similar functions to showcase the benefits that this topology has to offer.",
        "authors": [
            "Aklilu Aron"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157247",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Recovery of Herschel-Bulkley Fluid Parameters from\r\nVideo via Differentiable Simulations",
        "abstract": "Recreating the physical behavior of fluids from real-world footage remains a significant challenge, particularly for non-Newtonian fluids. This work introduces a novel method that combines neural radiance fields (NeRF), which map 3D scene coordinates to color and density using deep neural networks, with the material point method (MPM), a simulation technique that represents materials as moving points capable of large deformation. Our approach aims to accurately recover physical parameters and achieve high-fidelity 3D reconstructions from single-view videos of fluids, even those with complex rheological behaviors like shear thinning and thickening. In this study, we apply our method to a Herschel-Bulkley fluid, namely ketchup, under two different real-world conditions: a 50mm column collapse and being squeezed from a bottle. By leveraging the differentiable nature of NeRF and the fluid simulation capabilities of MPM, our approach extracts parameters from real-world footage after initially training on approximate geometry derived from virtual models. The actual video footage is then used to estimate initial velocities and retrieve constitutive parameters, including modulus, yield stress, and viscosity. The iterative optimization process, which integrates continuous feedback between the NeRF-MPM simulation and the video data, enables us to extract constitutive parameters from real footage and perform predictive simulations that closely reflect the behavior observed in the training videos. Key results include the retrieval of constitutive parameters, such as modulus, yield stress, and viscosity, as well as reconstructed videos that reflect the fluid behavior observed in the training video. The results demonstrate that our method can reconstruct the fluid’s flow behavior from limited perspectives, accurately enough to visually reproduce the flow, showcasing its flexibility and robustness. This work not only validates the approach through 3 a series of experiments but also highlights the potential for differentiable rendering and simulation techniques to advance our understanding and simulation of complex material dynamics, particularly in cases where direct measurements are challenging or impossible.",
        "authors": [
            "John M. Eastman"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157204",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "FESTIM V&V book",
        "abstract": "This book aims to provide a comprehensive overview of the verification and validation processes conducted for the FESTIM code. Through a series of verification and validation cases, we aim to demonstrate the accuracy, reliability, and applicability of FESTIM for simulating hydrogen transport phenomena.",
        "authors": [
            "Remi Delaporte-Mathurin",
            "Jair Santana"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156690",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Instability Scaffolding: Enacting Strategic Instabilities to Produce Authentic Premium Wine",
        "abstract": "Unstable conditions can be a risk to productions, potentially disrupting operations and rendering activities unpredictable. While a common organizational response is to minimize instability, I find instead that producers can also purposefully cultivate it to generate value—through strategic instabilities. My dissertation explores how strategic instabilities are enacted in productions of fine wine, articulating the practices and arrangements that facilitate working with unstable production conditions in productive ways—a process I refer to as instability scaffolding. My data are drawn from a 16-month ethnographic study of two field sites in the California premium wine industry, combined with archival data and industry interviews. In Chapter One, I explain why minimizing certain sources of instability, while potentially more efficient, would be considered inauthentic for premium wine productions. In Chapter Two, I look historically at the California premium wine category, and explain why and how working with instabilities of nature became a basis of its authenticity. This chapter examines the instability scaffolding (i.e., cooperative category framing work) performed in the California wine industry to enable such productions to become commercially viable, and identifies the intra-category mutualism that motivated competitors to support such productions. Chapter Three offers insight into the modern-day operations of a world-renowned fine wine producer. I identify the trajectory management work scaffolding this organization’s achievement of craft authenticity, turning production instability into productive instability so that high-quality wines are produced consistently year after year despite relying on unpredictable activities. Chapter Four explores a regional-level instability scaffolding allowing many producers to keep their operations logistically feasible despite working with unstable conditions. I show how vineyard proprietors and contract providers worked together to sustain craft authenticity at scale in the region through a process I theorize as contract custodianship. My dissertation concludes in Chapter Five with a discussion of instability scaffolding more broadly and its implications for further research. I highlight how my research contributes new insights into the multiple ways organizations can leverage complex interactions in product by skillfully engaging with them to express authenticity in productions at commercial scale.",
        "authors": [
            "Alan Zhang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157091",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Deformation and its surface expression in stressed planetary materials",
        "abstract": "This thesis investigates the response of planetary materials to changing stress fields, and resultant signatures of stress in geophysical properties observable from planetary surfaces. When forces change within rocky and icy layers of planetary bodies, constituent materials of these layers adjust on the microscale; energetically favorable alignment of microstructural materials builds across scales to result in deformation, preferred directions for material transport and wave propagation, and heat release. This work therefore explores the relationship between microstructure and stress conditions in order to connect geophysical observations to the underlying forces on subsurface materials, using both experimental and computational methods. The first two chapters investigate two-phase deformation, where a partial melt phase is present between grains of solid materials such as olivine (Chapter 2) or ice (Chapter 3). Chapter 2 finds that in partially molten rocky materials, microstructural melt aligns parallel to the maximum applied stress direction quickly over geological time, while crystallographic orientations require significant strain intervals to reset. This shows that we can use the melt-induced changes to properties in the deforming Earth, for example, as an indicator of short-term stress fields. Chapter 3 applies these findings to the evolution of icy systems through simulated deformation of ice-melt aggregates, suggesting that current seismic studies which do not correct for the orientation of melt may misinterpret deformation at the base of warm ice sheets. The final two chapters center on deformation mechanisms that may shape the properties of icy outer Solar System satellites as they orbit their host planets. Chapter 4 provides novel experimental constraints on meteoritic materials relevant to the cores of icy moons, finding that microstructural brittle deformation, and resultant energy release, occurs even at very small differential stresses. Acoustic emissions associated with this brittle deformation are also more energetic at lower confining pressures, indicating that smaller, lower-pressure icy moons might receive enhanced heat from core deformation. The final chapter (Chapter 5) investigates crustal processes on Titan, Saturn’s largest moon. This work models how tidal stresses interact with local topographic stresses to create fracture across Titan’s crust, creating pathways for sediment generation and fluid transport.",
        "authors": [
            "Cassandra Seltzer"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157122",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Predictive and Prescriptive Trees for Optimization and Control Problems",
        "abstract": "This thesis introduces novel methods to expedite the solution of a broad range of optimization and control problems using machine learning, specifically decision tree algorithms. In many practical settings, similar optimization and control problems often need to be solved repeatedly. We propose methods to leverage patterns from pre-solved problem instances using machine learning, leading to drastically faster solutions once training is complete. \r\n\r\nThe thesis is structured into four parts, each tackling different class of optimization or control problems. In Chapter 2, we propose a machine learning approach to the optimal control of multiclass fluid queueing networks (MFQNETs). We prove that a piecewise constant optimal policy exists for MFQNET control problems, with segments separated by hyperplanes passing through the origin. We use Optimal Classification Trees with hyperplane splits (OCT-H) to learn an optimal control policy for MFQNETs. \r\n\r\nIn Chapter 3, we study fluid restless multi-armed bandits (FRMABs), deriving fundamental properties and designing efficient numerical algorithms. Using these results, we learn state feedback policies with OCT-H and introduce a novel feature augmentation technique to handle nonlinearities.\r\n\r\nIn Chapter 4, we propose a machine learning framework for solving two-stage linear adaptive robust optimization problems with binary here-and-now decisions and polyhedral uncertainty sets. We also introduce novel methods to expedite training data generation and reduce the number of different target classes the machine learning algorithm needs to be trained on. \r\n\r\nIn Chapter 5, we introduce a prescriptive machine learning approach to speed up the process of solving mixed integer convex optimization (MICO) problems. We use a prescriptive machine learning algorithm, Optimal Policy Trees (OPT), instead of more commonly used classification algorithms. We demonstrate that OPT-based methods have a significant advantage in finding feasible solutions compared to classification algorithms.\r\n\r\nWe test our approach on various synthetic and real-world problems. Using the proposed methods, we can obtain high-quality solutions to a broad range of large-scale optimization and control problems in real-time – within milliseconds.",
        "authors": [
            "Cheol Woo Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157107",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Surgery Exact Triangles in Instanton Theory",
        "abstract": "The introduction of instanton Floer theory and Donaldson polynomial invariants in the 1980s revolutionised the study of low dimensional topology. Since then, many Floer theories have been introduced with different structural properties and qualitative features. One of these Floer theories, Heegaard Floer theory, is popular due to its computational ease and rich algebraic structure. One of the computational tools absent in other Floer theories is the integer surgery formula that computes Heegaard Floer homology of 3-manifolds obtained by surgery along knot(s) in them. This thesis establishes a new surgery formula in instanton Floer theory. The algebraic language to express this formula is that of the derived category of chain complexes. The first part of the thesis describes this surgery formula whose statement and proof are inspired by the Atiyah-Floer conjectures. The second part then contrasts with the Heegaard Floer analogue by showing that instanton and Heegaard Floer theory cannot agree over integers.",
        "authors": [
            "Deeparaj Bhat"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157113",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Accelerating Astrophysical Simulations with GPUs: A Case Study of Radiative Transfer in arepo-rt",
        "abstract": "Radiative transfer (RT) is a crucial ingredient for self-consistent modelling of numerous astrophysical phenomena across cosmic history. However, on-the-fly integration into radiation-hydrodynamics (RHD) simulations is computationally demanding, particularly due to the stringent time-stepping conditions and increased dimensionality inherent in multifrequency collisionless Boltzmann physics. The recent emergence of exascale supercomputers, equipped with extensive CPU cores and GPU accelerators, offers new opportunities for enhancing RHD simulations. We present the first steps towards optimizing the RHD solver AREPO-RT for such high-performance computing environments. We implement a novel node-to-node communication strategy that utilizes shared memory to substitute intranode communication with direct memory access. Furthermore, combining multiple internode messages into a single message substantially enhances network bandwidth utilization and performance for large-scale simulations on modern supercomputers. The single-message node-to-node approach also improves performance on smaller-scale machines with less optimized networks. Additionally, by transitioning all RT-related calculations to GPUs, we achieve a significant computational speedup of around 15x for standard benchmarks compared to the original CPU implementation. As a case study, we perform cosmological RHD simulations of the Epoch of Reionization, employing a similar setup as the THESAN project. In this context, RT becomes sub-dominant such that even without modifying the core AREPO codebase, there is an overall threefold improvement in efficiency. The advancements presented here have broad implications, potentially transforming the complexity and scalability of future simulations for a wide variety of astrophysical studies. This work serves as a blueprint for porting similar simulation codes based on unstructured resolution elements to GPU-centric architectures.",
        "authors": [
            "Erkin Emiel Verbeek"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157117",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Understanding Drivers of Deforestation using Games on Spatial Networks",
        "abstract": "As the impacts of climate change become more extensive and intense, effective actions for mitigation and adaptation become imminent. Since deforestation is a key driver of CO₂ emissions and forests constitute a crucial carbon sink, mitigating deforestation is an essential policy lever for governments. However, much of tropical deforestation results from actions of private entities that use the cleared land for activities such as palm oil tree cultivation, timber plantation, and agriculture. Often, the incentives to engage in (often illegal) deforestation within a forest concession are coupled with these activities and are also shaped by the activities in neighboring concessions. In this thesis, we focus on the problem of modeling these strategic interactions using game theory. We analyze a class of games in which agents engage in coupled activities over a spatial network and study a policy intervention to limit illegal deforestation.\r\n\r\nFirstly, we conduct equilibrium analysis of a game in which each agent decides the production levels of her coupled activities in the presence of network effects. Practically, these network effects are induced by spatial arrangements of concessions and their ownership structures. We consider the general case where network effects are heterogeneous, i.e. network effects influencing palm oil tree cultivation and time logging are described by different graphs. We provide a sufficient condition for existence and uniqueness of Nash equilibrium. This result follows by leveraging potential function of the game or via a general variational inequality. \r\n\r\nSecondly, we analyze how the spatial structure of concessions impacts the equilibrium outcome. In addition to the basic game in which each agent simultaneously engages in two activities, we consider a variation in which agents engage in one of the activities (but not both). We show that in both cases equilibrium structure can be expressed as a linear combination of weighted Bonacich centrality vectors -- a node-centrality measure that depends on the total number of walks that depart from a node (concession). Our analysis provides new insights on the drivers of illegal logging in forest regions where palm oil cultivation and timber logging are coupled.\r\n\r\nThirdly, we evaluate the impact of ``edge removal’’ intervention policy in which the boundary between two neighboring concessions is monitored or a buffer is created between them. We characterize the policy of a social planner who is interested in maximally reducing the illegal production of timber. Interestingly, we identify a regime shift (or phase transition) as the local network effect and level of coupling between activities vary. This result identifies conditions for which the social planner should incentivize specialization (enforce production of palm oil trees or timber) versus diversification (allow for both palm oil trees and timber) cultivation.",
        "authors": [
            "Jean-Baptiste Seby"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157109",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Advancements in Models and Algorithms for Management Science",
        "abstract": "Management science is an interdisciplinary field that leverages a variety of analytical techniques to inform effective decision-making within businesses and organizations. It is a dynamic field that is continuously innovating as data becomes increasingly available and businesses leverage new digital technologies. As a result, there is a constant need to develop models and algorithms to address unique decision-making settings. This thesis is composed of three independent chapters, each of which proposes novel modeling insights and algorithmic solutions for real-world problems.\r\n\r\nChapter 2 studies a mathematical model in online resource allocation where a decision-maker must efficiently allocate a scarce resource to patient and impatient customers. This study is motivated by recent advancements in on-demand online platforms (such as Uber and Instacart) where customers who are patient (e.g., can wait a few minutes for a ride) are offered a discounted price. Under this model, we develop a simple resource allocation policy that has provable theoretical guarantees under a competitive ratio analysis and is also easy to use in practice. Our work supports the managerial intuition that offering discounts for patient customers leads to more robust and efficient resource allocation.\r\n\r\nChapter 3 addresses the challenge of organizing a large corpus of documents into an expert-defined labeling scheme without manual annotation or labeled training examples. This work is motivated by a collaboration with a major pharmaceutical company to streamline root cause analysis of deviations in the manufacturing process. In investigating a new deviation, quickly finding related historical deviations is crucial, but such deviation reports are not organized in a way to facilitate this task. This chapter proposes an innovative methodology called Document Classification with Reference Information (DCRI), which crucially leverages the existence of reference information, documents which describe the taxonomy of interest but are not labeled examples themselves. Empirical results show that DCRI can produce highly accurate labels with minimal intervention from subject matter experts. Based on these empirical findings, we develop a mathematical model for the underlying data generating process and propose both numerical and theoretical finds that further justify the DCRI approach.\r\n\r\nChapter 4 studies a novel way of generating insights from black-box classification models by deriving simple conditions under which the model predicts confidently. Existing work on explaining binary black-box classifiers typically studies when the model predicts 1 or 0 without accounting for the confidence (i.e., probability) of the prediction. Our work argues that explaining when a model makes confident predictions is more useful to a practitioner as such predictions typically correspond to when a model is more accurate and reliable. We define a novel evaluation metric for black-box explainers which emphasize confident predictions and develop a local-search based methodology to find interpretable lists of if-then rules that optimize for this metric. Evaluation on six real-world datasets suggest that such rule-based explanations are effective at capturing highly confident data points. By targeting highly confident predictions of black-box model, our methodology generates rules that are more useful than existing approaches which only explain a classifier's binary predictions.",
        "authors": [
            "Yuanfan (Evan) Yao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157094",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Dimers, Trimers and their Superpositions in a Bose-Fermi Mixture",
        "abstract": "This thesis describes experiments on few- and many-body bound states in a Bose-Fermi\r\nmixture of ultracold 23Na and 40K atoms. We examine the formation of dimers and trimers in\r\na balanced, thermal mixture and their evolution into strongly interacting Bose polarons with\r\nhybridized dimer and trimer character when we instead immerse an impurity concentration\r\nof K into a dense quantum bath of Na.\r\nWe report a novel direct observation of a heteronuclear halo trimer, consisting of two\r\nlighter Na atoms and one heavier K atom, alongside the familiar NaK Feshbach dimer, using\r\nradiofrequency (rf) spectroscopy. We find that in proximity to a Feshbach resonance, the\r\ntrimer feature closely follows the dimer resonance across an order-of-magnitude variation\r\nin binding energy. We show that the measured binding energies are consistent with our\r\ntheoretical model of the trimer as having the structure of a Feshbach dimer weakly bound\r\nto one additional boson.\r\nWe then study the fate of impurities interacting with a bosonic quantum bath, the\r\nparadigmatic Bose polaron scenario. By preparing an initial attractive polaron state, we\r\nprobe previously inaccessible, highly-correlated Bose polaron states, again on the repulsive\r\nside of the Feshbach resonance. Deep within the condensate, the rf spectra no longer exhibit\r\ndiscrete dimer and trimer features as before, instead dominated by a single broad feature.\r\nWe attribute this to the impurity-boson coupling becoming stronger than the dimer-trimer\r\nenergy splitting, leading to hybridization of dimer and trimer states and, consequently, an effective level repulsion consistent with the spectra we observe. This experiment demonstrates\r\nthe remarkable interplay between polaron physics and bound-state formation in a quantum\r\nenvironment.",
        "authors": [
            "Alexander Chuang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157084",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Predicting Risk and Optimizing Resilience of Digital and Physical Supply Chains",
        "abstract": "A number of disruptions and related challenges have affected the landscape of global supply chains in the past decade. These include the COVID-19 pandemic, geopolitical tensions, and cross-industry cyber breaches, highlighting the need for resilient and adaptive supply chain management. This thesis explores the role of data, machine learning, and analytics in developing predictive risk models to evaluate supply chain-related risks and optimizing the supply chain to improve resiliency. This thesis focuses on the two primary industry application domains of cybersecurity and the global shipping industry.\r\n\r\nChapter 2 and 3 are motivated by the increasing prevalence of supply-chain related cyber breach incidents such as the SolarWinds breach in 2020. Chapter 2 develops the first predictive model for cyber risk that relies on innovative supply chain features. It utilizes large-scale data from more than 30,000 entity enterprises and their respective digital supply chain networks. In particular, this chapter develops descriptive features of the local supply chains of these entities, and then leverages these features to develop a supervised ML model for predicting whether an enterprise will experience a data breach incident. The results from this analysis demonstrate that local supply chain characteristics are significant predictors of data breach risk. Additionally, including supply chain features increases predictive power compared to baseline models that rely solely on internal enterprise features.\r\n\r\nChapter 3 introduces an innovative global supply chain network graph and cyberattacker framework for modeling cyberattacker behavior in supply chain network environments. Theoretical analysis of this model proves that certain local supply chain characteristics determine an upper bound on the probability that an enterprise is compromised in this framework. Furthermore, the supply chain graph is calibrated with real data and then used to train an unsupervised reinforcement learning (RL) attacker agent. The agent traverses the supply chain network graph by cyberattacking and compromising nodes with the goal of maximizing its reward. The trained agent is used to produce an unsupervised risk assessment of the company nodes by simulating attacks within the network graph. The assessment, which is validated using public breach data, is competitive with basic unsupervised models and can significantly improve predictive performance when included as a feature for supervised models. An attractive aspect of this innovative modeling approach is that it does not require access to historical breach data needed for supervised models and algorithms, as unfortunately, the currently available data on cyber breaches is very partial and sparse.\r\n\r\nChapter 4 develops a novel methodology for optimizing shipping container scheduling for the last leg in the shipping container global supply chain, called the \\textit{drayage trucking} delivery process. The work in this chapter details the drayage trucking process from end-to-end and highlights key sources of inefficiencies throughout the process. An integer programming (IP) model is introduced to schedule each step in the drayage trucking delivery process to improve efficiency and minimize additional costs that are incurred as a result of inefficiencies in the container delivery schedule, which are known as \\textit{accessorial charges}. The IP generates optimized schedules using industry delivery data, which are then compared with historical schedules. The results demonstrate that this approach can significantly decrease costs and improve container scheduling efficiency compared to current industry practices.",
        "authors": [
            "Kevin Hu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157125",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Case Study in Marketing a Real Estate Debt Fund through the Design and Preparation of a Private Placement Memorandum (PPM) and Investor Presentation",
        "abstract": "Private equity-backed real estate debt funds play a crucial role in providing capital to borrowers seeking financing for construction projects. These funds raise capital from investors, deploy it strategically, and actively manage debt investments to generate returns for their limited partners. The appeal lies in the potential for attractive yields and risk management strategies in a complex investment landscape. There are countless potential fund structures to address a range of investment strategies, risk profiles, investor appetites, geographic considerations, and manager experience and deal access. This study delves into the dynamics of capital raising for a real estate debt fund specializing in private construction loans. It covers the essential elements of the Private Placement Memorandum (PPM), including legal disclosures, investment terms, risk factors, and fundspecific details. This research aims to provide a real-world example of a fund designed according to current trends and market terms for use by a real-life investment manager, ProBuilder Financial LLC. The PPM and the associated investor presentation utilize best practices for presenting complex financial information in a clear and concise manner. Bridging theory and practice sheds light on the strategies, risk-reward trade-offs, and market implications associated with this capital-raising channel.",
        "authors": [
            "Richard Scott Poirier"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157081",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "From Opinion Dynamics to Collective Action: How Identity-Based Tolerance Leads to Political Extremism",
        "abstract": "Current sociological theories attribute the recent surge in political extremism to mechanisms of opinion “homophily” (i.e., like-minded individuals interact more while dissimilar ones might distance) and “assimilation (i.e., interactions homogenize opinions),” which collectively suggest a social world dominated by extreme views. Yet, this view contradicts empirical evidence showing that extremists still represent a minority and individual opinions remain largely stable. We resolve this apparent paradox by illustrating how extreme collective action can arise from a moderate majority that retains moderate opinions yet responds positively to recruitment by extremists. We break down this task into three steps. First, we theoretically distinguish between opinion homophily and identity homophily (i.e., individuals who share the same identity interact more). Second, we develop an agent-based model to manipulate the strength of identity homophily relative to opinion homophily, while excluding the effect of assimilation (i.e., holding opinions constant). Our model reveals that strong identity-based tolerance can create a “radicalized” structure, which allows extremists and moderates–who disagree in opinion but share an identity–to maintain stable relationships in emergent clusters; Further, the structure concentrates extremists at the center of the clusters, enabling them to form a critical mass that enlists a broader population. Finally, beyond confirming our expectations, we uncover unexpected model behaviors by exploring how the \"radicalized\" structure can transition between three other distinct structures the model generates. We show that homogeneous groups, often seen as indicators of polarization, could paradoxically be key to reducing organized extremism when dominated by moderates who can effectively mobilize collective action while marginalizing extremists.",
        "authors": [
            "Chen E. Liang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157090",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Automated Social Science: Language Models as Scientist and Subjects",
        "abstract": "We present an approach for automatically generating and testing, in silico social scientific hypotheses. This automation is made possible by recent advances in large language models (LLM), but the key feature of the approach is the use of structural causal models. Structural causal models provide a language to state hypotheses, a blueprint for constructing LLM-based agents, an experimental design, and a plan for data analysis. The fitted structural causal model becomes an object available for prediction or the planning of follow-on experiments. We demonstrate the approach with several scenarios: a negotiation, a bail hearing, a job interview, and an auction. In each case, causal relationships are both proposed and tested by the system, finding evidence for some and not others. We provide evidence that the insights from these simulations of social interactions are not available to the LLM purely through direct elicitation. When given its proposed structural causal model for each scenario, the LLM is good at predicting the signs of estimated effects, but it cannot reliably predict the magnitudes of those estimates. In the auction experiment, the in silico simulation results closely match the predictions of auction theory, but elicited predictions of the clearing prices from the LLM are inaccurate. However, the LLM's predictions are dramatically improved if the model can condition on the fitted structural causal model. In short, the LLM knows more than it can (immediately) tell.",
        "authors": [
            "Benjamin S. Manning"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157089",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Effect of Social Information on Reliance and Efficacy\r\nin AI-assisted Prediction",
        "abstract": "This work addresses an under-explored aspect of people's utilization of algorithmic decision support systems: How do people perceive and use these systems under social influence? Through a pre-registered randomized human-subject experiment, I study the effect of two forms of social information-direct conversations and summarized peer decisions----on users' reliance and effectiveness in leveraging algorithmic advice across a series of decision-making tasks, and how t he availability of local model explanations and performance feedback moderates this effect. I find t hat, on average, neither form of social information affects t rust directly, yet they both moderate t he extent to which feedback and model explanations influence trust in the algorithm. However, while social information can influence trust in the algorithm, I detect no effect on how effectively people utilize algorithmic advice. By describing this interplay between social information, algorithmic transparency, and user behavior, this work contributes to recent research on collective intelligence and sociotechnical approaches to human-AI interaction.",
        "authors": [
            "Mohammed Alsobay"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157105",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Implementation of Machine Connectivity in Low-Volume High\r\nVariety Manufacturing Line",
        "abstract": "This thesis provides a comprehensive analysis and implementation plan for enhancing machine connectivity within a manufacturing facility at SLB. The study investigates the existing limitations of the facility's connectivity infrastructure and proposes an advanced connectivity software suite as a solution, presenting a compelling business case for its implementation. The software’s scope involved DNC (direct numerical control), allowing for line-by-line feeding of CNC code to machine controllers, as well as machine data collection for real-time shop floor monitoring. The research emphasizes the development and implementation of an advanced network infrastructure designed to improve efficiency, security, and data handling capabilities. There is discussion regarding cybersecurity practices, specifically those related to industrial control systems that leverage CNC machining processes. The software implementation process is detailed, highlighting the necessary steps and information required for successful integration. These include: 1) securing connection to critical CNC machine controllers, 2) acquisition of hardware including local server and network switch, 3) server bring-up through remote imaging and installation of standard monitoring tools and 4) implementation of software on edge devices for CNC file transfer and machining data collection. Additionally, the thesis discusses the limitations encountered during implementation and outlines future steps to address these challenges.",
        "authors": [
            "Kanishk Pal"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157327",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Decoding divergence in marine protistan communities: from strain diversity to basin biogeography",
        "abstract": "Protists (microbial eukaryotes) in the global ocean are critical components of primary\r\nproductivity and nutrient recycling. Protists are genetically diverse and have distinctive\r\necological niches based on genetically-driven differences in physiological fitness. A deeper\r\nunderstanding of which dimensions of protistan genetic diversity translate to measurable phenotypic variation is needed to predict the impact of protists on marine biogeochemistry and\r\nprotists’ environmental change sensitivity. I cultured twelve strains of the coccolithophore\r\nGephyrocapsa huxleyi across temperatures, which revealed strain-specific differences in thermal optima and niche widths. I used traits measured during the experiments to design\r\na Darwin ecosystem model simulation, which demonstrated basin-specific biogeography of\r\nthermal optima and niche widths (Chapter 2). For seven of the twelve strains, I sequenced\r\ntranscriptomes at 3-5 temperatures to assess gene expression variation. Using the RNAseq\r\ndata, I developed a regression modeling approach to identify proteome allocation model parameters. Combining differential expression analysis, gene abundance normalization, and the\r\nregression model to explore the proteome allocation model parameter space, I probed differences in modeled strategies of G. huxleyi strains in response to temperature (Chapter 3).\r\nScalable workflows highlight the challenge and promise of meta-omic data to link community\r\nstructure to physiology. I developed a pipeline for metatranscriptome analysis and taxonomic\r\nannotation to address the lack of tools built specifically for microbial eukaryotes, and created mock communities to assess recovery success in protistan metatranscriptome analysis\r\nworkflows (Chapters 4 and 5). I applied these tools to a three-year metatranscriptomic\r\ndataset from Cape Cod Bay to investigate a recent emergence of a summer coccolithophore\r\npopulation in the 20-year time series, tracking shifts in nutrient physiology to identify potential bottom-up controls (Chapter 6). This dissertation advances approaches to constrain\r\nthe protistan taxonomic diversity that underlies shifts in global primary productivity and\r\nnutrient turnover. Specifically, strains of a single phytoplankton species revealed diversity\r\nrelevant to a global ecosystem model. Future work will clarify variability in protistan gene\r\ncontent and expression that may underpin both protists’ present ecological niches and their\r\nfuture climate change response.",
        "authors": [
            "Arianna Isabella Krinos Quinn"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157085",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Orbital stability in a classical pilot-wave system",
        "abstract": "The hydrodynamic bouncing droplet system, consisting of millimetric droplets bouncing on a vibrating fluid bath, displays many quantum mechanical phenomena on a macroscopic scale. These phenomena include tunnelling, diffraction and wave-like statistics. This thesis focuses on the features responsible for the quantisation of orbital radii, and rationalises this quantisation in terms of the stability of circular orbits arising in the presence of a rotating frame and a central force. We find that orbital quantisation is most pronounced when the waves generated by each bounce decay slowly. The wave decay rate, in turn, is related to the concept of path memory, the number of prior impacts with the bath that affect the droplet’s future dynamics. We conduct an analytical investigation into the stability of circular orbits using a generalised theoretical framework that allows for an exploration of classical pilotwave dynamics both inside and outside the experimentally accessible parameter regime. The exploration of parameter regimes beyond those accessible with the hydrodynamic system reveals much richer orbital dynamics. Our novel mathematical approach allows for evaluation of the integrals appearing in the stability problem in terms of Bessel functions of complex order, and thus facilitates asymptotic expansions of the stability problem in various limits. Within the experimental parameter regime, we demonstrate that in a rotating frame, circular orbits destabilise only via resonant instabilities, for which the growing perturbations oscillate at a frequency that is an integer multiple of the orbital frequency. Conversely, in a central force, non-resonant instabilities arise, for reasons detailed herein. Outside the experimental parameter regime, we show how the non-resonant instability leads to counter-intuitive scenarios; for example, circular orbits that are stabilised by increasing memory. In the limit of vanishing particle inertia, infinite path memory and a linear spring force, we demonstrate the intriguing possibility of infinitely many sharply quantised orbital states, where the allowed orbital radii exist in vanishingly thin intervals, and are stabilised by the combined influence of the time-averaged wave field and spring force. We demonstrate that these sharply quantised orbital states are only stable for higher memory. We then consider the effect of weak external forces on spin states, circular orbits arising in the absence of external forces, and show that the destabilisation of spin states depends in a complex manner on the type of external force applied. Finally, we show that the instability of large circular orbits is related to the in-line speed oscillations of free walking droplets in a manner that is independent of the external force.",
        "authors": [
            "Nicholas Z. Liu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157120",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Advances in Sparse and Low Rank Matrix Optimization for Machine Learning Applications",
        "abstract": "Numerous fundamental problems in operations research, machine learning, and statistics exhibit natural formulations as cardinality or rank constrained optimization problems. Sparse solutions are desirable for their interpretability and storage benefits. Moreover, in the machine learning setting, sparse solutions exhibit superior model generalization and have a natural interpretation as conducting feature extraction in high-dimensional datasets. On the other hand, since the rank of a matrix is equivalent to the cardinality of the matrix's vector of singular values, rank can be interpreted as the matrix generalization of sparsity. Accordingly, low rank solutions inherit similar desirables properties as sparse solutions while allowing for very flexible modelling capability. Unfortunately, optimizing over cardinality and rank constraints is non-convex and NP-Hard in general which has led to strong reliance on convex relaxations and heuristic methods which yield sub-optimal solutions.\r\n\r\nThis thesis advances both the theory and application of sparse and low rank matrix optimization, focusing on problems that arise in statistics and machine learning. We develop algorithmic approaches to problems exhibiting cardinality and rank constraints by leveraging techniques from mixed-integer and mixed-projection optimization. The proposed algorithms outperform existing convex relaxations and heuristics. Our rigorous analysis and empirical validation aim to contribute to both the theoretical foundations of optimization and the development of practical tools for complex challenges in statistics and machine learning.\r\n\r\nChapter 2 studies the Sparse Plus Low Rank Matrix Decomposition problem. We present an alternating minimization algorithm that computes high quality feasible solutions and outperforms benchmark methods, scaling to dimension n=10000 in minutes. We additionally design a custom branch and bound algorithm to globally solve problem instances of dimension up to n=25 in minutes. Chapter 3 examines the Compressed Sensing problem, for which we present a custom branch and bound algorithm that can compute globally optimal solutions. Our approach produces solutions that are on average 6.22% more sparse on synthetic data and 9.95% more sparse on real world ECG data when compared to state of the art benchmark approaches.  Moreover, our approach outperforms benchmark methods when used as part of a multi-label learning algorithm. Chapter 4 explores the problem of learning a partially observed matrix that is predictive of fully observed side information, which consists of an important generalization of the Matrix Completion problem. We reformulate this problem as a mixed-projection optimization problem and present an alternating direction method of multipliers algorithm that can solve problems with n = 10000 rows and m = 10000 columns in less than a minute. On large scale real world data, our algorithm produces solutions that achieves 67% lower out of sample error than benchmark methods in 97% less execution time.",
        "authors": [
            "Nicholas André G. Johnson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157126",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Molecular characterization of microbial interactions with labile\r\ndissolved organic matter",
        "abstract": "Marine microbes produce and consume labile dissolved organic matter (DOM), generating a carbon flux with significant implications for global carbon cycling and microbial ecosystems. Intracellular measurements of biological activity cannot fully capture microbial interactions with dissolved carbon. Better understanding this carbon flux thus requires direct and compound-specific characterization of metabolites, the small organic biomolecules that make up labile DOM. However, these measurements are challenging due to low metabolite concentrations, high ambient salt concentrations, and the complexity of labile DOM. More complete characterization of dissolved metabolites is therefore a standing challenge in the field. This in turn leaves many open questions with respect to the specificity of microbe-DOM interactions and the biotic and abiotic drivers of those interactions. This thesis addresses those challenges and questions. In Chapter 2, I explore the compound-specific uptake of metabolites by the copiotrophic gamma-proteobacterium Alteromonas macleodii, with a focus on metabolites derived from the cyanobacteria Prochlorococcus. I find that Alteromonas grows on 3-methyl-2- oxobutanoic acid, a valine intermediate, but not on the other cognate branched chain amino acid intermediates. This substrate selectivity is likely driven by transporter specificity. The distinct fate of these structurally similar molecules emphasizes the importance of compound-specific characterization of labile DOM. To expand our ability to make these compound-specific measurements, in Chapter 3 I develop a method for derivatizing carboxylate-, carbonyl-, and phosphate-containing molecules via aniline derivatization, solid phase extraction, and liquid chromatography-tandem mass spectrometry (LC-MS/MS). This method is able to quantify 51 different metabolites dissolved in seawater, 25 of which could not be detected previously, with pM to nM detection limits. I verify the utility of this method by applying aniline derivatization to phytoplankton culture filtrates and field samples. Additionally, I show that where dissolved metabolites can be quantified by multiple methods, the measurements obtained by aniline derivatization are in good agreement with measurements yielded by other methods. Finally, in Chapter 4 I use aniline derivatization to characterize the diel variability of labile DOM produced by phototrophic microbes. Here, I apply aniline derivatization to filtrate from cultures of Prochlorococcus grown under 24-hour diel light/dark conditions and sampled every two hours. I find that Prochlorococcus cells not only release metabolites into solution, but also take those metabolites up again, with diel rhythmicity. Together, this thesis shows that microbe-DOM interactions can be remarkably subtle and complex; expands our ability to quantify the metabolites that make up labile DOM; and demonstrates the importance of directly quantifying these dissolved metabolites to fully characterize microbial ecology and carbon cycling in the ocean.",
        "authors": [
            "Kathryn H. Halloran"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157080",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Implementing a Tiled Singular Value Decomposition: A Framework for Tiled Linear Algebra in Julia",
        "abstract": "High-performance computing (HPC) is essential for scientific research, enabling complex simulations and analyses across various fields. However, the specialized knowledge required to utilize HPC effectively can be a barrier for many scientists. This work introduces a hardware-agnostic, large-scale tiled linear algebra framework in Julia designed to enhance accessibility and usability without compromising performance. By providing a flexible abstraction layer, the framework simplifies the development and testing of new algorithms across diverse computing architectures. Julia language’s multiple-dispatch and type inference facilitate the development of type-agnostic, hardware-agnostic, and multi-use frameworks by allowing composability. Utilizing a tiled approach, the implemented framework improves data locality, parallelism, and scalability, making it well-suited for modern heterogeneous environments. Its practical benefits are demonstrated through the implementation of tiled QR-based singular value decomposition (SVD), demonstrating how it streamlines the development process and accelerates scientific discovery. The developed framework is used to implement an in-GPU tiled SVD and an out-of-core GPU-accelerated SVD. Furthermore, its extensibility is demonstrated by implementing a tiled QR algorithm. This work aims to democratize HPC resources by bridging the gap between advanced computational capabilities and user accessibility, empowering a broader range of scientists to fully leverage modern computing technologies.",
        "authors": [
            "Evelyne Ringoot"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157092",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "An Analysis Using State Space Global Coherence of Brain Dynamics in a Young Child Under Sevoflurane General Anesthesia",
        "abstract": "The dynamics of brain states under general anesthesia in infants are complex and exhibit significant developmental changes, particularly in the context of neurophysiological responses. Traditional EEG analysis has been valuable in tracking these changes, but there is a critical need for more precise, quantitative methods to assess neural synchrony and coherence in this vulnerable population. This thesis explores advanced state-space modeling techniques, specifically focusing on State Space Global Coherence (SSGC), to estimate global coherence (GC) during sevoflurane general anesthesia in an infant. Two different SSGC approaches were employed: one approach directly estimated GC from the data, while the other first estimated the covariance matrix and then used this matrix to compute GC. The SSGC approaches were first applied to a validation dataset that had been previously analyzed using SSGC for covariance estimation. This was done to ensure that my analysis was functioning correctly by validating it against a dataset with known outcomes before proceeding with exploratory analysis. Once this was certain, the next step involved applying this pipeline to EEG data from a 10-month-old infant—a dataset where SSGC had not been previously utilized. Following this, both the validation dataset and the infant dataset were used to compare the effectiveness of SSGC for covariance estimation versus direct GC estimation. The infant dataset, in particular, provided an opportunity to explore the utility of SSGC in a new context. Both datasets that the SSGC methods were applied to had a low signal to noise ratio. This revealed that direct GC estimation provided improved temporal resolution for GC and the ability to capture dynamic changes in coherence over time. In contrast, SSGC for covariance estimation produced results nearly identical to empirical GC, suggesting that it is more susceptible to noise. The resilience of direct GC estimation to noisy data highlights its potential as a robust tool for capturing the spatiotemporal dynamics of neural synchrony under anesthesia. This thesis emphasizes the importance of advanced modeling techniques in enhancing neurophysiological monitoring, with significant implications for improving pediatric anesthetic care and outcomes.",
        "authors": [
            "Sebastian A. Gallo"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157097",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Do High Street Retail Rents Align with the Economy? An Analysis of Retail Real Estate Pricing Dynamics Based on Macroeconomic Trends",
        "abstract": "This study closely examines the correlation between high street retail rents and key economic indicators, specifically Consumer Price Index (CPI) and Gross Domestic Product (GDP). Utilizing data on rent levels from prominent high streets globally, the analysis incorporates these macroeconomic indicators to discern patterns and relationships. Through methodologies such as multiple linear regression and Error Correction Model (ECM), the paper aims not only to analyze how high street retail rents align with CPI and GDP but also to explore the primary factors influencing these rents. In studying high street retail properties or considering the acquisition of such properties, this methodology can be used to determine whether a high street is susceptible to macroeconomic fluctuations. If not, it may be necessary to consider the uniqueness of the area or potential risks involved.",
        "authors": [
            "Yujian Xu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157106",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Development of Machine Connectivity Guidelines for Production\r\nFloor",
        "abstract": "This thesis introduces and uses a standardized method for assessing machine connectivity at manufacturing facilities and develops a roadmap for an organization looking to implement connectivity at its facilities. As technology rapidly advances and Industry 4.0 takes hold of manufacturing worldwide, it is essential for manufacturing companies to utilize the latest technology to maintain a competitive advantage by optimizing operations, improving productivity, and increasing throughput. In this work, an overview of machine connectivity and its benefits are presented, and technologies and security measures used for connectivity are explored. Upon compilation of this information, a comprehensive rubric was developed with six weighted connectivity criteria, each scored from 0 (no progress) to 4 (fully complete), from which a total connectivity score can be computed. The rubric serves as a guiding tool for gauging a manufacturing facility’s level of maturity with regards to connectivity, and helps identify areas of need both within a facility and within an organization as a whole. The connectivity levels of six different manufacturing facilities were assessed using the rubric. The results were compiled to understand the development of connectivity at different facilities across the organization. The learnings from this analysis are used to develop guidelines as the organization continues its push towards full connectivity across all of its facilities. The next steps in this initiative are to: 1) utilize the developed rubric to assess connectivity at all of its manufacturing facilities, 2) identify facilities in need of the most resources in order to plan and execute connectivity, and 3) encourage collaboration between facilities to expedite the connectivity implementation process.",
        "authors": [
            "Kenan Hayel Sehnawi"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157244",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "State and Dynamics Estimation in an Outdoor Multi-Drone Slung Load System",
        "abstract": "Over the past decade, aerial drones have been used to address problems in areas such as sensing and measurement, inspection, delivery, security, and defense. Adding a load attached to one or more drones using a flexible cable can significantly enhance the capabilities of these platforms. This work aims to develop a multi-drone platform, built on open-source tools such as PX4 and ROS2, that can be used to lift a general slung load in an outdoor environment. Various fidelity simulators, including a pseudo-photo-realistic Gazebo simulator, are developed alongside a functional real world platform for testing load pose estimation methods. A novel cable-based testing apparatus that enables drone translation is used to facilitate stability testing of a quasi-static formation control method for lifting a slung load. This work aims to be the first to use visual feedback to estimate a load’s pose in a multi-drone slung load system operating without external motion capture devices. In simulation, perspective-n-point-based visual estimation achieves position errors of 0.1 m, and geodesic distance attitude errors around 0 ◦ . Real world testing shows errors of 0.2 m and 5 ◦ respectively. Applying extended Kalman filter and unscented Kalman filter formulations, simulated position estimates average around an error of 0 m, while the error noise magnitude is only 6% of the cable length at 0.06 m. Achieving accurate load pose estimates without an inertial measurement unit mounted to the load requires a good cable dynamics model. This work concludes by presenting a novel model for the effect of cables in a drone-slung-load system. A method based on universal differential equations shows promising early results.",
        "authors": [
            "Harvey Merton"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157197",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Survey Techniques to Examine Morphological Evolution of Coastal Regions",
        "abstract": "Beaches are dynamic, changing with tides, winds, and waves. Here, a beach was mapped daily for 3 weeks from the dune to the low-tide water line on the Outer Banks of North Carolina at the US Army Corps of Engineers Field Research Facility in Duck. The 22,500 m2 area of interest was surveyed daily by a walker carrying a GPS-equipped backpack and occasionally with a lidar equipped drone. Surveys of the northern region of interest also were collected with a stationary terrestrial lidar mounted on the dune. The observed morphological events include the destruction and formation of a cusp field during which there was 1.4 m of erosion and accretion associated with bays and horns, and the formation over 7 days of a ~1-m high ridge and runnel system. The GPS-equipped backpack apparatus was used as ground truth for estimates made with the lidar systems. Along both cross- and alongshore transects the lidar elevations were within approximately 0.05 m of those estimated by the backpack surveys, with RMS errors less than 0.11 m.",
        "authors": [
            "Seth N. Ammons"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157143",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "HYPERION: A HYdrogen PERmeatION Experiment to Quantify Hydrogen Transport in Fusion-Relevant Molten Salts",
        "abstract": "The measurement of hydrogen transport properties of molten salts like FLiBe is crucial for the development of advanced nuclear technologies like lithium-bearing liquid immersion breeding blankets for fusion reactors. Tritium production and the quantification of its mobility in these materials is necessary for efficient operation of these technologies. A common method of measuring these properties is with hydrogen permeation experiments. Hydrogen permeation experiments involve measuring the flux of hydrogen permeating through a substance, and from this flux transport properties like the diffusivity and solubility of hydrogen in the molten salt can be derived with various models of the experimental setup. This thesis describes the process of fabricating and assembling a HYdrogen PERmeatION (HYPERION) experiment and provides preliminary results of the functionality as well as some issues and troubleshooting of the experiment. Using the code Finite Element Simulation of Tritium In Materials (FESTIM), the experiment was also modeled. The models were used to explore the design parameter space of the experiment to determine the experiment’s effectiveness in producing the desired result of accurately calculating the hydrogen transport properties of the molten salt. Through the process of modeling, the assumptions that were normally made when performing these experiments were called into question and their validity was quantified, suggesting that the experiments that have been previously conducted might have been significantly affected by these assumptions. Using these models could eventually improve the accuracy of measured transport properties for molten salts like FLiBe and other nuclear fusion-relevant molten salts and inform the design of hydrogen permeation experiments moving forward.",
        "authors": [
            "Jaron F. Cota"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157221",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Instrumenting Observability in a Decentralized Microservice Architecture",
        "abstract": "Software systems have increased in complexity over time, and with this increased complexity has come an increased need to keep these systems organized and functioning efficiently. Observability is closely attached to ensuring this correct and effective system function. Without system monitoring, it is difficult to pinpoint when errors occur and correct them at their sources. Monitoring systems also helps to understand a system from the outside by allowing developers to ask questions about the system’s state and function without needing to know the details of what comprises the system’s internal behavior. While there are existing solutions for observability frameworks, these solutions do not target microservice architectures, which are used more and more with expansive code bases, such as those likely to be employed in an industry environment. They also require extensive configuration to be fully integrated with a pre-existing system. As such, the challenge lies primarily in adapting observability solutions to a decentralized, microservice architecture found in an industry setting. The existing solutions also come with advantages and disadvantages for different situations, so they are often incomplete in addressing an entire system’s needs. The integrated system created here satisfies our system’s requirements of a consolidated observability platform while also enabling future customizations, thereby allowing problems to be identified more quickly and proactively.",
        "authors": [
            "Helen X. Liu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157254",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Refining Hardware of Desktop Fiber Extrusion Devices\r\nfor Affordable Manufacturing and Novel Fiber Prototyping",
        "abstract": "The Fiber Extrusion Device (FrED) is a hands-on desktop tool designed to facilitate the teaching of manufacturing engineering concepts through remote laboratory experiences. FrED simulates the continuous fiber draw process used in various industries, including fiber optics, synthetic textiles, medical devices, aerospace, and construction. This device translates industrial-scale fiber draw towers into a compact version, allowing users to experiment with different parameters to understand their effects on manufacturing processes. Over the past three years, successive groups of MEng students have refined FrED’s design with the goal of creating a robust, functional, and affordable device for in-house manufacturing at the MIT FrED Factory. While the 2023 model achieved significant cost reduction, it required further hardware and electronics refinement for stable and repeatable performance. This thesis encompasses two main objectives: enhancing the hardware design and assembly processes for the final 2024 educational FrED model, and developing an alternative design for an advanced FrED version suitable for academic lab settings to rapidly prototype synthetic fibers. The first objective was met by improving the two most dynamic sub-assemblies—the gearbox and extrusion system—to ensure smooth and consistent operation. Additionally, conjoining part tolerances and hardware insert locations and geometries within manufactured parts were verified and adjusted according to manufacturing standards. Multiple jigs were also designed and fabricated to facilitate the assembly process of the gearbox and extrusion sub-assemblies, and two new parts were created to enhance user operation of FrED. For the second objective, an enhanced version of FrED capable of handling a wider range of preform materials was developed by upgrading the extrusion sub-assembly to operate at temperatures over three times higher than the educational version. This feature had been previously attempted with older, more expensive versions of FrED but had not been pursued with the recent, more affordable iteration. The new high-temperature FrED successfully drew fibers from PLA, a biodegradable thermoplastic, using 3D printed preforms with distinctive geometries, demonstrating its potential for providing an affordable solution for rapid synthetic fiber prototyping in academic labs.",
        "authors": [
            "Kaili Glasser"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157149",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Bridging the Health Divide: Achieving Equitable Healthcare Access in Kenya through Artificial Intelligence",
        "abstract": "This research explores the innovative application of Artificial Intelligence (AI), particularly Generative Pre-trained Transformer (GPT) models, in designing culturally sensitive hospitals for rural Kenya. The research addresses the critical need for improved healthcare infrastructure in underserved areas, focusing on the potential of AI to create efficient, adaptable, and contextually appropriate hospital designs. The study employs a mixed-methods approach, combining qualitative analysis of cultural practices and healthcare needs with quantitative data on environmental factors and health statistics. A GPT model is developed and fine-tuned on a comprehensive dataset of Kenyan cultural information, healthcare data, and architectural knowledge. This AI model is then used to generate hospital design concepts that are evaluated against newly developed cultural sensitivity metrics. Key findings demonstrate the potential of AI to significantly reduce design time, improve space utilization, and enhance cultural appropriateness in hospital designs. The thesis also highlights the importance of human-AI collaboration, with local experts and community representatives playing crucial roles in refining and implementing AI-generated concepts. Challenges identified include data quality and availability in rural settings, the need for ongoing model refinement, and the importance of establishing ethical guidelines for AI use in healthcare design. The thesis concludes with a set of recommendations for implementing AI-driven, culturally sensitive hospital design processes in rural Kenya, including the development of specialized AI models, and establishment of collaborative design methodologies. These findings have significant implications for improving healthcare infrastructure in resource-constrained settings and offer a model for culturally sensitive, AI-driven architectural design in developing contexts globally.",
        "authors": [
            "Geoffrey Mosoti Nyakiongora"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157182",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Dynamics and Implications of ROS in Marine Systems",
        "abstract": "The reactive oxygen species (ROS), superoxide and hydrogen peroxide, play critical roles across diverse marine ecosystems, influencing redox chemistry and organismal health. The distribution and concentration of these compounds in the oceans may serve as important controls for various biogeochemical cycles. The contrasting physiological nature of ROS, serving as both integral compounds for cellular processes such as signaling and growth while inducing oxidative cell damage at elevated concentrations, has made interpretation of their roles in organismal and ecosystem health challenging. Despite the potential for these ROS to provide unique insights into the intricate interactions occurring at the interface between life and its surrounding environment, critical gaps in our understanding of these compounds in marine systems exist. In this thesis I explored two aspects of marine ROS. The first part is focused on advancing our understanding of the distribution of superoxide in the sea. As part of a multidisciplinary team, I developed a submersible chemiluminescent sensor (SOLARIS) capable of measuring ROS in situ to ocean depths greater than 4,000 meters. With the use of SOLARIS, I discovered that a broad diversity of sponges and corals are local hotspots of superoxide at depth. Then, I studied the distribution of superoxide in the stratified water column of the Baltic Sea and found large subsurface maxima in the aphotic zone. In the second part of this thesis, I probed the use of hydrogen peroxide as a monitoring agent of organismal health. I measured hydrogen peroxide and bromoform production by two seaweed species exposed to different stressors. An analysis of these signals suggests that hydrogen peroxide could serve as a non-invasive chemical signature for stress in seaweed meadows and farms. Lastly, I characterized hydrogen peroxide associated with different coral species during a Stony Coral Tissue Loss Disease transmission experiment. I determined that hydrogen peroxide does not predict infection before lesions are visible, thus hindering its utility as an early-stage signature of disease within corals. Altogether, this thesis extends our perspective on the distribution and controls on ROS in various marine systems and provides a baseline for using ROS dynamics to monitor organismal health.",
        "authors": [
            "Lina Taenzer"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157137",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The development and application of mass-spectrometry-based tools to monitor proteome remodeling in microbes",
        "abstract": "Outside of controlled laboratory environments, cells are continually sensing and adapting to highly variable environmental conditions in an effort to maintain cellular homeostasis and to maximize fitness in each condition. Although specific stresses elicit distinct cellular responses, the reshaping of the proteome is a central element in most cellular adaptation. This dynamic proteome remodeling involves a highly orchestrated combination of regulated proteins synthesis, degradation, and modification, each contributing to the overall goal of matching the capacity of the expressed proteome to the demands of the sensed environment. Although each pathway will contribute, ultimately, whether cells mount a response primarily driven by synthesis or by degradation hinges on the nature and duration of the stress, as well as the cell type involved. Understanding the balance of these contributions has historically been challenging. As such, there is a need for approaches that can quantitatively resolve the contributions of protein synthesis and protein degradation pathways in a wide array of cellular and environmental contexts.\r\nQuantitative proteomics via mass spectrometry stands out as a powerful tool for deciphering these questions, as it allows one to simultaneously monitor thousands of proteins. In this work, I leverage the power of quantitative proteomics coupled with metabolic labeling to investigate how microbes remodel their proteome during cellular adaptation. In chapter 2, I describe the development and characterization of these proteomic methods, including a detailed analysis of the variety of metabolic labeling schemes that can be employed in budding yeasts, which facilitate the bulk of my thesis work. In chapters 3 and 4, I apply these methods to the methylotrophic yeast, Komagataella phaffii, which grows robustly on a diverse set of carbon sources. As such, I use K. phaffii as a key case study to explore questions of cellular adaptation. I find that the K. phaffii expressed proteome varies greatly between cells grown in methanol, oleate, or glucose and, interestingly, that proteome remodeling strategies vary in a context-dependent manner. Specifically, I find that autophagic degradation drives proteome remodeling under nitrogen starvation conditions, with selective autophagic degradation of peroxisome supporting the cells transition from methanol media to glucose media. In contrast, I uncover that synthesis and growth-coupled dilution is the primary driver as K. phaffii adapts from methanol media to oleate media. Given the deep proteome coverage enabled by my methods, and my application of these methods in a wide variety of genetic backgrounds (6) and environmental conditions (5), these datasets also serve as a rich resource to identify conditions stimulating degradation of specific proteins, as well as the genetically defined pathways supporting these activities. Finally, in appendices 1 and 2, I highlight how these approaches can be applied across different microbial species to broadly characterize the proteomic consequences of nutrient and genetic perturbations. Overall, my work highlights how the development and application of powerful quantitative methods provide a global view of how proteome remodeling supports cellular adaptation, reveal deeper insights into pathways supporting turnover of specific proteins, and help to identify potential therapeutic targets to ameliorate protein-turnover related diseases.",
        "authors": [
            "Bertina Telusma"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157155",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Degradation Mechanisms and Applications in Ion Intercalation Materials",
        "abstract": "Lithium ion batteries (LiBs) are a pivotal energy storage technology that are widely adopted for their high energy density and safety. From a macroscopic level, LiBs operate at a micrometer lengthscale, but consist of many active material nanoparticles which participate in reversible electrochemical reactions that store and release energy. These particles control the crucial processes for energy storage in macroscopic devices, generating a process spanning multiple length and time scales in LiBs. However, despite the ubiquitous application of LiBs in many industries, degradation limits their lifespan, hindering their broader applicability in usages demanding high energy density and extended lifespans, such as electric vehicles (EVs). Dominant degradation occurs at the nanoparticle level involving various mechanisms, such as formation of resistive films on the particle surface or surface phase transformations in common LiB materials. The effects of degradation are observed at the macroscopic level from electrochemical responses such as voltage or current measurements. Bridging the gap between microscopic and macroscopic scales to extract particle level degradation mechanisms from electrode scale responses is essential for understanding LiB degradation. These methods can be used to quantify degradation in battery materials for second life use, designing degradation resistant materials, and more.\r\n\r\nHere, I propose a comprehensive multiscale framework that initially models LiB degradation at a single particle scale, using nickel rich materials as an example, then projects single particle degradation into population scale for both solid solution and phase separating materials. Furthermore, I analyze and design improved pulse diagnostics using hybrid pulse power characterization (HPPC) methods to extract physical microscopic degradation mechanisms from electrode-level responses. Overall, I set up a consistent framework modeling degradation from single particle to population level and vice versa in LiBs.",
        "authors": [
            "Debbie Zhuang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157236",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Computational design of a novel soft-Xray-based turbulence diagnostic in NSTX-U",
        "abstract": "Turbulence transport poses a significant challenge in fusion research. The measurement of turbulent fluctuations is critical for comprehending turbulence transport, predicting its behavior, and ultimately controlling it to maximize fusion gain. However, there is a notable scarcity electron temperature fluctuation diagnostics, including for high density tokamak plasmas and in spherical tokamaks. The ultimate aim of our research is to develop a novel diagnostic tool for temperature fluctuations. Before experimental exploration, conducting a numerical feasibility study is essential for the proposed diagnostic. The high spatial and temporal resolutions that are attainable using Soft X-ray (SXR) imaging makes it a promising candidate. The primary objective of the thesis is to assess the feasibility of an electron temperature fluctuation diagnostic based on SXR imaging.\r\n\r\nThe feasibility study involves gathering fluctuation data and constructing a numerical diagnostic model. This model computes synthetic SXR measurements, which are then reconstructed using tomographic algorithms to derive electron temperature fluctuations. These reconstructions are then compared against the ground truth to assess diagnostic performance. Optimization of performance is achieved by adjusting diagnostic parameters to identify the optimal set for feasibility analysis.\r\n\r\nThis study consists of two primary parts. First, we utilize a simplified toy model with circular plasma geometry and synthetic fluctuation data abstracted from gyrokinetic simulation fluctuation spectra and we employ a pseudolocal tomography algorithm for reconstruction and demonstrate reliable measurement of electron temperature fluctuations for X-ray detectors with sufficiently high signal-to-noise ratio. Second, we conduct a more comprehensive feasibility study using fluctuation data directly generated from gyrokinetic simulations, in a real (spherical tokamak) NSTX-U configuration with complex plasma geometry. Assumptions from the previous study, such as infinitely thin beam size, are relaxed to assess their impact on reconstruction. Additionally, we enhance the reconstruction algorithm using neural networks, enabling reconstruction of both electron density and temperature fluctuations, as well as cross-phase analysis. Overall, the study confirms the feasibility of the SXR diagnostic given that SXR detectors meet minimum requirements. Furthermore, we explore fluctuations generated from different gyrokinetic simulations, demonstrating the diagnostic's ability to differentiate fluctuations originating from different instabilities under the same configuration.\r\n\r\nThis research provides a theoretical foundation and guidance for developing a practical SXR-based electron temperature fluctuation diagnostic for experimental use. It outlines the measurable quantities, their limitations, and the minimum requirements for SXR hardware to ensure reliable measurements. This contribution significantly advances our understanding of plasma turbulence transport, addressing a key challenge in fusion research. However, the current study's limitations employ a simplified emissivity model. Utilizing a more comprehensive model incorporating atomic data could yield more robust conclusions. Additionally, incorporating real hardware parameters would enhance the reliability of the conclusion.",
        "authors": [
            "Xiang Chen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157154",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Adversarial Prompt Transformation for Systematic\r\nJailbreaks of LLMs",
        "abstract": "The rapid integration of Large Language Models (LLMs) like OpenAI’s GPT series into diverse sectors has significantly enhanced digital interactions but also introduced new security challenges, notably the risk of \"jailbreaking\" where inputs cause models to deviate from their operational guidelines. This vulnerability poses risks such as misinformation spread and privacy breaches, highlighting the need for robust security measures. Traditional red-teaming methods, involving manually crafted prompts to test model vulnerabilities, are labor-intensive and lack scalability. This thesis proposes a novel automated approach using Reinforcement Learning from Human Feedback (RLHF) to transform unsuccessful adversarial prompts into a successful jailbreak. Thus it learns a policy based on relation to existing jailbreak prompts that informs the generator LLM of what makes an adversarial prompt successful. This was implemented using Proximal Policy Optimization (PPO) and tested with both a classifier and judge reward model, attaining at best a 16% attach success rate on a target model. This research can be applied to any prompt at the word level and further analyzed on characteristics of toxicity. This work contributes to advancing LLM security measures, ensuring their safer deployment across various applications.",
        "authors": [
            "Kevin E. Awoufack"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157167",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploring the Internet Celebrity City: Social Media and Urban Space in China",
        "abstract": "“Internet celebrity space” offers a fresh perspective for studying urban spaces in the mobile Internet era as a new visual consumption space. The term \"Internet celebrity,\" or wanghong in Chinese, is utilized in modern Chinese media to refer to celebrities and the specific cultural and consumption trends linked to them. This concept has surfaced alongside the growth of e-commerce platforms, with the recognition that wanghong often engages in promoting products, services, or lifestyles to their followers. The internet celebrity spaces, or wanghong spaces, can elevate the popularity of certain areas and influence local neighborhoods, communities, and economies. Internet celebrity urbanism involves broadening this trend from certain locations to greater scales, encompassing entire districts or extending this status through urban scale. This thesis explores the impact of internet celebrity spaces in China. It is divided into three parts: Firstly, it demonstrates the phenomenon and background: study investigates the way Internet Celebrity spaces are represented in social media. Then, the studies focus on exploring the latest research and analyzing the research perspectives and methods to anchor the author’s research questions with appropriate approaches. Lastly, the influence of Internet Celebrity spaces is discussed through examining the case in Shanghai by observing internet celebrity spaces’ influence on street activity. With the analysis and conclusion, suggestions for future development are given.",
        "authors": [
            "Yufei Chen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157148",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Development and Execution of a Testing Strategy for Omnidirectional Wheels",
        "abstract": "Omnidirectional wheels enable robots to achieve holonomic motion; however, this often comes at the cost of increased rolling resistance compared to traditional caster wheels. The rolling resistance in omnidirectional wheels is higher than in many other wheels due to several factors including an irregular tread shape, material compliance, and friction in the bushinglike cross rollers during lateral motion. Testing standards exist for characterizing the rolling resistance, compressive strength, and other attributes of commonly used wheels such as caster wheels. However, there are no comprehensive testing standards or research that broadly characterize the performance of omnidirectional wheels. Here, test methods are described for characterizing the load relaxation, stiffness, and rolling resistance of omnidirectional wheels, and the results from these tests are presented. Test apparatuses for static loading and rolling resistance were created. Test results were analyzed to determine important factors for determining the ultimate compressive strength in static loading and the rolling resistance coefficient of an array of omnidirectional wheels, and results indicate wheel manufacturing methods and materials are the most important factors for determining these responses.",
        "authors": [
            "Michael J. Donnellan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157242",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Shape of Kubler: George A. Kubler in Peru, 1948-49",
        "abstract": "Yale art history professor George Kubler’s seminal 1962 publication The Shape of Time is, according to his own words, representative of a “crossroads between the history and anthropology of art.” This work does not stand alone, but is rather part of a larger corpus of study through which Kubler recurred to disciplines, methods and tools outside of what is traditionally considered art historical—including anthropology, architectural representation, and biology—in order to generate new readings and understandings of the history of South and Central American art. This thesis takes a look into a year of Kubler’s life in 1948-49, spent in Peru conducting archival research and field work on culture change with the Institute for Social Anthropology at the Smithsonian Institution and teaching a seminar on the use of archival sources in ethnology at Universidad Nacional Mayor de San Marcos in Lima; during this time, Kubler also engaged in the construction of an archive of his own. Drawing from correspondence and other records from the period in question, a series of lost episodes resurface, providing a reconstruction of various strata of 1940s Peruvian society: an increasingly cosmopolitan Lima stands in stark contrast to the underdeveloped, feudal Andean world, evidencing its colonial underpinnings. I contend that witnessing the coexistence of various temporalities within a single geographic territory had a significant impact on Kubler’s later theories on spatialized historical time.",
        "authors": [
            "Johann Schweig"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157181",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The rickettsial effector Sca4 has a conserved interaction with host clathrin and a tick cell specific role in infection",
        "abstract": "Intracellular bacterial pathogens secrete effectors to manipulate the host cell environment, create a hospitable niche, and promote infection. While many effectors interact with specific host machinery to perform a single distinct function, some effectors are capable of interaction with multiple host proteins to carry out multiple functions. Rickettsia species are obligate intracellular bacteria that cause vector-borne diseases that constitute an ongoing public health threat. As Rickettsia spp. have small genomes, and thus a limited coding capacity, multifunctional effectors may be an efficient way to manipulate their host environment. However, relatively few secreted effectors have been characterized in the Rickettsia genus and even fewer have been identified as multifunctional effectors. \r\n\r\nIn this work, I demonstrate that the rickettsial secreted effector Sca4 interacts with the host endocytic factor clathrin heavy chain. As previous work showed that Sca4 interacts with the host protein vinculin in mammalian cells, this discovery of the Sca4-clathrin interaction makes Sca4 one of the first multifunctional effectors to be identified in a Rickettsia species. When investigating the potential role of the Sca4-clathrin interaction, I found that clathrin promotes the cell-to-cell spread of R. parkeri in mammalian cells by acting in the recipient cell. However, the Sca4-clathrin interaction was found to be dispensable for efficient cell-to-cell spread. I investigated the role of this interaction in the tick arthropod vector and found that the Sca4-clathrin interaction is necessary for the efficient proliferation of R. parkeri in tick cells. These findings show that knowledge of the complete roles of rickettsial secreted effectors in both arthropod vector and mammalian hosts is needed to fully understand rickettsial pathogenesis.",
        "authors": [
            "Cassandra Joan Vondrak"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157209",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Automatic Generation of Chemical Kinetic Models for Biofuel Oxidation and Pyrolysis",
        "abstract": "Biofuels hold great promise for reducing greenhouse gas emissions and boosting engine performance. Modeling biofuel combustion and pyrolysis chemistry with chemical kinetic models allows for high-throughput evaluation of their performance under various conditions. These models, typically comprising hundreds of species and thousands of elementary reactions, provide quantitative predictions for the investigated systems. While manually creating such mechanisms is labor-intensive and requires extensive knowledge, reaction mechanism generation software, such as the Reaction Mechanism Generator (RMG), greatly facilitates model development by automating the selection of relevant species and reactions, as well as estimating their thermochemical, kinetic, and transport parameters. Despite the advances in these software packages and their success in modeling small, simple conventional fuels, their application to novel, under-explored biofuels is often limited due to a lack of knowledge in the relevant chemical space. This gap can be bridged by expanding the software's access to accurate thermochemical and kinetic parameters. However, these data are scarce, and their acquisition, typically via quantum chemical calculations, is challenging on a large scale. \r\n\r\nThis thesis addresses these challenges by developing automated workflows to enhance the calculation of accurate thermochemical and kinetic parameters, thereby extending the capabilities of RMG for biofuel modeling. First, an automatic thermochemistry calculation workflow is implemented and integrated into the chemical kinetic model development process. The significant improvement in computational capacity enables an iterative approach to model generation and refinement, where the thermochemistry of hundreds of molecules is refined in each iteration. This approach is validated through the modeling of light alkene combustion chemistry, resulting in a model that accurately predicts key combustion properties and outperforms other well-regarded models. This study highlights the necessity of sufficient refinement iterations for a comprehensive exploration of the relevant chemical space and the convergence of critical species and reactions in the chemical kinetic model. This approach is then applied to model less-studied biofuels, such as butyl acetate isomers and tetramethylethylene. By incorporating key kinetic parameters from literature and quantum chemical calculations, along with iteratively refined thermochemistry, the developed models demonstrate strong predictive capabilities. These models agree with experiments conducted after their development and reveal important reaction pathways in the studied systems.\r\n\r\nAdditionally, this thesis enhances the acquisition of accurate kinetic parameters through the simultaneous development of software, datasets, and data-driven models. RDMC, a cheminformatics software, is developed, featuring toolkits for elementary reaction analysis and end-to-end automated workflows for generating molecular and transition state conformers. A dataset covering nine reaction types relevant to combustion and pyrolysis radical chemistry is created using the RDMC workflow. Concurrently, another radical reaction dataset is curated, covering different reaction types. In total, the two datasets introduce high-quality elementary reaction data for over 11,000 radical reactions. Eventually, a graph neural network is trained on the new dataset for fast kinetic parameter estimation that can potentially benefit chemical kinetic model development.",
        "authors": [
            "Xiaorui Dong"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157195",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "GPU-Oriented Algorithms for Continuous Energy Monte\r\nCarlo Neutron Transport",
        "abstract": "The advent of graphics processing units (GPUs) has brought computing to new heights with deep learning models, now deployed ubiquitously and touching the lives of many. While GPU hardware may be ideal for deep learning, its full potential in various scientific computing applications has yet to be realized. Often, paradigm shifts in the data formalisms and algorithmic choices used to solve scientific computing problems must take place to fully leverage GPUs. A quintessential example of this shift has been the move towards matrix-free, high-order finite element formulations researched under the Exascale Computing Project. Similar groundbreaking shifts are only starting to take place in continuous energy Monte Carlo (MC) neutron transport simulations. These simulations play a crucial role in designing fission, fusion, and security systems that may play a pivotal role in the transition to a decarbonized world. This work contributes to adapting continuous energy MC neutron transport simulations for the GPU computing era. We first summarize some changes made to other scientific computing applications that led to performance gains on GPUs, which informed our independent development of a CUDA-based version of OpenMC, an open-source continuous energy MC neutron and photon transport code. Fortunately, the historical event-based MC simulation modality developed extensively through the 1980s for vector computers provides an excellent basis for GPU computing. Adapting a full-physics, continuous energy MC neutron transport simulation for GPUs is a feat only completed by a few institutions across the world, so we share some software development tricks that facilitated this task. We then identify a variety of algorithmic optimizations that improved the performance of the baseline CUDA application, and identify areas for further development. 3 Based on experience adapting a full-physics continuous energy MC code for GPU, we identify two pieces of the simulation which can be improved for GPU computing: resonance upscatter handling and unresolved resonance modeling. Our new method for modeling resonance upscatter is based on a novel, fundamental observation regarding the resonance upscatter effect. The relative speed tabulation (RST) method developed by other GPU MC researchers can be underpinned by a universal special function we have named the incomplete Faddeeva function, which is closely related to the incomplete Goodwin-Staton integral. Our research develops numerical algorithms for efficient, accurate computation of the incomplete Faddeeva function and identifies some properties of the function. We then present a specialized root-finding algorithm that takes advantage of the structure of the problem to efficiently sample the resonance upscatter effect on GPUs. This obviates the need to rely on RST tables or a zero kelvin pointwise cross section, freeing precious GPU memory while using a GPU-friendly memory access pattern. Continuing in the same direction, we focus on unresolved resonance region (URR) crosssection modeling, which was shown to induce a 30% computational efficiency degradation on GPUs. We review the requirements to model cross sections in the unresolved resonance regime, and provide what is to our knowledge the first rigorous demonstration that URR modeling can be reduced to a one-dimensional probabilistic model in addition to some expectation values of partial cross sections conditioned on the total. Through three asymptotic arguments covering different resonant behavior regimes, we show that the normal inverse Gaussian distribution is the natural choice for modeling the total neutron cross-section distribution. Rather than inducing a performance degradation, we show the new URR modeling technique in fact outperforms a pointwise infinite-dilute approach when it is used to model the URR region.",
        "authors": [
            "Gavin Ridley"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157192",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Origins of the East Greenland Coastal Current on the Northeast Greenland Shelf: a Comparison of Two Reanalysis Products",
        "abstract": "The East Greenland Coastal Current (EGCC) carries some of the freshest outflow from the Arctic southward along the East Greenland Shelf and into the Nordic Seas and subpolar North Atlantic. How this fresh water initially flows onto the Northeast Greenland Shelf (NEGS) and feeds the EGCC is not well known due in part to the lack of observations in the region. In this thesis, I use two ocean reanalyses, the Regional Arctic Ocean/sea-ice Reanalysis (RARE) and Global Ocean Physics Reanalysis (GLORYS) to explore the structure and dynamics of the ocean circulation on the NEGS. To validate the use of these products in the region, I compare the reanalysis products to the Fram Strait Arctic Outflow Observatory for the period of 2003-2019. In the mean, RARE is too warm and salty compared to the moorings, while the properties in GLORYS track more closely to the observations. However, the observed velocity field is better represented in RARE than GLORYS. From there, I analyze the cross-shelfbreak flow from 74°N to 81.5°N in the two reanalysis products, and conclude that transport onto the NEGS of waters fresher than 34 salinity is driven by an Ekman circulation that arises from along-shelfbreak winds and a widening shelf south of 81.5°N. The enhanced transport of fresh water also shifts the isohalines across the shelfbreak, directing a geostrophic flow onshelf between 81°N and 79°N. The convergence of fresh water on the NEGS initiates the EGCC as an identifiable and distinct feature around 80°N in RARE, uniting the EGCC along the southwest coast of Greenland and its northern counterpart, the Polar Surface Water (PSW) Jet. In GLORYS, the EGCC is not present throughout the domain, though there is a weak net southward flow on the NEGS. The EGCC in RARE is primarily buoyancy-driven, though the along-coast winds likely play a major role in maintaining the density front that supports the EGCC. Results from this thesis have implications for the transport and fate of Arctic and Greenland-sourced fresh water, and stratification in the high latitude North Atlantic and Nordic Seas.",
        "authors": [
            "Sara L. Vianco"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157172",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Brownian dynamics simulation of soft matter with hydrodynamics: methods for constrained systems and shear processing of 2D materials",
        "abstract": "2D materials are a rising class of soft matter material with a promising set of unique characteristics. The most ubiquitous 2D material, graphene, for example, possesses large surface areas, tunability, and unique electrical, optical, and catalytic properties while being lightweight, strong, and flexible. This has led to graphene seeing use in separations, biomedical applications, flexible electronics, and more. Meanwhile, synthetic 2D polymers, a relatively new field of study, represent a massive expansion of the design space for 2D materials and their applications. Solution processing of these materials is often an important step for synthesizing or applying them, necessitating knowledge of their behavior in suspensions and in flows. As these materials become more viable, our fundamental understanding of them must increase in tandem. This will inform us in designing these materials for our desired applications. However, especially when compared to their 1D counterparts, our understanding of 2D materials is lacking. It is the goal of this thesis to help fill in this gap in knowledge.\r\n\r\nIn Chapter 1, we discuss the basics of soft matter and methods for simulating which are the basis for understanding the work in this thesis. We present and discuss the governing equations for the movement of soft matter particles. We then discuss the simulation methodology and mobility tensor approximations used in this thesis along with some additional considerations.\r\n\r\nIn Chapter 2 we study methods for simulating constrained Brownian systems. We compare the current state-of-the-art method for these simulations, GMRES, to a different method called the projected conjugate gradient (PrCG) method. In particular, we compared PrCG and GMRES for rigid bodies, freely jointed chains, and immobile systems. We find that both methods exhibit the same linear computational complexity. We find that PrCG, however, exhibits some notable advantages over GMRES including lower precomputational and storage burdens, a guaranteed feasible iterate, and trivial extension to new constraint types due to the lack of a preconditioner. We use PrCG to solve a mixed constraint problem with rigid body and immobile particles, comparing to the analytical solution at large separations.\r\n\r\nThe remainder of this thesis studies the effects of self-attraction on self-avoiding, semi-flexible, athermal 2D materials (sheets) in shear flow. In Chapter 3, we give a background on rheology and 2D materials which are necessary for understanding the remaining chapters. We begin by discussing non-Newtonian fluids, specifically their applications and affect on the momentum balance presented in Chapter 1. Then, we give a brief introduction on simple shear and discuss how it is implemented in simulations. Finally, we give a brief introduction to 2D materials, their applications, as well as previous experimental, theoretical, and computational work.\r\n\r\nIn Chapter 4, we model self-interacting, self-avoiding, semi-flexible, athermal sheets in shear flow. We find a rich conformational landscape of 4 different behaviors --- flat, tumbling, 1D folded, and 2D folded --- which are well-delineated by several dimensionless groups representing the ratios between shear strength and interaction strength, and bending rigidity and interaction strength. We use these dimensionless groups to explain the observed behaviors, explain the folding behavior of 1D folded sheets, and calculate and explain the viscosity of a dilute suspension of these sheets. We use the conformational and rotational properties of the sheet simulations to explain this behavior, demonstrating a new explanation for the non-monotonic rheological properties of 2D materials which does not involve sheet-sheet interactions (which are rare in dilute suspensions) or thermal energy (which is often small in sheet systems). We also study systems with two initially stacked sheets in order to model, for example, shear exfoliation of 2D materials. We find three behaviors --- separating, waltzing, and flipping --- which are characterized by the same dimensionless groups as single sheets. We again explain these behaviors and calculate the viscosity of these sheets which again shows interesting non-monotonic rheological properties which we also explain using the conformational and rotational properties of the sheets.\r\n\r\nIn Chapter 5, we use simple time-dependent flow protocols to show how the properties of sheets can be controlled. Specifically, we use linear shear annealing simulations to show that the final conformational properties of a sheet suspension can be tuned continuously by varying the quench time. We also use our knowledge of the phase map of sheets to design flow protocols with step changes in shear rate to produce a target state of highly aligned, 1D folded sheets which represents, among other things, our predicted lowest possible viscosity for a sheet suspension.\r\n\r\nIn Chapter 6, we discuss potential future directions for the sheet model applied in Chapters 4 and 5. Specifically, we discuss loose ends from Chapter 4 and potential extensions of the model. We discuss potential benefits of and complications in exploring these directions.\r\n\r\nFinally, in Chapter 7, we summarize the discoveries presented in this thesis and provide concluding remarks.",
        "authors": [
            "William Tian Funkenbusch"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157226",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Women Nobel Laureates in STEM (2000-2023): Life Stories, Challenges, and How They Achieved Impact for Success",
        "abstract": "Science, Technology, Engineering, and Math (STEM) are the critical growth engines that develop the economy and society and improve our lives overall. However, women are underrepresented in STEM, which means 50% of the world's brain power is untapped. We know that, in general, women face unique barriers and challenges than men, such as gender bias and stereotypes. However, we know less about the unique obstacles and challenges women face in STEM and even less about overcoming the barriers in STEM. This research aims to identify the challenges faced by women in STEM and to gain a practical understanding of what women can do to evolve as leaders. As STEM is extremely broad, this thesis focused on studying the 11 female Nobel laureates who won the prize after 2000 under the three STEM-related Nobel categories: physics, chemistry, and medicine or physiology.\r\n\r\nFirst, a comprehensive literature review was conducted to understand the study results of existing barriers faced by women in STEM and the enablers that can increase the likelihood of women's success in STEM. Next, data were collected about the 11 Women STEM Noble laureates, including their biographies, life stories, newspaper reports, and interview transcripts. The thematic analysis was then adopted to analyze the collected data, in which four themes are identified and presented: 1) Overcome Barriers and Challenges; 2) Qualities of a Good Scientist; 3) Supportive Systems; 4) Impactful, Humanity, Innovative. Finally, the findings are summarized in relation to the research objectives to provide insights for women who want to pursue a STEM career.",
        "authors": [
            "Kedi Wu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157207",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design, simulation, and testing of a low cost laser\r\nmicromachining system for flexible and rapid tissue-on-chip\r\nfabrication.",
        "abstract": "This study introduces a novel approach to tissue-on-chip device fabrication using low-cost picosecond laser ablation, addressing critical limitations in current manufacturing methods such as soft lithography, particularly in terms of material compatibility, feature resolution, and scalability. We developed a comprehensive finite element method (FEM) model for the laser ablation process, incorporating key physical phenomena including laser-material interactions, heat transfer, and material removal dynamics. This model, validated against experimental results, accurately predicts ablation depths within 20% of measured values across a range of laser parameters. Our experimental setup, utilizing a cost-effective 10 kHz picosecond laser system, demonstrates superior capabilities in creating high-aspect-ratio microchannels exceeding 20:1, surpassing traditional manufacturing techniques. We achieve precise control over channel dimensions, with widths ranging from 20 to 500 micrometers and depths up to 1 mm, while maintaining sub-micron surface roughness (Ra < 0.8 𝜇m). The system’s versatility is showcased through the fabrication of complex structures such as Tesla valves and high-resolution text features, with a minimum feature size of 20 𝜇m. We present practical techniques for component selection and process parameter optimization 3 using our simulation, reducing expensive and time-consuming experimentation. This work establishes low-cost picosecond laser ablation as a viable and advantageous method for tissue-on-chip manufacturing. With fabrication times of 6-8 minutes for small features and less than an hour for a full chip, our method represents a significant advancement in rapid prototyping capabilities. These findings demonstrate that laser ablation is a powerful technique for manufacturing tissueon-chip devices, offering high resolution, flexibility, and scalability. This approach has the potential to overcome the limitations of traditional methods, enabling the next generation of sophisticated, physiologically relevant in vitro models for biomedical research and drug development. The successful development and validation of the FEM model, coupled with practical demonstrations, provide a solid foundation for further advancements in laser-based fabrication of tissue-on-chip devices, potentially accelerating drug discovery processes and enabling more accessible production of personalized medicine platforms.",
        "authors": [
            "Jorge A. Nin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157161",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Experimental Evaluation of Underwater Semantic SLAM",
        "abstract": "Autonomy is crucial for underwater vehicles due to the challenging and inaccessible nature of underwater environments. These environments pose significant difficulties for human-operated systems because of limited visibility, high pressure, and vast areas that are costly and risky to explore manually. Implementing autonomy in underwater vehicles presents unique challenges due to the marine environment's harsh and complex nature. Underwater communication is severely limited as water absorbs and scatters most electromagnetic signals used in terrestrial communications. This necessitates the use of acoustic communication, which has a lower bandwidth and is prone to delays and signal distortion. Similarly, GPS signals do not penetrate water, complicating navigation and creating dependence on inertial and sonar sensors, which suffer from noisy measurements that are guaranteed to drift over time. The unpredictable dynamics of underwater environments, including varying currents, lighting conditions and obstacles, further complicate autonomous navigation. As such, data collection while moving through a preplanned course is the traditional mission of the Autonomous Underwater Vehicle (AUV), defining the limitation of current technology. Higher-level missions such as search, surveillance, maintenance and manipulation require greater situational awareness, decision-making and navigation abilities, facilitated by processing semantic visual information and applying it to map generation and localization. To address the limited autonomy of current AUVs and enhance their capability for complex missions, this thesis presents the development and evaluation of a real-time, monocular visual-inertial semantic Simultaneous Localization and Mapping (SLAM) system for underwater environments, implemented on the cost-effective BlueROV2 platform. The research aims to enhance AUV autonomy and enable complex underwater missions through improved navigation and semantic mapping capabilities. Key contributions include the integration of a custom-trained object detector for underwater environments, adaptation of a hybrid SLAM algorithm combining Gaussian and Non-Gaussian landmarks for underwater operation, preliminary assessment of the SLAM system's accuracy using motion capture-based ground truth measurements, and comparative evaluation of the developed semantic SLAM system against state-of-the-art alternatives in an indoor pool experiment using the BlueROV2. This work addresses the challenges of underwater navigation and semantic mapping, offering a potential solution to extend the operational capabilities and mission complexity of affordable AUV platforms.",
        "authors": [
            "Thomas Jeongho Song"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157234",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Tailoring the angular and spectral reflectance characteristics of color-dynamic films by modifying their photonic texture and topcoat roughness",
        "abstract": "Controlling nano- and microscale morphology is essential for tailoring the appearance of structurally colored stretchy films. An effective approach for controlling the optical properties of such color-dynamic photonic films, which are manufactured holographically, is demonstrated using two simple control handles: the texture of the photonic structure and the surface roughness of a transmissive topcoat. Texture of the photonic structure affects the spectral signature and angular distribution of reflected light. Surface roughness of the topcoat affects the angular distribution of incident and reflected light. Fourier optics concepts are harnessed for modeling and predicting the optical characteristics of the materials as a function of their photonic texture and topcoat roughness. The model is verified with data obtained by imaging the angular scattering distribution and spectroscopic analysis of four representative combinations of photonic texture and surface coat roughness. The findings presented in this thesis validate the hypothesis that controlling texture of the photonic film and roughness of its topcoat allows for tailoring the visual appearance of structurally colored materials. This approach provides access to a rich design space of different appearances, including strong iridescence, color constancy with collimated light sources at small angles of incidence, pure and muted colors, and specular and highly diffuse reflections.",
        "authors": [
            "Andrew D. Blair"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157199",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Intangible Reverberations Following Mergers & Acquisitions",
        "abstract": "This study preliminarily investigates how merger and acquisition (M&A) activities affect employees as stakeholders of the company system - specifically in the areas of leadership, communication, company direction, project autonomy, and path for career growth.\r\nInterviews of 14 employees supporting the oil and gas industry were conducted to determine the effect (if any) that M&A activities had on their careers and any similarities in their experiences. This data was evaluated against research completed by Steigenberger & Mirc and Schweizer & Patzelt.\r\nWhile the hypotheses presented cannot be proven, recommendations for future research are provided to gain and evaluate additional information.",
        "authors": [
            "Laura N. Warren"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157238",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Architectures of Microbiality: From Diatoms to Diatom Houses",
        "abstract": "This dissertation examines the diatom—a microorganism found ubiquitously from oceans to kitchen countertops—as a resonant factor in understanding modernity. From the early 19th century to the mid-20th century, rapid advances in optical microscopy dramatically unveiled the microbial world, during which diatoms, owing to their astonishing biological properties, forms, and material possibilities, became the subject of wonderment by a diverse group of naturalists, artists, and architects. I contend that microbial ubiquity constitutes a crucial but often overlooked aspect of environmental history, which architecture has consistently failed to account for. \r\n\t\r\nThe narrative progresses from early fascination with the diatom’s aesthetic potential to geologists’ more serious-minded efforts, laying the foundations for Richard Neutra’s (1892–1970) insistence that the diatom was critical to a new type of modern architecture. Through the case studies of Marquis Panciatichi d’Aragona (1813–1897) and Jacob Whitman Bailey (1811–1857), among others, I explore how diatom-influenced works from the 19th to early 20th centuries underpinned a transboundary dialogue on the microbial and its architectural imaginations. This perusal historicizes the theoretical and technological conditions that made possible the emergence of modernist views of nature, epitomized by Neutra’s philosophy on environmental psychology and realism through his early 20th-century proposal to integrate diatoms into the very fabric of modern living spaces. \r\n\t\r\nThis dissertation examines diatoms within shifting epistemological frameworks, tracking their transition from scientific specimens to motifs in visual culture. It investigates how unseen natural elements were disseminated, accumulated, and manifested into distinctly perceivable forms, revealing that the understanding of diatoms expanded from isolated, object-focused studies to a concern for environmental relationships and geological transitions. Filling the world was not some grand narrative of human will but the disorienting, puzzling, and even frightful everywhere-ness of microbiality. The intersection of diatoms with architecture changed from aesthetics and form to a deeper engagement with sites and land, following the transformative reconception of the thickness of the earth’s surface. This dissertation reveals a condition of knowledge that architecturally and psychologically rewrote nature as an encounter of biological (un)consciousness and technological actualization.",
        "authors": [
            "Xuan Luo"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157141",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Cross-Shelf Exchange Driven by Dense Flow Down a Canyon",
        "abstract": "Laboratory experiments investigated the dynamics controlling the cross-shelf exchange in a prograde sloping canyon induced by dense shelf water descending into the canyon. This thesis is motivated by the dispersal of dense water generated by polynyas on the Arctic and Antarctic continental shelves. Laboratory results corroborate prior numerical results suggesting that canyons are hotspots of cross-shelf exchange. When the dense water descends a canyon, it induces an onshore return flow of offshore water into the canyon. This return flow is initially driven by the dense water eddies descending the canyon and acting like a bucket brigade. At later times, another mechanism may also be at play where large dense cyclonic (anticlockwise) eddies on the northern continental shelf may pull more dense water out of the canyon producing a region of low pressure, near the canyon head, which induces an increase in ambient flow into the canyon. The Burger number (Rossby radius of deformation/canyon width) and the dense water source location with respect to the canyon head affect the offshore ambient water velocity up the canyon. Additionally, as the offshore water reaches the canyon head, the offshore water volume flux becomes larger than the dense water volume flux, possibly due to the low pressure region described above. Understanding these dynamics in the Antarctica region is of global significance for two main reasons: 1. The offshore flowing dense water forms Antarctic Bottom Water and thus affects the global meridional circulation; 2. The onshore heat transport induced by the return flow drives glacial ice melt and therefore contributes to sea level rise.",
        "authors": [
            "Christian M. Mier"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157175",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Effects of the US Capitol Attack on political views in Argentina, Brazil, and Chile",
        "abstract": "Is it possible for major political events, such as the U.S. Capitol insurrection on Jan. 6, 2021, to influence political attitudes in other countries? Such events may act as framing devices that influence individuals to think somewhat differently about democracy and populism, primarily by reminding them of domestic shortcomings. Some previous literature has found international attitude effects from major events like terrorism or environmental disasters. In this study, I take advantage of the fact that the insurrection took place in the middle of a set of surveys administered to bureaucrats in Argentina, Brazil, and Chile. The events of Jan. 6 thus act as a type of exogenous shock, thus allowing for an interrupted time series analysis. I find that satisfaction with democracy generally declined across all three countries but only in Chile did support for democracy and elections fall and populist attitudes rise.",
        "authors": [
            "George Reuben Garcia III"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157200",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Beans to Bytes: Grey-Box Nonlinear System\r\nIdentification Using Hybrid Physics-Neural Network\r\nModels",
        "abstract": "The advancement of neural networks in the last several years has yielded some astonishing results. However, the applicability to system identification and modelling dynamical systems still has a great amount of room for exploration. This thesis reviews different neural network architectures and their application to complex non-linear dynamic system identification. In particular, it uses the intricate process of coffee roasting as a case study to explore and demonstrate these techniques. Coffee roasting is a complex process that requires precise control to achieve the desired coffee quality. The ability to develop models that represent a system, i.e. system identification, is of great value to industry. Coffee roasting poses several challenges for system identification from complex chemical reactions occurring inside the bean, to temperature trajectories being dependent on several states that cannot be explicitly measured, such as moisture content, or reaction rate, making it an ideal candidate for exploring the application and limitations of neural networks. The primary contributions of this study are a proposed \"grey-box\" model that augments previously established physics based models, as well as illustrating the limits of LSTM, Deep NARX models using \"one-step\" forward prediction techniques. Although the study focuses explicitly on coffee roasting, the conclusions drawn are applicable to other similarly complex industrial and manufacturing processes.",
        "authors": [
            "Morgen Pronk"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157179",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "From Shipyard to Sea: A Flexible System Design Approach\r\nto the Transition from Shipbuilding to Operations A Case Study Using the United States Coast Guard Offshore Patrol Cutter Program",
        "abstract": "The United States Coast Guard faces significant challenges transitioning new ships from shipbuilding to operations. Historically the low volume and irregular pace of major ship deliveries, combined with diverse homeporting factors, have resulted in anomalous post-delivery requirements. Today, a growing fleet, personnel shortages, and sweeping technological advancements are amplifying the complexity of post-delivery activities. At the same time, the Coast Guard is engaged in its largest shipbuilding effort since World War II, with seven acquisition programs scheduled to deliver 134 new ships over the next 15 years. In light of these factors the current approach, which places significant strain on crews, escalates costs, and delays operational use of the Coast Guard’s newest assets, warrants thorough examination. This thesis examines the issue through case study analyses using the Offshore Patrol Cutter (OPC) Program. The Coast Guard’s challenges are driven by three primary factors: the inherent uncertainty in ship construction, sociotechnical system dynamics associated with organizational management of pre-commissioning crews, and the ongoing evolution of technology. To address these challenges, this analysis employs an integrated approach, synthesizing principles and techniques from Architecting Innovative Enterprise Strategy (ARIES), Flexible Engineering Design (FED), and System Design and Management (SDM). This systems thinking approach aims to develop opportunities to reduce costs, improve schedules, and optimize workforce outcomes. The analysis recommends a three-phased strategy that could yield cost savings on the order of $400 million over the OPC Program’s lifespan, significantly mitigate risks associated with unforeseen shipbuilding developments, and enhance organizational outcomes regarding workforce, operational availability, and life cycle sustainment. The staffing of pre-commissioning crews is pinpointed as a pivotal discretionary event that triggers an exponential increase in system complexity and a surge in scope by introducing interdependent yet organizationally disparate requirements. Consequently, major personnel activities are decoupled from highly variable ship construction milestones. This paves the way for a paradigm shift from fixed to flexible approaches, replacing fragmented, ad hoc approaches with a flexible system architecture capable of continuous enterprise learning and improvement. Dynamic post-delivery activities are reimagined as a continuous business line, to professionalize the transition of new ships from shipbuilding to operations.",
        "authors": [
            "Jeremy A. Kime"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157239",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Impact of Process Substitution on Manufacturing Costs: A\r\nComparative Analysis of Sheet Metal Forming versus Extruded Steel Cutting",
        "abstract": "Sheet metal manufacturers continuously seek methods to enhance automation and reduce costs. This thesis explores process substitution and design standardization through a parameter-driven cost model and case studies applying Design for Manufacturability & Assembly (DFMA) principles. Specifically, it evaluates substituting conventional sheet metal components with extruded steel profiles and replacing manual press brake operations with automated tube laser cutting. The findings show that tube laser adoption across a broad range of channels can reduce costs by 49% to 79%, with a payback period of under two years, even in scenarios with fluctuating raw material prices. The study proposes strategies for maximizing tube laser utilization through product mix analysis, redesign for compatibility, and designing with tube laser as the primary method. A developed automation tool using clustering aids profile identification, though the study highlights the need for improved data management around C-channel dimensions to enhance process standardization. The investigation confirms that extruded steel can be a cost-effective alternative to large-scale channel products, providing solutions for industry transition through direct replacement, compatibility-focused redesign, or design guidelines optimized for extruded steel.",
        "authors": [
            "Omar Talal"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157164",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploring Optoelectronic Properties of Twisted and\r\nIntercalated Niobium Oxide Dihalides",
        "abstract": "2D materials, or layers of one-atom thick crystalline solids, offer a flexible solution for a variety of applications that require certain characteristics. As a result of modifications in physical and chemical design involving 2D materials such as stacking, twisting and ion intercalation, properties such as electrical conductivity, spin diffusion length, thermal conductivity, and mechanical strength observe more degrees of freedom than in their bulk material counterpart. Currently, small optical systems comprise of passive devices that are rigid in their light pathing design and require modulators to control light post-fabrication for use. These systems are confined by the material used to fabricate the device and their associated effective indices, which are determined pre-fabrication by the ultimate desired optical effect. However, 2D materials can exhibit tunable band structures that yield the optimal optical response, even post-fabrication. This thesis will discuss the properties of mechanically and chemically manipulated niobium oxydichloride (NbOCl₂) and niobium oxydiiodide (NbOI₂) ultrathin structures that have the potential to integrate into flexible optical systems.",
        "authors": [
            "Ashley Luo"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157202",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Natural Language Interface for Prescriptive AI Solutions\r\nin Enterprise",
        "abstract": "Despite advancements in causal inference and prescriptive AI, its adoption in enterprise settings remains hindered primarily due to its complexity and lack of interpretability. This work at the MIT-IBM Watson AI Lab focuses on extending upon the proof-of-concept agent, PrecAIse, by designing a domain-adaptable conversational agent equipped with a suite of causal and prescriptive tools. The objective is to make advanced, novel causal inference and prescriptive tools widely accessible through natural language interactions. The presented Natural Language User Interface (NLUI) enables users with limited expertise in machine learning and data science to harness prescriptive analytics in their decision-making processes without requiring intensive compute. We present an agent capable of function calling, maintaining faithful, interactive, and dynamic conversations, and supporting new domains.",
        "authors": [
            "Piero Orderique"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157220",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "An Energy and Area Estimation Plugin for Accelerator Architecture Simulation",
        "abstract": "Development of domain-specific hardware accelerators has been an important focus for high performance computing research in recent years, enabling significant gains in a variety of practical applications. Of particular interest is accelerator design for applications involving sparse data. Such accelerators inherently tend towards a diverse array of architecture designs, and often rely on custom simulators for evaluation. In addition to raw performance, energy consumption and chip area are both important considerations for evaluating accelerators. Accelergy is a tool that provides a good general framework for fine-grained energy and area estimation. However, output from simulation tools may not be compatible with Accelergy’s expected input format, which is the case for the custom simulator Accelsim. To address this gap, this work presents a streamlined plugin for processing Accelsim simulator output into Accelergy input, for the purpose of generating accurate and explainable energy consumption and area models for accelerator architectures. We demonstrate the plugin’s flexibility by performing energy and area estimates for two state-of-the-art hardware accelerators, ISOSceles and Trapezoid. Overall, this plugin is easy-to-use, self-contained, and supports a wide variety of configurable functionalities, making it an excellent general tool for running Accelergy on Accelsim output.",
        "authors": [
            "Wendy Wu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157205",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Feasibility of Vector Instruction-Set Semantics Using Abstract Monads",
        "abstract": "Formalizations of instruction-set semantics help establish formal proofs of correctness of both hardware designed to implement these instruction sets and the software implemented against this specification. One such prior work1 formalizes a specification of a subset of the RISC-V instruction-set architecture using a general-purpose language, Haskell, using its monad and typeclass support to abstract over effects. Another member of the same family is the RISC-V V extension, which specifies instructions for operating on multiple data elements in a single instruction, which is useful for domains with high levels of data parallelism, such as graphics rendering and machine learning. In this work I examine the question of whether the same prior work can be extended to formalize the semantics of the vector extension. I answer this question with a tentative “yes”, backed by a partial specification in Haskell of a small but nontrivial subset of this vector extension, a translation of the same specification into Coq using hs-to-coq², and work towards demonstrating the utility of this specification.",
        "authors": [
            "Arthur Reiner De Belen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157232",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Enhancing Roll Form Bending Processes through Experimentation and Informed Predictive Analysis: A Strategic Approach to Optimize Tooling",
        "abstract": "Sheet metal roll forming is a continuous bending process where metal strips pass through a series of rolls to achieve a specific cross-sectional profile. This technique is crucial in the automotive industry for producing high-strength, lightweight components with precision, consistency, and cost-effectiveness. This project aims to optimize Novelis’s aluminum roll forming process by employing Computer-Aided Engineering (CAE) tools, including UBECO Profil, AutoCAD, and Finite Element Analysis (FEA) software such as LS-DYNA. Initial simulations of a square tube profile identified key stations and led to performance enhancements through targeted adjustments. Stress and strain analyses demonstrated how operational factors, such as roll settings, influence section shapes and angles, facilitating the fine-tuning of roll forming station parameters. Using a Design of Experiments (DOE) framework, the study pinpointed critical factors to improve simulation accuracy and optimize roll forming settings. The results indicated that optimized stand height settings significantly improved the accuracy of the desired angles. These insights can be integrated within Novelis’ production line to boost production efficiency and roll performance. This research not only supports current operations, but also provides a foundation for future advancements in roll forming technology.",
        "authors": [
            "Sarvagnya Kompella"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157150",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Multi-Agent Reinforcement Learning for Autonomous Robotics",
        "abstract": "Technological advancements in autonomous robotics, including autonomous vehicles, have created new opportunities for innovative solutions to many everyday challenges. The impact of integrating robotic agents into real-world applications may be significantly enhanced by leveraging advancements in multi-agent autonomous systems. However, the coordination required in multi-agent systems demands complex motion planning to deconflict actions and prevent collisions of vehicles moving at increasingly high speeds. This thesis explores the application of multi-agent reinforcement learning (MARL) to autonomous robotics by teaching a central controller to navigate multiple agents across various environments without collisions. The simulated scenarios range from simple, obstacle-free environments to complex environments with obstacles configured to form narrow passageways or represent other complexities in dense urban environments. The findings demonstrate the potential of MARL to achieve high accuracy in navigating these different environments, highlighting the method's flexibility and adaptability across diverse settings and the resulting implications for applying MARL to real-world scenarios.",
        "authors": [
            "Caroline R. Vincent"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157178",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Motion Phantom Development for MRI",
        "abstract": "The development of magnetic resonance imaging (MRI) has enabled health care professionals to non-invasively visualize subjects' soft tissue for medical diagnosis. Since it's conception, artifacts due to patients' movement have shown themselves to be an issue and an assortment of tools and methods have been developed to help mitigate the effect of motion on MRI but such mitigation methods are generally only applicable on a case by case basis depending on the specific type of motion. As such, additional research is required to develop novel methods and a standardized method of testing, validating, and ultimately comparing mitigation strategies.\r\n\r\nThis work provides a design to develop a motion stage as well as build instructions for the Martinos head phantom which moves in four degrees of freedom (linear translation in the plane parallel to the floor, a head shaking \"no\" motion, and a head nodding \"yes\" motion) independently of one another to limited success. Only the translation in direction (into and out of the bore hole, along the z-axis) worked as expected, while the translation perpendicular to it (x-axis) did not. The total range of motion that head phantom was capable of turning in the head shaking/\"no\" motion was approximately 19 degrees, though the torque required is on the higher end (on the order of 0.06 N*m) and the position of the rotational actuator needs some reexamination. The head nodding/\"yes\" mechanism is more promising, allowing for a tilt downwards of 1 degrees and upwards of 2 degrees, but requires actuators capable of exerting 6N of force or more.",
        "authors": [
            "Kerlina Liu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157184",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Evaluating Adaptive Layer Freezing through Hyperparameter Optimization for Enhanced Fine-Tuning Performance of Language Models",
        "abstract": "Language models are initially trained on large datasets, enabling them to extract patterns and establish rich contextual connections. When dealing with data scarcity, transfer learning has become the go-to method to use these models in specialized downstream tasks via fine-tuning. However, fine-tuning on small datasets can lead to overfitting and a lack of generalization. Generalization is crucial when deploying models that perform a sensitive tasks in a real world environment, as it dictates how well it performs on unseen data. Conversely, overfitting is highly likely to occur when training on small datasets. This thesis proposes and evaluates a new method for fine-tuning language models by adaptively choosing specific learning rates for each transformer layer that provide higher performance on in-domain low-volume datasets. Additionally, we explore which layers inside the models usually hold more contextual information from pre-training that might be valuable to keep ‘frozen’ when fine-tuning on small datasets. This analysis provides insights into fine-tuning approaches during initial experiments when data is limited. Our results demonstrate limited performance gains on certain models while achieving more significant gains on others when fine-tuning using our proposed method. Additionally, our work also provides valuable insight into per-layer importance of language models by showing that certain layers have a stronger direct correlation with the overall model accuracy.",
        "authors": [
            "Reinaldo Figueroa"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157169",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design and Development of an Accelerated Material Synthesis\r\nPlatform for Automated Materials Research",
        "abstract": "Materials development is the foundation for innovation in many industries and fields, however, this process is traditionally slow and resource-intensive. Most often, new materials are developed and characterized on the time scale of years which can limit the pace of scientific and industry innovation. I address the material synthesis and characterization bottleneck by presenting a framework that I believe is suitable for smaller labs: Self-built, low-cost automation. The design philosophy is to de-risk the lab automation process by keeping costs low, failing fast, and leveraging common resources in electronic systems and additive manufacturing. I present an improved version of a low-cost but high-throughput inkjet material printer developed by Siemenn et al. and adapted to operation in the glovebox, hood, and benchtop environments. The tool is capable of depositing gradients of droplets with unique compositions at a rate of up to 1000 materials per minute, is self-built, and costs around $500. I also present a computer-vision-enabled high-throughput material characterization algorithm for stability quantification through color degradation. The synthesis and characterization methods are validated on a methylammonium lead iodide (MAPbI3) and formamidinium lead iodide (FAPbI3) perovskite material system. X-ray diffraction (XRD), X-ray photoelectron spectroscopy (XPS), and hyperspectral imaging measurements show equivalence between high-throughput synthesis and more traditional spin-coating methods. Results obtained through the high-throughput stability characterization method are aligned with stability trends reported in the literature and have an accuracy of 96.9% when compared to ground-truth degradation as measured by a domain expert.",
        "authors": [
            "Eunice I. Aissi"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157213",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Calculation of Zakat on Financial Assets for American Muslims: A Financial and Jurisprudential Approach",
        "abstract": "This thesis presents a comprehensive framework for calculating Zakat on modern financial assets specifically tailored for American Muslims. As one of the five pillars of Islam, Zakat is an obligatory form of charity for those who meet specific wealth criteria. However, applying traditional Zakat principles to contemporary financial instruments poses significant challenges, particularly within the context of the U.S. financial system.\r\n\r\nThe research addresses these complexities by developing methodologies that consider diverse financial instruments, valuation challenges, tax implications, accessibility issues, and Shariah compliance. The framework covers a wide range of assets, including cash and bank accounts, stocks, mutual funds, bonds, cryptocurrencies, retirement accounts (401(k)s, Traditional and Roth IRAs), Health Savings Accounts (HSAs), employee stock options, precious metals and jewelry, and real estate investments.\r\n\r\nBridging classical Islamic jurisprudence with modern financial realities, this thesis provides detailed calculation methodologies for each asset class, incorporating U.S.-specific considerations such as tax-deferred accounts and capital gains implications. The framework is designed to be adaptable to evolving financial markets and balances various scholarly opinions on contentious issues. To enhance accessibility, both comprehensive and simplified calculation methods are offered, catering to users with different levels of financial literacy.\r\n\r\nIn conclusion, this thesis makes a significant contribution to Islamic finance by offering a structured, principle-based approach to Zakat calculation that is both Shariah-compliant and applicable in the modern American financial context. It provides a valuable resource for American Muslims striving to fulfill their religious obligations amidst the complexities of the U.S. financial system and lays the groundwork for future research in Islamic finance in Western contexts.",
        "authors": [
            "Naveed Arsalan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157223",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Application of Koopman Operator Theory to Legged Locomotion",
        "abstract": "Nonlinearities from complicated robot systems and harsh contact dynamics have long impeded the effectiveness of optimal control strategies for legged robots. In this work, we present a linearized simple walking model using Koopman Operator Theory, and its usage in Linear Model Predictive Control (L-MPC). Various walking and contact models were evaluated, but ultimately the rimless wheel was selected due to its inherent stability and low dimensionality, and a nonlinear viscoelastic model was used to accurately capture floor contact and impact dynamics. Koopman models were developed using both Radial Basis Functions (RBFs) and neural network-generated observables for the passive rimless wheel. A novel actuation method with linear actuators, combined with the Control Coherent Koopman methodology, resulted in accurate linear models that effectively enabled L-MPC to control the wheel on flat ground. This model outperformed those created using the more traditional Dynamic Mode Decomposition with Control method. This work demonstrates the power of Koopman linearization to produce a unified set of linear dynamical equations that encompass various contact and non-contact configurations and demonstrates the effectiveness of the Control Coherent Koopman methodology in generating an accurate input matrix across these different contact modes.",
        "authors": [
            "Jasmine G. Terrones"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157163",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Redefining Urban Landscapes: A Methodological Approach to Transforming Underused Parking Spaces with Dynamic Urban Functions",
        "abstract": "This study presents an approach to identifying underutilized urban spaces, focusing on parking areas, and explores potential reutilization strategies in Greater Boston. Under the milieu of the information age, global urbanization, and technological development, the prosperity of urban data serves as the new method to approach urban proposals. The city, as a multifaceted artifact, is examined through the lens of advanced data-driven techniques, particularly deep learning. With the computer vision model, the underused surface parking lots will be automatically detected according to historical satellite imageries, highlighting a misalignment between the current infrastructure and the actual urban needs. This study then leverages miscellaneous urban factors to analyze the parking patterns. Associated with the multimodal system, there are possibilities underlying the usage of redundant surface parking. Considering the high rents and housing situation, these spaces could be transformed into housing units or even mixed-used districts, to alleviate the housing crisis in Greater Boston.",
        "authors": [
            "Jie Fan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157139",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Cellulose Nanofoams: 3D Printing and Characterization",
        "abstract": "In recent years, the advancement in cellulosic nanofoams has been considerable. Yet, their customization potential for diverse application requirements has been constrained by reproducibility challenges. Our research, therefore, focused on two primary objectives: enhancing the thermal regulation capabilities and mechanical properties of cellulose nanofibrils (CNF) nanofoams, and developing a reproducible methodology for printing customized three-dimensional (3D) structures using direct-ink-write (DIW) technology and molding.\r\n\r\nWe developed composite nanofoams using TEMPO-modified cellulose nanofiber (TCNF). The resultant composite nanofoams showcased remarkable properties such as ultra-low thermal conductivity, low density, outstanding flexibility, and infrared shielding capabilities.\r\n\r\nIn a bid to create robust and environmentally friendly nanofoams, we employed a crosslinking process with CaCl2. The crosslinked nanofoams were extraordinarily lightweight yet boasted superior mechanical properties, significantly amplified by the crosslinker. Remarkably, these freeze-dried T-CNF/CaCl2 nanofoams maintained their form and demonstrated admirable flexibility, even when subjected to weight exceeding thousands of times their own. Furthermore, transient characterization confirmed their excellent thermal insulation capabilities.\r\n\r\nIn conclusion, our research has pioneered the fabrication of sustainable, high-stability cellulose nanofoams. We have significantly enhanced the thermal management capabilities and mechanical performance of these nanofoams, marking a remarkable advancement in the field. The demonstrated sustainability, biocompatibility, ultra-light weight, high porosity, and deformability of the resultant nanofoams suggest considerable potential for diverse applications, including thermal insulation, shock and vibration damping, as well as tissue engineering.",
        "authors": [
            "Vineet Padia"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157245",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Uncertainty Quantification in Deep Learning Models of\r\nG-Computation for Outcome Prediction under Dynamic\r\nTreatment Regimes",
        "abstract": "G-Net is a neural network framework that implements g-computation, a causal inference method for making counterfactual predictions and estimating treatment effects under dynamic and time-varying treatment regimes. Two G-Net models have been successfully implemented: one that uses recurrent neural networks (RNNs) as its predictors, and one that uses transformer encoders (G-Transformer). However, one limitation of G-Net is that its counterfactual predictive density estimates do not take into account uncertainty about model parameter estimates. These uncertainty estimates are necessary for establishing confidence intervals around the effect estimation, enabling a robust assessment of whether the effects of two treatment options exhibit statistically significant differences. An important area of work is adding support for quantification of model uncertainty for conditional effect estimation. This thesis aims to add uncertainty quantification to both the RNN-based G-Net and the G-Transformer. To achieve this, we use two well-known techniques in uncertainty modeling, namely variational dropout and deep ensembling. We evaluate our methods using two simulated datasets based on mechanistic models. We demonstrate that G-Net and G-Transformer models with uncertainty quantification are better-calibrated and perform better for individual-level clinical decision making than their baseline counterparts.",
        "authors": [
            "Leon Deng"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157222",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Building a Scalable Electrification Infrastructure in\r\nLogistics",
        "abstract": "The transportation sector in the US contributes to about a third of all greenhouse gas emissions, about a quarter of which stems from road freight. A major driver of this environmental footprint remains a heavy reliance on trucking—the least fuel-efficient mode of transportation. A key pathway toward freight decarbonization, therefore, involves shifting from internal combustion engines (ICE) to electric powertrains in truck fleets. This work develops analytics-based solutions to support and assess the electrification of long-haul logistics operations, by applying the methods to PepsiCo’s operations in Texas.",
        "authors": [
            "Muhammad Ashhad Alam"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157190",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Discovering non-equilibrium mechanisms that regulatestructure and function of biomolecular condensates usingphase-field modeling",
        "abstract": "Biomolecular condensates are phase-separated assemblies in living cells that form through cooperative interactions between their constituents such as proteins and RNAs. They are emerging as important organizers of biochemistry in cells and are dysregulated in disease. Understanding the physical principles that shape the form and function of these condensates is a fundamental scientific challenge, which if addressed can provide novel therapeutic avenues to improve human health. The equilibrium principles behind condensate formation are well understood. Several studies have investigated the impact of multivalency, protein sequence, protein-RNA interactions, and the role of DNA in modulating biomolecular interactions that drive phase separation. However, the living cell is inherently out of equilibrium with non-equilibrium reactions that constantly burn ATP and turn over biomolecules. My thesis investigates how the interplay between biomolecular interactions and non-equilibrium reactions that turn over biomolecules affect the structure and function of biomolecular condensates. Phase-field modeling is used for these investigations, as this approach has been historically successful in answering similar questions in other fields such as material science. Prior work shows that proteins present in biomolecular condensates associated with RNA transcription undergo complex coacervation with the RNA product. The first project in this thesis investigates the interplay between complex coacervation and spatially heterogeneous RNA synthesis on condensate morphology and dynamics using a phase-field model. This simple model exhibits a rich variety of dynamical behaviors and steady states. It also provides a unifying framework to explain diverse experimental observations related to condensate morphology and dynamics such as vacuole formation, aspherical shapes, directed motion, and splitting-fusion behaviors. The second project investigates how transcription of messenger RNA (mRNA) by transcriptional condensates is modulated by other RNAs in the vicinity, such as long non-coding RNAs (lncRNAs). Our model reveals that lncRNA transcription in the vicinity can regulate mRNA transcription by altering the protein concentration and lifetime of transcriptional condensates, 3 promoting mRNA transcription from genes expressed at a low level and inhibiting transcription from highly expressed genes. This model provides a unifying framework to reconcile conflicting observations in the literature about transcriptional regulation by lncRNAs. The final project focuses on the fibrillar center of the nucleolus, an important condensate that is involved in ribosome biogenesis. Using a phase-field model that explicitly accounts for rRNA-protein interactions in the nucleolus and the non-equilibrium reaction of rRNA transcription, we show that the coarsening of fibrillar centers is arrested leading to a preferred size. Altering this size affects rRNA export and processing from the fibrillar centers. These predictions are validated by experiments. Using a combination of experiments and theory, we uncover the non-equilibrium mechanism that controls size control of fibrillar centers and the functional consequences of this size control on rRNA processing.",
        "authors": [
            "Pradeep Natarajan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157252",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Autonomy Work: Personhood, Expertise, and Activism of Disabled AI Data Workers in China",
        "abstract": "This dissertation examines the labor and life of disabled workers in China’s artificial intelligence (AI) data annotation programs. The study draws on 14 months of ethnographic fieldwork, conducted over three years, with disabled activists, disabled workers, employment advocates, tech company staff, and government officials. This is supplemented by five years of my professional experience in disability nonprofits. My primary field site was a disabled people-led NGO founded in 2006, which I refer to as ENABLE. In recent years, ENABLE has developed numerous projects with tech companies to hire people with visual and physical impairments as data annotators for AI systems and to design assistive technologies for the community.\r\n\r\nIn ENABLE’s case, what appears to be a familiar story of capitalist exploitation of disabled people turns out to be, instead, a story about the struggles of disabled Chinese people over different ways of being, living, and relating. I use the term “autonomy work” to describe disabled people’s labor to make “autonomous” machines (zidonghua) (Chapter 1), build an “autonomous” life (zizhu shenghuo) through work (Chapters 2 & 3), and design tools for “independent” navigation (duli chuxing) (Chapter 4).\r\n\r\nI argue that disabled activists seek to construct greater autonomy for their community by reconfiguring social relations in and around technology. I call this mechanism “rerouting.” Instead of a complete departure from asymmetrical power relations, my interlocutors “reroute” the pathways between different human and non-human nodes without changing the nodes per se. They do so within the sociotechnical systems they build, the technological institutions they navigate, the kinship structures they seek to remake through tech work, and the physical terrain they navigate with assistive devices, all in pursuit of multiple forms of autonomy. “Rerouting” contributes to the rich scholarship on the intersection of disability and technoscience by highlighting the effects of disabled people’s unorthodox knowledge and practices that bend the world towards disabled bodies and minds. Furthermore, it specifies a key mechanism through which these effects are realized. Disabled people hack lives, build access, and improvise affordances by reorganizing the pathways between objects, bodies, and environments that were originally designed with other intentions.\r\n\r\nWith deep knowledge and lived experience of the social issues they advocate for, disabled activists in China approach technology as a puzzle piece, not a magic bullet. They make technology useful for their lives, work, and activism by returning the technical to the social. Rather than displacing the slow work of social movements with neoliberal techno-solutionism, I show that this community-driven technological engagement is part of a larger effort to sustain that very slow work within a shifting political environment.",
        "authors": [
            "Di Wu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157144",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Sequence-Dependent & -Independent Effects of Intron-Mediated Enhancement (IME)",
        "abstract": "Introns are ubiquitous features of eukaryotic genes, and their precise removal from pre-mRNA transcripts by the spliceosome is an essential step in gene expression. Genomic deletion of an intron from a gene tends to reduce its expression, and addition of an intron tends to increase it. This phenomenon, termed Intron-Mediated Enhancement (IME), has been observed in many organisms, genes, and introns. IME can act at multiple levels to increase transcription rate, processing rate, export efficiency, translational efficiency, and stability of the processed mRNA. These stimulatory effects range across orders of magnitude depending on the context, and also on the identity of the intron, as has been shown in Arabidopsis thaliana. Presently, little is known how intron sequence may determine the mode or magnitude of effect on gene expression output in animals. In this study we report the design and execution of several massively parallel reporter assays (MPRAs), interrogating the effect of tens of thousands of synthetic and natural intron sequences on gene expression in the human HEK293T and HeLa cell lines. We observe that even with random internal sequence, most of these introns splice well and trigger IME. In the primary tested context, the average intron stimulates an eight-fold increase in both mRNA and protein output over intronless controls, suggesting that the enhancement is largely at the level of mRNA accumulation. We analyze the sequence features associated with highly-enhancing introns and demonstrate that the poly-uridine (polyU) content of an intron is positively correlated with its impact on host gene mRNA and protein level. In a second library of natural intron sequences, we observe that U12-type introns do not stimulate IME, while U2-type introns universally do. Surprisingly, we observe in both MPRAs that the enhancement from random introns is similar to or greater than the enhancement from natural sequences. In sum, we have developed a robust experimental platform for interrogating the sequence-activity relationship of IME, and used it to uncover new insights into this unsung sculptor of eukaryotic gene expression.",
        "authors": [
            "Emma J. K. Kowal"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157229",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Regulation of microRNA degradation in Caenorhabditis elegans via the E3 ubiquitin ligase EBAX-1",
        "abstract": "microRNAs (miRNAs) are short, ~22-nucleotide noncoding RNAs that base-pair to messenger RNAs (mRNAs) to direct their post-transcriptional repression through their associated Argonaute (AGO) proteins. Animal genomes encode hundreds of miRNAs that, together, regulate a majority of mRNAs and tune spatiotemporal gene expression programs. The production and degradation of many miRNAs occurs in a regulated manner, but molecular pathways of miRNA degradation are relatively poorly understood.\r\nSome rapidly degraded miRNAs owe their instability to a mechanism termed target-directed miRNA degradation (TDMD), whereby unusual miRNA binding sites with extensive complementarity to the miRNA promote a conformational shift in AGO, leading to the recruitment of an E3 ubiquitin ligase complex containing the substrate receptor ZSWIM8. The subsequent polyubiquitination and proteolysis of AGO liberates the miRNA, rendering it vulnerable to nucleases. TDMD underlies the instability of many miRNAs in diverse cell lines and animals.\r\nIn this work, I probe the biological scope of TDMD as a regulatory mechanism in the nematode Caenorhabiditis elegans, which tolerates homozygous loss of the ZSWIM8 ortholog, EBAX-1, and expresses some miRNAs that are subject to rapid, developmentally regulated decay. I have confidently identified at least 22 miRNAs destabilized by EBAX-1 across the worm life cycle. These included the embryonic miR-35–42 family as well as certain stress-responsive miRNAs that together constitute some of the shortest-lived miRNAs in this organism. In mutants of ebax-1, the accumulated miR-35–42 excessively repressed predicted target mRNAs and underwent 3′ trimming as they aged, though no consistent signature of 3′ trimming or tailing emerged for EBAX-1-sensitive miRNAs.\r\nA recent study reports that the destabilization of miR-35 at the embryo-to-L1 transition does not depend on that miRNA’s 3′ region, unlike canonical mammalian TDMD. To test the generality of this result for other EBAX-1 sensitive miRNAs, I assayed the behavior of seed- or 3′-based miR-43 variants in the presence and absence of EBAX-1. Intriguingly, the miR-43 3′ variants showed substantially reduced propensity to be regulated by EBAX-1. The requirement for 3′ pairing therefore varies between EBAX-1 sensitive miRNAs, raising questions about the molecular features of TDMD trigger RNAs that recruit EBAX-1 when extensive pairing is not crucial.",
        "authors": [
            "Michael William Stubna"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157215",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A synthetic biology platform for malaria parasites based on orthogonal transcriptional control",
        "abstract": "Malaria is responsible for half a million deaths each year in some of the poorest communities around the world. Furthermore, the evolution of drug resistance among malaria parasites threatens to continue this trend. However, our understanding of malaria parasite biology is held back by a lack of tools with which to study the function of their genes. In light of this, we have created systems that control gene expression in the malaria parasite Plasmodium falciparum using bacterial repressor proteins. These are the first tools to reliably control malaria parasite transcription and offer the most robust method of conditional gene expression in Plasmodium parasites to date. We develop automated DNA design software to apply this technology to study essential parasite genes for functional genomics and confirm compound-protein interactions for drug discovery. We hope these tools advance efforts to engineer and control malaria parasites in the future.",
        "authors": [
            "Pablo Cárdenas Ramírez"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157237",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Spatial Computing for Building Performance and Design",
        "abstract": "Accommodating urban population growth while reducing emissions from the built environment poses an unprecedented challenge to the architectural discipline. To enable more sustainable construction, the dissertation proposes a new computational design framework to investigate how building performance from an environmental and user perspective relates to spatial design. The dissertation surveys existing computational methodologies for design automation and identifies new opportunities and value propositions for architectural computing in design guidance, feedback, and optimization. Exploring methods that can be used to generate and optimize structural systems of buildings and interior layouts, a specific focus lies in the design of residential buildings. By applying generative design methods to building analytics, new ways for estimating the embodied carbon of a building and the environmental impact of system-level design choices can be explored.\r\nFirst, the research demonstrates how generative geometric algorithms can be coupled with structural simulations to accurately predict the structural material quantity and, through that, the embodied carbon of a building in early stages of design. Second, a new method for representing, analyzing, and generating spatial layouts – the hypergraph – is proposed, that captures the characteristics of any given floor plan. Unveiling new architectural opportunities through automatic geometry creation, the hypergraph shows potential to improve the quality of residential spaces in terms of environmental performance and access to daylight. Enabling new design tools for architects, it offers creative applications and new collaborative workflows for incorporating new spatial metrics in the design process. Allowing for new quantitative insights in building performance, the research demonstrates that spatial efficiency can outperform envelope upgrades in terms of carbon emission savings.",
        "authors": [
            "Ramon Elias Weber"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157180",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Shifting Paradigms: Data-Centric Approach for Marine Statics Correction using Symmetric Autoencoding",
        "abstract": "Deep learning has demonstrated remarkable performance in a wide variety of domains and is often leveraged for making high-stakes decisions. Parallel to its growing and beneficial presence in other domains, deep learning is gaining a notable reputation for solving challenging problems in geophysics. A key problem - given the escalating energy and geosequestration demands in present times - is marine statics correction. The traditional workflow for correcting marine statics has been based on a model-centric paradigm. This paradigm involves a series of transformations between non-commensurate spaces: first, inversion from seismic data space to velocity model space and second, forward modeling from velocity model space to seismic data space. Statics correction within this paradigm has severe drawbacks, mainly the high compute, time and labor cost, and inaccuracies stemming from errors in velocity model inversion or from unmet assumptions about subsurface structure. Overcoming these drawbacks was thus, the prime motivation for our study - where we chose to leverage deep learning as the core algorithmic tool to understand the limits of the model-centric paradigm and explore the performance horizons of a different, data-centric, paradigm to statics correction. The main feature of the data-centric paradigm is the direct mapping between commensurate data spaces, eliminating the need for intermediary transformations to and from velocity model space. Initial benchmark tests on the model-centric approach revealed the impact of inaccuracies in velocity model inversion as substantial nonzero timeshifts - exceeding 0.01s, and reaching values as large as 0.04s - for most arrivals in seismic data. These arrival time precision levels are unacceptable for good seismic imaging and time-lapse analysis; underscoring the need for an improved approach to marine statics correction. Consequently, we began our investigations into the data-centric paradigm. With the focus of disentangling the effects of varying seawater velocity from coherent subsurface geology in seismic records, we implemented an autoencoder algorithm, named SymAE. Notably, SymAE leverages the permutation symmetry of coherent subsurface information to perform the separation of information from nuisance variations. Once trained, SymAE is able to redatum selected subsurface and water velocity information in its latent space to produce statics-corrected seismic records. Our results show that for training datasets of increasing subsurface complexity, SymAE strongly converges all dynamic timeshifts to zero, aligning perturbed traces to reference traces. Crucially, SymAE delivers the required timeshift precision of 0.01 seconds for all arrivals - an achievement that the model-centric approach falls short of. This notable precision improvement using SymAE highlights how a streamlined data-centric paradigm outperforms the traditional model-centric paradigm of marine statics correction. This finding is pivotal as it is the foundation that lays the groundwork and opens the path towards the real-world deployment of SymAE for statics correction in challenging deepwater environments.",
        "authors": [
            "Brindha Kanniah"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157174",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Investigating the Illusion of Wetness: Cold Dry Stimuli in Sensory Perception",
        "abstract": "Humans lack specialized receptors for perceiving wetness and so it is a compound sensation based on changes in skin temperature and contact pressure that are sensed by thermoreceptors and mechanoreceptors in the skin. In addition to perceiving the wetness of damp fabrics in contact with the skin or the presence of sweat on the skin, humans can perceive wetness in the absence of any moisture, a phenomenon known as illusory wetness. The illusion has been shown to arise when the skin is in contact with a surface and is cooled.   This thesis is focused on understanding the variables that contribute to illusory wetness by first determining the difference threshold for perceiving the rate of skin cooling and relating this to perceived wetness. The results from the first two experiments showed that the difference threshold averaged 0.9 °C/s -1.06 °C/s at a reference value of 0. 5 °C/s. For perceiving wetness, the threshold averaged 1.08 °C/s - 1.41 °C/s. The latter finding indicates that the rate the skin cools exceeds some threshold value before it is perceived as being wet. A third experiment explored the role of temperature and surface material in the perception of illusory wetness. The results showed that temperature was the more critical valuable, with ratings of perceived wetness increasing as the temperature decreased further below the baseline skin temperature. These experiments have demonstrated the effect that rates of cooling have on perceiving illusory wetness and have contributed to a better understanding of the role of surface material and temperature on perceiving wetness during static contact. These findings are relevant to simulating wetness in prosthetic devices and virtual reality environments.",
        "authors": [
            "Ozioma Ozor-Ilo"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157152",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Red Teaming Language Conditioned Robotic Behavior",
        "abstract": "Natural language instruction following capabilities are important for robots to follow tasks specified by human commands. Hence, many language conditioned robots have been trained on a wide variety of datasets with tasks annotated by natural language instructions. However, these datasets are often limited in their size and hence the distribution and nature of the instructions given by real world users might be different from that in the datasets. This makes it unclear how these robots will perform in real world environments. Hence, a large scale evaluation with diverse instructions is needed to benchmark the performance of these robots. However, using humans to collect more annotations is prohibitively expensive. We show that recent large language models provide a scalable and inexpensive way to do such an evaluation. Moreover, there is a large performance drop in robots when evaluated on this larger set of instructions. We also show that we can use different prompts to LLMs to control properties such as diversity of the generated instructions.",
        "authors": [
            "Nishant Abhangi"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157255",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Automatic Bayesian Inference of Reaction Networks via Guiding",
        "abstract": "Jump process models based on chemical reaction networks are ubiquitous, especially in systems biology modeling. However, performing inference on the latent variables and parameters of such models is challenging, particularly when the observations of the system state are noisy and incomplete. This thesis presents CatalystFitting, a system for inferring the latent variables and parameters of stochastic reaction network models given observational data. CatalystFitting provides primitives for performing changes of measure on jump processes. Building on top of these primitives, CatalystFitting further provides a library of strategies for guiding a jump process to match an observation set. These strategies exploit the form of the underlying symbolic reaction network to automatically produce guides optimized to the particular reaction network structure of interest to the modeler, accelerating otherwise costly Bayesian inference procedures. We present inference results on a bistable switch system and a repressilator system.",
        "authors": [
            "Gaurav Arya"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157193",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Cycle Time Reduction for CNC Machining Workcells in High-Mix Low-Volume Manufacturing",
        "abstract": "The demand for the product under investigation exceeds the available manufacturing capacity, with the CNC milling workcell identified as the bottleneck operation. This research, conducted in an active, high-mix, low-volume production environment, focuses on evaluating and implementing improvements to CNC machining parameters to enhance the workcell's capacity. Key areas of investigation include machining speeds and feeds, depth of cut, machine settings, toolpath strategies, stepover percentages, and alternative tooling. The study specifically targeted the initial roughing operation, which uses a feed mill and is the longest milling process. Addressing the challenges of high mix and low volume, the research successfully optimized machining and CNC programming parameters, reducing total machining cycle times by 25% and resulting in a 33% increase in throughput. Additionally, the methodologies and findings from this work have provided a framework for implementing further milling process improvements outside of the roughing operation, demonstrating their applicability to similar production scenarios.",
        "authors": [
            "Brandon Christopher Sun"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157243",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Systems Thinking Approach to Hispanic Engineer’s Involvement in Corporate Diversity Networks",
        "abstract": "Affinity networks, also known as Employee Resource Groups (ERGs), are increasingly essential in today’s corporate world as they play a crucial role in fostering diversity, equity, and inclusion within organizations. These groups provide a platform for employees from underrepresented or marginalized communities to connect, share experiences, and find\r\nsupport. ERGs geared towards Hispanic employees are often advertised as not only a means to connect with others and provide a sense of belonging but are also often promoted as avenues towards successful professional development and growth for underrepresented employees. This research explores the perspectives of a group of experienced engineers from various technical backgrounds and industries to understand if there is a correlation between generational status for Hispanic Americans and their overall perceived benefits from participating in ERGs. The study provides a detailed literature review of relevant existing research on this subject, followed by semi-structured interviews of ten participants, and a thematic analysis approach used to analyze the data into the following five themes: diversity considerations for school and job selections, employee perspective on ERGs, sense of belonging and generational differences, the meaning of inclusiveness, and continued participation. Finally, a research conclusion and a series of recommendations are provided.",
        "authors": [
            "Enoch Chambe"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157208",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Power of Perception in Human-AI Interaction: Investigating Psychological Factors and Cognitive Biases that Shape User Belief and Behavior",
        "abstract": "This thesis investigates the psychological factors that influence belief in AI predictions, comparing them to belief in astrology- and personality-based predictions, and examines the \"personal validation effect\" in the context of AI, particularly with Large Language Models (LLMs). Through two interconnected studies involving 238 participants, the first study explores how cognitive style, paranormal beliefs, AI attitudes, and personality traits impact perceptions of the validity, reliability, usefulness, and personalization of predictions from different sources. The study finds a positive correlation between belief in AI predictions and belief in astrology- and personality-based predictions, highlighting a \"rational superstition\" phenomenon where belief is more influenced by mental heuristics and intuition than by critical evaluation. Interestingly, cognitive style did not significantly affect belief in predictions, while paranormal beliefs, positive AI attitudes, and conscientiousness played significant roles. The second study reveals that positive predictions are perceived as significantly more valid, personalized, reliable, and useful than negative ones, emphasizing the strong influence of prediction valence on user perceptions. This underscores the need for AI systems to manage user expectations and foster balanced trust. The thesis concludes with a proposal for future research on how belief in AI predictions influences actual user behavior, exploring it through the lens of self-fulfilling prophecy. Overall, this thesis enhances understanding of human-AI interaction and provides insights for developing AI systems across various applications.",
        "authors": [
            "Eunhae Lee"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157241",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Geo-UNet: A Geometrically Constrained Neural Framework for Clinical-Grade Lumen Segmentation in Intravascular Ultrasound",
        "abstract": "Precisely estimating lumen boundaries in intravascular ultrasound (IVUS) is needed for sizing interventional stents to treat deep vein thrombosis (DVT). Unfortunately, current segmentation networks like the UNet lack the precision required for clinical adoption in IVUS workflows. This arises due to the difficulty of automatically learning accurate lumen contour from limited training data while accounting for the radial geometry of IVUS imaging. We propose the Geo-UNet framework to address these issues via a design informed by the geometry of the lumen contour segmentation task, building anatomical constraints directly into the architecture. We first convert the input data and segmentation targets from Cartesian to polar coordinates. Starting from a convUNet feature extractor, we propose a two-task setup, one for conventional pixel-wise labeling and the other for single boundary lumen-contour localization. We directly combine the two predictions by passing the predicted lumen contour through a new activation (named CDFeLU) to filter out spurious pixel-wise predictions. Our unified loss function carefully balances area-based, distance-based, and contour-based penalties to provide near clinical-grade generalization in unseen patient data. We also introduce a lightweight, inference-time technique to enhance segmentation smoothness. The efficacy of our framework on a venous IVUS dataset is shown against state-of-the-art models. We will make the code repository for this project available soon after approval from industry collaborators.",
        "authors": [
            "Yiming Chen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157219",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "From Capture to Storage: Understanding the Viability\r\nand Challenges of Carbon Capture and Sequestration\r\nInitiatives",
        "abstract": "This thesis explores the implementation of Carbon Capture and Sequestration (CCS) technologies, focusing on the stages of capture, transportation, and sequestration. Utilizing a system dynamics model, the research evaluates CCS's effectiveness and economic viability across various scenarios, including those outlined by the International Energy Agency (IEA). The baseline model suggests that even under favorable assumptions, CCS permanently sequesters only a small fraction of total global emissions.\r\n\r\nThe economic analysis reveals a slight decrease in total costs, attributed to the learning curve, but offset by increasing costs as more complex projects are undertaken. The model also highlights the energy penalty associated with high energy requirements for capture. Additionally, the alignment of capacities across capture, transportation, and sequestration phases is important because discrepancies can lead to inefficiencies and bottlenecks.\r\n\r\nThis research acknowledges limitations, including the use of aggregated data and assumptions across many parameters. These limitations emphasize the need for further research to refine these estimates and enhance the model's accuracy. Despite these challenges, the model serves as a beneficial tool for testing policy interventions and assessing the potential of CCS as a component of global climate strategy.\r\n\r\nOverall, the findings highlight the complexities and challenges of deploying CCS technologies at scale, emphasizing the importance of coordinated policy, technological innovation, and infrastructure development. This research provides a foundation for future studies and policy discussions to better understand CCS's role in achieving climate goals.\r\n\r\nDisclosure: The following content is the author’s, and responsibility is taken for all content. Noting this, it was generated by the author with the assistance of an AI-based system to\r\naugment the effort.",
        "authors": [
            "Lauren James"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157138",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Quantifying Emissions and Costs of Geologic Hydrogen: An Integrated Lifecycle Emissions and Techno-economic Approach",
        "abstract": "In the pursuit of sustainable energy solutions, this thesis explores the lifecycle emissions and economic feasibility of geologic hydrogen production. This research extends Brandt's 2023 study of 'prospective' lifecycle assessment (LCA), enhancing the underlying open-source LCA model used in this work and adding a preliminary techno-economic analysis (TEA). The findings demonstrate that geologic hydrogen developments should have emissions intensities that compare favourably to all other hydrogen production pathways. The value of lifetime emissions intensity for Brandt’s Baseline case is estimated at 0.40 kgCO2e/kgH2, representing an increase of ~6% over Brandt’s estimation. The study also highlights the potential for geologic hydrogen to achieve competitive levelized costs (estimated at $1.45/kg), making it a promising candidate in the hydrogen economy. It finds that to achieve the best possible emissions and economic results, proponents of geologic hydrogen developments should seek to maximise the productivity of each well. It also studies the impact of the United States regime of production tax credits for hydrogen, finding that the fivefold increase in the magnitude of credits for meeting employment conditions is generally more impactful than lowering emissions intensity. The thesis underscores the importance of continued refinement of LCA and TEA models to understand geologic hydrogen resources better and ensure they are developed appropriately.",
        "authors": [
            "Timothy Blackford"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157240",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Affordable Fiber Extrusion Device for Educational\r\nPurposes: Design Improvements, Controls Development,\r\nand Manufacturing Scale-up",
        "abstract": "The Fiber Extrusion Device (FrED) is an affordable desktop tool intended for engineering education. It mimics the fiber draw process, allowing students to study topics such as data acquisition, control systems, computer vision, data analytics, and smart manufacturing. As an educational tool, the goal of the device is to replicate the practical laboratory experience in remote learning scenarios. FrED has gone through multiple iterations, yet several outstanding issues remain. Building on the 2023 team’s progress, the 2024 project objectives include refining the design, developing controls, scaling up manufacturing, designing the assembly line, managing inventory, creating educational content, and conducting user testing and pilot runs. This thesis specifically details the author’s contributions to enhancing mechanical designs, advancing control systems, increasing production capacity, and planning educational materials. Mechanical components in the frame, the cooling system, and the diameter measurement system were redesigned to improve stiffness and stability. Local PID controllers were implemented for the DC motor and heater, effectively closing the feedback loop for fiber diameter control. The production target of manufacturing 35 FrED units was successfully achieved within the planned timeframe, with the packaging design optimized for efficient shipping. Additionally, an assembly manual, a graphical user interface, and control activities were developed as part of the educational content. Three user testing sessions were conducted to gather feedback.",
        "authors": [
            "Yiqian Zhang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157159",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Improving Technology Adoption Process in Accounting and Finance Using Systems Thinking Methods",
        "abstract": "In the era of digital transformation, Accounting and Finance (A&F) functions face the challenge of making well-informed decisions about which technologies to adopt, which processes to prioritize, and why. These decisions require stakeholders to carefully evaluate available options, assess their implications and tradeoffs, and align diverse preferences to make well-supported investment choices. Conducting this process in a siloed and unstructured manner can lead to inefficiencies.\r\nThis study explores the application of Systems Thinking (ST) and Systems Engineering (SE) methods, developing an integrated framework that combines Rich Picture, Object-Process Diagram (OPD), Design Structure Matrix (DSM), and Multi-attribute Tradespace Exploration (MATE) to enhance the technology adoption decision-making process within A&F functions. The focus is on Internal Audit (IA) as a case study for a simplified model and demonstration. While empirical data collection and hypothesis testing were not conducted due to data and time constraints, qualitative insights were gathered from industry practitioners.\r\nKey findings suggest that the integrated framework can potentially reduce the time and effort needed to reach technology adoption decisions. Providing a structured and comprehensive approach ensures that the decision-making process is more holistic, unbiased, and quantifiable. This can also offer post-implementation benefits, as the technologies adopted align better with the organization’s requirements and preferences, resulting in improved efficiency and effectiveness.\r\nThis study extends the practical application of ST methodologies into A&F. By presenting this integrated framework, it contributes to the foundation for future research on applying ST to improve the technology adoption decision-making in A&F.",
        "authors": [
            "Albert Y. Chun"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157257",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Hofstadter Physics and Composite Fermionic Phase in Moiré Systems",
        "abstract": "This thesis explores the intricate electronic phenomena in Moiré systems, particularly focusing on twisted bilayer transition metal dichalcogenides (TMD). These systems, with their unique superlattice structures and strong electron correlations, provide fertile ground for investigating novel quantum states. A key focus is on understanding Hofstadter physics and the emergence of composite fermion phases in these materials. In this work, we first develop a continuum model to describe the low-energy electronic structure of twisted TMD bilayers, emphasizing the role of the Moiré superlattice in modifying the band structure and introducing non-trivial topological properties. We analyze the resulting Hofstadter spectrum under an external magnetic field, revealing the rich fractal pattern and the impact of valley polarization induced by the magnetic field. Building on this framework, we delve into the concept of composite fermions, particularly in the context of the fractional quantum Hall effect (FQHE). We extend Jain’s composite fermion theory and the Chern-Simons field theory to Moiré TMD systems, proposing the existence of an anomalous composite fermion liquid state at half-filling. Through a detailed mean-field analysis, we demonstrate that this state, characterized by a strong valley polarization and an effective magnetic field arising from Berry curvature, could be energetically favored under certain conditions. Our findings suggest that Moiré TMDs are promising candidates for realizing fractional Chern insulators and other exotic quantum phases, opening up new avenues for experimental exploration and potential applications in quantum technology.",
        "authors": [
            "Shuhan Ding"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157248",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Sensitivity Analysis of Self-Loosening Behavior forMesoscale Bolt Assemblies Under Cyclic Lateral Loading",
        "abstract": "This study aims to enhance the understanding of self-loosening in mesoscale bolt assemblies, specifically those with characteristic dimensions ranging from 100 to 3,000 micrometers. These bolts pose unique design challenges due to the small difference between their nominal dimensions and manufacturing tolerances. This work discusses the design of new instrumentation to test multimesoscale bolt assemblies under various loading conditions, an area previously focused only on larger bolts. A case study was conducted in collaboration with a mesoscale multi-bolt system that was experiencing self-loosening failures. This system was tested to determine its susceptibility to the self-loosening failure mode. An experimental study was conducted to identify the sensitivities of the system to geometric and loading environment parameters. A set of hypotheses were proposed as a way to facilitate new learnings about the system’s sensitivities to four different parameters. The findings from the experimental study provide valuable insights into how different geometric configurations and types of loading conditions contribute to the performance of mesoscale multi-bolted systems. Through these investigative efforts, the study successfully identified the existence of a critical displacement threshold for self-loosening in mesoscale multi-bolted systems that is sensitive to factors such as clamp length, amplitude of input displacement load, and screw position.",
        "authors": [
            "Alejandro Martinez"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157214",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploiting irregular parallelism to accelerate FPGA routing",
        "abstract": "In the era of hardware specialization, field-programmable gate arrays (FPGAs) provide a promising platform for computer architects, combining the programmability of software with the speed and performance of hardware. Despite this, compiling hardware programs onto FPGAs can be incredibly time-consuming, making it hard to develop and iterate on complex FPGA programs. Of particular relevance is the routing phase, which takes a circuit’s technology-mapped netlist and routes its signals using the switches and wires present on a given FPGA architecture, often with a target of minimizing critical path delay. This optimization problem is known to be NP-hard, and existing algorithms for approximating it exhibit very little regular parallelism.\r\nThis thesis accelerates the routing phase of VTR 8.0, a commonly used, open-source research tool for FPGA CAD flow. We show that despite the lack of regular parallelism, routing still exhibits significant irregular parallelism. This parallelism can be exploited on parallel architectures that provide hardware support for ordered tasks and fine-grained speculation, such as the Swarm architecture. Using Swarm, we exploit the parallelism present at the core of VTR’s algorithm, achieving a 35.9x speedup on a single routing iteration of a large benchmark (cholesky_mc) on 256 cores.",
        "authors": [
            "Alan Y. Zhu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157224",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Knowledge and the City: Redefining Islamic Urbanism, 762–1067",
        "abstract": "This study demonstrates that the rapid urbanization of the Islamic world in its first five centuries can be attributed in part to the development of an independent class of city administrators who ensured that urban life thrived even in the most tumultuous of political times. This dissertation subverts existing historical models of urbanism, which were developed for medieval Europe, by excavating a theorization of the city from the political writings of the philosopher al-Farabi (d. 950), who argues that cities require the administrative wisdom of learned men trained in law. To historically corroborate al-Farabi’s theory, which has been cast as utopian, I identify these learned men in the historical record as the ʿulamaʾ. I demonstrate that early Islamic learning was a complex but ordered system—even before its institutionalization—first by articulating its delineations via a praxis of personally conferring and acquiring ʿilm (knowledge). This praxis was, I demonstrate, informed by a widely held view that ʿilm was metaphysically substantiated. The ʿulamaʾ—those marked by ʿilm—inherited their legal authority from the Prophet via the transmission of hadith and thus did not rely entirely on the political vesting of the caliph or amir to carry out Islamic law on the level of the city. I demonstrate that the ʿulamaʾ, with their independent legal authority, served as city administrators via two primary positions—the qadi (judge) and the muḥtasib (officer of public order)—and various other positions delegated by these two offices. Just as the system of early Islamic learning was regularized across the Islamic world, so too was the administration of cities by the ʿulamaʾ. Through city administration, the ʿulamaʾcultivated favorable living conditions in cities. Their relative independence from the state allowed for a continuity in city administration—and thus a continuity in urbanism—that survived the many political upheavals that came to define the Islamic world in the tenth and eleventh centuries.",
        "authors": [
            "Courtney Lesoon"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157142",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Insights on Serology, CRISPR Diagnostics, and Machine Learning Architectures for Biological Sequences",
        "abstract": "Fueled by technological breakthroughs, advancements in our understanding of infectious agents offer unprecedented potential for their early detection, intervention, and ultimately, eradication. This dissertation focuses on combining cutting-edge immunological, diagnostic, and computational approaches to confront infectious diseases more effectively, with a particular emphasis on SARS-CoV-2. The first two chapters delve into the immunological aspects of SARS-CoV-2, exploring the dynamics of antibody responses during primary infection and reinfection. First, we explore the dynamics of antibody responses during primary infection, revealing a “switch-like” relationship between antibody titer and function. Next, we investigate the humoral immune response following reinfection, identifying specific biomarkers that differentiate between primary infection and reinfection, offering potential tools for monitoring disease spread and understanding immunity. The subsequent chapter shifts focus towards technological innovation in diagnostics, presenting a novel bead-based method for CRISPR diagnostics that leverages a split-luciferase reporter system for enhanced sensitivity and a highly deployable bead-based platform for multiplexed pathogen detection. This work represents a significant advancement in rapid, scalable, and portable diagnostic tools. Finally, the dissertation culminates with a leap into computational biology, introducing ’Janus,’ a subquadratic state space model designed to efficiently handle large biological sequences. Janus demonstrates superior performance in genomics and proteomics tasks, outperforming existing models with significantly fewer parameters, thus paving the way for more efficient and accurate modeling of protein behavior and other biological processes. Collectively, these works contribute to the broader field of infectious disease research with new immunological insights paired with advances in technological and computational solutions.",
        "authors": [
            "Sameed Muneeb Siddiqui"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157162",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Layer-by-Layer Nanoparticles for Cytokine Delivery",
        "abstract": "In the past decade, cancer immunotherapy has been a promising therapeutic strategy for cancer treatment. However, immunotherapy has failed to improve responses in certain cancers such as ovarian cancer (OC). The action of cytokines in the tumor microenvironment (TME) is key to regulating immune responses, but dose-limiting toxicities limit the application of cytokines in cancer therapy. One promising approach to improve treatment with cytokines are nanoparticles (NPs) which, when modulated via layer-by-layer (LbL) assembly, can provide many of the desirable characteristics of cytokine-delivery vehicles including tumor cell targeting, subcellular localization, and improved pharmacokinetics. In this thesis, we address some aspects of NPs that have limited their clinical utility including manufacturing, control over self-assembly, and mechanistic understanding of their interactions in biological environments The focus here was on using liposomal LbL-NPs coated with a bilayer of poly-L-arginine (PLR) and poly-L-glutamate (PLE). The coating of NPs with PLR/PLE enables targeting towards cancer cell surfaces which allows for extended extracellular presentation of cargos. This ability is used for targeted delivery of a potent immunostimulant – interleukin-12 (IL-12) to disseminated tumors in metastatic OC. Aspects on the manufacturing of other lipid-based nanocarriers such as discoidal assemblies and immune stimulating complexes (ISCOMs) are also explored. We show that employing a bottom-up approach to produce lipid-based NPs from mixed micelles allows for greater control over NP self-assembly. With this procedure, we generated immune stimulating complexes (ISCOMs) co-loaded with monophosphoryl-lipid-A (MPLA) via a scalable approach for clinical-scale manufacturing of the adjuvant termed Saponin MPLA NanoParticles (SMNP). Moreover, we discover that this approach allows for precise control over liposome size from 50 nm to 1 µm with minimal polydispersity. Lastly, by exploiting the lipid headgroup charge repulsion, we find that multivalent charged lipids yield discoidal lipid nanoparticles through this approach. Unlike previous attempts to generate lipid-based discs, this new class of NPs termed charge-stabilized nanodiscs (CND) do not require disc-stabilizing agents such as proteins or polymers. CNDs are shown to be promising drug delivery vehicles, especially when coated with PLR/PLE via the LbL technique where they have greater tumor accumulation than LbL-coated liposomes. On the use of LbL-NP for cytokine delivery via PLR/PLE coated NPs, we found that covalent conjugation of IL-12 to the liposomal core of LbL-NPs greatly improves targeting and retention of IL-12 in peritoneally-disseminated OC tumors, enabling immunological and therapeutic effects not observed with free cytokine treatment. Mechanistic investigations revealed that these LbL-NPs rapidly accumulated in tumor nodules upon intraperitoneal (i.p.) administration, wherein shedding of the LbL coating allowed for gradual release of IL-12-lipid 3 conjugates via lipid extraction by serum proteins present in interstitial fluid. Upon a single dose of IL-12 conjugated to LbL-NPs using an intraperitoneally disseminated OV2944 highly-metastatic (HM-1) mouse model, we observed a dramatic increase in T cell levels within the ascites and the tumor nodules dispersed within the i.p. space which was not observed with either free cytokine or unlayered IL-12-NPs. When evaluated for its effectiveness in this highly aggressive model, two doses could significantly enhance survival compared to even five times (5x) the amount of free cytokine. Remarkably, while the model was non-responsive to checkpoint inhibitor (CPI) therapy with anti-PD1 and anti-CTLA4, when combined with LbL-IL-12-NPs, we achieved complete responses with robust immune memory induction. The mice were able to rapidly clear rechallenges with fresh cancer cells in the i.p. space. Towards the clinical translation of LbL-IL12-NPs, we demonstrate that LbL assembly is readily performed via microfluidic mixing technology amenable for clinical-scale manufacturing. We also find that we can titrate the polymer amount used to omit time-consuming purification steps. We also find that the LbL film conformation is key to maintaining therapeutic efficacy as thicker films hinder IL-12 delivery. Lastly, we uncover that the binding target of PLE on the surface of cancer cells is SLC1A5, a glutamine amino acid transporter.",
        "authors": [
            "Ivan S. Pires"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157260",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Characterizing Speech Motor Pattern in Minimally\r\nVerbal Adults with Autism Spectrum Disorder via\r\nSurface Electromyography",
        "abstract": "Minimally verbal adults with Autism Spectrum Disorder (mvASD) experience significant speech production challenges linked to impaired motor skills. Despite the prevalence of these speech difficulties, the underlying motor mechanisms remain poorly understood. This thesis investigates the neuromuscular activity associated with speech motor movement in mvASD using surface electromyography (sEMG). By capturing and analyzing sEMG signals with 8 electrodes from key facial muscles during speech production tasks, this study provides insights into the distinct motor patterns exhibited by mvASD individuals compared to neurotypical controls. The sEMG data was collected while 25 participants, including 10 mvASD individuals and 15 neurotypical controls performed a series of carefully designed speech tasks. Features such as Root Mean Square (RMS) values, Pearson correlation coefficients, and eigenvalues from auto and cross correlation matrices were extracted to measure muscle activation and coordination complexity. The results reveal that mvASD individuals exhibit higher RMS values and greater synchronization between sEMG channels, indicating stronger muscle activation and tighter coupling among facial muscles. Furthermore, the analysis of eigenvalues suggests lower complexity in motor coordination among mvASD participants, reflecting fewer degrees of freedom in muscle control. These findings were supported by classification models, which demonstrated that features from diadochokinetic tasks were more effective in distinguishing mvASD from neurotypical individuals.",
        "authors": [
            "Nishat Fahmida Protyasha"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157217",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Compact Capabilities: Developing and Evaluating a Field-Portable Neutron Resonance Capture Analysis System",
        "abstract": "Technological advances in the thorium fuel cycle and other advanced reactor concepts suggest their possible commercialization for nuclear power use in the next ten years. Although the thorium cycle shares many aspects with the uranium and plutonium fuel cycles, it introduces the requirement for the nondestructive assay of multiple isotopes (²³8U, ²³²Th, ²³³U, ²³⁵U, or ²³⁹Pu) in varied concentrations and chemical or physical forms. Current methodologies used for safeguarding the uranium and plutonium fuel cycles are either unsuitable for quantifying many of these isotopes or lack the ability to differentiate between them effectively. This work presents an experimental evaluation of a portable Neutron Resonance Capture Analysis (NRCA) system sensitive to isotopes with neutron capture resonances in the epithermal range (1-100 eV). NRCA is a technique traditionally used for nuclear data collection and nondestructive assay of archaeological materials, typically conducted at large accelerator facilities with beamlines in excess of ten meters. This research miniaturizes the system to a two-meter beamline using a portable deuterium-tritium neutron generator. It builds upon the foundation of a portable Neutron Resonance Transmission Analysis (NRTA) system, utilizing capture gamma rays to generate a signal, in contrast to the neutron transmission measurements of NRTA. The NRCA technique is evaluated in this novel, portable configuration first using nonradioactive samples for optimization and then progressing to depleted uranium and thorium salt samples. Through a research partnership with Pacific Northwest National Laboratory, the technique was tested using highly enriched uranium, 233U and high-assay, low-enrichment uranium (HALEU) samples. Field portability tests demonstrated its ability to operate safely in field conditions, with operator doses remaining well within occupational limits. The system was able to identify multiple mid- and high-Z materials by reconstructing their neutron resonance profiles in experiments as brief as 20 minutes. It successfully differentiated between nuclear fuel cycle isotopes in composite samples as small as 2 grams, with limited success in quantifying the areal densities of uranium and thorium. These results suggest that NRCA, especially when used in concert with NRTA and other neutron-interrogation techniques, has the potential to rapidly and nondestructively quantify and characterize isotopes of interest in support of safeguards material accountancy.",
        "authors": [
            "Jill M. Rahon"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157134",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Deep Learning Multimodal Extraction of Reaction Data",
        "abstract": "Automated extraction of structured information from chemistry literature is vital for maintaining up-to-date databases for use in data-driven chemistry. However, comprehensive extractions require reasoning across multiple modalities and the flexibility to generalize across different styles of articles. Our work on OpenChemIE presents a multimodal system that reasons across text, tables, and figures to parse reaction data. In particular, our system is able to infer structures in substrate scope diagrams as well as align reactions with their metadata defined elsewhere. In addition, we explore the chemistry information extraction potential of Vision Language Models (VLM), which allow powerful large language models to leverage visual understanding. Our findings indicate that VLMs still require additional work in order to meet the performance of our bespoke models.",
        "authors": [
            "Alex Wang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157191",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Systems Engineering for Carbon Capture and Storage",
        "abstract": "Carbon Capture and Storage (CCS) is a crucial technology in the mission to achieve NetZero carbon emissions by midcentury. By capturing and storing CO2 from large industrial sources and power plants, CCS mitigates the impact of existing industrial activities while maintaining energy security and economic stability. The study underscores the necessity of a systematic approach to CCS system design and development to meet stakeholder requirements. It highlights the versatility of CCS in addressing emissions across various sectors, its ability to be retrofitted to existing infrastructure, and its potential for immediate emissions reduction compared to the longer timelines required for integrating renewable energy sources.\r\nThis study analyzes CCS systems holistically, identifying primary components and alternative options for capture, transport, storage, and utilization. It reveals that the transport type significantly impacts system utility, with pipelines being the most effective. The analysis also indicates that CCS systems capturing CO2 from power plants, ammonia, and chemical production facilities and utilizing onshore pipelines and saline aquifers offer high utility and low cost. The Gulf Coast and Permian & Midcontinent regions show better performance due to existing infrastructure and storage capacity. The study emphasizes the benefits of staged CCS development for broader deployment, technology maturation, and cost recovery. Sensitivity analyses suggest that future technology advances could further improve CCS system performance and economic viability.",
        "authors": [
            "Tiantian Zhang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157140",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Using Adaptive Parsing to Integrate Dialogue Scripts in Game\r\nDevelopment",
        "abstract": "For people without programming experience, integrating their work into the main project forms a common bottleneck in video game development. Particularly for dialogue writing, existing approaches for moving the text into the codebase are either highly tedious or excessively heavyweight for faster paced projects. Given that writers often initially produce loosely-formatted scripts, this thesis describes Game-DAP, an adaptive parsing system that accounts for the variation in individual dialogue writing styles. Examinations of pre-existing systems and a survey conducted on developers form a basis for a syntactic model of the information commonly encapsulated by dialogue scripts. This model lends itself to a design for the parsing process used by Game-DAP, which aims to provide as much flexibility to writers as possible with those assumptions as a baseline. User testing results informed the evaluation of the system, focusing on its accuracy, flexibility, and accessibility from the perspective of various authors. Although this analysis revealed several classes of inputs that Game-DAP struggles to process with full correctness, the more successful cases and instances of positive feedback suggest that a refined approach to this kind of domain-specific parsing could provide great value in the creative writing process of game dialogue.",
        "authors": [
            "Temi Taylor"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157203",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Mapping the Cellular Landscape of the Brain: A Scalable Approach to Comprehensive Microscopy Data Analysis",
        "abstract": "Recent advances in intact tissue processing and imaging have enabled the generation of whole brain microscopy data at subcellular resolution, revealing intricate morphological details of cells at unprecedented scales. Given that cellular morphology is strongly linked to distinct functional states of cells, in-depth morphological analysis of such data offers immense potential for understanding their roles in brain development and disease. However, the lack of scalable computational techniques poses a substantial challenge in achieving comprehensive morphological characterization. To efficiently and accurately analyze cellular morphology, we need to process terabyte-scale three-dimensional (3D) data, which inevitably complicates downstream analysis workflows with existing methods.\r\n\r\nTo address the challenge, we developed an end-to-end scalable framework that seamlessly strings each step of the analysis pipeline together, enabling comprehensive fluorescence microscopy data analysis. The framework, termed MorPheT (Morphology Phenotyping Tool), serves as an all-in-one solution, offering a suite of analysis modules spanning from image pre-processing to precise cell detection, atlas alignment, morphological phenotyping, and interactive visualizations. MorPheT employs an ensemble method using both supervised and unsupervised approaches to maximize feature learning for unbiased morphological characterization. A novel deep neural network (ALNet) was designed to capture the long-range contextual dependencies inherent in 3D training data during supervised learning. Unsupervised learning leverages complementary features from the supervised approach, demonstrating the powerful synergy of this ensemble method.\r\n\r\nWe applied MorPheT to two main projects. First, we profiled brain-resident macrophages (BRMs) and created the first fetal mouse brain atlases across multiple developmental stages, revealing distinct regional growth patterns of BRMs throughout development. We also demonstrated MorPheT’s effectiveness in characterizing microglia distribution patterns and morphological properties brain-wide in both control and neurodegeneration mouse brains. In the second project, we investigated cFos+ cells in a memory engram study, showcasing MorPheT’s utility for brain-wide analysis of engram cells. By examining regions hypothesized to hold memory engrams for contextual fear conditioning memory, we identified brain regions where engrams for a specific memory are distributed. Taken together, MorPheT is a powerful tool for cell profiling and mapping across the brain, and we anticipate it will help democratize computational analysis for large-scale microscopy datasets, making advanced analytical approaches more accessible to the broader scientific community.",
        "authors": [
            "Minyoung E. Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157135",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Uncertainty-Inclusive Contrastive Learning for Leveraging\r\nSynthetic Images",
        "abstract": "Recent advancements in text-to-image generation models have sparked a growing interest in using synthesized training data to improve few-shot learning performance. Prevailing approaches treat all generated data as uniformly important, neglecting the fact that the quality of generated images varies across different domains, datasets, and methods of generation. Using poor-quality images can hurt learning performance. In this work, we present Uncertaininclusive Contrastive Learning (UniCon), a novel contrastive loss function that incorporates uncertainty weights for synthetic images during training. Extending the framework of supervised contrastive learning, we add a learned hyperparameter that weights the synthetic input images per class, adjusting the influence of synthetic images during the training process. We evaluate the effectiveness of UniCon-learned representations against traditional supervised contrastive learning, both with and without synthetic images. Across three different finegrained classification datasets, we find that the learned representation space generated by the UniCon loss function leads to significantly improved downstream classification performance in comparison to supervised contrastive learning baselines.",
        "authors": [
            "Fiona X. Cai"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157230",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Cognitive Underpinnings of Legal Complexity",
        "abstract": "Across modern civilization, societal norms and rules are codified and communicated largely in the form of written laws. Although principles of communicative efficiency and legal doctrine dictate that laws be comprehensible to the common world, legal documents have long been attested to be incomprehensible to those who are required to comply with them (i.e. everyone). Why? This thesis investigates this question using the tools of cognitive science. Chapter II approaches the question from the comprehender side, documenting the cognitive and linguistic factors that make legal documents difficult to understand for non-lawyers. Corpus analyses reveal that legal contracts are laden with psycholinguistically complex structures at a strikingly higher rate than nine baseline genres of English. Experimental evidence further reveals that some of these structures, such as center-embedded syntax, inhibit recall and comprehension of legal content more than others, suggesting that difficulties in understanding legal content result largely from working-memory limitations imposed by long-distance syntactic dependencies as opposed to a mere lack of specialized legal knowledge. Chapter III extends these results to other legal genres and investigates the cognitive and linguistic profile of law over time. Analyzing every law passed by congress between 1951 and 2022 with matched texts from four different genres, we find that laws have and continue to be disproportionately laden with psycholinguistically complex structures relative to baseline genres of English, suggesting that top-down efforts to simplify legal texts over this period have largely failed. 3 Chapters IV and V turn to the producer side, investigating why legal actors write in a complex manner in the first place. We find that lawyers likewise struggle to recall and comprehend legal content drafted in a complex register and prefer simplified legal documents to complex documents across virtually every dimension. We further find that people tasked with writing official laws write in a more convoluted manner than when tasked with writing unofficial legal texts of equivalent conceptual complexity, whereas people editing a legal document do not write in a more convoluted manner than when writing from scratch. From a cognitive perspective, these results suggest law to be a rare exception to the general tendency in human language towards communicative efficiency. In particular, these results indicate law’s complexity to be derived from its performativity, whereby low-frequency structures may be inserted to signal law’s authoritative, world-state-altering nature, at the cost of increased processing demands on readers. From a legal perspective, these findings call into question the coherence and legitimacy of legal theories and principles whose validity rests on the notion of law being comprehensible to laypeople, such as ordinary meaning, fair notice, and modern variants of textualism. From a policy perspective, this work informs long-standing efforts to simplify legal documents for the public at-large, which, despite bipartisan support, have remained largely intractable. Finally, from a field-building perspective, this thesis lays the foundation for a broader interdisciplinary research program that uses insights from cognitive science to inform long-standing and cutting-edge questions of legal doctrine and policy.",
        "authors": [
            "Eric Martínez"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157156",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Molecular, Genetic, and Process Approaches for Improving Secreted Pharmaceutical Protein Quality in Komagataella phaffii",
        "abstract": "Biopharmaceutical products constitute a significant portion of the global bioeconomy. Compared to traditional synthetic small-molecule drugs, recombinant therapeutic proteins offer advantages like enhanced specificity and reduced side effects, and there has been tremendous growth in their innovation thanks to modern DNA technologies and AI-driven algorithms. While mammalian platforms such as Chinese Hamster Ovary (CHO) cells are commonly used for their high production titer and capability for complex post-translational modifications, thier high cost of goods manufactured can greatly constrain biopharmaceutical global accessibility. The yeast Komagataella phaffii is the prime candidate for next-generation biomanufacturing for reasons including simpler host biology, reduced time to market, and better sustainability. Nevertheless, product quality, such as size/charge variants and non-human glycosylation, can be of major concern for proteins secreted from this host organism. This thesis explores three different engineering approaches aimed at improving the quality of both aglycosylated and glycosylated proteins, with a particular focus on monoclonal antibodies, the leading class of protein biopharmaceuticals by both sales and innovation. Firstly, we demonstrated significant quality improvements through molecular sequence engineering of aglycosylated monoclonal antibody backbones. By making informed, conservative mutations to two or three amino acid residues, we greatly reduced product-related variants from proteolysis and N-terminal variations. We further showed the comparability between yeast- and CHO-secreted products, providing a framework for rapid product development with this unconventional yeast. Secondly, we applied CRISPR-Cas9 gene editing technology to humanize the glycosylation pathway of K. phaffii. We achieved homogeneous G0 glycosylation on a reporter peptide by resolving a previously unreported synthetic lethality via a transcriptomics-informed approach. Key challenges for monoclonal antibody glycosylation were also identified through further comprehensive pathway engineering. Lastly, we examined the performance of glycoengineered K. phaffii strains under varied process conditions. Employing a machine learning algorithm, we improved the desired glycan abundance on a subunit vaccine candidate. The process-robustness of engineered strains suggests the potential of this host as a viable commercial biomanufacturing host.",
        "authors": [
            "Yuchen Yang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157251",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Regulation of the hif-1-dependent hypoxic stress response by C. elegans",
        "abstract": "All aerobic organisms need a way to sense oxygen levels and respond accordingly when in an unfavorable environment. In almost all metazoans, oxygen is both sensed and regulated by the HIF-1 (hypoxia inducible factor) transcription factor that is activated in periods of hypoxia and goes on to regulate hundreds of genes allowing for appropriate adaptations to hypoxia. HIF-1 activation results in changes at the cellular, tissue and whole organism levels such as increases in glycolysis, vascularization and erythropoiesis; HIF-1 is a critical factor in human development as well as progression of numerous diseases including ischemic stroke, COPD and cancer. HIF-1 is negatively regulated by the O2-dependent prolyl hydroxylase EGL-9 (known as EGLN, PHD, or HIF-PH in mammals). In normoxic conditions, EGL-9 uses ambient O2 to hydroxylate HIF-1. Hydroxylated HIF-1 is recognized by the von Hippel-Lindau (VHL-1) tumor suppressor protein, a component of an E3-ubiquitin ligase complex that targets HIF-1 for proteasomal degradation. In hypoxic conditions, EGL-9 is unable to hydroxylate HIF-1; stabilized HIF-1 enters the nucleus to regulate the expression of target genes that coordinate the hypoxia response. Increased activity of HIF-1, produced by either hypoxia or an egl-9(lf) mutation, induces the hypoxic stress response, which coordinates numerous adaptive changes in C. elegans, including retention of eggs in the uterus, decreases in locomotion and defecation rates, and increased resistance to not only hypoxia but also other stresses including oxidative stress and ER stress. By identifying suppressors of the egl-9(lf) mutant phenotype of egg retention, we have identified two independent pathways that regulate aspects of the hypoxic response in C. elegans. First, we discovered that loss of the conserved nonsense-mediated decay (NMD) pathway, an RNA surveillance mechanism that degrades aberrant mRNA transcripts with premature termination codons, suppressed the egl-9(lf)-induced changes in egg laying and defecation and caused increased hypoxia sensitivity. Other aspects of the egl-9(lf) phenotype, such as resistance to oxidative stress and changes in locomotion, were not affected by NMD-pathway mutations, indicating that NMD modulates specific aspects of the hypoxia response. Secondly, we found that loss of the neprilysin metallopeptidase, nep-2, suppressed the egl-9 Egl phenotype through the degradation of multiple neuropeptides including the known NEP-2 target SNET-1. Our findings reveal two different pathways that function downstream of egl-9 to regulate aspects of the hypoxic stress response, both providing a new pathway with which to study the neuromuscular control of egg laying using NEP-2, and critically showing the integration of the evolutionarily conserved hypoxic-stress response and nonsense-mediated decay pathways.",
        "authors": [
            "Calista Sorine Diehl"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157246",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Neutronic-Thermal Simulation of Micro Reactor Designs for the Purpose of Analyzing the Impact of Thermal Expansion and Hydrogen Migration in Metal Hydride Moderator",
        "abstract": "The recent increased interest in microreactor designs has presented the opportunity to take advantage of the smaller core dimensions to perform steady state neutronic-thermal coupled simulation with the inclusion of an additional physics system. This work accomplishes this, by adding thermal expansion and zirconium hydride-based hydrogen diffusion to the neutronic-thermal simulation of multiple heat pipe microreactor designs. Microreactors’ smaller cores are inherently characterized by more leakage than gigawatt-scale reactor cores. The inclusion of thermal expansion’s representation in the coupling system may reveal neutronic or thermal impacts of geometric expansion that have yet to be noted for these smaller scale geometries. This is the impetus for the work on thermal expansion. The work on hydrogen diffusion is inspired by the common use of zirconium hydride in microreactor designs as a moderator. This material provides high density of hydrogen with high melting point, but features a well documented increase in mobility of hydrogen within the zirconium lattice at high temperatures. Coupling this migration of hydrogen within the neutronic-thermal simulation is performed in order to identify and analyze neutronic and thermal impacts due to the movement of hydrogen within the moderator. Additionally, a heat pipe failure case is simulated for each microreactor geometry studied, aimed to analyze the impacts of multipipe failure on both thermal expansion and hydrogen diffusion, as well as their downstream neutronic-thermal effects.",
        "authors": [
            "W. Reed Kendrick"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157171",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Development of a Computational Tool for Simplifying Engineering Tradeoff Analysis for the Design of Cost-Optimized, Time-Variant, Electrodialysis Reversal Desalination Systems",
        "abstract": "This study presents an analytical tool for characterizing a wide swath of the designspace for time-variant electrodialysis reversal brackish water desalination (TEDR) while avoiding the computation time oft required by mechanistic models of electrodialysis reversal (EDR) and time-variant processes. In place of explicit computation, this paper proposes a simplifying assumptions to simulate desalination power and production rate of a TEDR process without explicit computation, enabling rapid year-long simulation and system optimization. The output of the model is compared to experimental data from a pilot TEDR system and found to have good agreement between desalination power and production rate. Disagreement between the modeled and experimental pressure losses suggesting additional losses in the experiment which may be accounted for in future work. Two case studies, one case for potable water in the American Southwest and another case for irrigation water in the Middle-East and North Africa (MENA) region, compare the results from 54 optimized systems. The results illustrate the complexity of system design and selection, elucidating tradeoffs between different models of electrodialysis (EDR) stacks, operating modes, and system configurations. The output of this model will enable system designers to confidently design and implement cost-effective TEDR systems to combat rising global freshwater scarcity.",
        "authors": [
            "Jeffrey Costello"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157166",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "From the body to the brain: Studying drug delivery and physiological interactions using MRI",
        "abstract": "The brain is in continuous communication with the rest of the body. Nerves connect the peripheral and central nervous system, and complex vasculature networks selectively permit passage of small molecules with an exogenous origin into the brain parenchyma. Although brain-body interactions underpin a host of cognitive and physiological phenomena, they are often overlooked in studies of brain biology and mental function. We studied aspects of the interaction between brain and body using functional and molecular magnetic resonance imaging (MRI), in combination with other tools. In a first project, we examined properties of the blood-brain barrier (BBB). The BBB is a highly selective collection of endothelial cells and tight junction proteins that restrict passage of extracerebral substances from the blood vessels into the brain tissue. We disrupted and bypassed the BBB to deliver an MRI contrast agent and quantitatively assessed the resulting contrast dynamics. We discovered that individual brain regions display method-independent susceptibility to BBB disruption and washout, suggesting principles for calibrating drug delivery and understanding the propensity for chemical exchange across the BBB. We then used one of the widefield brain delivery techniques to apply a novel contrast agent for the study of the cholinergic system, a neurochemical pathway important for motor control mechanisms in both the central and peripheral nervous systems. Kinetic modeling of probe distributions revealed intrinsic localization of cholinergic enzymes. Finally, we applied related neuroimaging tools to an animal model of substance abuse, a pathology for which brain-body interactions are particularly engaged but underappreciated. We designed a study to investigate the role of the insula, a cortical mediator of peripheral physiological signals, in responses to opioid exposure. With molecular imaging approaches, we show the insula shapes drug-dependent brain phenotypes and physiological responses during substance exposure and withdrawal. In all, this work serves as a demonstration of the power of quantitative neuroimaging methods for multifaceted investigation of brain and body relationships.",
        "authors": [
            "Miranda Dawson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157253",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Technoeconomic Analysis of Geothermal District Heating\r\nin the Boston, MA area.",
        "abstract": "This study conducts a comprehensive technoeconomic analysis of geothermal district heating (GDH) in the Boston, MA area, with a specific focus on the MIT campus. The research begins by reviewing the evolution of district energy systems, highlighting various use cases, technologies, and policy developments. It then defines the system problem and establishes a framework for implementing a geothermal district heating system at MIT. The analysis examines the economic viability and decarbonization potential of the GDH system, identifying various system architectures and phased campus sector implementation scenarios. These scenarios are compared to a 'business as usual' reference case. The study reveals that the recommended implementation scenario, MG-E-N-W, not only offers the lowest cost but also achieves the lowest emissions. Over a 30-year period, this scenario presents a net present value (NPV) savings of more than $700 million and 2 million MTCO2e compared to the reference case, making it the most economically and environmentally favorable option for MIT's campus energy system transformation.",
        "authors": [
            "Joseph Estep"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157136",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Practical Exocompilation for Performance Engineers in\r\nUser-Schedulable Languages",
        "abstract": "High performance computing libraries provide efficient implementations of common computational kernels. Traditionally, such libraries are written in C or assembly. User-schedulable languages provide performance engineers a productive way to optimize these kernels with welldesigned interfaces which provide users control over performance-relevant decisions and automate unnecessary concerns. Often, this is a tradeoff: too much control with too little automation is tedious to program, and too much automation with too little control will hinder obtaining peak performance. The principle of exocompilation advocates for one end of the extreme: to give performance engineers maximal control over code execution so they can maximize performance, its current implementation in existing systems is impractical to use. This thesis broadly explores ways to make exocompilation a practical solution for performance engineers. We show that providing more control does not necessitate sacrificing automation, as long as the language is designed so that users can build their own automation. We explore the necessary design features to enable such a system, demonstrate the types of automation users can build in the system, and brainstorm ways to further push the amount of control user-schedule languages expose to the user.",
        "authors": [
            "Kevin Qian"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157187",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Technology Performance Curves to Inform Government and Private Investment",
        "abstract": "Forecasts of technological progress are used to inform decisions in the public and private sectors that shape the modern technology landscape on a global scale. Technology performance curves are the quantitative, model-based representations of technological change employed in industrial, economic, and integrated assessment models to inform decision-making processes. Technology performance curves have evolved from their origins in the 1920s modeling of airframe manufacturing labor cost to consider mechanisms of technological progress, including learning-by-doing, learning-by-searching, economies of scale, and exogenous improvement. Examining changes to the performance and prevalence of technologies can provide insight that is relevant for product strategy and market forecasts. This knowledge can also help estimate the potential impact of government market policy and funding for research and development. This thesis seeks to consolidate the available literature on the various models of technology performance curves into a conceptual framework that can be used to understand the features and limitations of models, and their potential use cases.",
        "authors": [
            "Matthew R. Roberts"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157158",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Analyzing Remote Sensing-Derived Normal Difference Vegetation Index to Predict Coastal Protection by Spartina alterniflora",
        "abstract": "Coastal vegetation can provide protection to the coastline through its root structures, which reduce soil erosion, and its stem structures, which dissipate wave energy. The drag a plant induces could be used to quantify the amount of coastal protection that is provided. This study combined field measurements and drone surveys to develop methods for quantifying vegetation drag, focusing on Spartina alterniflora (S. alterniflora), a smooth cordgrass native to the study site: Waquoit Bay National Estuarine Research Reserve. The drag of a single plant is proportional to frontal area. The drag per bed area is proportional to the drag of a single plant and the number of stems per bed area. This study collected plant samples over the growing season to generate allometric relationships between tiller height and individual plant biomass and frontal area, which provides a way to translate remotely-sensed measures of biomass into stem count and frontal area per bed area. The frontal area was measured through digital imaging of individual plants. The elastic modulus of the stem was also measured using an Instron testing machine. For sixteen 1m x 1m test plots, Normalized Difference Vegetation Index (NDVI) extracted from drone multispectral imagery was compared to measured stem count and estimated biomass. The study compared two different years and three time points within a growing season [August 2022; June, August, October 2023]. In addition, at three plots the stem count was manually altered by cutting out 50% and 100% of the plants. This study found that while NDVI can be used to determine the abundance of S. alterniflora, there are several limitations that cause the correlations to be case-specific. Limitations to NDVI-S. alterniflora correlations included: (1) saturation, (2) species inhomogeneity of the area tested, (3) shoot density inhomogeneity of the area tested, and (4) environmental conditions.",
        "authors": [
            "Samantha C. Garber"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157228",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On-Stack Replacement Across User-Kernel Boundaries",
        "abstract": "In large, distributed computations with small amounts of work done at each node, networking latencies quickly add up, especially in comparison to the time taken to execute small tasks. As such, lowering network latencies is crucial to getting good performance. Previous research has shown that often the largest contributors to network latencies are data copies between kernel and application buffers. Conventional wisdom argues that to solve this problem, one should move the networking stack out of the kernel and into the user space or networking hardware. Instead, we build upon an alternative approach, known as LakePlacid. LakePlacid mitigates the kernel-user boundary overhead issue by moving the most important application logic out of the user space and into the kernel. This thesis proposes and implements a key improvement to LakePlacid. Because only part of the application logic is migrated to the kernel, some packets necessarily must be resolved in the standard user space application. The system discussed in this thesis allows packets which cannot be handled in the kernel to seamlessly continue in user space via on-stack replacement, thus preventing side effects from being executed erroneously. This system for on-stack replacement is very general, allowing execution to switch between code versions at any conditional, and it is novel in its ability to switch stacks across the user-kernel boundary. With this change, LakePlacid is able to better maintain the semantics of user applications, making it more feasible in practice.",
        "authors": [
            "Katherine Mohr"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157170",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Verifying Correctness of the Number Theoretic Transform and Fast Number Theoretic Transform in F⋆",
        "abstract": "As engineers continue to develop more sophisticated algorithms to optimize cryptographic algorithms, their often simple mathematical specifications become convoluted in the algorithms, from which a class of correctness bugs arise. Because cryptographic algorithms often secure sensitive information, their correctness, and in turn their security is a top priority. The Number Theoretic Transform (NTT) is an algorithm that enables efficient polynomial multiplication and has recently gained importance in post-quantum cryptography. This thesis presents a proof of correctness of the NTT in F⋆ , a proof-oriented programming language that extracts to OCaml, and shows that we can use the NTT to perform polynomial multiplications. We provide an implementation of the Cooley-Tukey fast NTT algorithm and a proof that it matches the original NTT specification. This thesis also presents a representation of polynomials in the F⋆ subset Low*, which extracts to performant C code.",
        "authors": [
            "Rick R. Ono"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157189",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Labeling Schemes for Improving Cilksan Performance",
        "abstract": "While race detection algorithms like SP-bags have provably good theoretical properties, large overheads exist in practice, which urges the need for performance optimization. In this thesis, I propose labeling schemes as a method of circumventing many of the expensive operations in Cilksan, an implementation of the SP-bags algorithm. The proposed labeling schemes give strands of a parallel program labels during the execution of Cilksan, allowing Cilksan to shortcut the processing of certain memory accesses if the label comparison allows. I describe and prove correctness for two labeling schemes, the procedure labeling scheme and the prefix labeling scheme, implement both in Cilksan, and measure their performance. While the results show that the overhead of maintaining labels is too high in my implementation, the labeling schemes manage to circumvent many of the memory access operations, suggesting the merit of a more performant implementation of the same schemes.",
        "authors": [
            "Satya Holla"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157231",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Modeling Sandbar Effects on Nearshore Waves and Morphological Change using SWAN",
        "abstract": "Numerical model simulations (Delft3D SWAN) are used to examine the impact of small alongshore variations in the bathymetry of an outer sandbar (in about 5-m water depth) on the nearshore wave field as the shallow (< 3 m) bathymetry changes from near alongshore uniform to strongly spatially variable to understand wave driven morphologic evolution. Waves were observed at Duck, NC with an array of 14 pressure gages between 1- and 3-m water depth spread over 250 meters alongshore. Bathymetry was measured between the dune toe and about 8-m water depth on September 26 and October 2, 2013. The bathymetry evolved from roughly alongshore uniform on September 26 to strongly alongshore variable on October 2. Between these dates incident significant wave heights ranged from 0.5 meters to 2.3 meters, with incident angles from 20 degrees north to 5 degrees south of shore normal. Simulations were run with observed bathymetry for both the outer bar and inner shallow bathymetry, with smoothed outer bar and observed shallow bathymetry, and with digital elevation model bathymetry to determine the effects of outer bar and shallow bathymetry on wave evolution.",
        "authors": [
            "Charles E. Murman"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157153",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Automatic Generation of Chemical Kinetic Models including Macromolecules in Multiphase Systems",
        "abstract": "Detailed chemical kinetic models are indispensable tools for unraveling the complexities of industrial and environmental chemistry systems. Many important industrial and environmental chemistries involve thousands of species and hundreds of thousands of complex pathways, which are difficult to resolve manually. To address this challenge, automatic mechanism generation software has been developed. Previous studies have demonstrated the promising quantitative agreements of automatically generated mechanisms with experimental data. However, previous studies have primarily focused on small molecules in single-phase systems, overlooking the complexities of multiphase systems and macromolecules commonly found in industrial and environmental processes. This thesis introduces advancements in three key areas of automatic mechanism generation: Part I extends the current framework of automatic mechanism generation to tackle the longstanding issue of polymer fouling in the industrial system. Two detailed kinetic models are presented: one for anaerobic fouling and the other for aerobic fouling in distillation columns. Modeling innovations are introduced, which allow one to construct models including thousands of chemical reactions occurring in the liquid and film phases, vapor-liquid equilibria of hundreds of molecules, transport between the phases, and flows between the trays. All of these factors significantly affect the fouling rate. Most of the critical model parameters are derived from quantum chemistry calculations. The modeling method is validated using experimental film growth measurements made with a quartz-crystal microbalance. These models clarify the mechanistic details of the fouling process. Part II develops machine learning models for predicting thermochemical parameters in gas and liquid phases. A decision tree model based on subgraph isomorphism for gas-phase radical thermochemistry is presented. The model demonstrates improved accuracy compared to the existing empirical model and reliable uncertainty estimates for both interpolation and extrapolation tasks. Additionally, the effectiveness of active learning for building models for solvation-free energy is explored under various compositions of initial training sets and uncertainty estimation methods for data acquisition. The possibility of aiding data acquisition with unsupervised learning for active learning is also assessed. Part III adds new features and enhances the performance of multiple packages under the Reaction Mechanism Generator software suite, originally developed by the MIT Green Group. New tools are developed to facilitate thermochemical data augmentation, multiphase 3 simulation for automatic mechanism generation, and the automatic implementation of quasisteady state assumptions during the simulation of detailed kinetic models. A new species and reaction selection algorithm is developed to enable the automatic generation of mechanisms for molecular growth systems. Various speed improvement techniques are applied to improve both the simulation speed and sensitivity analysis of large-scale detailed kinetic models. By addressing these key areas, this thesis contributes to the advancement of automatic mechanism generation, paving the way for more accurate and efficient modeling of complex chemical systems.",
        "authors": [
            "Hao-Wei Pang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157258",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Impact of Government Policies in Middle Eastern\r\nCountries on Digital Platform Startups",
        "abstract": "In the last decade, the financial sector has changed significantly. The introduction of new technologies and mobile applications transformed the entire industry, leading to the rise of financial technology (fintech startups). Fintech startups offer a wide range of products/services, such as digital payments, Buy Now, Pay Later (BNPL), crowdfunding, peer-to-peer lending, etc. Middle East and North African (MENA) countries have seen significant growth in the number of fintech startups and the total investment value in these companies. For example, in Egypt, Fawry is the biggest payment service provider; it covers nearly 25% of Egyptian customers and has more than 3 million daily operations. Also, some fintech companies in MENA became unicorns, such as Tabby of Saudi Arabia and MNT-Halan of Egypt. The increased penetration of fintech in MENA countries has consistently raised concerns about data security, consumer protection, and financial stability that these companies can cause. This always raised a couple of questions for the financial sector authorities or regulators: how these authorities can increase the number of these companies to support financial inclusion and growth of financial sectors and, at the same time, alleviate the dangers and concerns that these fintech companies present. This thesis provides a comprehensive analysis of the growth of fintech startups in the MENA region, focusing on four countries: Egypt, Saudi Arabia, UAE, and Jordan. Then, the study investigates the fintech regulations in these countries. This study aims to understand how recent regulations have impacted the growth of fintech startups through qualitative insights and case studies from four countries. The study reveals the following: First, Jordan's fintech regulations are still in their early stages. Despite having some fintech regulations, significant regulations such as data protection and cyber security laws still need to be made available. The absence of some fintech regulations might cause investors and entrepreneurs not to launch or expand their fintech businesses in Jordan. Second, in Egypt, the fintech regulations align with investors' and entrepreneurs' expectations; however, the economic conditions-budget deficit and currency fluctuations might hinder the growth of the fintech sector in Egypt. Third, for Saudi and UAE, the fintech ecosystem and regulations encouraged entrepreneurs to start and grow their businesses and customers to increase the adoption of fintech products and services. The development of regulations, laws, and guidelines in both countries contributed to the growth of the fintech sector and, at the same time, safeguard customers.",
        "authors": [
            "Mohamed Mamdouh Ali Osman"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157157",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Efficacy of Different Analysis Algorithms for Summarizing Online Deliberations",
        "abstract": "For the past decade, online deliberation platforms like Polis have expanded the reach of deliberative democracy, which calls for political decisions to be based on the results of fair and balanced discussions among citizens, by enabling larger deliberations. However, as these discussions often generate a large volume of comments, which is infeasible for policymakers to thoroughly review, these platforms often include analysis algorithms that distill the conversation into a small set of comments, which policy-makers can use as the base of citizen input into decision-making. While Polis currently provides a clustering-analysis summary of the discussion, two newer aggregation algorithms, inspired by computational social choice theory and abstract argumentation theory, have recently been proposed. These algorithms seek to provide more representative (i.e. portraying all perspectives) and consistent (i.e. comments within a perspective do not oppose each other) summaries of the discussion, respectively. Still, though these newer algorithms may have theoretical advantages over Polis’s current methods, they have yet to be evaluated in a real-world application. Through a randomized controlled trial of all three approaches using a nationally representative sample, we compare their practical effectiveness, as measured by participants’ subjective experiences regarding how well these summaries represent their concerns. We find that the computational social choice-inspired algorithm consistently outperforms Polis’s current methods in this regard, though future theoretical work is still needed to fully adapt this approach to a real-world setting.",
        "authors": [
            "Naveen Venkat"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157168",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Engineering Medical Devices to Improve Oral Delivery of Biopharmaceuticals",
        "abstract": "The dynamic mechanics of the gastrointestinal (GI) tract, including gut contractions, variable pH, and degradative enzymes, significantly challenge the development of oral delivery systems for biologic drugs by compromising their reliable delivery and therapeutic efficacy. While recent advances in oral delivery systems offer improved absorption through tissue penetration, their clinical translation remains tenuous due to the uncertainty of actuation-based delivery in variable environments and inherent design complexity. Inspired by the compression-based toxin delivery system of the stonefish, we developed a simple oral delivery device that harnesses GI mechanics to reliably actuate and systemically deliver biologic drugs. By synchronizing device actuation with gut contractions, the device and tissue work in tandem to ensure the complete transfer of a loaded therapeutic from the device to the tissue, bypassing physical and biochemical barriers and maximizing absorption. Through ex vivo and in silico experiments, we engineered the geometry of the device to achieve safe and targeted injections in the gut. An ex vivo electromechanical simulation model revealed the effectiveness of gut contractions for device actuation, and extensive in vivo experiments involving minipigs demonstrated comparable biologic drug delivery efficacy to subcutaneous injection. Harnessing the dynamic mechanics of the GI tract to improve oral delivery could transform drug administration and significantly enhance the lives of many patients.",
        "authors": [
            "Shonit Nair Sharma"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157196",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Mechanisms and Implementation of Thermo-Optical Annealing in Silica Fiber Sensors for Radiation-Induced Attenuation Mitigation",
        "abstract": "In the context of quench detection systems for fusion superconducting magnets, temperature sensors based on optical fibers provide an effective solution for rapid, distributed measurement, with low sensitivity to electromagnetic interference. At the cryogenic temperatures and high radiation doses associated with this application, however, optical fibers undergo radiation-induced attenuation (RIA): light-absorbing point defects form within the silica glass structure, reducing the longevity and effectiveness of these sensors. In this work, we investigate the underlying microscopic defects and mechanisms of RIA and assess strategies for mitigation, namely, annealing via heat treatment (thermal annealing) and annealing via light propagation through the fiber (optical annealing, or “photobleaching”). We design a white light absorption spectroscopy setup with in-situ irradiation and optical annealing, working at liquid nitrogen temperature and different post-irradiation warm-up rates. For the pure silica core and F-doped cladding fibers studied, the RIA spectrum obtained is decomposed into known radiation-induced defect absorption bands, highlighting the key role of self-trapped holes in RIA at telecommunication wavelengths. Furthermore, absorption spectroscopy experiments are performed to show that thermal annealing at liquid nitrogen temperature is negligible, validating the transferability of the experimental results obtained at 77 K to 20 K applications. The decomposition of RIA into different defect contributions is supported by cold post-irradiation electron paramagnetic resonance (EPR) spectroscopy of fiber preform fragments, which reveals the presence of two types of paramagnetic centers: self-trapped holes and E'_gamma centers. The post-irradiation transient grating spectroscopy (TGS) technique is adapted to glass samples with continuous cooling at liquid nitrogen temperature and in-situ optical annealing. With this technique, we could observe the changes in thermal and acoustic properties resulting from the evolution of defect populations, with the potential to complement other experimental techniques to better understand RIA build-up and annealing kinetics. To improve the modeling of thermo-optical annealing, we propose future experiments including isothermal annealing tests and a larger exploration of optical annealing parameters. Our RIA build-up and annealing tests can help companies aiming to operate optical fibers under irradiation at cryogenic temperatures optimize their heat treatments to restore fiber transmission and the prevention of RIA during operation.",
        "authors": [
            "Aurelien Y. M. Legoupil"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157206",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "From Hurdles to Highways: Overcoming Barriers to Robotics Adoption in Supply Chains",
        "abstract": "Macroeconomic events are putting unprecedented pressure on the warehouse industry. Among these are labor shortages, increased operating costs, and the desire for greater customization and higher throughput from these facilities. Focused on these challenges and strategic issues for warehouse applications, this thesis investigates the obstacles to implementing robotic automation in supply chains. The thesis explores this environment and the lens of using three common integration methods. These are the traditional purchase, lease, and emerging robotic-as-a-service (RaaS) model. With these methods in scope, the study incorporates a multicriteria decision-making framework (MCDM) that is built based on an analytical hierarchy process (AHP) and combined with the technique for order of preference by similarity to the ideal solution (TOPSIS). From this framework, the research identifies key decision criteria and their impact on selecting the most suitable integration strategy for automation.\r\n\r\nThrough a literature review, the study identified the essential criteria for the project design decision. These include infrastructure requirements, system capabilities, usability, provider reputation, project duration, and the total cost of ownership. We then gained insight from industry professionals familiar with automation integration using a focused field study. Furthermore, we underlined practical issues and general opinions on the criteria and how well they correspond to their integration plans. The results highlight notable trade-offs in the decision criteria, emphasizing the need for a more tailored strategy to make automation adoption more efficient.\r\n\r\nThis thesis provides an effective decision support system to guide the choice of appropriate automation solutions. It helps clarify how decision makers give the most importance to different criteria when implementing robotic automation. The research findings offer helpful details for practitioners navigating the challenging warehouse automation environment. This, therefore, encourages better informed and more efficient decision-making procedures.",
        "authors": [
            "Bartholemew Hegarty"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157194",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Comparison of Machine Learning-Based Methods for Narrowband Blind Adaptive Beamforming",
        "abstract": "There are many different approaches to beamforming and interferer cancellation. The earliest methods of beamforming assumed prior knowledge of the receive array geometry and of the incoming signal directions. This information is normally found via array calibration. Blind source separation methods do not require this information and therefore are more robust to array calibration errors. Traditional blind source separation methods generally leverage some intrinsic characteristic of the signal, such as constant envelope properties or second or higher order statistics. Traditional blind source separation methods such as CMA, SOBI, JADE, and FastICA tend to be highly effective at beamforming datasets with moderate to large sample supports, but they do not perform well when they only have access to a limited number of data samples. They also bear the disadvantage that the appropriate algorithm must be selected based on the properties of the expected signal. Machine learningbased methods are of interest because they show promise in low sample support regimes, and because they offer the possibility of a ‘one size fits all’ solution that can adaptively recognize and exploit different signal features. This thesis describes the performance of two machine learning-informed beamforming methods — Classification-Based Transfer Learning (CBTL) [1] and Denoising-Based Transfer Learning (DBTL). CBTL and DBTL are evaluated with respect to each other and with respect to traditional blind beamforming methods across a variety of signal detection environments, and are found to offer superior or equivalent performance in a majority of environments.",
        "authors": [
            "Lara Shonkwiler"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157218",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Impact of Process Replacement on Sheet Metal\r\nProduct Design: The Use of Steel Extrusions Versus\r\nFormed Sheet Metal",
        "abstract": "The sheet metal manufacturing industry, with its rich history and legacy, continues to seek innovative methods to enhance automation and reduce costs in an increasingly competitive market. Design for Manufacturability & Assembly (DFMA) has emerged as a strategy to simplify product designs, thereby improving manufacturing eOiciency and reducing production costs. This research suggests the use of extruded steel profiles as an alternative to traditional sheet metal components that pose challenges for automation, particularly heavy gauge narrow channels. Additionally, it advocates for replacing manual press brake operations with advanced automated tube laser technology. The proposed shift not only simplifies the manufacturing process but also aligns with the broader goal of global cost reduction and process standardization, which are essential for enhancing New Product Introduction (NPI) eOiciencies. The findings demonstrate that maximizing the application of tube laser technology across a diverse range of channels and products can lead to significant cost savings, ranging from 49% to 79%, with a payback period of less than two years. Even under fluctuating raw material prices, the tube laser method remains economically advantageous. Moreover, redesigning products to enhance compatibility with tube laser technology has shown to increase the automation compatibility of an example product to 100%, underscoring the importance of incorporating DFMA principles from the early stages of product design.",
        "authors": [
            "Chenyu Yuan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157151",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "SongGen: Framework for Controllable AI Song Generation through Interactive Songwriting and Artist Emulation",
        "abstract": "We propose SongGen, an AI-based song-writing and song co-creation framework. Building upon existing AI tools like Suno.ai, SongGen features a chat interface with a trained AI songwriter assistant, emulating the traditional back-and-forth of human collaboration. The system offers enhanced capabilities for greater control over the songwriting process, including concept ideation, lyric generation and editing, real-time song generation, and granular instrumental specification. Comparative evaluations demonstrate SongGen’s superiority in key metrics such as steerability, expressiveness, personalization, and user satisfaction. We also present an extension of the SongGen framework for artist emulation and on-demand song generation. Future development aims to incorporate voice-based interaction and real-time voice conversion, enabling music artists to guide fans in creating personalized songs.",
        "authors": [
            "Ajay Arora"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157249",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Nature-Centered Materiomics: Experimental and\r\nComputational Design",
        "abstract": "As of the year 2020, the accumulated mass of anthropogenic materials now outweighs all living biomass on Earth. Industrial material production simultaneously contributes nearly 30% of global greenhouse gas emissions each year, which in conjunction with solid waste accumulation and deterioration of ecological processes, threatens the livelihood of current and future generations of both human and non-human species. This is in dramatic contrast with natural materials, which consistently outperform human engineering, yet are invariably produced using abundant, renewable sources of energy and upon their disuse, decompose to fuel new growth. Nature effectively forms sustainable supply chains with no waste by leveraging both the constituents of materials and their structural organization at multiple scales, architecting common and abundant building blocks into a variety of high-performing composites. In this thesis, we present a nature-centered materiomics approach to emulate this in the design of novel sustainable materials. We leverage both computational and experimental strategies to consider multiple length-scales and time-scales across the processing, structure, properties, and performance of material systems with minimal ecological impact. First, we demonstrate machine learning strategies for harnessing functional geometries in natural materials and demonstrate how interpretable models can be leveraged toward novel material design. Next, we develop a platform for the fabrication of tunable biocomposites composed of renewable and biodegradable feedstocks, and consider Bayesian optimization as an approach to guide composite optimization and design. Finally, we extend the fabrication system to hybrid-living materials and demonstrate dynamic bio-welding capabilities in the strongest mycelium-based material in the literature to-date. Altogether, these contributions enhance multiscale understanding of nature-centered material design and pave the way for future innovations that align human engineering with regenerative material cycles.",
        "authors": [
            "Sabrina C. Shen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157176",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Heat Pipes for the Thermal Management of High\r\nFrequency Transformers in the Navy integrated Power\r\nElectronics Building Block",
        "abstract": "The development of the integrated Power Electronics Building Block (iPEBB) is key to the full electrification of future United States Navy ships. The creation of this modular, universal power converter takes full advantage of modern electronics; however, the high heat generation of these components, 9.6 kW from the MOSFET switches and 624 W from the transformer, makes thermal management crucial to their successful implementation. As a result of additional requirements, indirect liquid cooling using a detached cold plate is being studied; however, preliminary analysis revealed concerns regarding the hot spot temperatures of the transformer using this approach. This thesis explored the feasibility of using heat pipes to supplement the cooling provided by the cold plate to maintain iPEBB transformer core and coil temperatures below 100°C and 155°C respectively. First, experiments and analytical solutions were used to provide accurate estimates for the thermal conductivity values of the 3F36 ferrite and litz wire in the transformer. Then, a standalone thermal model of the transformer was built in StarCCM+ and used to test various cooling solutions, including forced airflow and heat pipe configurations. The proposed design utilized 16 copper-water heat pipes configured to provide alternative paths of heat flow for the regions of the transformer furthest from the cold plate. Shapal HiM Soft Machinable AlN ceramic was utilized to provide high voltage insulation, and electromagnetic simulations were used to estimate the induced losses in the heat pipes as a result of high frequency coil operations. Using a half-iPEBB thermal model, the final configuration, coupled with the cold plate cooled by 22°C deionized water at a flow rate of 0.37 kg/s, achieved a core maximum temperature of 99.7°C, coil maximum of 93.2°C, and MOSFET maximum of 144.6°C, all within their respective limits, while only adding a net weight of 0.29 kg to the iPEBB. The thermal results of this study showcase the effectiveness of heat pipes in the iPEBB and invite further analysis and experimentation to validate the electromagnetic implications of the concept. These results also contribute to the general ongoing study of heat pipe usage near high-frequency electronics.",
        "authors": [
            "David Hernandez"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157227",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "CAD-Based Geometry Representations for Monte Carlo Fusion Neutronics Methods and CSG vs. DAGMC Performance Tradeoffs in OpenMC",
        "abstract": "Fusion reactors utilizing deuterium and tritium fuel produce high-energy 14.1 MeV neutrons, necessitating a thorough understanding of their behavior for effective reactor design. Neutron transport codes play a critical role in determining key parameters such as tritium breeding ratio, neutron wall loading, and heat deposition, vital for assessing operational considerations. Monte Carlo (MC) radiation transport methods have become standard in fusion neutronics due to their ability to handle energy and angular variables continuously. However, manual modeling of complex fusion geometries with traditional constructive solid geometry (CSG) methods remains labor-intensive, prompting the integration of computer-aided design (CAD) models into MC radiation transport. This thesis investigates the integration of CAD-based geometry representations into MC radiation transport, focusing on computational performance implications of the Direct Accelerated Geometry Monte Carlo (DAGMC) approach. This work examines different neutronics model representations, including CSG, Unstructured Mesh (UM), and DAGMC for the practical solutions they can provide for fusion neutronics needs. Tracking algorithms associated with each representation are explored, highlighting UM and DAGMC’s versatility in the way they integrate with CAD-based design processes. Performance comparison between CSG and DAGMC geometries in OpenMC is analyzed by evaluating particle simulation rates and memory usage across four progressively complex fusion-like models. Performance results reflect positively on DAGMC transport, but areas of future work are identified for more comprehensive results. From the lens of computational performance, this study contributes to determining the viability of CAD-based geometry representations for use in fusion-relevant MC radiation transport.",
        "authors": [
            "Katelin Du"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157233",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "FrED Manufacturing - A Study in Affordable Manufacturing to Scale using Desktop Sized Fiber Extrusion Device",
        "abstract": "FrED (Fiber Extrusion Device) Factory is a manufacturing facility at MIT which educates its students on fundamental and advanced manufacturing principals. The factory produces multiple FrED devices, which are \"desktop fiber extrusion systems that mimic continuous fiber draw process for hands-on learning and/or laboratory experience on data acquisition, control system, and smart manufacturing. It allows learners to perform experiments, vary manufacturing parameters and control system, collect data, and perform analysis.\" [1] This year’s thesis work builds off of the progress from 2023, which aimed to produce a low cost variant of earlier versions of the FrED. In 2024, the aim for the lab was to implement design refinements, design for manufacturing, design the assembly line, design packaging, develop supply chain using Tulip, develop educational content, perform user testing, and execute pilot runs. The focus of this thesis will be on design refinements related to graphical user interface (GUI), inclusion of threading for improvement to program speed, and characterization of performance related to diameter control as well as advancements in educational content development, user testing, production level assembly, and pilot runs. The results of this thesis include significant improvements made to the FrED device such as a user-controlled GUI as well as close-loop controls. Furthermore, key components of the device were quantified such as fps rate of the USB camera and motor stability which aided in understanding how diameter control and modulation can be implemented in future work. At the time of submission, there were inherent complications still not understood about the FrED that limited its potential as an end user product. Some complications included reliability of the diameter reading from the USB camera, physics of the hot glue preform, and motor speed assumptions which did not perform well under close-loop testing (spool speed going to 0 in order to make the diameter larger consequently prevents the camera from reading any future diameter measurements which is problematic). In terms of pilot runs, user testing, and educational content development, the results were promising. 78.3% of the 23 user testing respondents at Venture Cafe said they were interested in receiving a FrED and getting access to more learning content. Suggestions were made by the users for future work and implementation. Educational content was developed for mass flow and data acquisition, however, a formal pilot run session where this could be tested for feedback was not performed.",
        "authors": [
            "Rachael S. Rosko"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157212",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Distributed Singular Value Decomposition Through\r\nLeast Squares",
        "abstract": "Singular value decomposition (SVD) is an essential matrix factorization technique that decomposes a matrix into singular values and corresponding singular vectors that form orthonormal bases. SVD has wide-ranging applications from principal component analysis (PCA) to matrix completion and approximation. Methods for computing the SVD of a matrix are extensive and involve optimization algorithms with some theoretical guarantees, though many of these techniques are not scalable in nature. We show the efficacy of a distributed stochastic gradient descent algorithm by implementing parallelized alternating least squares and prove theoretical guarantees for its convergence and empirical results, which allow for the development of a simple framework for solving SVD in a correct, scalable, and easily optimizable manner.",
        "authors": [
            "Freddie Zhao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157145",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Study on Deploying Large Language Models as Agents",
        "abstract": "This thesis investigates the deployment and utilization of Large Language Models (LLMs) as agents, exploring their potential in automating workflows and enhancing user interactions. The study begins with an in-depth analysis of language models, tracing their evolution from pure statistical models to advanced neural network architectures like Transformers and their bidirectional variants. It then delves into the operational framework of LLM agents, detailing user interactions, environmental considerations, memory management, task planning, and tool use. The study addresses critical limitations in LLM inputs, such as the context window and introduces Retrieval-Augmented Generation (RAG) as a solution to extend the model’s capability. Key APIs provided by OpenAI for deploying GPT models are discussed, highlighting their functionalities and applications. Finally, the practical application of LLMs in creating Robotic Process Automation (RPA) workflows is demonstrated through a divide-and-conquer methodology, showcasing the efficiency, scalability, flexibility, and accuracy of this approach. This comprehensive study underscores the transformative impact of LLMs in automating complex processes and enhancing user experiences through intelligent agent deployment.",
        "authors": [
            "Jiannan Cao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157177",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Mechanics of Three-Dimensional Micro-Architected Interpenetrating Phase Composites",
        "abstract": "The design of modern composite materials, as used in a wide range of engineering applications, is largely derived from a traditional framework based on laminates. While resulting in desirable strength and stiffness properties, the laminate-based structure leads to a high degree of anisotropy and unique failure modalities like interlaminar failure, limiting the performance of these composites under complex loading conditions. Meanwhile, recent work in the field of architected materials has yielded a thorough understanding of geometry-dependent material behavior, enabling the development of highly robust architectures with tunable (an)isotropy. However, such advances have focused primarily on describing the response of lightweight architected geometries comprised mostly of air. The effect of adding a load-bearing matrix is not well understood. Here we investigate the effect of geometry and constituent material properties on the mechanics of 3D-architected interpenetrating phase composite (IPC) materials, i.e., two-phase materials consisting of an architected structure surrounded by a matrix. Using computational homogenization, we first predict how resultant coupled stress states in the composite change with the material properties of each individual phase and contextualize the results within traditional stiffness scaling laws. We then demonstrate two robust fabrication pathways for realizing polymer- and carbon-based centimeter-scale architected IPCs with micro-scale features. Using these prototypes, we study the mechanical behavior of the fabricated composites under uniaxial compression, with particular emphasis on the non-linear and failure regimes. We show that independent of the material system, the presence of a load-bearing matrix distributes the stress in the composite, contributing to a high-strength, globally stretchingdominated failure behavior, regardless of nodal connectivity. Moreover, the development of a 3D, highly tortuous pathway for stress delays or prevents catastrophic failure of the traditionally brittle architecture phase, resulting in energy dissipation performance of the composite that exceeds the sum of its individual constituents. Finally, we demonstrate that the composite stress state can be architected using geometric design of the IPC and introduce an example of tunable mechanical response in an architected composite inspired by traditional auxetic metamaterials. Altogether, this work broadens our established understanding of the link between architecture and mechanical performance by considering the framework of interpenetrating phase composites, creating the foundation for a new class of strong, resilient, and programmable materials with architected stress states.",
        "authors": [
            "Andrew Y. Chen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157198",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Development of Elastic Resistive Force Theory & Applications to Uprooting",
        "abstract": "Granular intrusion processes such as sand locomotion, uprooting, and digging are commonly present. While these phenomena can be accurately modeled via discrete element methods and continuum models, this accuracy comes at a great computational cost, especially for large systems. Granular Resistive Force Theory (RFT) is a reduced-order, rateindependent model that has been shown to successfully capture the motion of rigid intruders in granular media, with a reduced computational cost. RFT is based on a rate-independent theory that calculates the force experienced by a body using its direction of velocity. This makes it difficult to handle scenarios that are near-stagnant which occur frequently in uprooting of plants. To overcome this limitation, we introduce elastic RFT (eRFT) which is based on a rate-independent plasticity flow-rule–like criterion, and pair it with deformable intruders. We focus on modeling uprooting processes which inherently have flexible intruders and are often dynamically controlled. This allows us to address both previously mentioned shortcomings of RFT (stagnancy and flexible intruders) at once. By combining eRFT with a nonlinear beam theory to represent slender, inextensible roots we create a speedy computational tool. Using MATLAB, we simulate various uprooting scenarios to better understand anchoring mechanisms of different root geometries. We showcase the validity of eRFT results by comparing them to experimental data. To implement eRFT in ABAQUS, we make use of an existing user subroutine which allows the study of a broader range of intruder materials and shapes. While the subroutine has its limitations, initial comparisons to computational and experimental results are demonstrative.",
        "authors": [
            "Lale Yilmaz"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157201",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Target Design and Optimizations for Spent Fuel Transmutation",
        "abstract": "There are six long-lived fission products (LLFPs) identified in nuclear spent fuel, which account for at least 99% of the long-term radiotoxicity once actinide recycling is completed. This thesis examines the feasibility of using proton beams to transmute LLFPs into shorterlived or stable isotopes. While long-term storage for high-level waste would still be necessary, transmuting the LLFPs can reduce the volume of waste material that needs to be stored. The objectives of this research are to explore the design of a proton transmutation facility, as well as to determine the optimal LLFP target-blanket material configuration for maximizing the transmutation efficiency. This thesis analyzes the use of intermediate energy beams of 18-70 MeV from commercial cyclotrons for transmutation. This thesis also analyzes the use of 1000 MeV proton beams to generate a substantial number of secondary neutrons through spallation interactions with target materials. The secondary neutrons produced from the spallation process are utilized by the LLFP materials, while surrounding blanket materials are selected to enhance the transmutation efficiency. PHITS, a Monte Carlo transport code, is employed to computationally model the interactions between LLFP materials and the proton beam. In this thesis, PHITS is used to estimate the flux-energy spectrum and the number of atoms irradiated in the LLFP target during beam interaction. This data is then post-processed using a 0-dimensional analysis in FISPACT to estimate the transmutation rate for each LLFP. PHITS is also used to find the depletion rate of the LLFPs for the 18-70 MeV beam case and for spallation-induced transmutations in the 1000 MeV case. Geant4, a Monte Carlo transport toolkit, is used to calculate the production rate of particles attributed to the spallation process. Analysis of the performance of commercial cyclotrons with energies of 18-70 MeV indicates that transmutation rates increase with higher proton beam energy. A cyclotron with a beam current of 10 mA and beam energy of 70 MeV running continuously can transmute 15.401 ± 0.069 g/year of Tc-99. However, Tc-99 is produced at a rate of approximately 8.54 kg/year in a 1 GW reactor, suggesting that a single commercial cyclotron beam is currently not viable for transmutation purposes. A proposed tank design with a lead/Tc-99 target that is surrounded by LLFP pins and heavy water is considered for the spallation study. Although using Tc-99 as a target directly transmutes 0.893 ± 0.002 kg/year from transmutation attributed to spallation, using lead as a target instead approximately doubles the transmutation rates in the LLFP regions for almost all of the LLFP isotopes. In both cases, the depletion rate of the LLFPs is greatly increased compared to using a commercial cyclotron of 70 MeV. A proton spallation source 3 with a beam current of 10 mA and beam energy of 1000 MeV, using a Tc-99 target, achieves a transmutation rate of approximately 10.9 kg/year of Tc-99 in the LLFP pins through secondary neutrons produced by the spallation process. In contrast, using a lead target achieves a higher transmutation rate of around 20.0 kg/year of Tc-99 in the LLFP pins. This work was supported by the DOE ARPA-E Project under the award number DEAR0001578.",
        "authors": [
            "Grigor Tukharyan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157147",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "satdatagen: a Python Library for Satellite Sensor Task\r\nScheduler Support",
        "abstract": "The number of objects in Earth’s orbit is increasing rapidly, raising urgency for intensified observations of satellites and other resident space objects (RSOs) to manage space traffic and prevent collisions. Current methods for RSO detection and tracking rely on ground-based and space-based observatories with optical or radar sensors, but these telescopes require complex scheduling to achieve surveillance of all objects. Previous works have implemented scheduling algorithms and machine learning models that optimize the assignment of tasks to the sensors for RSO observations. However, prior methodologies rely on different datasets, making it hard to make comparisons across methods. This paper presents satdatagen: a software package that generates datasets that can be used as inputs to sensor task schedulers. The datasets generated from the satdatagen library are intended to be used as a baseline input to satellite sensor task schedulers. The datasets contain information about every satellite that passes in view of the sensor such as its angle of altitude and its brightness. Additionally, actual cloud cover data is included for optical telescopes that need to take visibility into account while scheduling observations. satdatagen is simple to use, and does not require excess outside knowledge from developers of scheduling tools.",
        "authors": [
            "Adina H. Golden"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157185",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Autonomous UAV Navigation using Millimeter Wave\r\nRadar",
        "abstract": "We present the design, implementation and evaluation of MilliNavigator, an autonomous navigation system for drones capable of mapping, path-planning, self-localizing, and navigating in indoor environments by leveraging strategically-placed millimeter wave anchors. Autonomous drones are an increasingly relevant tool for completing and automating hard-to-reach tasks. State of the art navigation systems rely primarily on cameras and GPS for environmental perception and self-localization. These solutions can impose restrictions on existing systems, which limit their navigable environment to well-lit, outdoors, and unobstructed paths. This thesis presents MilliNavigator, the first system to use millimeter wave radar and anchor-aware path planning to achieve high accuracy, 6DOF, online localization. By generating a localization precision score map from known anchor deployments, the system jointly optimizes travel distance and localization performance. We implemented and evaluated MilliNavigator on a drone built with commercial, off-the-shelf parts. We ran over 165 successful missions across 7 different tag deployments. Our system successfully achieved 7.9cm overall median error and had a 90th percentile error of less than 21cm.",
        "authors": [
            "Joshua I. Herrera"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157188",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Design Study Using Simulation Techniques in Roll Form\r\nProduction",
        "abstract": "Sheet metal roll forming is a continuous bending process where metal strips are fed through a sequence of rolls to achieve a specific cross-sectional profile. This method is vital in the automotive industry for producing high-strength, lightweight components with precision, consistency, and cost-efficiency. This project focuses on optimizing Novelis’s aluminum roll forming process using Computer-Aided Engineering (CAE) techniques, including UBECO Profil, AutoCAD, and Finite Element Analysis (FEA) tools such as Ansys and LS-Dyna. Initial simulations on a square tube profile were key in identifying critical stations, leading to performance improvements through targeted adjustments. Stress and strain analyses revealed how operational factors, such as roll adjustments, affect the section shapes and angles, facilitating the refinement of roll forming station settings. With a Design of Experiment (DOE) framework, the study identified key variables to enhance simulation output accuracy and optimize roll forming settings. The team successfully built a digital twin of the new roll forming line, which accurately predicted the final product's geometry and provided precise recommendations for machine settings to achieve the desired shape. Novelis can apply these insights to enhance their software, thereby potentially increasing production efficiency. This approach not only supports current operations but also lays the foundation for future research and development advancements.",
        "authors": [
            "Joo Won Lee"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157160",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Microporous Polymer-Metal Organic Framework (MOF)\r\nHybrid Materials for Separations",
        "abstract": "Membrane-based separation holds significant promise for reducing the high energy consumption associated with traditional thermal-based separation processes in the chemical industry. Recent advancements in microporous materials, such as polymers of intrinsic microporosity (PIMs) and metal-organic frameworks (MOFs), have demonstrated performance improvements over conventional polymers. Mixed-matrix membranes (MMMs) have emerged as a potent strategy, combining the processability of polymers with the superior separation properties of MOFs to create high-performance membranes. Additionally, the integration of MOFs into polymers can mitigate stability issues such as plasticization, swelling, and physical aging. This thesis investigates MMMs based on PIM-1 and its derivatives, along with UiO MOFs, for gas and organic solvent-based separations. The studies focus on enhancing polymer–MOF interfacial compatibility, understanding penetrant transport, and addressing key challenges in MMM design and fabrication. A longstanding challenge with MMMs fabrication is poor polymer–MOF compatibility, leading to particle agglomeration and non-selective interfacial voids. To address this, the strategy of decorating polymers and MOFs with compatible functional groups was explored. By studying UiO-66-NH2 MOF and carboxylic acid-functionalized PIM-1 (PIM-COOH), it was demonstrated that MMMs with compatible functional groups exhibit enhanced polymer–MOF interaction and plasticization resistance. To further understand transport within these MMMs, self-diffusivities of gases were measured using pulsed-field gradient nuclear magnetic resonance and compared to macroscopic diffusivities obtained from permeation and sorption analysis. The PIM–MOF material platform was also extended to solvent-based separations.To understand solvent transport through microporous polymers, intrinsic properties of swollen polymers were obtained both experimentally and computationally, and these properties were correlated with solvent transport metrics. Finally, MMMs composed of PIM-COOH and UiO MOFs with systematically increasing pore apertures were evaluated for their solvent nanofiltration performance. Key challenges such as MOF instability and non-ideal polymer–MOF interfaces were identified. In summary, this thesis delves into the structure-property relationships of microporous materials for gas and solvent-based separations, offering insights that can guide the future design of advanced composite membranes for challenging separations.",
        "authors": [
            "Wan-Ni Wu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157250",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Biometric and Biomechanical Sensing for Violin Performance Analysis",
        "abstract": "Expressive violin performance demands the coordination of multiple physical and physiological processes. Students, especially those engaged in infrequent private lessons, often struggle to manage these demands. Outside of lessons, they lack access to the resources and external feedback that technology has made readily available in other learning settings. In this study, we propose the Expressive Violin Performance Sensing (EVPS) system as a solution to this issue. The EVPS system uses low-cost and accessible electronic sensors to provide objective, quantitative insights into the physical and physiological aspects of a violinist’s performance. Results from experimental trials reveal that the EVPS system provides relatively reliable data on expressive violin performance. While the general measures of physicality did not reveal significant differences between players of distinct skill levels, physiological and specific physical measurements aligned well with predictions. The successful utilization of low-cost sensors in the EVPS system highlights their potential for use in future performance analysis studies, challenging the precedent of relying on expensive, medical-grade systems.",
        "authors": [
            "Aria Kydd"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157216",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Light Water Reactor Loading Pattern Optimization with Reinforcement Learning Algorithms",
        "abstract": "In 2023, Commercial Nuclear Power Plants (NPPs) in the USA, comprising Light Water Reactors (LWRs) such as Pressurized Water Reactors (PWRs) and Boiling Water Reactors (BWRs), remained the largest single source of carbon-free energy. They provided approximately half of the nation’s carbon-free electricity and under 20% of total electricity throughout the year. Ensuring the competitiveness of these nuclear assets is crucial for maintaining their role in providing dispatchable clean energy alongside renewable sources. The recent commissioning of Vogtle Units 3 and 4 marked the first new NPPs connected to the grid in over three decades, highlighting the high costs associated with nuclear technology and underscoring the need to improve their economic competitiveness. Optimizing the fuel cycle economics through enhanced core Loading Pattern (LP) is a key strategy to address this challenge. Since the 1960s, optimizing the LP for LWRs has been a major focus in nuclear engineering, but the large search space has posed significant difficulties. Computational methods from Stochastic Optimization (SO) have been used to tackle this issue, yet they often fail to outperform expert-designed solutions preferred by utilities. Deep Reinforcement Learning (RL), a subset of Deep Learning focused on decision-making, has shown promise in surpassing human-expert solutions in fields such as gaming and robotics. This thesis investigates the use of RL to improve automated tools for solving the PWR LP optimization problem, with the goal of developing efficient decision-support tools for core designers to generate more economical loading patterns. We present a novel approach using deep RL to solve the LP problem and compare it with traditional SO-based methods. Our findings indicate that the LP problem benefits from a global search to rapidly identify promising directions, followed by a local search to efficiently exploit these directions and avoid local optima. Proximal Policy Optimization (PPO), a type of RL algorithm, adapts its search capabilities with learnable policy weights, making it effective for both global and local searches, which contributes to its superiority over SO-based methods. Additionally, we introduce a new method called PEARL (Pareto Envelope Augmented with 3 Reinforcement Learning) to tackle multi-objective optimization challenges. PEARL demonstrates greater efficiency in identifying Pareto fronts without requiring additional designer intervention, compared to traditional single-objective scaling methods. Finally, we extend PEARL to a novel paradigm called physics-informed RL by integrating statistical techniques and physics knowledge to enhance algorithm performance. As problem complexity increases, classical methods sometimes fail to find feasible solutions. Incorporating physics-informed insights becomes crucial for discovering high-quality and diverse solutions more efficiently. These results highlight the potential of AI advancements in the nuclear field. A deep understanding of AI tools is essential to fully leverage their capabilities. Our approach achieved a cumulative benefit of over 4 $million per year per plant compared to using off-the-shelf AI solutions. While further work is needed to translate these theoretical benefits into real reactors, these algorithms promise to enhance the competitiveness of future nuclear fleets. In doing so, they could make a substantial contribution to achieving carbon neutrality by increasing the amount of clean electricity on the grid.",
        "authors": [
            "Paul R.M. Seurin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157146",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Streamoscope: A Low-Cost, Open-Source, USB-3-Capable Streaming Data Acquisition System for Low-Field MRI",
        "abstract": "Magnetic Resonance Imaging (MRI) is a powerful, safe imaging technique based on using magnetism to provide contrast between soft tissues. Portable, low-field MRI is a growing area that has already demonstrated value in both educational and clinical domains. Low-field MRI systems need to acquire data with sample rates in the tens of megahertz, which can make the data acquisition system the bulk of the overall cost of low-cost systems. This work presents the Streamoscope: an open-source data acquisition system designed for low-field MRI that streams two 14-bit resolution channels at 60 megasamples per second over USB-3 into Python. It is approximately $300 in parts, about a quarter of the price of the cheapest data acquisition system on the market that would work in our case study. The Streamoscope can stream full-sample-rate raw MRI data into a computer to be processed in Python, enabling real time imaging. The system has been validated by generating 2D images of a phantom on a system with an 8 MHz Larmor frequency.",
        "authors": [
            "Joseph W. Feld"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157183",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Carbon Capture Efficiency in Natural Gas Combined Cycle Power Plants: Analyzing the Effects of Variable Load Operations",
        "abstract": "Natural gas power generation retrofitted with carbon capture technology is poised to play a crucial role in ensuring energy reliability amidst the transition to variable renewable energy resources. While natural gas generation is used primarily for baseload power, it is expected to transition towards an intermittent power generator, serving as a load-following resource during periods of low renewable energy availability. It will be critical to understand how start-up, shutdown, and load-following behavior may impact system performance and influence future grid design. \r\n\r\nThis thesis performs a comprehensive literature review to establish context on various techniques of carbon capture technology. Post-combustion carbon capture, specifically absorption-based technology, remains the preferred candidate for retrofitting natural gas plants due to its technical maturity, scalability, relatively high capture efficiencies, and ease of retrofitting. The literature highlights that absorption-based carbon capture units exhibit degraded performance during non-steady-state operating conditions. Specifically, cold start-ups result in lower capture efficiencies and higher heat rates, although hot start-ups incur significantly less performance reduction. \r\n\r\nThe literature review findings are integrated into GenX, a grid optimization tool, to evaluate natural gas combined cycle power plants equipped with carbon capture technology. The modified optimization models are run using the ISO New England grid system, and results suggest that incorporating advanced start-up penalties for natural gas plants reduces operational flexibility in an emissions-constrained environment. As capture efficiencies decrease and heat rates increase during start-ups, utilizing natural gas plants becomes more expensive due to the additional emissions and reduced thermal efficiency. Comparing models with different levels of performance degradation during start-up suggests that installing less gas capacity could be optimal, with those units operating at higher capacity factors to mitigate start-up penalties. Under modest emissions constraints, natural gas units may be operated continuously even during periods of renewable energy surplus. Harsher start-up penalties applied to natural gas plants likely increase the incremental value of alternative energy technologies, although natural gas retains a critical role in the energy mix.",
        "authors": [
            "Caleb M. Knight"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157211",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Characterization of Microbial Primary and Secondary Metabolism in the Marine Realm",
        "abstract": "This thesis applies meta-omics data analysis to elucidate the ecological roles of marine microorganisms in diverse habitats and includes the development of new bioinformatics tools to enhance these analyses. In my second chapter, I applied genome mining tools to analyze the gene content and expression of biosynthetic gene clusters (BGCs). The analysis of BGCs through largescale genome mining efforts has identified diverse natural products with potential applications in medicine and biotechnology. Many marine environments, particularly oxygen-depleted water columns and sediments, however, remain under-represented in these studies. Analysis of BGCs in free-living and particle-associated microbial communities along the oxycline water column of the Cariaco Basin, Venezuela, revealed that differences in water column redox potential were associated with microbial lifestyle and the predicted composition and production of secondary metabolites. This experience set the stage for my third chapter, in which I developed MetaPathPredict, a machine learning-based tool for predicting the metabolic potential of bacterial genomes. This tool addresses the lack of computational pipelines for pathway reconstruction that predict the presence of KEGG modules in highly incomplete prokaryotic genomes. MetaPathPredict made robust predictions in highly incomplete bacterial genomes, enabling more accurate reconstruction of their metabolic potential. In my fourth chapter, I performed metagenomic analysis of microbial communities in the hydrothermally-influenced sediments of Guaymas Basin (Gulf of California, Mexico). Previous studies indicated a decline in microbial abundance and diversity with increasing sediment depth. Analysisrevealed a distribution of MAGs dominated by Chloroflexota and Thermoproteota, with diversity decreasing as temperature increased, consistent with a downcore reduction in subsurface biosphere diversity. Specific archaeal MAGs within the Thermoproteota and Hadarchaeota increased in abundance and recruitment of metatranscriptome reads towards deeper, hotter sediments, marking a transition to a specialized deep biosphere. In my fifth chapter, I developed MetaPathPredict-E, a deep learning-powered extension of MetaPathPredict for eukaryotic metabolism predictions. Eukaryotic metabolism is diverse, reflecting varied lifestyles across eukaryotic kingdoms, but the complexity of eukaryotic genomes presents challenges for assembly and annotation. MetaPathPredict-E was trained on diverse eukaryotic genomes and transcriptomes, demonstrating a robust performance on test datasets, thus advancing the study of eukaryotic metabolic potential from environmental samples",
        "authors": [
            "David Edward Geller-McGrath"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157088",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Single-molecule diagnostics to support curative interventions for tuberculosis and HIV",
        "abstract": "Tuberculosis (TB) and the human immunodeficiency virus (HIV) are two of the leading causes of death worldwide. Tuberculosis is curable, but because of the difficulties of diagnosing it, many people with TB—and the majority of those killed by it—never begin treatment. HIV can be treated with lifelong medication. But if drug resistance develops or treatment is interrupted, the virus resurges. This could be prevented by an HIV cure that either clears HIV from the body or keeps the virus suppressed without continued therapy. Next-generation diagnostics will play a central role in supporting access to existing TB cures and future HIV cures. In this thesis, I describe the advancement of digital enzyme-linked immunosorbent assay (ELISA) protein detection methods in service of curing these two deadly infectious diseases.\r\n\r\nExisting TB diagnostics rely heavily on sputum, which is highly infectious, leading to increased TB cases among health care workers and limiting access to places with appropriate biosafety precautions. We developed a multiplexed Single Molecule Array (Simoa) digital ELISA that can diagnose TB from biomarkers in urine. Our assay is highly sensitive, as demonstrated in diverse cohorts totaling approximately 600 individuals.\r\n\r\nSimoa is a robust and widely used platform, but its accessibility is limited because it relies heavily on advanced microwell and imaging technology. We developed a new digital ELISA platform, called Molecular On-bead Signal Amplification for Individual Counting (MOSAIC), that performs the final readout step with a flow cytometer, bringing digital ELISA within reach of many hospitals and other health care centers. In addition to reducing instrumentation and cost, MOSAIC also allows for greater sensitivity and higher-order multiplexing than Simoa. It is, to our knowledge, the most sensitive protein measurement technique ever developed, with attomolar limits of detection.\r\n\r\nFinally, I describe the application of MOSAIC toward the development of HIV cures and longer-acting antiretroviral medications. These depend on a deeper understanding of the biology of HIV, and when they are ready for clinical trials, will also need highly sensitive tests to characterize the virus-host interactions and determine whether they are working. We developed ultrasensitive Simoa and MOSAIC assays for 20 circulating host and viral proteins and measured them in a cohort of 17 individuals with HIV whose treatment was interrupted, to evaluate which biomarkers could predict when the virus would rebound. Baseline levels of these biomarkers did not predict viral rebound, but changes over time did, highlighting the need for scalable personalized approaches.\r\n\r\nHIV and TB are two of the great diseases in the world. The next generation of diagnostic technologies, a urine test conducted on expensive instrumentation, and newly identified circulating biomarkers will not in themselves solve these problems. But these more sensitive assays are one step closer to the true biology of these diseases, and these advances in accessibility bring this this ultrasensitive monitoring one step closer to the clinic.",
        "authors": [
            "Tyler J. Dougan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157100",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Towards depth-resolved multi-cubic-centimeter field of view endoscopic camera for intraoperative nerve identification.",
        "abstract": "One out of every five peripheral nerve injuries in the United States has an iatrogenic origin. These injuries can cause chronic neuropathies, paresthesia, and varying functional losses.\r\nTo reduce the risk of nerve injury, surgeons meticulously identify and track nerves within the surgical field using white-light magnification. However, small (sub-millimeter diameter)\r\nand buried nerves are often difficult to identify with this approach. This has motivated a long-standing effort to develop improved nerve visualization technologies that are deployable in both open and minimally invasive surgical workflows. Fluorescence imaging is the most commonly explored strategy, and multiple exogenous fluorophores that bind to nerve-specific targets have been developed. However, fluorescence imaging has several limitations, including a disrupted workflow (due to the need for specialized lighting) and a significant regulatory burden. For these reasons, fluorescence-based nerve visualization has not yet been clinically adopted.\r\n\r\nPolarization-based optical coherence tomography (OCT) approaches to nerve visualization would inherently mitigate each of these translational challenges. First, OCT imaging is not affected by room light and thus can be used simultaneously with surgical lighting. Second, OCT is label-free and avoids regulatory pathways associated with new drug development. However, because OCT offers high-resolution, three-dimensional imaging. A surgical OCT system supporting video-rate acquisition of cubic centimeter fields would require signal capture bandwidths that are several orders of magnitude higher than what is available today. It is unlikely that this gap will be addressed through incremental advances in existing OCT platforms.\r\n\r\nIn this thesis, we present a radically different OCT platform designed to aggressively reduce signal capture bandwidths while also simplifying the optical and electronic subsystem designs. The proposed approach is contour-looping (CL-) OCT (pronounced cloaked). It retains the depth-sectioning capability upon which OCT is based but discards the requirement of comprehensive three-dimensional imaging, which results in impractical signal capture bandwidths. As such, CL-OCT defines a strategy for low-bandwidth depth-sectioned imaging that may be sufficient for specific imaging tasks such as nerve identification. Importantly, the CL-OCT platform is compatible with a camera-based (i.e., scan-free) deployment that is advantageous for endoscopic deployments. In the second component of this thesis, we provide extensive theoretical and experimental studies on how optical amplifiers can be used in OCT to address sensitivity challenges of high-speed surgical OCT platforms like CL-OCT. Together, these lines of research define a new approach to meeting the need for OCT-based solutions for intraoperative nerve identification. This technology, if successfully translated, may lead to a lower incidence of iatrogenic nerve injury.",
        "authors": [
            "Yong-Chul Yoon"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157108",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Looking at the Map, Together: Modeling Treatment Center Location Selection and its Effects on Access to Gene Therapy in Brazil",
        "abstract": "Choosing at how many and which treatment centers to offer a gene therapy to patients is\r\na crucial decision which impacts how far the treatment has to be transported and how far\r\npatients have to travel to receive treatment. Many gene therapies are for patients with severe diseases that make it difficult to travel. On the other hand, cold chain requirements\r\nmake shorter transportation preferable for gene therapies, and few centers have prior experience handling them. Using multi-criteria optimization modeling paired with local input,\r\nthis thesis explores different approaches to the gene therapy treatment center location selection decision and how these approaches would affect patients’ geographic accessibility to\r\ntreatment.\r\nWe focus on Brazil and a specific gene therapy product as our case study. We interview\r\nlocal pharmaceutical company employees to understand the stakeholders involved in this\r\ndecision and the approaches being considered. We model how these approaches would affect patients’ geographic accessibility to treatment and discuss potential modifications to\r\nour model. Finally, by means of an interactive workshop, we explore the decision-making\r\ndiscussion between stakeholders in choosing which approach to follow.\r\nWe find that the approaches under consideration result in a wide range of geographic accessibility for patients. Early stage decisions have impacts across stages, and even therapies,\r\ndue to a reluctance to select new locations. Patients in the northwest of Brazil would need\r\nstakeholders to consider candidate locations beyond government reference centers or those\r\nwith gene therapy experience, in order to have a treatment center nearby. Regarding facilitation, we find that quick, low-stakes modeling and joint discussion could allow stakeholders\r\nto consider approaches they might not otherwise consider.",
        "authors": [
            "Sarah R. Wertheimer"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157099",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Curve of Inflation Expectations and Firms’ Investments",
        "abstract": "Using rich survey data on Italian firms, this paper studies the formation mechanisms of inflation expectations at different forecasting horizons. Starting from empirical evidence embedded in firms’ inflation expectation curve, we obtain 3 main findings: (1) firms extrapolate for long forecasting horizons, (2) inflation forecasts overreact (underreact) at long (short) forecasting horizons, (3) long-term inflation expectations impact investment decisions. Specifically, we find that a 1% wedge between the 4-year and 1-year ahead expected inflation is associated with a 0.8% increase in the probability of investing. What motivates this result? After ruling out alternative channels of (1) an increase in expected demand, (2) a decrease in supply of input goods, and (3) an improvement in financing conditions, we claim that a decrease in the perceived cost of capital is the main driver.",
        "authors": [
            "Giuditta Perinelli"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157104",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Envisioning Water: Sustainability and Future-Making in Dubai and Los Angeles",
        "abstract": "The following dissertation explores how the future of water is being imagined, planned and\r\nprepared for in two dryland cities – hyper-arid Dubai and semi-arid Los Angeles – as the\r\nclimate changes and as they face increasing pressures to become more ‘sustainable.’ Both\r\nDubai and LA are cities that have long been deemed unsustainable, but are aiming to\r\nbecome sustainability leaders. Dubai, which relies on energy-intensive desalination and has\r\nhigh water consumption, including in ubiquitous urban greening, is investing heavily in\r\nachieving efficiencies and powering water through clean energy. Los Angeles, which\r\nsources the majority of its water through aqueduct systems from faraway places where\r\nwater is becoming increasingly taxed, is looking to produce more of its water supply\r\nlocally, and especially through wastewater recycling. Throughout the dissertation, I trace\r\nthe plans, projects, and policies being introduced in this vein to consider how\r\n‘sustainability’ initiatives play out and get negotiated through the socio-political and\r\npolitical economic structures in the two cities to unique effects.\r\nTo get at sustainability’s variegated forms and effects, I first view sustainability as a\r\n“boundary object” (Star and Griesemer) and “technology of imagination” (Pederson et. al).\r\nTreating sustainability as a “boundary object” that is shared but viewed differently by\r\nactors enables me to hone in on the interests and forces - sometimes countervailing - that\r\nshape sustainability projects. Treating it as a technology of imagination allows me to get at\r\nthe imaginative effects that sustainability projects constitute. Second, I consider how these\r\ninterests, forces, and effects emerge from and get mediated through entrenched structures\r\nlike bureaucratic systems, accumulation regimes, and sunken investments, which produce\r\na stickiness to infrastructures and infrastructural visions that renders change challenging,\r\nslow, and incremental. As such, I show, for instance, how Dubai’s highly centralized\r\ngovernance structure and foreign-investment development model produce an emphasis on\r\nsustainability’s enhancement of the city-state’s competitiveness agenda that can belie\r\nlarger eco-realities, while LA’s fragmented institutional, regulatory, and financing scapes\r\ncomplicate collaboration on recycling projects which span across and exceed individual\r\ninstitutional mandates.\r\n\r\nFinally, alongside the municipal projects I focus on, I also look at visions of the future by\r\nartists, designers, and architects to get at how the arts might provide alternatives that in some cases could help get beyond the stickiness of sustainability as it is currently being\r\nimagined.",
        "authors": [
            "Nadia Christidi"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157118",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Optimizing Wildfire Suppression: A branch-and-price-and-cut approach",
        "abstract": "In periods of intense, synchronous wildfire activity, fire system managers must make rapid fire prioritization decisions over a disperse geographic area with limited suppression resources. This thesis defines the Wildfire Suppression and Crew Assignment Problem, which optimizes resource allocation to triage fires based on damage risk, crew availability and spatiotemporal dynamics. We formulate a two-sided set partitioning model on time-space-rest networks for crew assignments and time-state networks for fire damage, with linking constraints between both; this representation can encode a broad class of non-linear wildfire spread models and diverse suppression objectives. To solve it, we develop a two-sided column generation algorithm that generates fire suppression plans and crew routes iteratively. We embed it into a branch-and-price-and-cut algorithm to retrieve an optimal integer solution, using novel special-purpose cuts that augment generalized-upper-bound cover cuts and a novel branching rule that leverages dual information from the linking constraints. Extensive computational experiments show that the algorithm scales to practical problems that remain otherwise intractable. The optimization methodology can provide high-quality solutions by jointly optimizing wildfire triaging and crew assignments, resulting in enhanced wildfire suppression effectiveness.In periods of intense, synchronous wildfire activity, fire system managers must make rapid fire prioritization decisions over a disperse geographic area with limited suppression resources. This thesis defines the Wildfire Suppression and Crew Assignment Problem, which optimizes resource allocation to triage fires based on damage risk, crew availability and spatiotemporal dynamics. We formulate a two-sided set partitioning model on time-space-rest networks for crew assignments and time-state networks for fire damage, with linking constraints between both; this representation can encode a broad class of non-linear wildfire spread models and diverse suppression objectives. To solve it, we develop a two-sided column generation algorithm that generates fire suppression plans and crew routes iteratively. We embed it into a branch-and-price-and-cut algorithm to retrieve an optimal integer solution, using novel special-purpose cuts that augment generalized-upper-bound cover cuts and a novel branching rule that leverages dual information from the linking constraints. Extensive computational experiments show that the algorithm scales to practical problems that remain otherwise intractable. The optimization methodology can provide high-quality solutions by jointly optimizing wildfire triaging and crew assignments, resulting in enhanced wildfire suppression effectiveness.",
        "authors": [
            "Jacob Wachspress"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157098",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Contours of the Cloud: Dissecting the Real Estate Investment Decisions of Data Center Operators",
        "abstract": "This thesis investigates the real estate investment decisions of data center operators, with a focus on how key infrastructure characteristics influence data center development. Using a sequential econometric approach, the research applies both a logit and a hedonic model to evaluate the importance of various factors. The logit model explores the likelihood of data center development at the county level, highlighting geographical characteristics. The hedonic model examines the impact of specific site attributes, such as proximity to power infrastructure and fiber, on the scale of data center facilities in megawatts. The findings suggest that colocation data centers prioritize connectivity, electrical infrastructure, and urban proximity, while the location of hyperscale facilities is more variable and less predictable. This study enhances our understanding of how modern technological demands, particularly in the AI era, shape real estate strategies and offers insights into future trends in digital infrastructure investments.",
        "authors": [
            "Robert Logan Fawcett"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157114",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "State Estimation in Dynamical Robotic System with Non-Gaussian Noise",
        "abstract": "State estimation is critical for robot operation. Most estimation algorithms assume that the robotic sensor measurements are contaminated by Gaussian noise. However, in practical applications, the noise is often non-Gaussian, heavy-tailed, or even multi-modal. In this thesis, we develop algorithms that perform state estimation in dynamical systems with arbitrary noise and prove their theoretical guarantees. We tackle two challenging state estimation problems: multi-model point cloud registration and state estimation in polynomial dynamical systems, both contaminated by non-Gaussian noise. In the multi-model 3D registration problem, we are given two point clouds picturing a set of objects at different poses (and possibly including points belonging to the background) and we want to simultaneously reconstruct how all objects moved between the two point clouds. We propose a simple approach based on Expectation-Maximization (EM) and establish theoretical conditions under which the EM approach recovers to the ground truth. We evaluate the approach in simulated and real datasets ranging from table-top scenes to self-driving scenarios and demonstrate its effectiveness. For state estimation in polynomial systems corrupted by arbitrary noise, we develop a new filtering approach called the Generalized Moment Kalman Filter (GMKF). The GMKF formulates the prediction and update steps as polynomial optimization problems (POP) and solves them using moment relaxations, carrying over a possibly non-Gaussian belief. In the linear-Gaussian case, GMKF reduces to the standard Kalman Filter. We demonstrate that GMKF performs well under highly non-Gaussian noise and outperforms common alternatives, including the Extended and Unscented Kalman Filter, and their variants on matrix Lie groups. We also showcase applications to challenging landmark-based and lidar-based robot localization problems.",
        "authors": [
            "David Jin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157096",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Ultramafic Alteration and the Cooling of Earth and Mars",
        "abstract": "This thesis deals with the influence of `ultramafic' rocks over the climate of planets. Ultramafic rocks, rich in Mg and Fe, are the most common rocks on Earth but exist primarily in the mantle and rarely outcrop on the surface. They are incredibly unstable under Earth's surface conditions where they are altered via incongruent reactions which form clay minerals, iron oxides, and ultimately release cations to the ocean. Due to their instability, they play an out-sized role in Earth's long-term carbon cycle. My first chapter investigates a hitherto unappreciated mechanism by which ultramafic rocks serve as a carbon sink, through the formation of high-surface-area clays and the resultant burial of organic carbon. I use a combination of mineral weathering models and proxy data to show that this mechanism has contributed to the glaciations of the Palaeozoic (541 - 252 Ma).\r\n\r\nUnlike Earth, igneous rocks on the Martian surface are frequently of ultramafic composition. My second chapter argues that the alteration of these Martian ultramafic rocks was fundamental in the cooling of the planet from a habitable surface with liquid water to a cold and icy planet, largely devoid of an atmosphere. I show that the same high-surface-area clay minerals which bury organic carbon on Earth are prevalent enough on Mars to store the bulk of its initial 1-4 bar atmosphere as adsorbed methane. I postulate that this methane was formed abiotically during hydrothermal alteration of ultramafic rocks, a process which is observed in ultramafic systems on Earth. I show that this framework reconciles the histories of \\dc{} and atmospheric loss-to-space on Mars.\r\n\r\nMy final chapter quantifies the effects of the alteration of ultramafic and mafic rocks across the Taconic orogeny in Newfoundland, Canada. This collision exposed one of the most well-studied ultramafic bodies on Earth, the Bay of Islands ophiolite, and closely preceded global cooling in the Middle-Late Ordovician (470-445 Ma). I present a new method, leveraging both geochemical analysis and modelling of basin sediments, to infer ancient silicate weathering fluxes. I show that the relative weathering rate in this region increased dramatically during the Taconic orogeny. This method could be applied throughout systems with tectonically-driven changes in surface lithology to build a fuller understanding of the forces which modulate Earth's climate. \r\n\r\nMy work asks as many questions as it answers but tries to honestly portray the uncertainties associated with the application of quantitative methods in noisy, geologic systems. I hope that in trying to meaningfully constrain these processes I plant seeds of inquiry from which myself and others can one day make more concrete statements of the cause and effect between tectonics and climate.",
        "authors": [
            "Joshua Murray"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157093",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Studies in biotic persistence and the taxonomic stability of traits over geological time",
        "abstract": "It is increasingly recognized in evolutionary biology that biotic processes and pathways can be viewed as being under selection as well as organisms or populations. This view is particularly relevant when considering the history of the Earth’s biosphere over geological timescales, and the evolution of groups interacts with the evolution of processes in shaping the biosphere over time. This thesis considers a novel selection mechanism proposed to be operating on clades based on their age and tests its presence in marine animals over the Phanerozoic (Chapter 2); it also seeks to understand the interaction between some microbial traits and lineages over geological time as well as considering the implications of this interaction on the traits’ longevity. Chapter 3 considers the production of photoprotective pigment scytonemin, and Chapter 4 considers microbial iron oxidation. In these two chapters, I describe a metric called “clade fidelity” of a trait to describe its tendency to be associated with certain lineages and vertically inherited within them throughout the trait’s history, and I examine the relationship between a trait’s clade fidelity and its ecological context as well as evolutionary fate. The case studies in the thesis show that the proposed theoretical frameworks are applicable in practice and carry considerable explanatory power for the understanding of evolutionary processes on a scale of planetary history.",
        "authors": [
            "Erik Tamre"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157121",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Impact of Central Bank Real Estate Purchases on Asset Prices",
        "abstract": "This paper estimates the impact of central bank real estate purchases on asset prices, demonstrating an increase of 0.1% to 0.2% of Real Estate Investment Trust (REIT) prices in the hours following a typical intervention of 0.014% of market capitalization. At longer horizons, the purchases do not appear to have a significant aggregate effect. The primary identification strategy exploits the nature of the Bank of Japan’s (BoJ) policy rule, which triggers purchases when the Tokyo Stock Exchange Real Estate Investment Trust index falls below a certain threshold. Alternative research designs that exploit the counter-cyclical nature of the BoJ’s policy rule and cross-sectional variation in the eligibility of REITs for BoJ purchases are also considered. Overall, these findings are inconsistent with the predictions of canonical and recent models of asset pricing.",
        "authors": [
            "Quentin Batista"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157083",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Analysis of Seoul Apartment Prices during Population Decline Era",
        "abstract": "Since the early 2020s, South Korea has faced a population decrease due to the lowest birth rates globally, but the apartment prices in capital regions covering Seoul, capital city of South Korea and Gyeonggi-do, have ironically shown a consistent upward trend. This thesis explores the persistent rise in apartment prices despite diminishing population in Seoul, providing insights into the economic and social factors affecting this trend. Through an analysis of the characteristics of Seoul apartments, including the unique Jeonse system, and the impacts of population trends by region, this research demonstrates the broader implications of single person household trends and aging population demographics. Furthermore, comparative case studies from Japan and France supports the relationship between aging populations and housing markets. By applying various indices related to apartment prices, this study demonstrates the correlations between apartment prices and demographic changes, consequently exploring the potential future scenarios for the housing market in Seoul.",
        "authors": [
            "Moohyun Cho"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157103",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Quantifying the Severity of a Cybersecurity Incident for Incident Reporting",
        "abstract": "In the field of cybersecurity, the lack of standardized data collection and incident reporting\r\nmethods pose significant challenges to address and respond to incidents affecting critical\r\ninfrastructure. Various initiatives aim to resolve this issue by mandating the collection of\r\ndata on cyber incidents; however, there is often a lack of clear guidelines on how the collected\r\ndata will be utilized effectively.\r\nThis paper introduces the Cyber Incident Severity Scale (CISS), a framework designed\r\nto guide the selection of relevant data for analysis and communicate the severity of a cybersecurity incident. By drawing insights from established scales in other fields, such as\r\nnatural disasters and public health, this research produces a single score for a reporting\r\nentity which can be aggregated to determine the overall severity of an incident. The ability\r\nto swiftly assess and score an incident is a critical tool to quantify incident severity and\r\nprioritize response, support policy development, and bolster the overall security of critical\r\ninfrastructure.",
        "authors": [
            "Chelsea Foushee Conard"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157124",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Location, Location, Substation? How Battery Energy Storage Systems (BESS) Can Create Value in Unexpected Places",
        "abstract": "The transition to renewable energy is a critical step in reducing global carbon emissions, yet it introduces new challenges for the aging electrical grid, particularly in urban areas. Battery Energy Storage Systems (BESS) are emerging as key infrastructure in this transition, capable of enhancing grid resiliency, managing peak loads, and facilitating the integration of renewable energy sources. Federal and state incentives and a recent sharp decline in the cost of battery cells have made BESS development economically viable. This thesis explores the potential of BESS to create public and economic value in underutilized urban spaces through the exploration of a hypothetical redevelopment proposal for the Alewife MBTA Complex in Cambridge, Massachusetts.\r\n\r\nThe Alewife MBTA Complex presents significant challenges for redevelopment due to the high cost of demolishing the decaying existing structure. However, its proximity to a major substation and the increasing local demand for electricity make it an ideal candidate for a BESS project. This thesis demonstrates how integrating energy storage into the redevelopment of the site can enable an otherwise financially infeasible project.\r\n\r\nThe paper provides an overview of the BESS development process, detailing each phase from creating a business strategy to disposition. It offers insights into the common challenges encountered, and how these might be navigated to optimize project outcomes. By breaking down the development timeline and key decision points, this thesis serves as a practical guide for real estate professionals to gain familiarity with Battery Energy Storage Systems. \r\n\r\nThrough detailed financial modeling and analysis, including sensitivity testing, this research quantifies the expected financial performance of a BESS project at the Alewife site. The study concludes that BESS can unlock ‘found value’ in sites with little other economic potential. The findings suggest that incorporating BESS into real estate development projects can provide substantial public benefits, including enhanced grid resilience, lower energy costs, and increased property values, making it a strategic tool for urban planners and developers.",
        "authors": [
            "Neal Schutt"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157127",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Lessons From President Moon Jae In’s Housing Policy and The Road to Affordable Home Ownership in Seoul, South Korea",
        "abstract": "A fundamental goal of housing policy is to provide a safe and quality place to live for the population. This thesis studies the provision of affordable homeownership in Seoul, South Korea and particularly for non-homeowners and first-time buyers who did not have an opportunity to participate in the housing boom that the previous generations experienced. For Seoul, 58% of the population is non-homeowners. First, this thesis provides a brief introduction to the Korean housing history. Second, it discusses the housing policy under President Moon Jae In, and how housing prices soared under his administration due to misguided efforts. Finally, it describes the necessary path towards mitigating the housing affordability crisis that has been created in Seoul using both supply and demand side arguments.",
        "authors": [
            "Kibong Cho"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157082",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Contract, the Contractor, and the Capitalization of American Building",
        "abstract": "The heroic claims of twentieth-century architects notwithstanding, modern American architecture was built by general contractors. This new type of builder was unknown to US Americans before the Civil War, but by the turn of the twentieth century they commanded a powerful position in the widening gulf between architects and the construction of their buildings. Operating at the critical inflection point between projection and materialization, paper and concrete, contractors appealed to investment-minded clients as fellow businessmen, offering them what neither craft builders nor professional architects could deliver: a completed building, for a fixed price, on a guaranteed schedule.\r\n\r\nThis dissertation tells the story of how building became contracting in the United States during the long nineteenth century. Known to legal historians as the age of contract, the nineteenth century gave rise to a constellation of juridical and economic ideas that revolved around a vision of social relations modeled on market exchange and possessive individualism. Revealing the ideological and institutional foundations of today’s construction industry, the dissertation shows how nineteenth-century thinking about contract, freedom, value, and risk shaped the architectural building contract, the limits of the architecture profession, the practice of general contracting, and thus the modern relationship between architecture and building.",
        "authors": [
            "Chelsea Anne Spencer"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157336",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Self-similar singularity formation and wellposedness theory for compressible fluids and dispersive PDE",
        "abstract": "In this thesis, we study different problems related to singularity formation and local wellposedness of fluid equations and dispersive PDE. Regarding singularity formation, we construct radially symmetric smooth selfsimilar profiles for the compressible Euler equations which exhibit an implosion type singularity in finite time. This will be the first part of the thesis. The second part of the thesis consists on doing a non-radial stability analysis around those profiles to show singularity formation for adequate small perturbations of the profile. In particular, this stability analysis also allows to conclude existence of singularities for periodic initial data. The stability also allows to obtain singularity formation for the corresponding equation with dissipation: the compressible Navier-Stokes equations. Moreover, the self-similar profiles constructed are also intimately related to dispersive equations, and we will show how to use them to prove finite time singularity formation for some supercritical defocusing NLS equations, using its hydrodynamical formulation. The third part of the thesis consists of the study of a different dispersive equation: the Zakharov– Kuznetsov equation. The equation is a generalization of the KdV equation to higher dimensions with applications in plasma physics. We improve the deterministic local wellposedness in the cyilnder both in the deterministic and the probabilistic setting.",
        "authors": [
            "Gonzalo Cao Labora"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157346",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "From Words to Worlds: Bridging Language and Thought",
        "abstract": "What do we understand when we understand language? Human language offers a broad window into the landscape of our thoughts. We talk about what we see, believe, and imagine, posing questions and communicating our plans. Language, in turn, stocks our mental inventories with new concepts and theories, communicating ideas that we might not otherwise have discovered by thinking on our own even over the course of a lifetime. How do we make meaning from language, and how, in turn, does the meaning we construct from language draw on the other resources and capacities of human thought, from perception, to mental simulation and decision making? This thesis proposes a computational framework for modeling language-informed thinking, organized into two parts. In the first, I overview the overarching framework that makes up the backbone of this thesis, Rational Meaning Construction, which proposes how natural language can construct arbitrary expressions in a flexible, symbolic, and probabilistic language of thought that supports general inferences. I present examples and experiments demonstrating the range of this theory, modeling how concrete propositions and questions in language can update and query beliefs about many different domains of knowledge. In the second section, I turn to language that communicates more abstract conceptual knowledge – generic background concepts and theories that we can learn from language, and which give us building blocks for representing more concrete beliefs. I present three models that build on the basic premises of Rational Meaning Construction to learn new lexical concepts and theories from language. The first models how we can learn new theories from generic sentences that explicitly communicate or implicitly presuppose abstract knowledge. The second elaborates on this model to also incorporate environmental feedback alongside information from language. The third suggests how we can learn the meanings of new words from scratch, with very little linguistic data, using principles of both representational and communicative efficiency to guide learning. I conclude by discussing a open questions that this thesis raises about how we learn and understand language, and outline future directions that might make progress on answering them.",
        "authors": [
            "Lionel Catherine Wong"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157326",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Cheaper Than A Funeral: Considering Ibogaine’s Psychedelic Journey and Therapeutic Potential",
        "abstract": "The past decade has seen a surge of interest in psychedelic compounds as therapeutic medicine. Ibogaine, an indole alkaloid extracted exclusively from an endangered family of shrubs from Central African nations of Gabon and Cameroon, is a psychedelic currently being studied for its unique therapeutic potential. It is also considered the most extreme of the psychedelic drugs currently known to researchers. For the past fifty years, it’s been used to treat severe substance use disorders, particularly with highly addictive opioids and stimulants. In the past ten years, American special operations forces veterans have begun to take ibogaine to treat traumatic brain injuries (TBI). Anecdotal evidence has suggested that the permanent, downstream symptoms TBI patients experience after these injuries are effectively managed after a single ibogaine treatment. Advocacy from the special operations veterans community prompted Stanford University researchers to embark on the first-ever U.S.-based clinical trial of ibogaine to treat TBI. The study, published in January, 2024, further evidenced decades of evidence of ibogaine’s clinical use potential. Yet questions still remain about whether or not ibogaine’s cardiac toxicity can effectively be managed in human patients, as well as the true therapeutic utility of the prolonged period of dreamlike consciousness ibogaine produces in patients. This thesis examines the cases of three patients–all United States military veterans–undergoing ibogaine therapy, examining how the biological impacts of ibogaine, as well as their psychedelic experiences, may have saved their lives.",
        "authors": [
            "Noah Daly"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157210",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Nipah: The history, and future, of one of the world’s most lethal viruses",
        "abstract": "The Nipah virus kills around three quarters of people who contract it, making it one of the most lethal viruses known to infect humans. The virus first emerged in 1998, when hundreds of pig farmers in Malaysia fell ill with fevers and encephalitis, or brain inflammation. Nipah has caused smaller outbreaks in nearby Bangladesh nearly every year since then. The Malaysian farmers appeared to have been infected directly from their pigs, rather than from each other. For a time, there was no clear evidence that Nipah could spread from humans to other humans. That changed in April of 2004, when investigators responding to a Nipah outbreak in a remote district in Bangladesh discovered that the virus was spreading person to person. Pteropus fruit bats, which are native to South Asia, were identified as the natural reservoirs of the Nipah virus. Researchers have spent the last two decades studying the virus’ transmission in bats and how the virus spills over into humans. Institutions across the world have even recently started developing Nipah vaccines. Scientists believe the Nipah strains that currently circulate in humans are likely not transmissible enough to ignite a pandemic in people. That could change. Whether the virus one day evolves to spread better within humans, or hits a particularly susceptible place and thrives, officials worry about what could happen if Nipah ever affects larger populations. The Nipah virus is just one of many zoonotic pathogens that scientists are studying to understand how humanity can prepare for future deadly pathogens.",
        "authors": [
            "Alex Gabriel Viveros"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157115",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Beyond the Ovaries: Renaming a common yet neglected hormonal condition could be the key to unlocking better care for patients",
        "abstract": "PCOS is a common hormonal condition found in 10 to 19 percent of people with ovaries. It frequently causes irregular periods and ovulation and is one of the most common forms of female infertility. However, the effects do not stop there. People with PCOS are at higher risk for a slew of health complications: insulin resistance, sleep apnea, depression, and anxiety. They are also more likely to develop metabolic syndrome—a combination of high cholesterol, high blood pressure, diabetes, and high waist-to-hip ratios. Together, many of these symptoms are risk factors for fatty liver disease or heart attacks and strokes. \r\n\r\nDespite the commonness and potential seriousness of the condition, many patients go undiagnosed, and those with diagnoses frequently go under-treated. The reasons for this are aplenty. PCOS’s cause is unknown. It has no known cure. It looks different from patient to patient. Its research is underfunded. Physicians do not learn much about it in medical school. \r\n\r\nBut one reason at the root of it all, some experts say, is how tightly this condition has been intertwined with reproduction and fertility. Over the past decade, researchers and physicians who specialize in the condition have been pushing for everyone to recognize PCOS for what it is: a full-body endocrine syndrome with wide-reaching effects on health and quality of life. And one way to combat these is to change something fundamental about the condition: its name.",
        "authors": [
            "Lily Stewart"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157123",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Kent Kiehl’s Search for the Criminal Brain America’s self-proclaimed “psychopath whisperer” says he can predict criminality in incarcerated people. Is the legal system buying it?",
        "abstract": "Since the 19th century, researchers have attempted to uncover the biological roots of criminality. The process has been both scientifically dubious and ethically fraught. While biological theories of criminal behavior faded after World War II, they arose again in the 1990s and early 2000s, when new brain imaging techniques collided with a growing interest in understanding how biological drivers of crime, if they exist, could be analyzed to understand, and even predict, criminal behavior. This thesis examines the research and claims of a prominent neuropsychologist within that historical context. He claims to have conducted promising brain research on incarcerated people that could uncover biological markers of criminal behavior, or even predict future criminality. Yet methodological and ethical questions have been raised about his research. Is it scientifically valid to have a brain-based view of criminal behavior? Is it ethically valid to assume that criminal behavior can be decoded from the brains of people incarcerated in a system that disproportionately impacts people of color and those from low socio-economic backgrounds? His critics are doubtful.",
        "authors": [
            "Sarah Rebecca Hopkins"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157116",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Trouble on the Range: When Does a National Park Become a Bison Zoo?",
        "abstract": "Yellowstone National Park is often credited for bringing American bison back from the brink of extinction. In 1902, there were merely 25 individual bison in the park, but now, Yellowstone’s herd fluctuates between 3,000 and 5,500 animals. Over the past century, the national park’s conservation effort pushed bison into the public spotlight. The animal has become a symbol of the great American west, and recently, bison were named the US National mammal.\r\n\r\nMany of Yellowstone National Park’s bison reside in the park’s northern range, a 380,000-acre network of valleys, mountains, and river basins. One of these valleys, Lamar, is a hotspot for bison viewing, but, unbeknownst to many casual tourists, the area has also long-been the center of an intense scientific debate. \r\n\r\nBefore thousands of bison covered the floor of Lamar Valley, a different hooved mammal stood in their place. Over the 19th and 20th centuries, hunting pressure, federal policy, and unnatural predator-prey relationships made Yellowstone’s northern range a haven for elk herds. As they proliferated in peace, elk chewed through the northern range’s preexisting ecosystems. Their appetites took a severe toll on native flora, which in turn, shrank habitats for other wildlife. Debates about park management and range science broke out between independent scientists and Yellowstone officials. The disagreements lasted for decades. But in the late 1990s, a whirlwind of decisions reduced (and maintained) elk herds to a more manageable level. Scientists thought that finally, the northern range’s native flora and fauna might have a chance to recover. \r\n\r\nFor many years, it seemed like an ecological revival was beginning. But not in some places. Regrowth in regions of the northern range where bison heavily grazed were lagging behind. A growing body of research suggests that bison are having a similar adverse effect on Yellowstone’s ecosystems as the historic overabundance of elk. In Lamar Valley, many riverbanks are still devoid of trees, beavers are few and far between, and non-native species are increasingly prevalent. \r\n\r\nYellowstone officials disagree with this consensus. Instead, they point to research showing how bison positively impact the landscape. In 2023, the park released a bison management proposal that has only intensified the debate. The proposal dismissed a large body of research as insignificant, going on to suggest an increase to the size of the park’s bison herd. In addition to concern about ecological degradation, many independent researchers are perplexed as to why Yellowstone — the world’s first national park — is seemingly intent on diminishing or ignoring the significance of legitimate scientific research.",
        "authors": [
            "Sophia Hartley"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157119",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Phight for Phage: Understanding Bacteriophage Therapy in Aquaculture and Human Health",
        "abstract": "In the wake of the antibiotic resistance crisis, alternative options to prevent and treat bacterial infections are desperately needed. Researchers across the world are turning to the most abundant \r\nbiological particle on our planet: bacteriophage. Often called phage, these microscopic viruses infect bacteria, and their high specificity and incredible abundance may make them viable treatment options. Scientists have known about phage for over a century, but renewed interest over the past few decades has spurred a wide variety of research into the biology and applications of these viruses. The benefits, and some of the challenges, of phage therapy for both \r\naquaculture and human health are discussed here.",
        "authors": [
            "Eva Cornman"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157095",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "No One Wants To Be A Parasitologist: The Shrinking Field of America's Least Favorite Animals",
        "abstract": "Parasites have a bad rap. Most people think of them as scary, gross, or both, but they are also diverse creatures that have evolved in and on every animal and ecosystem on the planet. Parasitism is the most successful way of life for an animal — representing more than 40% of all species — and the wormy and crawly creatures it encompasses are vastly understudied. An increasing volume of research shows that parasites play important ecological functions, from keeping animal populations in check to stabilizing food chains to driving evolution and biodiversity. While parasites can cause horrible human suffering, especially in countries without reliable clean water or sanitation systems, only a fraction of parasites affect humans, with estimates as low as 0.1%. \r\n\r\nAs climate change and habitat loss threaten animals, so too do they endanger the parasites that live on and inside them. At the same time parasite biodiversity faces shrinkage, the field of parasitology reckons with its own crisis: membership in the American Society of Parasitologists has declined by 76% in the past 50 years, and many of the world’s most important parasitologists are elderly or dead. To revitalize the field, parasitologists are charming younger generations with parasite Pokémon cards and stuffed animals and attempting to integrate parasites into global conservation programs. One main question is on parasitologists’ minds: How can they convince people to discover, catalog, and understand the world's parasite biodiversity before parasites, the field’s leaders, and their valuable knowledge die off?",
        "authors": [
            "Hannah Richter"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157101",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Protein spatiotemporal dynamics in gene regulation and disease pathology",
        "abstract": "A cell orchestrates billions of proteins to the right place at the right time to perform diverse cellular processes. Over the decades, this field has been evolving by integrating advances in microscopy, biochemistry, and molecular biology to unravel the intricate mechanisms governing protein spatiotemporal dynamics as well as the functional consequences. This thesis focuses on the physical motions of proteins at a length scale of tens of nanometers to several microns, where the apparent diffusion and the condensate dynamics of assembly and disassembly are specifically studied. In the studies presented in this thesis, the functional relevance of protein motion is exemplified in the context of gene regulation and disease pathology. We find that the apparent diffusion of transcription factors (TFs) is preferentially partitioned into slowly diffusing states by interacting with RNA, leading to enhanced chromatin occupancy and gene expression (Oksuz et al., 2023). The assembly and disassembly dynamics of transcriptional condensates are coupled to the active RNA synthesis, linking gene expression and the spatiotemporal organization of transcriptional proteins in a feedback loop (Henninger et al., 2021). In addition to transcriptional proteins, we find insulin receptors (IRs) are incorporated in dynamic condensates in normal cells to perform metabolic signaling transduction. In insulin-resistant cells which could occur in chronic diseases such as type 2 diabetes (T2D), IR signaling is dysregulated, associated with diminished IR condensate dynamics of assembly and disassembly (Dall’Agnese et al., 2022). Furthermore, pathogenic signaling reduces the mobility of key proteins–both inside and outside of condensates—that act in many cellular functions. Such reduced protein mobility under diverse pathogenic stimuli, termed proteolethargy, may account for diverse cellular dysregulation seen in chronic disease (Dall’Agnese, Zheng, Moreno et al., 2024).",
        "authors": [
            "Ming Zheng"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157596",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "From Research to Search: Technologies and Techniques of Legal Research, 1880-1980",
        "abstract": "In 1964, the Ohio State Bar Association (OSBA) embarked on a project to harness computer technology to automate legal research. After three years of investigation, it established the Ohio Bar Automated Research (OBAR) organization and contracted a local computer company, Data Corporation, to develop an electronic legal research service. Despite initial skepticism and mounting costs, these lawyers and technologists managed to launch a working service by 1969. The service – also named OBAR – was available through remote consoles placed in law firms, libraries, and government offices. By 1973, an improved system was relaunched as Lexis, a soon-to-be-national legal information retrieval service. Lexis went on to become a staple of American legal practice while OBAR gradually faded out of the picture. This dissertation tells the story of the OBAR system and its promise of automating legal research. \r\nWhat did it take for lawyers to begin using and trusting computer technology for their work? I argue that the automation of legal research required both conceptual and material rearrangement. Legal research was a deeply social activity supported by an intricate infrastructure of people, technologies, and techniques. To be trusted and used, the computer had to be constantly charged with meanings, often contradictory ones. It was presented as a tool that would be integrated into an existing legal research process and a technology that would overhaul legal research. The computer was attributed mechanical qualities, like being objective or operating according to instructions, and human ones, like being sophisticated and capable of conversation. These contradictory meanings, along with the gap between promise and reality, were constantly sewn together as part of the computerized system’s development and marketing process. \r\nTo capture the process of automation, this dissertation traces legal research practices before the computer, the development process of the new technology, and the competing notions of trust and credibility in its early years. The first section traces the splintering of legal research into a distinct task that could be taught, delegated, and automated. In the first chapter, I focus on print legal research technologies and legal research instruction through the first half of the 20th century. I show that innovations in legal research went hand-in-hand with a reallocation of legal work among lawyers and non-legal staff. Examining legal research manuals shows that instruction in types of law book gradually gave way to a more systematic approach to legal research. The second chapter considers the history of legal research work through an examination of the law office and the distribution of labor within it. It shows that the development of legal research into a distinct task that could be delegated was intertwined with social, professional, and technological developments at mid-century. The third chapter describes how the specter of automation focused bar associations’ attention on legal research practices. It shows that legal research fit into a social and professional setting. Lawyers relied on an array of technologies and personnel to produce answers to legal questions. As a whole, the section argues that three factors joined to make legal research into a distinct task, thus making its automation possible: the development of instructional materials and courses on legal research, the growth and bureaucratization of law firms, and the introduction of women and machinery into the law office in the 20th century.\r\nTwo chapters and two short excursuses make up the second section, which focuses on the development and early adoption of the OBAR system. In chapter four, I examine the entanglement of technological choices and ideals in the process of developing the OBAR system in the 1960s. I show that the focus on direct use by lawyers was meant to cast suspicion on human judgment while touting the computer as an objective and trustworthy tool. Excursus one unpacks OBAR’s promise of an interactive system. It shows that at the same time as the system was likened to human dialogue, it offered a substantially different interaction with court cases, a process that altered the epistemic and social setting of legal research. Chapter five considers the reactions of OBAR’s early users as communication consoles were placed in law firms and libraries across Ohio in the 1970s. Relying on call reports and correspondence, I examine controversies around the system’s accuracy and credibility. Excursus two tells the story of what came out of the system’s promise in light of later developments. Focusing on the chasm that developed between lawyers and technologists in defining the system in the 1970s, it explains how an approach that focused on the system as a product prevailed over an approach that viewed the system as a service to the profession. To become a successful national product, Lexis had to shed its connections to the organized bar and give up any social aspirations.",
        "authors": [
            "Alex Reiss Sorokin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157589",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Optical and core-level X-ray spectroscopy of correlated two-dimensional materials",
        "abstract": "The intersection of low-dimensionality and strongly correlated electrons in van der Waals (vdW) materials offers a rich landscape of ordered phases and associated excitations for potential applications in nanoelectronics. The coupling between distinct degrees of freedom in correlated materials provide routes to realize novel functional properties, which can be further manipulated by the high tunability intrinsic to vdW materials through, e.g., heterostructures and doping. However, identifying the mechanism of correlated phases poses a fundamental challenge due to coexistent and competing orders. This requires detailed knowledge of the microscopic interactions/excitation spectra, methods to disentangle the individual roles of coexistent orders, and selective probes of symmetry-breaking within different coupled degrees of freedom. In this thesis, we demonstrate the utility and complementarity of resonant X-ray spectroscopy and symmetry-selective optical probes in combination with appropriate external tuning parameters (e.g. strain, pressure, ligand substitution, layer number) for revealing the origin of correlated phases in low-dimensional vdW materials. We first investigate the triangular lattice antiferromagnet NiI₂. Frustrated exchange interactions result in a helimagnetic ground state and spin-induced ferroelectric order, making bulk NiI₂ a type-II multiferroic. Using a combination of optical spectroscopic probes, including Raman, magneto-optics, and second harmonic generation, we demonstrate the persistence of multiferroic order to the single-layer limit. We then aim to resolve the microscopic magnetic interactions and their interplay with the lattice symmetry to identify the origin of the magnetic ground state. Towards this goal, we investigate the magnetic ground state and transition temperature versus hydrostatic pressure and layer number, and directly probe the evolution of magnetic/structural orders with resonant magnetic X-ray scattering/structural diffraction, respectively. From these results, we demonstrate the central role of interlayer exchange interactions and their coupling to the structural symmetry in driving the magnetic ground state of NiI₂. We next investigate the broader class of triangular lattice nickel dihalides, NiX₂ (X = Cl, Br, I), to identify the origin of sharp optical excitations, i.e. excitons, in nickel-based vdW magnets. We employ Ni-L₃ edge resonant inelastic X-ray scattering (RIXS) to access a q-resolved and site-specific view into the excitation spectra. We identify the sharp excitons with spin-forbidden intra-configurational multiplets of octahedrally-coordinated Ni²⁺, which become renormalized by Ni-X charge transfer. We also observe a finite dispersion of these excitations, demonstrating a multiplet delocalization that is controlled by the ligand-tuned charge transfer gap in a process analogous to ground state superexchange. These results establish the microscopic origin of these excitons and provide a mechanism to explain their possible coupling to the magnetic order/excitations. Finally, we study the iron-based superconductor FeSe, which displays a rotational symmetry breaking electronic nematic phase in proximity to unconventional superconductivity without magnetic order. To understand the origin of nematicity, we investigate the ordering of the orbital degrees of freedom using X-ray linear dichroism with in-situ uniaxial strain tuning, electronic transport measurements and structural diffraction. We observe a lattice-independent orbital polarization acting as the primary nematic order parameter. This resolves the orbital origin of nematicity in FeSe and suggests that anisotropic spin fluctuations are the mechanism of unconventional superconductivity.",
        "authors": [
            "Connor Alexander Occhialini"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157567",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Decoding Dark Matter Halos through the Lens of Machine Learning",
        "abstract": "Dark matter (DM) constitutes about 85% of the matter in the Universe, yet its particle nature remains one of the greatest outstanding questions in astrophysics. DM halos act as the scaffolding within which galaxies form, but the specific mechanisms through which they influence galaxy evolution are not fully understood, especially at galactic scales. While cosmological simulations and astrophysical surveys have made significant strides in constraining DM properties, upcoming surveys will generate terabytes of complex, high-dimensional data. It is thus imperative to develop new methodologies capable of interpreting and linking this data with theoretical models. Machine learning techniques, coupled with advancements in cosmological simulations, present a transformative opportunity. In this thesis, I conduct a multi-scale investigation into the nature of DM and its role in shaping galaxies by integrating advanced machine-learning techniques with cutting-edge cosmological simulations. First, I employ simulation-based inference and graph neural networks to infer the mass density profiles of DM halos in dwarf galaxies from their stellar kinematics. Next, I develop a generative model using normalizing flows and recurrent neural networks to reconstruct the mass assembly histories of DM halos in cosmological simulations. Furthermore, I utilize variational diffusion models and Transformer-based neural networks to perform point-cloud modeling of satellite populations under alternative DM models. Finally, I create synthetic surveys for the Gaia surveys from Milky Way-like simulations, bridging the gap between simulations and observations. This thesis demonstrates the transformative potential of machine learning techniques to probe the DM properties and galaxy formation. The methodologies developed herein provide new avenues for interpreting vast and complex astronomical datasets and offer insights that could lead to a deeper understanding of the fundamental nature of DM.",
        "authors": [
            "Tri V. Nguyen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157579",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Studies on the synthesis of bisindole Aspidosperma alkaloids",
        "abstract": "I. Introduction and Background on Aspidosperma Alkaloids\r\nA brief overview of monoterpene indole Aspidosperma alkaloids is discussed. The biosynthesis of the characteristic pentacyclic core from tryptamine and secologanin is summarized. Some representative examples of total syntheses of Aspidosperma alkaloids are discussed.  Synthetic strategies for the synthesis of bisindole members of the family are also examined.\r\n\r\nII. Total Synthesis of (–)-Voacinol, (–)-Voacandimine C, and related congener, (−)-methylenebisdeoxoapodine\r\nWe describe the first total synthesis of complex aspidosperma alkaloids (–)-voacinol and (–)-voacandimine C via a late-stage C7-methylenation strategy inspired by a biogenetic hypothesis. We envisioned rapid access to these natural alkaloids from a common, symmetrical precursor assembled by methylenation of a D-ring-oxidized variant of the structurally related natural product (–)-deoxoapodine. Chemoselective N9-oxidation of a pentacyclic deoxoapodine precursor enabled the synthesis of the corresponding hexacyclic C8-aminonitrile. Stereocontrolled methylenation of a C8-enamine derivative of deoxoapodine, accessed by ionization of the C8-aminonitrile, afforded a symmetrical dodecacyclic bisaminonitrile as a versatile precursor to these bisindole alkaloids. Final-stage, biosynthesis-inspired, controlled reductive opening of the oxolane substructures of this dodecacyclic intermediate provided a unified approach to (–)-voacinol and (–)-voacandimine C, while direct reduction of the same intermediate afforded the structurally related (–)-methylenebisdeoxoapodine.\r\n\r\nIII. Progress Toward the Total Synthesis of Voacandimine A\r\nWe describe our work toward the total synthesis of bisindole Aspidosperma alkaloid, voacandimine A. Key features of the synthetic progress include two routes for monomer synthesis, two methods for complex fragment assembly to form the bisindole structure, and strategies to address the stereochemistry of the ring fusion.",
        "authors": [
            "Taylor Pinto"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157599",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Radiation Shielding Design and Radioactive Waste Assessment of Horizontal Compact High Temperature Gas-Cooled Reactor",
        "abstract": "With the objective that nuclear power plants utilizing small High Temperature Gas-Cooled Reactors (HTGRs) can provide economic, environmentally favorable and reliable electricity and heat for community and industrial purposes, Boston Atomics LLC initiated the design of Horizontal Compact HTGR (HC-HTGR). This work addresses shielding, activation analysis and the decommissioning cost assessment as an integrated part of the design process.\r\nReinforced regular and borated concrete were considered as shielding materials for the reactor building and Reactor Cavity Cooling System (RCCS) tanks. It was found that for locations of the reactor building where the dose rates during normal operation were greater than the Nuclear Regulatory Commission (NRC) limit of 0.1 rem/hr, 175 cm of borated concrete is required. The shielding concerns motivated the decision to separate RCCS tanks from the reactor room with a 75 cm borated concrete wall to ensure that the radiation levels do not exceed the NRC limit. Additionally, several shielding options were proposed to protect steam generator modules from radiation-induced activation.\r\nThe activation analysis was performed for the key equipment and graphite reflector components of the HC-HTGR design. The core barrel made of Incoloy 800H was characterized as a class C waste component after 40 years of reactor operation. It was proposed that 2.25Cr-1Mo alloy be considered as barrel material to decrease activity levels. The reactor pressure vessel (RPV) and RCCS tubes made of carbon steel were characterized as a class A waste component. The graphite reflector components are characterized as Class C level waste.\r\nFurthermore, this work discusses the neutron irradiation effects and their impact on the integrity of the barrel, RPV, and graphite reflector against material property changes. It was found that 2.25Cr-1Mo alloy has a higher radiation resistance due to the higher iron content in the composition. Based on the results, the reactor vessel is safe from radiation damage for 32 years of operation. The data evaluated for the graphite reflectors indicate that the components should be replaced after 20 years before they pass the turnaround point. \r\nThe concentrations of radionuclides computed during activation analysis were used to predict the radiation levels from beta and gamma sources that could be encountered during the disposal of the core barrel and RPV. Based on the obtained data, it is clear that if the barrel is not replaced during operation, the radiation dose rate will remain above acceptable levels, requiring a more rigorous disposal approach. The radiation levels are reduced for the reactor vessel as it was exposed to a lower flux and radiation-induced activation. A similar analysis was performed to derive the exposure dose rate from gamma and beta rays that can be detected by a sensor of a refueling camera. Beta particles will deposit most of the energy in a graphite layer, and the camera will register negligible dose rates. The gamma ray estimates indicate that a more enduring refueling machine is required. \r\nThe results of this work provide the disposal costs for HC-HTGR immediate dismantlement and after a given decay period. Overall, the disposal costs of core barrel, RPV and graphite reflector are $13 million for HC-HTGR design after 40 years of full operation if the billable charge limits are set on radioactivity levels. If this option is not considered, the total disposal costs grow up to $225 million. However, extending the storage up to 10 years would decrease the activity, reducing the cost of disposal.",
        "authors": [
            "Anna Kudriavtseva"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157583",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Natural Language Control for for Visually Interactive Decision Support Tools in Supply Chain Management",
        "abstract": "Supply chains are complex networks where changing one variable can have unforeseen\r\neffects on the entire chain. Interactive supply chain visualizations are useful for understanding these effects, and can lead to decreased cost. However, these interactive visualizations\r\ncan require technical and domain expertise to operate and understand. A solution for this\r\nis natural language interfaces, allowing users to use natural language commands to control\r\nthe visualization. Additionally, natural language interfaces can be difficult to implement,\r\nand require applications specific programming or training. This thesis proposes integrating\r\na pre-trained large language model as the natural language interface. An example application is created using an existing supply chain network visualization application. Various\r\nlarge language models are then evaluated for usability, functionality, and accuracy. We find\r\nthat a state of the art commercial model is able to practically fulfill the role of a natural\r\nlanguage interface, but that open-source large language models are not currently capable of\r\nfunctioning in this way.",
        "authors": [
            "Willem J. Guter"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157594",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The structure of hadrons and other potential phases of QCD",
        "abstract": "Quantum chromodynamics (QCD) is a mathematical theory describing subatomic particles called quarks and gluons, and the strong force that binds them together into protons and neutrons. This thesis centers on two major thrusts of modern QCD research: (1) uncovering the inner quark and gluon structure of the proton, and (2) mapping out other phases of matter that quarks and gluons form as we vary pressure and temperature. To study these topics, we develop, utilize, and synergize tools in quantum field theory (analytics), lattice gauge theory (numerics), and phenomenology (comparing theory to experiment). Specifically, we use new and existing techniques to access precision information about the inner structure of the proton, via the study of transverse momentum distributions, energy correlators, and diffractive processes at colliders. Additionally, we develop new analytic and numerical techniques for studying QCD phase structure inspired by non-Hermitian physics, and probe the possibility of new exotic phases near the QCD phase transition.",
        "authors": [
            "Stella T. Schindler"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157581",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Coherent Terahertz Control and Ultrafast Spectroscopy of Layered Antiferromagnets",
        "abstract": "The central theme of modern condensed matter physics is to understand the emergent phenomena arising from interactions among Avogadro’s number of particles in quantum materials, alongside efforts to control their properties. While powerful transport, thermodynamic, and spectroscopic tools have been developed, they often fall short to reveal the intricate interplay among electronic, spin, orbital, and lattice degrees of freedom. A promising approach involves selectively perturbing one degree of freedom while observing responses in others, made possible by ultrafast lasers with femtosecond time resolution. These advancements not only showcase the capability of ultrafast experiments in understanding complex material properties but also demonstrate the manipulation of ordered phases at ultrafast timescales, thereby opening a laboratory for studying materials in nonequilibrium regime. This dissertation contributes to the ongoing effort of developing new ultrafast spectroscopy tools, utilizing them to probe lattice, magnetic, and electronic properties, and gaining active control over them. Specifically, it investigates the induction of a new magnetic state with net magnetization using intense low-energy terahertz (THz) pulses in the van der Waals antiferromagnet FePS₃. Critical fluctuations near the phase transition are found to enhance both the magnitude and the lifetime of this new state. Additionally, a broadband two-dimensional (2D) THz spectroscopy technique is developed and employed to study interactions among low-energy collective excitations and to directly identify phonons that induce the new magnetic phase. Furthermore, time-resolved spectroscopy in the visible and near-infrared spectral range is utilized to detect a bound state between phonon and electronic states in the sister compound NiPS₃, and to capture a magnetostriction effect in FePS₃ using coherent phonon spectroscopy, that was elusive to conventional diffraction experiments. Finally, second harmonic generation spectroscopy with microscale spatial resolution, is employed to study the multiferroic material NiI₂, demonstrating its persistence down to the single atomic layer — a first of its kind. These findings and tools can potentially be extended to frustrated quantum magnets to control their magnetic phases and potentially detect their collective modes. The 2D nonlinear spectroscopy utilized in this dissertation is gaining attention both theoretically and experimentally as a promising tool for detecting fractionalized spin excitations.",
        "authors": [
            "Batyr Ilyas"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157588",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exciton Dynamics and Anisotropy in 2D Metal Organochalcogenolate Semiconductors",
        "abstract": "Silver phenylselenolate (AgSePh) is a novel hybrid organic-inorganic two-dimensional (2D) semiconductor that belongs to the broader class of metal organochalcogenolates (MOCs). Since its blue-emitting excitonic properties were discovered in 2018, AgSePh has attracted attention from the scientific community. From a fundamental science perspective, AgSePh provides an excellent platform for exploring many-body interactions among quasiparticles (such as excitons, phonons, and photons) due to its large exciton binding energy, strong exciton-lattice interactions, and natural photonic cavity structure. From a technological standpoint, its narrow blue emission, a tunable bandgap through composition control, chemical robustness, in-plane anisotropy, and low-cost, scalable synthetic methods make AgSePh promising candidate for photonic and optoelectronic applications. However, we do not yet fully understand how its excitonic properties arise at a fundamental level. The central aim of this thesis is to elucidate the correlation between structure, inorganic composition, organic ligands, and excitonic properties in these novel hybrid 2D semiconductors. First, we present the synthesis, structural and optical properties of 2D AgEPh (E = S, Se, Te) single crystals, colloidal nanocrystals, and thin films. Importantly, the growth of millimeter-sized single crystalline 2D AgEPh (E = S, Se, Te) enables their crystal structure determination via single crystal X-ray diffraction: AgSPh in P2₁, AgSePh in P2₁/c, and AgTePh in P2₁/c. Second, we explore the underlying mechanism of light emission in AgSePh and AgTePh. Despite having the same crystal structure, these compounds exhibit strikingly different excitonic properties: AgSePh shows narrow photoluminescence (PL) with a minimal Stokes shift, while AgTePh exhibits broad PL with a large Stokes shift. Using time-resolved and temperature dependent optical spectroscopy, combined with sub-gap photoexcitation studies, we demonstrate that the exciton dynamics in AgSePh films are dominated by the interaction of free-excitons with extrinsic defect states, whereas the dynamics in AgTePh are dominated by intrinsic exciton selftrapping behavior. Third, we study alloying between AgEPh. we demonstrate that AgSePh and AgTePh form homogeneous alloys with tunable excitonic properties across all compositions, whereas AgSPh and AgSePh/AgTePh exhibit a miscibility gap. These observations are elucidated by density functional theory calculations and correlated with crystallographic information. Fourth, using polarization-resolved micro-absorption, reflectance, and photoluminescence spectroscopy, combined with the GW plus Bethe-Salpeter equation calculations, we reveal multiple low-lying excitons with in-plane anisotropy in AgSePh and AgTePh. This showcases the richness of excitonic physics in these materials, which arises from their low-symmetry crystal structures. Finally, we show that the electronic and excitonic structure of AgSePh can be engineered through organic functionalization, resulting in giant excitonic anisotropy and a completely different absorption spectrum in 2D AgSePh-F₂(2,3). This divergence in excitonic properties is attributed to the semi 1D Ag chains in AgSePh-F₂(2,3), in contrast to hexagonal 2D Ag network in AgSePh. This finding can be generalized to other blue-emitting 2D AgSePh-R compounds which exhibit either AgSePh-like or AgSePh-F₂(2,3)-like absorption spectra. Overall, this thesis advances the understanding of the structure-composition-excitonic property relationships in these emerging hybrid semiconductors, paving the way for future investigations into this exciting material family.",
        "authors": [
            "Woo Seok Lee"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157590",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Collagen-mimetic peptides for diagnosis and analysis",
        "abstract": "Collagen, the most abundant protein in the human body, is an essential scaffold for tissue development, regulation, and homeostasis. As a major structural component of the extracellular matrix, collagen is not static. Rather, it is highly diverse and dynamic, actively participating in tissue physiology. Collagen can be a challenging protein to study due to its massive size and heterogeneity across subtypes. A valuable tool to study and better understand collagen is a technology known as collagen-mimetic peptides (CMPs), which are synthetic peptides that mimic the natural structure of collagen. These peptides can be applied to study collagen structure and function, from its macromolecular architecture in tissues to the significance of molecular modifications on its amino acid sidechains. This thesis explores the application of CMPs in diagnostic applications, in which CMPs detect aberrations in native collagen, and analytical contexts, in which CMPs act as a simplified system to understand collagen biochemistry. Chapter 2 investigates the ability of CMPs to identify collagen remodeling in a mouse model of pulmonary fibrosis, demonstrating their potential as non-invasive diagnostic tools for fibrotic diseases. Chapter 3 analyzes the collagen-rich desmoplastic reaction surrounding PDAC in murine models and human samples, highlighting the utility of CMPs in characterizing tumor microenvironments. Finally, Chapter 4 examines the structural implications of threonine phosphorylation on collagen stability, showcasing the value of CMPs in studying posttranslational modifications. The findings discussed in this thesis lay a foundation for future CMP applications in targeted drug delivery and biomaterials design.",
        "authors": [
            "Isabella M. Borgula"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157566",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Photocatalysis in a New Light: A Biohybrid Approach for Improved Reactivity with Tunable, Low-Energy Light Excitation",
        "abstract": "Since the advent of photoredox catalysis, much thought has been devoted to the development of exciting reaction modalities and the catalysts which perform these reactions. Less thought has been placed into the specific aspects of light absorption as the key step in photocatalytic mechanisms. Natural photosynthetic systems drive the high-energy reactions of photosynthesis with efficient and broadband energy capture. They provide a blueprint toward optimizing these processes in synthetic systems. In photosynthesis, both light capture and reactivity have been optimized by separation into distinct sites. The dominant process by which absorbed sunlight is transferred between these sites is through resonance energy transfer, which is highly efficient over long distances. This work highlights that light capture and energy transfer are crucial steps for the design of highly efficient photocatalysts in the future.\r\nChapter 1 describes the relevant structures in natural photosynthesis as inspiration for synthetic approaches, the different mechanisms of energy transfer, and examples of photocatalytic systems that harness such excitation transfer processes to improve performance. Chapter 2 reports the synthesis of a biohybrid photocatalyst inspired by the modular architecture of photosynthetic apparatus which conjugated a photosynthetic light harvesting protein to a transition metal photocatalyst. Spectroscopic investigation found that absorbed photoenergy was efficiently funneled from the light harvester to the photocatalyst. The utility of the biohybrid photocatalyst was demonstrated via an increase in yields for two test reactions, including enabled reactivity at red wavelengths where the photocatalyst alone does not absorb. Chapter 3 establishes the power of incorporating nature’s design into non-natural photoenzymatic catalysis, generalizing the approach to other systems and methodologies. Photoenzymes require high-intensity light to function because of the poor absorption properties of their photoactive intermediate. A conjugate composed of a covalently linked photoenzyme and light antennae separates light capture from catalysis. Spectroscopic characterization of the conjugate showed the presence of efficient energy transfer from the light-harvesting components to the photoenzyme. In the presence of energy transfer, a maximum ~4-fold increase in product yields was observed as well as enabled reactivity. Chapter 4 highlights spectroscopic exploration into emerging molecular catalyst species. Finally, Chapter 5 provides an outlook to the future possibilities of the topics presented herein.",
        "authors": [
            "Paul T. Cesana"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157600",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Precision Metrology with Ytterbium Ions for New Physics Search",
        "abstract": "Modern physics faces a growing discrepancy between the success of the Standard Model and the body of evidence pointing to the New Physics beyond it. A powerful method of New Physics searches is using quantum sensing tools based on Atomic, Molecular, and Optical physics. In particular, the modern optical atomic clocks demonstrate unprecedented accuracy and precision. Complementary to high-energy searches with particle colliders, the atomic clocks are used to place stringent bounds on tests of fundamental physics. One of the possible candidates for physics beyond the Standard Model is a carrier of a fifth force. Such a hypothetical particle that mediates interactions between leptons and quarks can potentially be detected in a tabletop atomic clock experiment. In particular, the isotope shift measurements may show sensitivity to coupling induced by such particles. In this thesis, we describe the efforts to place bounds on this particle using isotope shifts of optical transitions in Ytterbium. We conduct the isotope shift experiment by measuring ions one at a time and in a co-trapped configuration following the protocol of correlation spectroscopy. We study the systematic uncertainty budget for both types of measurements. We apply the King plot method to isotope shift spectroscopy data and observe the King nonlinearity. Using the analysis of the nonlinearity patterns, we determine the significance of the second source of the King nonlinearity with a currently unknown source.",
        "authors": [
            "Evgenii Kniazev"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157564",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Radioactive Atoms and Molecules for Fundamental Physics",
        "abstract": "The Standard Model (SM) of particle physics and the theory of General Relativity represent two of the greatest achievements in physics in the past century. However, despite their success, many experimental observations remain unanswered: What is the nature of Dark Matter and Dark Energy? Why is there so little antimatter in the Universe? Why is gravity so weak compared to the other fundamental forces? These questions point to the existence of new phenomena waiting to be discovered. High-precision laser spectroscopy experiments using atoms and molecules emerged as a fruitful approach for searching for new physics effects. Recently, atoms and molecules containing short-lived radioactive isotopes have been proposed as particularly sensitive laboratories to search for physics beyond the SM, especially at the nuclear level. However, many atoms containing very short-lived isotopes are still out of reach for spectroscopic investigations, while radioactive molecules have been completely inaccessible experimentally until recently.\r\n\r\nIn this thesis, I will present a series of pioneering experiments aimed at harnessing the power of radioactive atoms and molecules to explore nuclear phenomena, both within and beyond the SM. I will start by describing the first-ever precision laser spectroscopy investigation of a radioactive molecule, radium monofluoride (RaF). I will present measurements of the vibrational, rotational, and hyperfine spectrum of RaF, proving its high sensitivity to minuscule nuclear effects. These experiments allowed the quantification of a feasible laser-cooling scheme for RaF and the observation of the effect of the distribution of nuclear magnetization inside the Ra nucleus on the energy levels of RaF. To our knowledge, this is the first time this effect was observed in a molecule, opening the way for using molecules to benchmark ab initio nuclear theory. Finally, I will present measurements of the ionization potential of RaF, showing its suitability for Rydberg states studies and precise quantum control using external electric fields.\r\n\r\nI will then present the theoretical calculations and the status of an experiment aiming to measure hadronic parity violation using single molecular ions inside a Penning trap. The experiment's goal is to use the external magnetic field provided by the trap to fine-tune molecular energy levels of opposite parity close to degeneracy, thus increasing the signal produced by parity violating nuclear properties. The sensitivity to the sought-after signal is expected to be increased by more than twelve orders of magnitude compared to atoms. This amplification will allow the observation of yet-to-be-measured parity violating effects in a molecule. These measurements will be critical to guide our understanding of electroweak nuclear phenomena.\r\n\r\nFinally, I will show preliminary results obtained from a novel experiment with the goal of enabling laser spectroscopy studies of atoms and molecules containing radioactive nuclei with lifetimes of 1 ms and below. Such isotopes can't be currently studied spectroscopically. Using an event-by-event Doppler reconstruction, our approach could overcome most of the challenges encountered by state-of-the-art experimental techniques, allowing us to extend our reach toward unexplored regions of the nuclear chart. Such short-lived isotopes are of great importance for our microscopic understanding of nuclei as well as for constraining the properties of nuclear matter.",
        "authors": [
            "Silviu-Marian Udrescu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157592",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Ultrafast dynamics in quantum materials probed by time-and-momentum-resolved techniques",
        "abstract": "The interactions of quasiparticles in quantum materials give rise to intriguing phenomena, including magnetism and superconductivity. However, these interactions are often challenging to understand due to the intertwining of multiple degrees of freedom, such as charge, spin, orbital, and lattice. To fully understand such strongly correlated systems, a suite of experimental techniques that respectively probes various degrees of freedom and simultaneously resolves multiple channels, including energy, momentum, time, and space, are highly desired. This poses a significant challenge for the entire community. In this dissertation, I will focus on a series of experiments performed on quantum material systems utilizing several multi-resolution techniques. Ultrafast electron diffraction (UED) and time-and-angle-resolved photoemission spectroscopy (trARPES) are tools that I co-developed with my colleagues at MIT in the past several years. Supplemented by the time-resolved X-ray diffraction (trXRD) setup at free electron laser facilities around the world, they provide direct access to lattice (UED and trXRD) and electronic (trARPES) structures in quantum materials on an ultrafast timescale of a few hundred femtoseconds. The first part of the dissertation will briefly introduce assorted aspects of ultrafast phenomena as well as the fundamental principles and instrumentation of the several time-andmomentum-resolved techniques. Following the introduction to these time-and-momentumresolved techniques, the second part of the thesis focuses on the coherent acoustic phonons in quantum materials observed with UED. The crystalline lattice is the building block of any solid-state system and, thus, the most important aspect in condensed matter physics research. The study of coherent acoustic phonons, the fundamental coherent excitation of the lattice, could be traced back to the 1980s when solid-state ultrafast lasers were first developed. However, the knowledge about the excitation mechanism was not complete. In this part of the thesis, I will introduce a new pathway for launching coherent acoustic phonons: magnetostriction, and discuss the spin-mediated shear oscillator enabled by this mechanism in van der Waals antiferromagnet. I will further discuss the original methodology I developed that uses coherent acoustic phonon detected with UED as a picosecond timescale \"lock-in\" experiment that senses nano-scale mechanical motions in ultra-thin quantum materials. The last part of the dissertation will focus on charge density wave (CDW) phase transitions in quantum materials. CDWs are systems where strong interplays between electrons and phonons drive the phase transition that causes the modulation of charge density and is thus accompanied by periodic lattice distortions. In this dissertation, I will focus on systems with multiple interacting CDW orders. These systems are ideal platforms for studying the interplays among multiple order parameters. The suite of probes, including UED, trXRD, and trARPES, offers a comprehensive view of CDW systems from both phononic and electronic perspectives. This part of the thesis will examine a series of CDW materials with multiple CDW orders, including ErTe₃, EuTe₄, and, CsV₃Sb₅. Via a series of ultrafast multi-messenger experiments, I will survey various origins and behaviors of CDW interactions and answer longstanding questions about the nature of CDW ground states in these quantum materials. The overarching theme of this dissertation is to establish a paradigm of problem-solving in quantum materials research via a combination of multiple channels acquired from a suite of ultrafast momentum-resolved techniques. Coherent phonons and CDW systems are two of the richest playgrounds in the ultrafast regime. I am going to investigate various cases where an ultrafast laser pulse decodes the intertwined degrees of freedom in quantum materials. The insight developed in these case studies may be carried over to other quantum material systems with emergent quantum states, such as superconductivity and magnetic orders.",
        "authors": [
            "Yifan Su"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157571",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Explorations in two dimensional strongly correlated quantum matter: from exactly solvable models to conformal bootstrap",
        "abstract": "This dissertation presents two projects that touch upon the role of quantum mechanics in classifying phases of matter and their transitions. In the first project, we set out to answer: is it possible to find a lattice model in the Ising universality class that realizes the Kramers Wannier symmetry in such a way that it squares to 1, rather than a lattice translation as in the usual Ising model? Using insights from symmetry-protected topological phases of matter, we answer in the affirmative, with the caveat that the symmetry, beyond being non-onsite, actually acts on a Hilbert space that is not a local tensor product. The second concerns the nature of the Neel-VBS deconfined quantum critical point. This is thought to be described by the noncompact CP¹ model, which we argue to be continuously connected to the theory accessed by the 2 + ε expansion for the O(3) NLSM. To shed light on the nature of the DQCP, we perform conformal bootstrap studies of the O(3) model in 2 < d < 3.",
        "authors": [
            "Robert A. Jones"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157573",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Nonreciprocal phenomena in superconductivity",
        "abstract": "This thesis introduces and studies several unusual phenomena that arise in low-dimensional systems in the presence of a magnetic field.  The first example that we discuss is nonreciprocal superconductivity, which occurs upon simultaneous breaking of inversion and time-reversal symmetries.  Nonreciprocal superconductors describe certain classes of unconventional superconductors that include certain kinds of mixed-pairing and finite-momentum ones. They also occur in engineered systems exhibiting s-wave pairing-based superconductivity, for which we put forward several simple proposals. We demonstrate several striking observable consequences of nonreciprocal superconductivity. These include current rectification in normal metal-nonreciprocal superconductor junctions and the Josephson diode effect, for which we propose a simple and universally applicable mechanism. With the advent of novel low-dimensional symmetry-breaking materials, such as multilayer graphenes and twisted cuprates, as well as modern experimental possibilities involving engineered systems,  nonreciprocal phenomena could eventually become an indispensable tool for revealing the nature of superconducting orders.\r\n\r\nThe second part of this thesis concerns doped Mott insulators in a magnetic field, described by a triangular-lattice Fermi-Hubbard model in the limit of strong interaction. This is relevant for many novel materials, such as moiré transition metal dichalcogenides bilayers. We predict a new bound state, spin polaron, formed by binding a doped hole with a magnon (spin flip). Spin polarons have large effective mass and are spin 3/2 quasiparticles. The mechanism for their formation is kinetic frustration, and therefore their binding energy is proportional to the hopping t, which is the largest energy scale within a single Hubbard band. We then propose a new phase diagram for the triangular lattice Hubbard model in a magnetic field as well as multiple experimental signatures. We hope that the prediction of the spin polaron, which has since been experimentally confirmed, will give rise to novel mechanisms for superconductivity and correlated orders in doped Hubbard models.",
        "authors": [
            "Marharyta Davydova"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157580",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Symmetry and its Signatures in Quantum Many-Body Dynamics",
        "abstract": "Symmetry has long been a defining feature in our understanding of statistical or manybody systems. By making appeals to universal properties associated with global symmetries and topology, one may describe universal properties of “typical” states and dynamics in equilibrium, even when keeping track of the precise dynamics of a particular many-body system is impossible. This challenge of tracking allowable states and dynamical transitions is only exacerbated for non-equilibrium systems, where one cannot rely on the same notions of typicality. Further, when driven out of equilibrium by external interactions, quantum orders constructed from highly sensitive correlations between states are liable to vanish. Despite these conceptual and practical difficulties, the rise of quantum technologies and accompanying theoretical developments has motivated a surge of interest in dynamical quantum phenomena. The recent developments in the field of quantum many-body dynamics provide satisfactory accounts of many interesting phenomena, including failures of the Eigenstate Thermalization Hypothesis, various dynamical and mixed-state phases of matter, and measurement-induced dynamics and phase transitions. Many of these results are explained for specific systems or within different conceptual frameworks, however these results rarely generalize. In this thesis, I attempt to unify many aspects of quantum many-body dynamics under the same conceptual framework through an investigation of the universal signatures of symmetry in quantum dynamical systems. This is accomplished via a mapping between the averaged dynamics and the low-energy spectrum of an effective Hamiltonian in a “doubled Hilbert space,” comprised of two copies of the original space. This provides a general and versatile framework to qualitatively understand both familiar and novel universal properties of dynamical phenomena like charge diffusion, sub(super)-diffusion of multipole moments in systems with short and long-range interactions, charge and multipole, and even measurement-induced phase transitions. By expanding into a doubled Hilbert space, one may capture the subtleties of non-equilibrium physics, and particularly dynamical phases, within the framework of equilibrium physics and phases. In this work, we examine how to understand various symmetry-constrained dynamical phases and phase transitions using through a dual description of symmetry-constrained equilibrium phases and symmetry-breaking transitions in an enlarged Hilbert space.",
        "authors": [
            "Olumakinde Ogunnaike"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157598",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Ultrafast Terahertz Spectroscopy for the Manipulation and\r\nElucidation of Correlated Quantum Materials",
        "abstract": "Light-matter interactions are at the heart of quantum mechanics. The photoelectric effect, blackbody radiation, and the hydrogen emission spectrum were all experimental observations using light and its interaction with matter which led to the discovery of the quantum mechanical nature of the universe. In modern research, the interactions of light with matter plays a significant role in both understanding the properties of, or controlling various aspects of quantum materials, a class of materials whose macroscopic properties are only understood through quantum mechanics. Quantum materials are often categorized into two classes: topological materials or strongly correlated materials, though the cross-over and interplay between these two aspects is a significant field of study as well. Strongly correlated materials exhibit exotic physical phases such as magnetism, superconductivity, or heavy fermion formation due to the strong interactions of electrons. Many of these properties hold significant promise for application, yet the ability to predict correlated physics from a theoretical standpoint is still at a young stage of development. To this end, experimental efforts to demonstrate and understand the interplay between different degrees of freedom in a material (spin, charge, lattice, and orbital) are essential for progressing in this direction.\r\n\r\nIn this thesis, a variety of light-matter interactions using ultrafast techniques are explored in a set of quasi two-dimensional strongly correlated materials. These are bulk materials, whose properties are strongly founded in the two-dimensional layers stacked on top of one another. A variety of Optical-Pump Terahertz-Probe spectroscopic methods are used to drive a system out of equilibrium while monitoring the low-energy physics in the terahertz (THz) spectral range. This part of the electromagnetic spectrum is essential to understanding many aspects of strongly correlated physics. For example, the charge carriers in a metallic (or photoexcited) material have a strong spectral weight here and many of the collective modes of insulating phases, such as phonons or magnons, occur at these energies as well. Specifically, the collective modes of two van der Waals antiferromagnets are excited coherently with the use of ultrafast optical pulses. In the antiferromagnet NiPS3 a new mechanism for launching a coherent magnon is discovered. In the multiferroic, antiferromagnet NiI2, evidence for a new type of quasiparticle, an electromagnon-polariton, is demonstrated in a non-equilibrium sample. Further, preliminary data regarding the measurement of a new type of Kondo hybridization gap (a pseudogap) in the kagome strange metal Ni3In is reported using the photoexcited dynamics and the Rothwarf-Taylor bottleneck model.",
        "authors": [
            "Clifford Allington"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157570",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Spectroscopic study of emergent electronic phases in transition metal based compounds",
        "abstract": "Antiferromagnets with non-relativistic spin splitting are outstanding candidates as the next generation of spintronic materials owing to their electron-volt (eV) scale spin splitting, ultrafast spin dynamics and nearly vanishing stray fields. Achieving voltage-based control of spin polarization in antiferromagnets is of great interest for realizing energy-efficient and compact devices for information storage and processing. Spin spiral type-II multiferroics exhibit an inversion-symmetry-breaking antiferromagnetic order which directly induces ferroelectric polarization, allowing for symmetry protected cross-control between spin chirality and polar order. This intrinsic coupling between the magnetic and dipolar order parameters results in record-strength magnetoelectric effects. Two-dimensional materials possessing such intrinsic multiferroic properties have been long sought for harnessing magnetoelectric coupling in nanoelectronic devices. The recent discovery of intrinsic magnetic order in atomically-thin van der Waals (vdW) materials has created new opportunities for the study of collective spin phenomena in free-standing two-dimensional (2D) systems and nanoscale devices. Among possible multiferroic vdW materials, several families have been identified, and of particular promise is the magnetic semiconductor NiI₂. The multiferroic state of NiI₂ is characterized by a proper-screw spin helix with given handedness, which couples to the charge degrees of freedom to produce a chirality-controlled electrical polarization. We use a suite of optical technique which reveal an ordered magnetic, polar state that persists down to the ultrathin limit of monolayer NiI₂.\r\n\r\nRecent development of spin-group formalism has identified a new class of magnets with nontrivial spin textures, including even-parity d, g, or i-wave altermagnet and odd-parity p-wave antiferromagnets. The chiral magnetic order in NiI₂ breaks Inversion-Time-Reversal-Translation (P Tτ ) symmetry, and Spin-Rotation-Translation (Uτ ) symmetry, allowing for spin splitting even in the absence of spin-orbit-coupling (SOC). We provide direct evidence that the spin polarization in a spin spiral type-II multiferroic exhibits p-wave (odd-parity) character and directly couples to the spin chirality, enabling electrical control of non-relativistic spin splitting. Our findings represent the first observation of a p-wave antiferromagnet, and open a new frontier of voltage-based switching of non-relativistic spin splitting in vdW antiferromagnets.",
        "authors": [
            "Qian Song"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157577",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Investigating Fine-Tuning of Language Models for Multiple-Choice Questions",
        "abstract": "This thesis investigates the positional and contextual bias of large language models (LLMs) when used to answer multiple-choice questions (MCQs). Given the increasing use of generative language models in fields ranging from cybersecurity to biomedical research, it is important to understand the causes of their behavior in order to mitigate biases and prevent errors. One known method of improving the performance of LLMs is fine-tuning, wherein a model is additionally trained on data from a specified distribution or subject area. We specifically investigate training data properties related to positional bias in fine-tuned language model performance on correctly answering MCQs. To improve model efficiency, we used parameter-efficient fine-tuning, specifically LoRA (Low-Rank Adaptation), which reduces the dimensionality of weight matrices used in the model’s layers. We verify that if the training data for the model possesses the same qualities and distributions as the test data, the LLM will achieve the best performance. In our experiments, we scaled and balanced our fine-tuning datasets and learned that both processes improve the accuracy on test sets of MCQs.",
        "authors": [
            "Ivy A. Wang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157591",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploring the Intersection of Physics Modeling and Representation Learning",
        "abstract": "Representation Learning has evolved into a multi-purpose tool capable of solving arbitrary problems provided enough data. This thesis focuses on two primary directions: (1) Harnessing the power of deep learning for applications in fundamental physics and (2) using physicsinspired tools to improve and shed some light on otherwise large-scale, inscrutable black-box algorithms. We explore a collection of applications that improve different aspects of nuclear and particle physics research across its many stages, from online data selection to offline data analysis. We also tease out how deep learning can open up entirely new avenues of research through the lens of mechanistic interpretability to (re)derive fundamental theory as well as new ways to reinterpret physics measurements. Lastly, we study how physics tools can be useful to better understand the dynamics of deep learning as well as provide a solid foundation for algorithms and training paradigms that expand the frontier of machine learning.",
        "authors": [
            "Ouail Kitouni"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157597",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Instrumental Effects in 21 cm Cosmology: One-point Statistics and Power Spectrum with the HERA Interferometer",
        "abstract": "The epoch of reionization (EoR) signifies a critical phase in the universe’s evolution, marking the shift from a predominantly neutral intergalactic medium to the ionized state observed today. A key aspect of studying the EoR involves observing the redshifted 21 cm line emission with radio telescopes. A significant challenge in this endeavor is isolating the faint 21 cm signals from bright foreground emissions and systematics. This collection of works focuses on understanding the impact of instrumental systematic effects on statistical measurements, such as the one-point statistics and power spectrum, using the Hydrogen Epoch of Reionization Array (HERA). First, I investigate one-point statistics measured from image cubes based on HERA Phase I observations after foreground removal for the first time. I highlight the influence of systematics on these measurements, by measuring the second and third moments. These analyses show that, despite efforts to mitigate systematics, the residual systematics still cause deviations in the measurements from the expected values. In addition, I evaluate EoR models against observational data, suggesting the second moment measurements likely reject the cold reionization model characterized by inefficient X-ray heating. The third moment, which captures non-Gaussianity features of the signals, is significantly diminished by the instrument response and further reduced by the foreground removal process, making it challenging to probe non-Gaussianity. However, there remains the potential to detect some skewness at low redshifts. One potential systematic for HERA involves calibration errors stemming from per-antenna perturbations due to feed misalignment. I have simulated these calibration errors by modeling realistic perturbed primary beams for HERA Phase II observations. The chromatic calibration errors are critical since they can cause foreground emission to contaminate the region of Fourier space expected to be dominated by cosmological signals. I then present the work focused on developing a method to mitigate the calibration errors and foreground leakage, thereby recovering the clean EoR window.",
        "authors": [
            "Honggeun Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157569",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Advanced Reconstruction Techniques for CUORE: Searching Beyond the Standard Model with Cryogenic Calorimeters",
        "abstract": "Located within the Laboratori Nazionali del Gran Sasso (LNGS), the Cryogenic Underground Observatory for Rare Events (CUORE) is an experiment primarily searching for neutrinoless double beta decay in ¹³⁰Te. It is the largest operating sub-kelvin cryogenic detector array, instrumenting 988 TeO₂ detector channels at temperatures below 20 mK. CUORE uses the cryogenic calorimeter technique, resolving the thermal signatures from nuclear/particle interactions within crystal absorbers for precise determination of deposited energy. This work establishes methods and analysis techniques to treat CUORE as a segmented detector in aggregate, with a focus towards identifying and reconstructing track-like signatures induced by high-energy through-going particles traversing the detector array. Implementations of such high-multiplicity techniques are used to validate that CUORE can resolve the remaining underground flux of muons within LNGS. This result demonstrates CUORE’s unprecedented size and acceptance as compared to previous cryogenic calorimeter arrays, and has applications towards future searches for neutrinoless double beta decay for which muon-induced backgrounds are non-negligible. Additionally, these methods open up new avenues for CUORE to search for exotic beyond-the-Standard Model particles and interactions, such as particles with fractional electric charge. If realized in nature, fractionally charged particles (FCPs) could be present within the underground flux of cosmic radiation and would leave faint track-like signatures across the detector. We report on a search for FCPs using the first tonne-year of CUORE’s exposure, finding no excess of FCP track candidates over background, and setting leading limits at 90% C.L. on the possible underground flux of FCPs with charges between 1/24 − 1/5 that of an electron. Lastly, we introduce differentiable programming methods for the end-to-end training of neural ordinary differential equations to model thermal pulse dynamics within CUORE calorimeter channels. These methods and results improve understanding of detector response, enable improved in situ background characterization, and open novel opportunities for CUORE and future tonne-scale cryogenic calorimeters to search for physics beyond the Standard Model.",
        "authors": [
            "Daniel W. Mayer"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157595",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Optimization under ecological realism reproduces signatures of human speech perception",
        "abstract": "Recent advances in machine learning have made real-world perception tasks feasible for computers, in many cases approaching levels of performance similar to those of humans. In particular, optimizing models for ecologically realistic training datasets has helped to yield more human-like model results. In the field of speech recognition, models trained under realistic conditions with simulated cochlear input reproduce some characteristics of human speech recognition. However, it is unclear how similar the behavior of these models is to that of humans across the many ways in which speech can be manipulated or degraded, since human and model behavior have not been extensively compared. In this paper, we address this question by comprehensively testing a neural network model trained in ecological conditions across a large set of speech manipulations, comparing its behavior to that of humans. We find that training in ecological conditions yields a fairly good overall match to human behavior, with some discrepancies that can be largely resolved by training specifically on these conditions. The results support the idea that the phenotype of human speech recognition can be understood as a consequence of having been optimized for the problem of speech recognition in natural conditions.",
        "authors": [
            "Annika K. Magaro"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157565",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Total Synthesis of Verticillin A and Application of Diazene-Directed Fragment Assembly to the Synthesis of Heterodimeric Epidithiodiketopiperazine Derivatives",
        "abstract": "I. Total Synthesis of (+)-Verticillin A We report the first total synthesis of (+)-verticillin A, completed in 16 steps. Our initial strategy of late-stage sulfidation on a dimeric substrate produced an undesired diastereomer of ETP. We were able to access an ETP with the desired diastereoselectivity by effecting sulfidation on an epimerized, monomeric substrate. In order to install a disulfide with the desired facial selectivity, we developed a stepwise sequence involving stereoselective formation of a C15-benzhydryl disulfide followed by intramolecular sulfidation at C11. Because ETPs are unstable to carboncentered radicals and irradiation with UV light, we developed conditions to reduce the disulfide and protect the resulting thiols as alkylsulfides prior to cobalt reductive dimerization and photochemical desulfonylation. Finally, deprotection of the thiols and oxidation delivered the ETP natural product (+)-verticillin A. \r\n\r\nII. Synthesis of Heterodimeric ETP Derivatives Using Diazene-Directed Fragment Assembly\r\n\r\nWe report the development of a novel route to heterodimeric ETP derivatives using diazenedirected fragment assembly. This is the first application of diazene-directed coupling to the synthesis of dimeric diketopiperazine alkaloids Our group’s initial route to heterodimeric ETP derivatives relied upon reductive cobalt dimerization, which produces a nearly statistical mixture of homo- and heterodimeric products. In contrast to the initial route, the diazene-based approach disclosed herein enables selective heterodimerization. To demonstrate the utility of heterodimeric ETP derivatives, we have synthesized an ETP-diazirine photoaffinity labelling probe, which we hope can be used to study the interactions of ETPs with cellular targets.II. Synthesis of Heterodimeric ETP Derivatives Using Diazene-Directed Fragment Assembly\r\n\r\nWe report the development of a novel route to heterodimeric ETP derivatives using diazenedirected fragment assembly. This is the first application of diazene-directed coupling to the synthesis of dimeric diketopiperazine alkaloids Our group’s initial route to heterodimeric ETP derivatives relied upon reductive cobalt dimerization, which produces a nearly statistical mixture of homo- and heterodimeric products. In contrast to the initial route, the diazene-based approach disclosed herein enables selective heterodimerization. To demonstrate the utility of heterodimeric ETP derivatives, we have synthesized an ETP-diazirine photoaffinity labelling probe, which we hope can be used to study the interactions of ETPs with cellular targets.",
        "authors": [
            "Walker Knauss"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157574",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Systematic Studies on the Chelating Ligand Effects\r\nof Novel Borafluoronium Ions",
        "abstract": "This study explores the synthesis and characterization of borafluoronium ions via a ligand-based strategy using bidentate amine and phosphine bases as chelating agents to cationic boronium ions.The borafluoronium complexes A–C were synthesized in high yields (80%–95%) and characterized using NMR spectroscopy and single crystal X-ray diffraction. Further investigations into the coordination of other bisphosphine ligands, such as dppe, rac-BINAP, and Xantphos, resulted in the formation of Lewis adducts rather than the desired borafluoronium ions. The challenges in isolating these species are attributed to steric and chelate effects inherent of the ligands, with NMR analysis providing insights into the coordination chemistry and stability of these complexes.This work advances the understanding of borafluoroniumion formation and the impact of ligand structure on their properties.",
        "authors": [
            "Marissa D. Allen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157568",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploring New Frontiers in High Energy Physics: Boosted Resonances Decaying To Quarks, Foundation Models, and Heterogeneous Computing at the CMS Experiment",
        "abstract": "In this thesis, we introduce machine learning (ML) tools to optimize data taking and analysis at data-intensive scientific experiments, focusing on the CMS experiment at the Large Hadron Collider (LHC). A path to a foundation model for LHC physics is described, where self-supervised learning is enabled through the re-simulation of decaying partons. The first experiments with remote operation of GPUs in LHC experiments are presented. These tools will help equip experiments at the High-Luminosity LHC (HL-LHC) to perform precision measurements and searches for new physics, for example, low mass resonances decaying to quarks. In this context, a search for narrow resonances decaying into quarkantiquark pairs produced with high transverse momentum is presented. The analysis is based on data collected in Run 2 with the CMS detector at the LHC in proton-proton collisions at √ 𝑠 = 13 TeV. Resonance candidates are reconstructed as large-radius jets and identified using a state-of-the-art jet tagging algorithm. This analysis presents the most sensitive limits for new spin-1 bosons coupling universally to quarks and spin-0 bosons coupling preferentially to heavier quarks. The invariant jet mass spectrum is probed for a potential narrow peaking signal over a smoothly falling background. Upper limits at 95% confidence level are set on the coupling of narrow resonances to quarks as a function of the resonance mass. For masses between 50 and 300 GeV, these are the most sensitive limits to date on all possible mediators. Using conventions on s-channel dark matter mediators, limits are set on dark photons and dark matter in the context of the relic density.",
        "authors": [
            "Jeffrey Krupa"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157578",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Squeezing the Quantum Noise of LIGO below the Standard Quantum Limit",
        "abstract": "The year 2015 marked the first detection of a gravitational wave signal from a pair of black holes located 410 megaparsecs (1.3 billion light-years) away. Their merger unleashed an immense amount of energy, with the peak emission rate surpassing the combined power of all luminous stars in the observable universe. Unlike stars, the merger of two black holes does not emit electromagnetic radiation like visible light but instead illuminates the universe with gravitational radiation. These waves traveled freely for over a billion years before being captured by the twin Laser Interferometer Gravitational-Wave Observatory (LIGO) detectors. Upon reaching Earth, these waves caused a minuscule length change between the LIGO mirrors, on the order of 10^(−18) m, a thousand times smaller than a proton.\r\n\r\nThe unprecedented sensitivity of LIGO requires an extremely low noise level. The design of LIGO as an interferometer converts the gravitational-wave signal to an optical signal, which is measured on photodiodes along with other noises. One of the noise sources is the quantum noise due to the quantum vacuum fluctuations of the light itself. Besides the light, the mirror also has quantum-mechanical features and experiences quantum back-actions when we probe it with light. Knowing the position of the mirror very well would inevitably perturb its momentum, which prevents us from precisely making the next measurement of the position. This is fundamental physics dictated by Heisenberg’s uncertainty principle. In the case of continuous measurement like LIGO, the quantum back-action leads to an apparent sensitivity limit known as the Standard Quantum Limit (SQL). It tells us how precisely we can measure an object with light.\r\n\r\nThe SQL applies when using uncorrelated photons or coherent light to measure the object, such as a laser beam. However, introducing quantum correlations through squeezed light, a technique called squeezing (Chapter 2), can circumvent this limit. Squeezed vacuum, a non-classical light state, exploits quantum correlations between photon pairs to reduce vacuum fluctuations in one quadrature at the cost of another. By manipulating the quantum correlation between light and the mirror, the squeezed vacuum can potentially reduce quantum noise below the SQL, a concept explored in frequency-dependent squeezing. This thesis develops a first-principle model of quantum noise in LIGO (Chapter 3) and investigates how squeezing can mitigate it while considering practical factors like optical losses and mode-mismatch (Chapter 4). These theories are constructed with a bottom-up approach. Experimental details on generating and utilizing frequency-dependent squeezing for LIGO are also discussed (Chapter 5), culminating in the observation of LIGO’s quantum noise below the SQL (Chapter 6).\r\n\r\nBesides squeezing, increasing optical power can also reduce quantum shot noise. Nevertheless, maintaining high power levels (fractions of megawatts) in LIGO is challenging due to experimental imperfections, such as unintended point absorbers on the mirror coating. This thesis analyzes the thermoelastic distortions caused by these absorbers, which limit achievable optical power in current and future gravitational-wave detectors (Chapter 7).",
        "authors": [
            "Wenxuan Jia"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157572",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Search for High-Frequency Gravitational Waves with a Modified Axion Detector",
        "abstract": "ABRACADABRA-10cm has had great success as a lumped-element axion dark matter pathfinder experiment, with two published axion searches and an extensive background investigation. Now, using the electrodynamics of gravitational waves and a simple change of pickup structures, we are using the ABRACADABRA detector to search for high-frequency gravitational waves in the kHz to MHz range. These higher frequencies may indicate signs of in-spiraling primordial black holes, or other beyond the standard model phenomena. With careful calibration used to distinguish between the two signals, we introduce the first simultaneous search for both axions and gravitational waves using a lumped-element axion detector. In this thesis I will present on the high-frequency cryogenic ABRACADABRA-10cm detector, the background investigations of the detector and the design and first data from the ABRACADABRA-10cm high-frequency gravitational wave search.",
        "authors": [
            "Kaliroë Mabelle West Pappas"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157593",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Emergence, Formation and Dynamics of Hot QCD Matter",
        "abstract": "Understanding the dynamics of Quantum Chromodynamics (QCD) in quantitative detail is one of the main frontiers in particle physics. While the last century gave us the formulation of the theory of nuclear interactions, QCD, as well as that of the rest of visible matter encoded in the Standard Model of Particle Physics, much remains to be understood. In particular, the hot QCD matter produced in high energy collisions of heavy ions presents a unique challenge to theory and phenomenology due to the vast number of different phenomena that take place in such a collision, and even more so because it is an out-of-equilibrium process. In this thesis, we make progress in two concrete directions in the vast landscape of hot QCD physics. The first one is quarkonium transport inside quark-gluon plasma (QGP), the high temperature phase of QCD. Over the past two decades it has been realized that a significant fraction of quarkonium suppression in high energy heavy ion collisions comes from dynamic dissociation and recombination processes, instead of static screening of the interaction potential as originally proposed by Matsui and Satz. Our contribution is the formulation of the precise correlation functions in QCD at finite temperature that describe the dissociation and recombination processes of heavy quarkonium in QGP, as well as their calculation in weakly coupled QCD and strongly coupled N=4 supersymmetric Yang-Mills theory. We also formulate the Euclidean version of these correlation functions so that they may be calculated using Lattice QCD techniques. In this way, our results provide the necessary ingredients to carry out an analysis of the suppression of ϒ states in heavy ion collisions in terms of the parameters of the QCD lagrangian.\r\nThe second contribution we make is the development of tools to understand the process of hydrodynamization in QCD kinetic theory and their application to a simplified description where only a subset of the QCD scattering mechanisms are included. By doing this, we learn that the process of hydrodynamization in this theory, and specifically, how memory of the initial condition is lost, follows the recently proposed Adiabatic Hydrodynamization scenario.\r\nConcretely, hydrodynamization proceeds through a sequential process in which a monotonously shrinking set of low-energy states dominate the dynamics, where the opening of an energy gap relative to the ground state(s) signals the start of each stage of this process. The hydrodynamic attractor is reached when only one low-energy state remains as the ground state, and the system approaches local thermal equilibrium following the adiabatic evolution of this low-energy state.",
        "authors": [
            "Bruno Sebastian Scheihing Hitschfeld"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157563",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On the physics of intranuclear organization",
        "abstract": "Eukaryotic nuclei, despite their diverse and crowded chemical milieu, can achieve precise spatiotemporal organization of their contents and chemistry, despite lacking access to membrane-bound organelles. It has recently become apparent that the cells accomplish this feat by leveraging physical processes such as liquid-liquid phase separation driven by multivalent macromolecular interactions to form biomolecular condensates which can serve as membrane-less organelles for the precise, vectorial organization of intranuclear contents. In particular, the hierarchical and functional packaging of DNA into chromatin is mediated by phase separation. Epigenetic modifications of histone proteins, which DNA wraps around to form nucleosomes, are key determinants of nucleosomes’ condensability and chromatin’s higher-order structure. Chromatin structure, by regulating access of transcriptional machinery to the genome, in turn, has broad implications for cellular processes such as gene regulation and cellular differentiation. Furthermore, there exists a bi-directional feedback between 1D epigenomic sequence and 3D chromatin structure as the former is spread and maintained by enzymes that have a “reader-writer” functionality that allows them to similarly modify nucleosomes close to each other in sequence but not necessarily in space. Recent advances suggest chromatin has the properties of a viscoelastic network and exhibits non-trivial dynamics. Therefore, the dynamics of chromatin structure and the spread and maintenance of epigenetic marks are intimately and inextricably linked yet poorly understood. Part I of this thesis is devoted to understanding the complex interplay between chromatin structural dynamics and stochastic reaction networks describing histone modifications. Furthermore, given the prominent role phase separation plays in intranuclear organization, we devote Part II of this thesis to study the impact of competition between specific and non-specific interactions on liquid-liquid phase separation coupled to percolation and thereby attempt to elucidate the molecular grammar of phase separating biomolecules and evolutionary pressures that shape them.",
        "authors": [
            "Amogh Sood"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157585",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Beyond Color: Lattice Gauge Theory for Strongly-Coupled Physics",
        "abstract": "Quantum Chromodynamics (QCD) is the prototypical strongly interacting Quantum Field Theory (QFT). It is the interaction that yields the strong nuclear force that binds protons and neutrons together. The underlying mathematical picture of QCD is known exactly: it is an 𝑆𝑈(3) gauge theory coupled to six flavors of fermions (the quarks). Despite this, it remains difficult to compute QCD observables because QCD is strongly-coupled, and typical perturbative methods used in QFT only work in specific regimes of validity for QCD. The most successful ab initio method to study QCD is Lattice Gauge Theory (LGT). This computational formalism computes observables by discretizing spacetime to render the path integral tractable. The primary focus of LGT in the 40 years since its inception has been the study of QCD, as the theory has direct physical relevance to so much of our universe, and the desire to understand QCD has driven many conceptual breakthroughs and advancements in LGT. Despite the focus on QCD, lattice methods find significant utility in studying other strongly-coupled gauge theories related to and unrelated to QCD. This thesis will focus on applying LGT to strongly-coupled physics inside and outside of QCD and on developing techniques within LGT that may be used to better understand said theories. First, the spectral function reconstruction problem in LGT is considered, and a new method for spectral function reconstruction in LGT is presented. Spectral functions describe the energy states of a theory: bound states, resonances, and continuum thresholds. The presented reconstruction method uses the analytic properties of the retarded Green’s function to constrain the full set of spectral functions that may be reconstructed from LGT data using the Nevanlinna-Pick interpolation problem. Next, two theories will be numerically studied using LGT. The first is the Standard Model Effective Field Theory (SMEFT). The SMEFT process that is considered is neutrinoless double 𝛽 (0𝜈𝛽𝛽) decay, a hypothetical decay of two neutrons into two protons and two electrons. LGT is used to compute non-perturbative matrix elements for the unphysical 𝜋⁻→ 𝜋⁺𝑒⁻𝑒⁻ transition, which contributes to nuclear 0𝜈𝛽𝛽 decay, and for the decay of the dinucleon 𝑛⁰𝑛⁰ → 𝑝⁺𝑝⁺𝑒⁻𝑒⁻. Connections to Effective Field Theory studies of 0𝜈𝛽𝛽 decay will also be discussed. Finally, adjoint QCD (QCD₂), the theory of a Majorana fermion coupled to a 𝑆𝑈(𝑁) gauge field in the adjoint representation in 1+1 spacetime dimensions, will be studied using LGT. QCD₂ is a well-studied QCD-like theory whose properties have been crucial in the study of confinement. Lattice methods are used to compute the static quark potential, string tensions, and the low-lying spectrum of the theory, which will provide input that may be used to understand better QCD₂ and the confinement mechanism in general.",
        "authors": [
            "Patrick R. Oare"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157576",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Numerical and Analytical Methods in Low-Dimensional Strongly Correlated Quantum Systems",
        "abstract": "The study of low-dimensional strongly correlated quantum systems lies at the intersection of intricate theoretical models and practical numerical methods, offering deep insights into condensed matter physics. This thesis explores the application of various numerical and analytical methods to these systems. It addresses universal behaviors and phase transitions, exemplified by the phenomenon of multiversality. Specifically, the transition from a 1D Luttinger liquid to a charge density wave insulator, characterized by partly Kosterlitz-Thouless transition and partly Ising transition, is analyzed using both analytical renormalization group calculations and numerical density matrix renormalization group simulations. Additionally, the thesis introduces a statistical smoothing spline method to pinpoint transition points systematically. The work extends to quantum dynamics, presenting a generic theoretical framework for analyzing quantum-classical adiabatic dynamics with learning algorithms. A provably efficient adiabatic learning (PEAL) algorithm with favorable scaling properties is developed. The algorithm is numerically validated on the 1D Holstein model, demonstrating its precision in predicting dynamics. Furthermore, the thesis derives a Hamiltonian lattice formulation for the 2+1D compact Maxwell-Chern-Simons theory, providing an analytical solution that aligns with continuum theories and facilitating future numerical applications. Through these explorations, the thesis underscores the complementary roles of numerical and analytical methods in advancing the understanding of complex quantum systems.",
        "authors": [
            "Changnan Peng"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157584",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Towards Perceptual Augmentation",
        "abstract": "This thesis explores the concept of perceptual augmentation, focusing on expanding human sensory capabilities beyond their biological limitations. It challenges traditional approaches to sensory enhancement by emphasizing the importance of perception over mere sensory input. Drawing inspiration from the diverse sensory abilities found in nature, the research aims to develop methods for meaningful augmentation of human perception that can impact daily life. The study adopts an ecological approach to perceptual augmentation, grounded in Gibsonian ecological psychology. Key principles include providing correct mental models of augmentation devices, leveraging environmental training and natural tasks, emphasizing multisensory interfaces with sensorimotor feedback, and creating affordances that mimic the natural world. This approach seeks to facilitate perceptual learning through natural interaction with the environment, rather than relying on extensive explicit training.\r\nThe thesis presents early work in exploring and evaluating individual principles of this ecological framework for perceptual augmentation. While acknowledging the gap between the proposed theoretical approach and current research outcomes, the studies conducted focus on augmenting perception for specific tasks such as pitch interval perception, pilot situation awareness, and sleep staging.  The research does not yet demonstrate a generalized, \"all-purpose\" augmented sense, but lays groundwork for future investigations, including a proposed experiment to mitigate age-related hearing loss using the developed principles.",
        "authors": [
            "Sam Chin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157710",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Last-Meter Delivery: Solving the Unattended Delivery Challenge from Streets to Doorsteps",
        "abstract": "The rise of e-commerce has led to a surge in package deliveries, resulting in the proliferation of unattended delivery methods to address the \"last-meter\" problem – the challenge of delivering packages from the roadside or sidewalk to the customer's front door. This thesis proposes a methodology for implementing Large Language Model (LLM), and Vision Language Model (VLM) to enable delivery robots to identify the final delivery target and navigate the complex terrain from the curb to the front door. The proposed solution aims to enhance the autonomy and safety of last-mile delivery systems, addressing the \"last-meter\" challenge and improving the customer experience.\r\n\r\nThis thesis presents a comprehensive overview of the last-meter delivery concept, aiming to bridge the gap between the roadside/sidewalk and the customer's front door. It begins by introducing the significance of last-meter delivery in the growing e-commerce industry and the challenges posed by unattended deliveries. The thesis then reviews the existing literature on autonomous and unmanned delivery systems, multimodal delivery approaches, and the application of large language models and vision language models in robotics. This research identifies the advancements and gaps in the field that the proposed methodology aims to address.\r\n\r\nThe thesis primarily focuses on leveraging Large Language Models, the Segment Anything Model, and the open-source Florence-2 vision foundation model to enable the transmission of customers' delivery instructions to the final delivery target in the context of last-meter delivery. It outlines the methodology for data preparation, object detection and labeling, as well as the integration of Large Language Models to handle customer instructions and coordinate delivery target. It also describes the experimental design and methodologies employed to validate the effectiveness of the proposed system. This includes the use of a last-meter dataset and the evaluation of last-meter scene and target coordinate identification.\r\n\r\nThe thesis concludes by summarizing the key findings and contributions, discussing the broader implications of the proposed methodology, and suggesting directions for future work, such as enhancing system robustness and scalability.\r\n\r\nKEYWORDS: Last-Mile Delivery, last-meter Delivery, Large Language Models (LLM), Vision Language Models (VLM), Robotics, Segment Anything Model (SAM), Open-Vocabulary\r\nObject Detection (OVD).",
        "authors": [
            "Wen-Xin Xiao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157723",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Multi-bounce Returns for Specular Surface Mapping from Consumer-grade Flash LiDAR",
        "abstract": "This thesis proposes an approach to leverage multi-bounce returns of a flash LiDAR on portable smartphones for 3D specular surface reconstruction. This is an important research problem as most traditional LiDAR systems fail to detect specular surfaces. As mirror and glass are everywhere, vision systems failing to detect specular surface can be detrimental. Applications like mapping may become inaccurate, and more critically, robots could crash into undetected windows during navigation, leading to potentially fatal outcomes. We perceive this work can impactfully enhance the robustness of specular surface detection, with LiDAR complementing any kind of vision system, particularly image-based.\r\n\r\nTraditional LiDAR systems typically assume that all returns are single-bounce, which can lead to inaccurate representations of specular surfaces like mirrors or glass, often causing them to appear as though there is a hole. In contrast, this approach models the multi-bounce paths, providing a more accurate reconstruction of these specular surfaces.\r\n\r\nWe operate with a consumer-grade LiDAR that does not require manual calibration and can be operated in real-time on an affordable and portable smartphone. Consumer-grade LiDAR multi-beam flash LiDAR is challenging with its coarse resolution, co-located sensors, and multiplexing setup. In face of these challenges, we propose to solve the association problem with the `reciprocal pair’ algorithm, which can discern different types of bounces from the multi-bounce returns.\r\n\r\nThe algorithm is shown to detect over multiple consecutive frames for dense mirror mapping. In addition to 3D reconstruction, we show multi-bounce returns help to enhance performances on applications such as segmentation and novel view synthesis. Our method can be combined with these state-of-the-art learned-based model, enhancing its robustness by discerning ambiguous scenarios. In general, this approach can map various specular surfaces like mirrors and glasses, without making assumption about particular specular surface shapes, and can operate on non-perpendicular specular-diffuse surface pairs.",
        "authors": [
            "Tsung-Han Lin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157707",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Textile Macroelectronics: Architecting Sensate and Computational Fabrics across Scales",
        "abstract": "Textiles are omnipresent and among the oldest forms of art and culture in human civilization. They serve as our protective skin, the interface between our bodies and the environment, and a medium for self-expression and collective experience. As electronics become more compliant, miniaturized, and low-cost, textiles provide an ideal substrate for technology integration, further driving the era of ubiquitous computing. My research fuses recent advances in functional materials, digital fabrication, hardware systems, and immersive technologies to demonstrate Textile Macroelectronics architecture and develop sensate and computational fabrics across scales.\r\n\r\nIn this dissertation, I propose a ubiquitous computational textile framework—a synergy between functional device selection, textile structures, fabrication tools, and system architecture—that integrates a distributed network of sensing and computational elements as primitives or raw materials in the manufacturing process of electronic textile products. In the first part of the dissertation, I present several methods, artifacts, and implementations of sensate textiles using functional fibers and digital machine knitting. I argue that to promote the disruption and adoption of sensate textiles and achieve seamless integration, we require a better hierarchical understanding of textile construction and fiber-fabric properties, as well as ways to integrate electronics and functionalities with industrial textile fabrication processes. By controlling functional and common yarn inputs, along with knitting structures and patterns, I can architect fabric forms and aesthetics while tuning their electrical and mechanical properties. With this approach, I have developed a set of custom proxemic and tactile textile interfaces based on capacitive and piezoresistive sensing for musical expression, human-computer interaction, activity recognition, and multi-sensory experiences in various forms such as cloth, footwear, mats, carpets, and large-scale architectural facades.\r\n\r\nIn the second part of the dissertation, I will discuss my work in exploring flexible, stretchable, and soft printed circuit technologies, incorporating multi-modal sensing with distributed computation to address scalability issues inherent in large and dense sensate textiles. These efforts have led to unique power, interconnection, and networking paradigms that allow us to transition from application-specific sensate textiles to generic computational fabrics that can be tailored and programmed for various applications. Finally, through these collective and complementary efforts, I aim to demonstrate an ecosystem of fabric artifacts that will lead us toward an Electronic Textile Gaia—a vision where sensing and intelligence are seamlessly interconnected and integrated into the fabric of everyday life, from in-body, on-body, room-scale, to architectural textiles, for applications ranging from physiological and physical activity monitoring to interactive media and built environments.",
        "authors": [
            "Irmandy Wicaksono"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157711",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Public Interest Computing: a Pluralistic Design Language Foundation for Societal-Machine Alignment",
        "abstract": "The proliferation of algorithmic systems, including artificial intelligence (AI), in decision-making contexts necessitates a critical examination of their alignment with societal and environmental values. The reciprocal relationship between these norms and emerging AI technologies calls for a structural conceptualization of algorithmic systems that extends the scale of human-centered considerations. This dissertation introduces “Public Interest Computing, a Pluralistic Design Language,” which enables a novel design space for value-sensitive algorithmic ecosystems, fostering what we term “Societal-Machine Alignment.” The research is structured in three interconnected parts. First, we establish a comprehensive theory of Public Interest Computing, grounded in the planning and capability approach to human development. Second, we present a series of Public Interest Computing systems that instantiate and refine the proposed theoretical framework. These systems, co-designed with communities, demonstrate societal-machine alignment through five key design dimensions. Farm Pulse System exemplifies substantive fairness for at-risk farmers by enabling restorative justice through recourse in climate change adaptation decisions. Boomerang exhibits incentive alignment, promoting equitable designs of reputation systems in AI data markets. The Prototype Tasks System illustrates computationally mediated cognitive alignment, creating a level playing field for workers. The Beyond Boundaries framework enables environmental alignment, providing a platform for public discourse on climate change. Our analysis using Gobo focuses on value alignment, investigating ways to increase human agency in interactions with invisible algorithms on online platforms. Each system serves as an empirical testbed, providing critical design insights that shaped the theory and engineering of Public Interest Computing.\r\n\r\nThe third part demonstrates the interplay between the developed Public Interest Computing systems and policy by applying the Pluralistic Design Language to real-world scenarios. We illustrate\r\nthe bidirectional relationship between technology and policy, showing how Public Interest Computing informs policy decisions (“AI for Policy”) and, conversely, how policy shapes the responsible development of AI systems (“Policy for AI”). This symbiotic relationship opens new avenues for\r\nevidence-based policymaking, with Public Interest Computing serving as a foundation. By synthesizing the insights gained from this demonstration, we offer a principled approach for future\r\nresearch and practice, paving the way for a more informed and responsible design of algorithmic\r\nsystems that aligns with societal values and priorities.\r\nPublic Interest Computing and its Pluralistic Design Language serve as a guiding lens, leading us\r\ntowards a future where societal values and algorithmic ecosystems are inherently aligned. Public\r\nInterest Computing is not an end in itself but a means for understanding, reflection, and adaptation, ensuring that as technology advances, so does our commitment to aligning it with the greater\r\ngood.",
        "authors": [
            "Snehalkumar 'Neil' S. Gaikwad"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157721",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Laboratory studies of atmospheric photochemistry in indoor and outdoor environments",
        "abstract": "Secondary organic aerosol (SOA), fine particulate matter formed through indirect photochemical reactions, influences the climate and contributes to air pollution harmful to human health. While these two effects act at different scales, they are governed by similar chemical processes. This work investigates the atmospheric photochemistry of indoor and outdoor environments, giving particular attention to the reactions that lead to SOA formation, notably those involving oxidant and peroxy radical (RO2) chemistry.\r\nFirst, this thesis examines the oxidation of dimethyl sulfide (DMS), which represents a large natural source of sulfur to the atmosphere and affects the climate. Using varied chemical conditions across numerous environmental chamber experiments, we characterize aerosol formation from the oxidation of DMS, as well as two related compounds, dimethyl sulfoxide and dimethyl disulfide. We also measure key rate constants crucial to understanding the formation and fate of hydroperoxymethyl thioformate, an important recently-discovered DMS product.\r\nSecond, this work investigates the indoor air quality implications of 222 nm germicidal ultraviolet lamps (GUV222). While these lamps are effective at reducing the spread of airborne pathogens, they lead to the formation of ozone (O3), a harmful air pollutant. Through environmental chamber experiments, we quantify the GUV222-driven production of O3, OH, oxidized products, and SOA, and further demonstrate that GUV222 causes new particle formation. Based on these results, we recommend that GUV222 lights be operated at their lowest effective level.\r\nFinally, we pivot to examine assumptions embedded within the relationship between chamber experiments and SOA parameterizations in global chemical transport models. We represent historical laboratory experiments in a box model, enabling explicit estimates of the unmeasured RO2 and oxidant chemistry that influences aerosol formation. This work shows that reaction conditions are dynamic, changing within and between experiments, and demonstrates that RO2 isomerization is implicitly built into SOA parameterizations, even without its explicit representation.\r\nOverall, this thesis connects multiple areas of indoor and outdoor atmospheric photochemistry, improving our understanding of marine organosulfur chemistry, the impacts of GUV222 lamps, and the relationship between laboratory chamber measurements and the modeling of aerosol on a global scale.",
        "authors": [
            "Matthew B. Goss"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157732",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Imagine Yourself: Explorations in Fostering Personal Expression with Generative AI",
        "abstract": "Generative Artificial Intelligence (AI) technology has been promoted with many exciting promises to enhance human creativity. However, it has also been shown to amplify human bias and perpetuate harmful stereotypes. In the new age being ushered in by this technology, this thesis explores how educators and designers can use this technology to support young people in exploring and expressing aspects of their unique identities. In particular, I use a design based research methodology to iteratively create Imagine Yourself, a new digital experience adapting off-the-shelf text-to-image generation technology to support young people creating personal representations and stories.\r\nImagine Yourself combines OpenAI’s Dall-E 3 image generation technology with Scratch, a rich environment for young people to imagine and create interactive multimedia stories, animations, and more. Guided by a core value of designing for belonging, this project explores how experiences with generative AI can be designed to foster young people’s creative process in creating personally meaningful stories reflecting their own unique identities, experiences, and cultures. I discuss the iterative design process of creating Imagine Yourself in tandem with creative workshops, aiming to support more diverse representation within the image generation output and invite a tinkerable and iterative process of creating. I discuss observations and feedback from creative workshops with young people and adults, creating with Imagine Yourself. Finally, I conclude with reflections on the design process as well as a discussion of challenges, limitations,  opportunities, and open questions for future work incorporating generative AI into young people’s creative learning experiences.",
        "authors": [
            "Karishma Chadha"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157722",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Timbral Transformations",
        "abstract": "From folk songs to festivals, cafes to concert halls, and religious rituals to recording studios, the flute has long had a shapeshifting, cross-cultural presence. This thesis leverages 21stcentury technologies not only to explore and extend the timbral versatility of flutes, but also to underscore the performative, fluid, and ever-evolving nature of timbre more generally. At the core of the project is the creation of sequences of discrete sounds that interpolate between semantic categories and a collection of fixed media compositions based on those sequences, both of which consist entirely of flute sounds that have undergone varying degrees of electronic manipulation. By means of digital signal processing techniques, the flute wavers in and out of a multitude of sonic identities. Sometimes, it masquerades as another familiar object or interface (e.g., a ticking clock) or abstractly evokes a concept or phenomenon (e.g., a storm); at other times, it beckons toward the ethereal or ineffable, resisting indexical identification altogether. With source materials warped, layered, and splayed across the frequency spectrum, such concerns as “the real” and “the true” begin to move out of focus, making way for attention to embodied phenomenological experiences of sound. As this thesis positions compositional practice as a form of research, its outputs range from the conceptual to the creative and the computational. In addition to the music at its core, the project interfaces with gender studies in its original exposition on timbre and timbral identity, includes a rigorous set of experiments with human and machine listeners, and makes original applications of multimodal language models not before seen in musicology or music theory. A live performance incorporating each of these project vectors and an audience discussion following the event offer further opportunities for reflection and critique.",
        "authors": [
            "Jessica Shand"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157706",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Piezoelectric single crystal based one-dimensional phased array for breast tissue imaging",
        "abstract": "Ultrasound is widely used in clinical practice because it is safe, non-invasive, non-ionizing, low-cost, and provides real-time imaging, monitoring, and therapy. However, conventional ultrasound probes are rigid, pressure-required, and operator-dependent. Replacing rigid transducers with conformable ultrasound transducer arrays can allow image acquisition on curved body parts, improve image quality, and enable functions such as long-term monitoring. In this thesis, I propose a conformable ultrasound breast patch (cUSBr-Patch) consisting of a one-dimensional (1D) phased array and a nature-inspired patch design, which offers large-area, deep tissue scanning and multi-angle, repeatable breast imaging while avoiding the drawbacks of conventional ultrasound imaging technologies. I used a Yb/Bi-doped PIN-PMN-PT single crystal as the active element due to its superior piezoelectric properties (d33 = 2,800 pC/N, εr = 7,000, k33 = 0.93). I then fabricated a 1D phased array transducer consisting of 64 elements with an operational frequency of 7.0 MHz. The 1D array exhibits promising acoustic performance with i) a maximum imaging depth of 80 mm, ii) contrast sensitivity of 3 dB, iii) axial/lateral resolutions of 0.25/1.0 mm at 30 mm depth, and iv) a larger field of view than the commercial handheld linear probe at depths of approximately 30 mm or deeper, indicating a potential reliable capability to detect early-stage breast tumors. Beyond this, comprehensive in vitro experimental studies establish that the cUSBr-Patch can provide accurate and reproducible imaging of different phantoms. The clinical trials reveal that the patch exhibits a sufficient contrast resolution (~3 dB) and axial/lateral resolutions of 0.25/1.0 mm at 30 mm depth, allowing the observation of small cysts (~ 0.3 cm) in the breast. This research develops a first-of-its-kind ultrasound technology for breast tissue scanning and imaging which offers a non-invasive method for tracking real-time dynamic changes of soft tissue.",
        "authors": [
            "Wenya Du"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157727",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Wearable Gut and Brain Interfaces for Valence Detection and Modulation",
        "abstract": "Emotion detection interfaces have shown promise in mediating our emotional health through improved diagnosis, self-tracking, social support systems, mindfulness, and biofeedback training; however, the most popular methods falter when distinguishing between positive and negative valence - or lead to privacy or social issues. Brain and gut interfaces can serve as an alternative, but often require complex setups with many electrodes, large datasets, and the usage of significant training to achieve benchmark emotion detection performance. I present novel, wearable gut- and brain-interfaces for valence detection and modulation that can be made feasible with as few as two electrodes, minimal training and statistical analysis. I coin and define the area of gut-brain computer interfacing (GBCI), while further developing the field of affective brain-computer interfacing (aBCI). I take a novel approach by using the stomach signal and motivational direction models as an alternative to traditional affective modalities and models. I present Joie, a joy-based electroencephalography (EEG) brain-computer interface (BCI); JoyNet, a neural network for joy detection with EEG; and KALM, an EEG, electrodermal activity (EDA) and respiration rate multimodal fusion model. I also present Serosa, an novel electrogastrography (EGG) GBCI which non-invasively records indices of gastric neurons that can be correlated with emotional states and provide a new affect detection modality. This thesis presents findings and innovations in research and application: first, offline affect detection models which contextualize neural with embodied modalities and evaluate how each signal influences affect detection performance. Second, novel real-time interfaces are implemented and evaluated with placebo-controlled laboratory studies. Third, I present a novel neuroethics discussion which uses socioecological models to anticipate harms and I reflect on the works in this thesis.",
        "authors": [
            "Angela Vujic"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157740",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Comparison of Finite Element Methods and Satellite InSAR for Monitoring Deformations of a Large Tailings Dam",
        "abstract": "Following the recent catastrophic failure of several mine tailings dams there has been much interest in the use of numerical modeling and remote sensing for monitoring the safety and stability of these structures. This thesis presents a case study that investigates the accuracy of InSAR measurements and the predictive capabilities of finite element models using ground truth surface and sub-surface monitoring data applied to the Zelazny Most (SW Poland) copper tailings storage facility.  This site has a well-documented history of lateral deformations in a critical section (XVIE) of the East dam that have been attributed to a deep-seated translation mechanism of shearing through the underlying Pliocene, glacial clays. Since 2014, operators of the facility have constructed a series of stabilizing berms at this critical section. We investigated the accuracy of InSAR over this period, ending in 2019, by analyzing 186 ascending Sentinel-1 C-band images and 219 descending images using Persistent Scatterer Interferometry and SARProzTM software, comparing results with two surface geodetic benchmarks. Finite element analyses of the structure required a 2D model of section XVIE. We developed and integrated a stratigraphic model for the foundation soils, the complete construction history of the dam (since 1975), and selected input parameters for constitutive models to represent the soil behavior (foundation soils, tailings, dyke and berm materials) using PlaxisTM software. Our results show that InSAR achieves very consistent agreement with geodetic measurements for vertical (Up-Down) and lateral (E-W) surface deformations, over a time period where construction was limited to raising of the dyke near the crest of the dam and berm construction at the toe. The InSAR data are also insightful in showing relatively uniform lateral deformations occurring over the face of the dam, consistent with the interpreted translational failure mechanism. In contrast, it has proved much more challenging to predict subsurface deformations by FE analyses. The computed movements reflect accumulation of deformations over multiple stages of construction and involve shearing through the complex foundation stratigraphy.  We were able to achieve credible estimates of lateral deformations within the range of laboratory shear strength properties published in the literature and using the Hardening Soil (HS) model for non-linear shear stress-strain properties. However, the predictions of surface settlements and lateral deformation are much less reliable and depend on undocumented properties of the tailings, phreatic conditions in the tailings and details of the construction history.",
        "authors": [
            "Robert Henry Fetell"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157720",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Tomorrow's Typography",
        "abstract": "This thesis is an exploration for new tools for typography that investigates how emerging (AI) technologies can contribute to the type design practice in a meaningful way. I created computational design experiments focusing on three areas: (A) design automation, (B) interfacing, and (C) creative exploration. A lot of care has been put in understanding the current scene through expert interviews, workshops, talks and surveys. With pose estimation, generative visual AI, and large language models that operate on text, I explore whether typographic shapes can be created and manipulated with different modes of expression, in a playful, intuitive and collaborative way.",
        "authors": [
            "Vera van de Seyp"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157735",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Impact of Vegetation Morphology on Turbulence and Bedload Transport",
        "abstract": "By promoting sediment deposition and retention, aquatic vegetation can contribute to river bank stabilization, biodiversity, as well as carbon sequestration. The morphology and distribution of aquatic plants influence the velocity field, turbulence intensity, and sediment transport in wetlands, which impacts the erosion and deposition processes. By combining physical and numerical experiments, this thesis quantified how vegetation geometry impacts turbulence and sediment transport near the bed.\r\n\r\nIn aquatic canopies, turbulence generated at the stem scale, and for submerged canopies, also in the canopy shear layer, could contribute to the near-bed turbulence. Results of flume experiments using a constant channel average velocity revealed that bedload transport was predominantly correlated with near-bed turbulence, but was also weakly correlated with near-bed velocity. First, in emergent canopies, if vegetation was not clustered, turbulent kinetic energy (TKE) and bedload transport did not depend on the arrangement and stem diameter(s) and can be predicted from plant biomass and velocity. If vegetation was clustered in patches, TKE and bedload transport decreased with increased clustering and can be predicted from plant biomass, patch geometry, and velocity. Second, in submerged canopies, for constant channel velocity, submerged canopies could enhance or reduce bedload transport, depending on their degree of submergence. With increasing submergence, H/h (defined as the ratio of flow depth H to canopy height h), the near-bed velocity and TKE decreased, and the source of near-bed turbulence shifted from stem wake to the shear layer at the canopy top. A model to predict near-bed TKE in submerged canopies was developed and used to explore bedload transport under more realistic conditions with constant energy slope and flexible vegetation. For a constant energy slope, the denser the canopy, and/or the larger fraction of flow depth occupied by the canopy (decreasing H/h), the greater the sediment transport was reduced compared to unvegetated beds. This thesis provides essential parameterizations of vegetation to hydrodynamic and morphodynamic models, which can be used to predict the vegetation conditions that promote or diminish erosion, offering a useful guide for river and coastal restoration.",
        "authors": [
            "Tian Zhao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157716",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Advancements in Management Science: Applications to Online Retail, Healthcare, and Non-Profit Fundraising",
        "abstract": "Management science is an evolving-field that requires novel models and algorithms, combining methods from statistics, optimization, and machine learning. This thesis presents advancements in management science across three domains: revenue management, healthcare, and non-profit funding platforms. The chapters in this thesis develop rigorous algorithms and techniques which are relevant in practice, and present data-driven insights into each of the application areas. \r\n\r\nChapter 2 studies a personalized dynamic pricing problem commonly faced by online retailers. Customers arrive sequentially to the selling platform, and for each arrival the seller must make an immediate pricing decision for that customer. The seller aims to learn the demand as a function of price and customer covariates through price experimentation, while simultaneously earning as much total revenue as possible. Previous work on this topic have adopted a classical online learning setup, where the retailer begins the selling horizon with no information about the problem and gains all knowledge about the demand function from the online selling phase. However, this assumption is often not true in practice. Many retailers already possess some information about their product's demand from market research or previous sales data, and not utilizing this information is clearly suboptimal. The chapter develops a novel framework that allows the seller to incorporate historical data on pricing decisions and realized demand, and moreover enables one to study the effect that certain characteristics of this historical dataset have on online selling performance. Using this framework, a dynamic pricing algorithm is proposed which effectively uses both historical and real time data, and achieves provably optimal performance. Furthermore, a new distance measure is developed to quantify how close the historical pricing decisions are to being optimal. Using this distance measure, the chapter shows a surprising inverse relationship between this measure and the achievable online performance.  \r\n\r\nChapter 3 focuses on applying causal inference techniques to study the treatment efficacy of different antibiotics on patients with urinary tract infection. Up to 50% of women will experience a urinary tract infection (UTI) in their lifetime, making it the third most common indication for antibiotic treatment in the United States. Though national treatment guidelines encourage using one of three antibiotics as the first-line treatment, other second-line and alternative antibiotics are still commonly prescribed in practice. Studies on the efficacy of first-line versus second-line and alternative antibiotics for UTI are limited and dated. The chapter presents a retrospective cohort study using the claims database from Independence Blue Cross to determine the relative efficacy and adverse event rates between different categories of antibiotics. By combining causal inference techniques with automated feature extraction using the Observational Medical Outcomes Partnership (OMOP) common data model, evidence is found which supports the use of guideline-recommended first-line treatments for uncomplicated UTI. Specifically, the rate of treatment efficacy is higher for first-line antibiotics relative to alternatives. Surprisingly, the analysis also finds evidence which supports increased efficacy of first line agents relative to second-line antibiotics, which are of broader spectrum, albeit the effect difference is smaller compared to the comparison between first-line antibiotics and alternatives. This large-scale cohort study which includes a comprehensive collection of covariates provides much-needed evidence to support the continued recommendation of first-line drugs for the treatment of UTI. The chapter also suggests the feasibility for performing complex causal inference analyses using automated feature engineering packages for OMOP-formatted datasets.\r\n\r\nChapter 4 studies an online matching problem where sequentially arriving donors must be matched to projects needing funding on peer-to-peer philanthropic crowdfunding platforms such as DonorsChoose.org. Empirical studies have shown that (i) donors have heterogeneous preferences over the projects, and (ii) many return to make more than one donation. Facing such donors, the platform’s aim is to match each donor to one of their preferred projects so as to maximize the total donation without over-funding any projects and without knowing the arrival pattern. Previous work in the literature have not studied the effect of returning donors on algorithm performance. The chapter shows an upper bound on the best achievable worst-case performance of any online algorithm which reveals the relationship between donor return rate and algorithm performance. Furthermore, numerical analysis shows that a simple known algorithm achieves a performance that improves with the number of returning donors without differentiating between the original and return donors. The algorithm is intuitive and straightforward to implement, and the results shed light on the practical value that returning traffic can bring for fundraising platforms.",
        "authors": [
            "Chen Wen (Sabrina) Zhai"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157734",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Temporal Telepresence: Immersive Interfaces for TeleAbsence",
        "abstract": "To store the past in a simulation may enable greater understanding of ourselves, our stories, and our histories. The urge to capture our past into networks of photographic, written, filmed, and object-based narratives has long been a means for individuals to identify change, growth, and gain perspective on themselves. Using a dataset of human narratives derived from records and ephemera, this thesis explores a novel approach to preserving and interacting with memories. We present an interactive system of objects and applications that supports intergenerational memory preservation by enabling individuals to actively explore the relationship between personal artifacts, photographs, the spaces of their past, and their memories. This system integrates personal digital twins, photogrammetry, Gaussian splatting, and tangible interfaces to create a new way of experiencing the past, based on interactivity with architectural artifacts and simulations from an individual’s life. Using an iterative participatory design process, we developed a set of multisensory interaction experiences that allow individuals to explore their relationship to autobiographical memory. The system dynamically links autobiographical memories with the environments where they took place, responding to text, photo, and object-based interactions. This experience invites individuals to modify their recollections by exploring how photo, video, and 3D space relate to the experience of revisiting narratives from the past. Applications of this system include assisting with dementia, aging, memory loss, and Alzheimer’s. Our initial studies were promising. When using the simulation system, individuals spent more time reminiscing, discussing more memories, and experiencing greater presence in their recollections than without the interactive paradigm. The system also encouraged family members to reinforce their memories by actively re-encoding them through the simulation interfaces. Results demonstrated that presence in memories seemed more vivid, detailed, and spatially accurate than before the intervention. The result is a new memory-sharing experience that benefits individuals and families by allowing them to understand how their interactions with the past can be enriched through the integration of artifacts and simulations that impact the development of autobiographical memory.",
        "authors": [
            "D. Pillis"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157709",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Risk-Benefit Assessment of Pandemic Virus Identification",
        "abstract": "Pandemic Virus Identification (PVI) aims to assess unknown viruses for their pandemic potential in immunologically naive human populations. While proponents argue that PVI could facilitate targeted spillover prevention and accelerate medical countermeasure development, critics raise concerns about biosafety and biosecurity risks. This thesis presents a comprehensive mathematical framework to evaluate the benefits, biosafety risks, and biosecurity risks associated with PVI research.\r\n\r\nUsing a combination of mathematical modeling and expert elicitation, we developed a structured approach to estimate the potential impacts of PVI. Our framework suggests that identifying a single pandemic-capable virus through PVI could potentially save lives by reducing natural pandemic risks. However, this benefit is substantially outweighed by the estimated anthropogenic risks from potential accidental pandemic events and deliberate misuse scenarios. The overall expected value of identifying a single pandemic-capable pathogen was estimated to be strongly negative. \r\n\r\nSignificant uncertainty exists in many key parameters estimated through surveys, with wide confidence intervals reflecting the lack of consensus among experts. Expert opinions varied considerably on topics such as the likelihood of funding for medical countermeasures and the potential for deliberate misuse of pandemic agents. This modeling work primarily aims to provide exploratory estimates to guide future work. \r\n\r\nOur findings underscore the urgent need for improved governance of research involving potential pandemic pathogens. This study provides a quantitative basis for ongoing discussions about the balance between scientific advancement and public safety in high-risk areas of life sciences research.",
        "authors": [
            "Geetha Jeyapragasan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157708",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Beyond-the-Ice: Designing Games for Facilitating Deeper Conversations",
        "abstract": "In this age of constant communication, we’ve never been more connected, yet all of our numerous, fast, and convenient connections lack the depth and intimacy we truly crave. The desire for more authentic social experiences necessitates vulnerability, honesty, and risk; but introducing such dynamics presents a great challenge in the context of the wider landscape of public discourse. Designers across disciplines have suggested using games to facilitate stronger social connection, since the structures within games can expose players to alternate social norms and encourage risk-taking. However, few have designed games that specifically foster more intimate forms of dialogue or offer scaffolding for players to see the act of sharing authentically and listening deeply as ways to play. In this thesis, I explore the novel intersection between play, intimate conversation, and technology by presenting a variety of prototypes and fully developed games that employ innovative mechanics designed to facilitate authenticity, vulnerability, complexity, and subjectivity. This work builds on formal knowledge from the social sciences, HCI, and game design, as well as informal knowledge from facilitation, gathering practices, party games, and Tarot, by presenting five distinct design principles aligned with theories grounded in past work: 1) Make emotional disclosure special; 2) Scaffold responsiveness; 3) Approach depth through fun; 4) Empower “the work” through constraints and permissions; 5) Center objects to feel with. Following a thorough Research through Design (RfD) method, I designed 15 unique prototypes and proof-of-concepts which explore various aspects of the five principles. Two of the games were designed, developed, playtested, and evaluated – Analogia, a card game that uses generative images to inspire emotion-rich conversations and Crossroads, a digital game where players are guided to unlock a secret insight by co-creating generative images inspired one another’s real experiences. This work contributes two well-tested games that evoke five compelling principles; a series of mechanics for stimulating dialogue (dual-stimulus, bridge-and-tunnel, image scrying, listener roles); and pilot data from playtests that demonstrate the ability and challenge of these mechanics to create conversational outcomes. Additionally, both spotlighted games creatively employ generative artificial intelligence (AI) to help mediate player interactions through image interpretation and co-creation. Although this is a thesis about conversation games, it critically engages with the current social zeitgeist, provides widely applicable insights and presents nuanced ways to think about the future of social-technical systems that seek to encourage deeper, more authentic ways of connecting.",
        "authors": [
            "Cassandra Lee"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157712",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Interoceptive Interventions: Interfacing with Inner States",
        "abstract": "This thesis explores the emerging frontier of Human-Computer Interaction (HCI) that moves beyond traditional interfaces to directly modulate internal bodily processes, emotions, and cognitive states. As HCI progresses towards further integration between human and machine, this thesis investigates novel technologies that interface with interoceptive systems to influence subjective experiences and mental states. In this thesis, I introduce \"Interoceptive Interventions\" — tools designed to modulate physiological states. These tools interface with and alter internal physiological conditions, thereby influencing emotional and behavioral states.\r\n\r\nI present three individual proof-of-concept wearable prototypes “Frisson”, “ReCode”, and “Somnia” - grounded in neuroscience theories and evidence from embodied cognition. Frisson is a system targeted to elicit aesthetic chills and their downstream cognitive effects. I showcase experimental evidence of chills’ impact in modulation of emotional state, negative beliefs and amelioration of anhedonia in depression. Next, I present ReCode, a system which modulates baroreceptor activity and causally influences sympathetic activity (fight or flight response) and has consequential effect on perceived emotion and anxiety ratings. Finally, I present Somnia, a system which stimulates the vestibular system to influence sleep onset. These prototypes target specific pathways to enable on-demand emotion elicitation, emotion regulation and sleep regulation for users, while also providing potential non pharmacological interventions for conditions like insomnia, depression, and anxiety.\r\n\r\nThis thesis aims to make a twofold contribution: First, it introduces a conceptual framework that highlights how interfacing with unconscious bodily processes opens up new possibilities for human computer interface design. Specifically, by gently actuating core physiological dynamics linked to consciousness and psychology, there is potential for such tools to deliver a promising new paradigm for digital wellness interventions. Second, the interoceptive modulation tools developed in this work provide a platform for researchers to experimentally engineer physiological processes underlying emotions and sleep. This could allow examining causative pathways between physiology and psychology beyond correlational observations and developing interventions for affective/sleep disorders. Researchers and designers can build on this to advance a generation of augmented technologies that empower users to self-regulate the body and the mind.",
        "authors": [
            "Abhinandan Jain"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157724",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Secure Computation in Decentralized Systems",
        "abstract": "Decentralized systems like Bitcoin and Ethereum are real-world examples of secure distributed systems deployed at scale. Over the past decade, these systems and others have proven to provide a trust-minimized solution for computing. They ensure the correct execution of code (correctness), maintain the integrity of stored data, and remain consistently available (availability). Additionally, they allow any user to interact without the risk of censorship.\r\n\r\nHowever, while decentralized systems guarantee security properties like integrity, correctness, and availability, they do not provide privacy. In this regard, they are strictly worse than assuming full trust in a centralized server, since any node in the network must see all data. Furthermore, in many of these open systems (also known as 'permissionless' networks), there are no restrictions on who can operate a node. This means that decentralized systems, and public blockchains in particular, cannot operate on private data, greatly limiting the kinds of use-cases they can support.\r\n\r\nThis dissertation explores solutions to mitigate the privacy concerns associated with modern decentralized systems, focusing particularly on blockchains. The research employs Secure Multiparty Computation (MPC) techniques to address these issues, demonstrating how MPC, which already shares a similar distributed trust threat model, can enhance privacy in decentralized systems. More specifically, this thesis focuses on the following key areas in decentralized systems:\r\n\r\nAccess Control Mechanisms and Confidential Smart Contracts: The thesis begins by exploring access control mechanisms on blockchains, and from that builds up to the concept of confidential smart contracts -- arbitrary programs that execute both correctly and privately.\r\n\r\nIdentity Management and Authentication: Building on access control and confidential smart contracts, we examine identity management and authentication within decentralized networks. We develop a highly efficient Threshold ECDSA protocol that runs in the server-aided MPC model.\r\n\r\nPerhaps more importantly, we revisit the server-aided MPC model itself, which sits somewhere between the dishonest and honest-majority MPC paradigms, and show that a confidential smart contract is a real-world realization of the server in this model. We thus theorize that dishonest MPC protocols in general can be practically improved under this model, and argue that because there is a real-world counterpart, this model is realistic.\r\n\r\nAn Improved Distributed Point Function (DPF) and ORAM: A major theoretical contribution of this work is a novel three-party Distributed Point Function (DPF) construction. This leads to state-of-the-art Oblivious RAM (ORAM) and Distributed ORAM (DORAM) protocols, which are important building blocks in MPC.\r\n\r\nPrivacy-Preserving Digital Currencies: Using this DPF construction, we revisit the problem of privacy-preserving digital currencies, proposing a solution in the account model. This approach challenges the current consensus that privacy in blockchains requires a UTXO model.\r\n\r\nSecure Inference with private retrieval: Lastly, the thesis explores how Large Language Models (LLMs) can perform secure inference while retrieving data from private, distributed databases. This method represents a step towards building secure decentralized AI systems that respect user privacy.",
        "authors": [
            "Guy Zyskind"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157739",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On the non-microbial sources and sinks of dissolved metabolites in seawater",
        "abstract": "Dissolved marine metabolites are small (<1000 Da) organic chemicals that remain in seawater when passed through a filter (typically <0.2 µm pore size). Their name implies their biological function: to be produced and consumed by cellular metabolism. These chemicals are the flows of the “microbial loop”—the principle that most of the photosynthesized matter in the ocean is exchanged, respired, and restructured by single-celled organisms. Metabolites have critical biological utility, so they are considered extremely labile; estimates of the time each spends outside cells range from hours to days. Their concentrations are drawn down by their consumers to nanomolar and picomolar levels, making measurement difficult. However, improved techniques to measure metabolites simultaneously and at extremely low concentrations avail the question of what happens to metabolites outside the cell membrane. Conventionally, representations of labile DOM exchange networks avoid that question—metabolites’ short lifetimes imply their flows lead from one organism to the next. This thesis begins to interrogate that assumption, asking if there are other processes that could change the seawater exometabolome on time scales that are relevant to microbial life. In Chapter 1 I discuss the ways ambient metabolite pools could be affected by animals, chemistry, and physics. In Chapter 2 I investigate the photolysis of metabolites and examine metabolomic techniques’ suitability for such experiments. In simulated sunlight, 11 of 57 metabolites decayed to some extent in artificial or natural seawater, and tryptophan and kynurenine may decay rapidly in the mixed layer of an oligotrophic ocean. For Chapter 3, I captured five species of migratory zooplankton and measured metabolites in their dissolved excreta. Four species survived the experiment and produced 43 metabolites, many at a rate that should be measurable in field samples. Chapter 4 harnesses the previous two chapters, plus a model for physical mixing, to probe a field dataset comprising 60 metabolites from Hydrostation S (south of Bermuda). Based on eight profiles over the course of two days, I posit: (1) copepods alone can supply the entire demand of >20 compounds to the mixed layer; (2) mixing is rapid enough to erase input signatures in the mixed layer; and (3) photochemistry is a slow leak of metabolites to forms whose lability is yet unknown. Chapter 5 reflects on how metabolites break the microbial loop—and suture it together with more ecological richness than with elemental fluxes alone.",
        "authors": [
            "Noah Paul Germolus"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157715",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Designing for Connection with Inner Processes",
        "abstract": "At a time of division, it is more important than ever that we help each other feel truly understood. Today's online ecosystems offer us many new ways to communicate personal stories, often through fast-paced, reactive channels, but few if any technologies enable us to share what I posit to be a crucial component of how we implicitly understand each other: our inner processes, e.g., how we form our values and identities, navigate unspoken tensions in a community, or feel that something resonates with us.\r\n\r\nThis thesis explores inner processes as a resource for the design of systems that support human connection, interpersonal understanding, and reflection. Through a series of design iterations, I weigh approaches to eliciting inner processes, choosing media to externally, evocatively represent them, and encouraging perspective-taking behavior by guiding users through each other's inner processes. I approach this topic through three streams of projects, grounded in literatures that outline guidelines for successful perspective-taking and the development of interpersonal closeness, and that assert the value of creative play in surfacing and communicating inner processes, supporting perspective-taking, making room for new social norms, and enabling reframing.\r\n\r\nFirst, I present our collaborative work on Closer Worlds, a two-player, AI-assisted game in which players generate a world they might both want to live in in order to scaffold an emotionally intimate conversation about their memories and shared values. Next, to better understand inner processes entangled with creative practice, I conduct interviews with creative practitioners about the relationships they build through their practice, and design and develop prototypes for implicitly retracing inferred versions of one's own or another person's creative process, capitalizing on room for interpretation. Prototypes include Sjuzet, a compass that anchors the latent space of a user's creative writing to a local map in order to prompt reflection as a user physically wanders through memories, and Pull It Together, a material speculation on textile swatches whose wear and tear modulates to correspond to invisible sociocultural tensions. Finally, I shift my focus to explicitly, informatively trading inner processes in my design of Metaswap, an asynchronous, written activity in which strangers compare annotations about inner processes that arise as they tell personal stories about an uncertainty they are working to resolve in their lives.\r\n\r\nMaking inner processes explicit and prompting revisitation of them offered both benefits and drawbacks for connection and reflection, but revealed important questions. A mixed-methods analysis across this work presents tensions in the human and machine instinct to make inferences and assumptions about others, and offers opportunities for interpersonally insightful, vulnerable, and trusting conversation when computer-mediated communication and sense-making systems produce deep content rather than deep interactions. Through this work, I hope to lay the foundation for future research on technology's role in supporting interpersonal understanding at a time when so many subjectivities collide and are summarized at the speed of data.",
        "authors": [
            "Jessica Rachel Mindel"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157719",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Quantifying the effects of sunlight on the fate of oil spilled at sea",
        "abstract": "Oil spilled at sea is transformed by sunlight-driven photochemical reactions. The transformed oil has different properties and behavior in the environment compared to the fresh oil, resulting in different fates and effects. My work in this thesis was to put numbers on these changes, with the goal of better predicting where oil goes and how it behaves in diverse spill scenarios. First, I focused on how sunlight generates water-soluble compounds from oil, which can lead to the dissolution of oil-derived compounds in seawater (photo-dissolution; Chapter 2). To find out whether photo-dissolution could be an important fate process during an oil spill, I used a combination of experiments and photochemical rate modeling to calculate photo-dissolution rates for the 2010 Deepwater Horizon spill (DwH) in the Gulf of Mexico (GoM). I found that photo-dissolution likely converted ~8% of the floating surface oil to dissolved organic carbon during DwH, a fraction similar in magnitude to other well-recognized fate processes. Moving beyond DwH, I evaluated the sensitivity of oil photo-dissolution and photochemically-altered oil physical properties to temperature. I found that if a spill like DwH had occurred in 5°C water rather than the exceptionally warm 30°C water of the GoM, 7x less oil could have dissolved via photo-dissolution and the viscosity of the remaining insoluble oil could have been 16x higher, resulting in lower entrainment of oil into the water column as small droplets (Chapter 3). The net result is that more oil would stay at the sea surface in a cold-water spill. Finally, I determined photo-dissolution rates for diverse oil products beyond the light crude that spilled during DwH (Chapter 4). I found that oil photo-reactivity could be predicted from oil chemical composition. I also found that photo-dissolution likely affects oil mass balance in spills of light oils forming thin slicks but not in spills of light or heavy oils forming thick slicks. Overall, this work advances our understanding of how oil changes in the environment upon sunlight exposure. This information can be applied to better predict, evaluate, and mitigate the effects of oil spilled at sea on marine ecosystems, including humans.",
        "authors": [
            "Danielle Haas Freeman"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157736",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Towards a Single Bio-molecule Detector Based on CMOS\r\nNanofluidic Platform",
        "abstract": "Cytokines secretion is a core component of the function of many cell therapy products: It affects the tissue repair capacity of induced Pluripotent Stem Cells (iPSCs) and Mesenchymal Stem cells (MSCs) and the tumorigenicity of Chimeric Antigen Receptor (CAR) T-cell therapies. Ideally, we would be able to continuously monitor the secretome of these cell therapies as they are transformed and expanded in manufacturing.However, state-of-theart techniques for monitoring typically low concentrations of cytokines require either Mass Spectroscopy (MS) or immunoassays like Enzyme-linked Immunosorbent Assay (ELISA). We propose the use of CMOS technology to build a proteomic platform with a single biomolecule resolution. A prototype chip has been designed and fabricated using standard foundary process incorporating a new implementation of a Solid State Nanopore (SSN) of size 55nm×162nm×100nm (w×l×h) with nanofluidic access channels that bridge the buffer solution between the assay space in the packaging structure – a poly carbonate/Polydimethylsiloxane (PDMS) package- and the nanopore on the chip. A silicon Single Photon Avalanche Detectors (SPADs) was also implemented and placed near the nanochannels to utilize fluorescence labeling imaging techniques. In addition, a read-out amplifier that achieves a midband gain of 36.2 dB at a 3 dB bandwidth of 0.1-3.6 MHz is also implemented on the same silicon die, paving the way to superior performance compared to ionic current read-out systems used earlier for electrical biomolecule detection, thanks to low parasitics as a result of integration. The aforementioned modalities integrated on a single chip open the space for the use of CMOS platforms in the electrical and optical interrogation of biomolecules, opening a new horizon for near real-time biomarker assays. The following thesis builds on earlier work that was performed in [1][2] with the objective of expanding on different techniques to interface and characterize the performance of these modalities, especially after post-processing the chips with the aid of tools at MIT.nano. The thesis explores the further deployment of integrated SPAD in a Fluorescence Lifetime Imaging (FLIM) system to image fluorescence-labeled molecules, showcasing the capabilities of the CMOS nanofluidic platform to detect biomarkers such as cytokines.",
        "authors": [
            "Ahmed S. Zikrallah"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157733",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Listening by Synthesizing",
        "abstract": "Generative audio models offer a scalable solution for producing a rich variety of sounds. This can be useful for practical tasks, like sound design in music, film, and other media. However, these models overwhelmingly rely on deep neural networks, and their massive complexity hinders our ability to fully leverage them in many scenarios, as they are not easily controllable or interpretable. In this thesis, I propose an alternate approach that relies on a virtual modular synthesizer; a computational model with modules for controlling, generating, and processing sound that connect together to produce diverse sounds. This approach has the advantage of using only a small number of physically-motivated parameters, each of which is intuitively controllable and causally interpretable in terms of its influence on the output sound. This design takes inspiration from devices long used in sound design and combines it with state-of-the-art machine learning techniques. In this thesis, I present three projects that use this formulation. The first is SynthAX, an accelerated virtual modular synthesizer that implements the core computational elements in an accelerated framework. The second, CTAG, combines the synthesizer with an audio-language model into a novel method for text-to-audio synthesis via parameter inference. This method produces more abstract sketch-like sounds that are distinctive, perceived as artistic, and yet similarly identifiable to recent neural audio synthesis models. The third is audio doppelgängers, sounds generated by randomly perturbing the parameters of the synthesizer to create positive pairs for contrastive learning, encompassing more of the variety found in real-world recordings, with controlled variations in timbre, pitch, and temporal envelopes. This method offers an efficient alternative to collecting real-world data, producing robust audio representations that compete with real data on established audio classification benchmarks. This thesis contributes tools for understandably generating rich and diverse sounds, using them and their parameters for sound design and understanding at scale.",
        "authors": [
            "Manuel Cherep"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157728",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Creativity and Justice: Leveraging Creative Learning Principles to Co-Design Just Futures With and For Young People",
        "abstract": "Young people who live in underserved and under-resourced communities and have access to a creative learning environment are poised to create positive change within their communities. Their lived experiences make them experts on the issues their communities are currently facing, and the creative learning environment lends itself as a space where young people can prototype, improve, and implement solutions. Young people can use their imagination and creativity to seek justice and re-imagine their communities.\r\n\r\nThis dissertation examines the Youth Activism and Advocacy program, which I designed using a transformative justice framework, in collaboration with the Clubhouse Network, a global network of after-school centers in historically under-resourced communities. Young people in ten communities around the world used their creativity, lived experiences, and civic imagination to develop and sustain social justice campaigns in their communities.  This dissertation addresses the following research questions: (1) How might we cultivate and support constructionist learning environments that serve young people from communities that have been marginalized? (2) How might we use computational tools to support creative learning while developing and amplifying social justice campaigns? (3) How might we use Human Centered Design methods to allow for meaningful participation and engagement from youth who have been marginalized?\r\n\r\nWhile there were multiple pathways into and motivations for engaging in community action projects, all of the young people gained technical, organizational, and leadership skills that can be applied in future education and career pursuits. The outcomes of the Youth Activism and Advocacy program are complex and intertwined, prompting a call to action to further examine how civic engagement and creative learning can broaden participation in STEM and computing fields—and support youth in making a positive impact in their communities, moving them towards greater justice.",
        "authors": [
            "Jaleesa Trapp"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157714",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Mechatronic Design and Evaluation of a Two-Degree-of-Freedom Powered Ankle-Foot Prosthesis with Myoneural Interfacing Capabilities",
        "abstract": "Recent advancements in neural interfaces and sensing technologies have opened new possibilities for enhanced prosthesis control. The agonist-antagonist myoneural interface (AMI) connects residual muscle pairs to emulate natural dynamics, while electronic osseointegrated prostheses for the rehabilitation of amputees (eOPRA) allow direct measurement of neural signals through implants. Additionally, magnetomicrometry enables precise, real-time measurement of muscle length. These innovations motivate the development of more sophisticated prosthetic designs, including two degrees of freedom (2DoF) ankle systems. \r\n\r\nThis Ph.D. thesis advanced bionic limb technology through three primary aims. First, a comprehensive characterization study of human-scale actuators was conducted, including brushless motors of different sizes. Using a custom-built dynamometer, the performance of these actuators was evaluated across their full operating range. Building upon this foundation, an innovative bionic ankle-foot prosthesis with enhanced capabilities was designed and fabricated. This advanced prosthetic system achieved biological fidelity in terms of range of motion, torque output, and angular velocity, thus enabling more natural and adaptable gait patterns. To validate the efficacy of the system, a subject with AMI constructs was fitted with the prosthesis and underwent a series of locomotion tasks, including level-ground ambulation and obstacle traversal. \r\n\r\nThis work pushed the boundaries of bionic limb function and advanced the restoration of natural locomotion after lower limb amputation, providing valuable insights into the potential of combining advanced prosthetic design with neural interfacing techniques.",
        "authors": [
            "Tsung-Han Hsieh"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157737",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Matters of Illuminance - Transforming Light into Material Artifacts",
        "abstract": "This research explores a process to transform light into physical artifacts. It develops a series of custom software systems to capture images of sunlight moving through a building and transform them into three-dimensional forms. It uses digital manufacturing methods to create the three-dimensional forms out of glass. The aim of this work is 1) to construct a methodology for recording light’s interaction with architecture as three-dimensional forms 2) to produce glass sculptures that exist in a fine art setting and contribute to the lineage of 21st century light artists. The academic contribution of this research builds upon the autographic design framework defined by Dietmar Offenhuber. Offenhuber describes the autographic design process as “the practice of shaping the conditions that allow traces to emerge and guiding their interpretation to demonstrate causality and evidence”.1 The technique I use to transform light into three-dimensional forms follows the four steps of the autographic design process. The goal of this technique is to provide a repeatable process and data format that captures information about light’s interaction with architecture at specific locations. The process produces three-dimensional forms, physical glass sculptures, and media that guide their interpretation, which can be interpreted to provide insight on the design and history of the building. The artistic contribution of this research produces glass sculptures that physicalize the shapes of light I observed and recorded at the location. The goal of these sculptures is to create meaningful physical artworks that reflect the nuanced shapes and subtle aesthetic qualities of natural light. Exhibiting the sculptures in spaces that are abundant with natural light creates new interactions between the glass and the light, offering unique visual experiences that change over time. I bolster these artworks with experiential accounts of my time spent in the building. The artwork I produced as part of this research was exhibited at the Wiesner Gallery at MIT and aims to exist in a fine arts setting, contributing to the lineage of Light & Space artists such as Larry Bell and Robert Irwin.",
        "authors": [
            "Dexter Callender III"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157713",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Accelerating Practical Engineering Design Optimization with Computational Graph Transformations",
        "abstract": "Multidisciplinary design optimization has immense potential to improve conceptual design workflows for large-scale engineered systems, such as aircraft. However, despite remarkable theoretical progress in advanced optimization methods in recent decades, practical industry adoption of such methods lags far behind. This thesis identifies the root causes of this theory-to-practice gap and addresses them by introducing a new paradigm for computational design optimization frameworks called code transformations. Code transformations encompass a variety of computational-graph-based scientific computing strategies (e.g., automatic differentiation, automatic sparsity detection, problem auto-scaling) that automatically analyze, augment, and accelerate the user’s code before passing it to a modern gradient-based optimization algorithm. This paradigm offers a compelling combination of ease-of-use, computational speed, and modeling flexibility, whereas existing paradigms typically make sacrifices in at least one of these key areas. Consequently, code transformations present a competitive avenue for increasing the adoption of advanced optimization techniques in industry, all without placing the burden of deep expertise in applied mathematics and computer science on end users. The major contributions of this thesis are fivefold. First, it introduces the concept of code transformations as a possible foundation for an MDO framework and demonstrates their practical feasibility through aircraft design case studies. Second, it implements several common aircraft analyses in a form compatible with code transformations, providing a practical illustration of the opportunities, challenges, and considerations here. Third, it presents a novel technique to automatically trace sparsity through certain external black-box functions by exploiting IEEE 754 handling of not-a-number (NaN) values. Fourth, it proposes strategies for efficiently incorporating black-box models into a code transformation framework through physics-informed machine learning surrogates, demonstrated with an airfoil aerodynamics analysis case study. Finally, it shows how a code transformations paradigm can simplify the formulation of other optimization-related aircraft development tasks beyond just design, exemplified by aircraft system identification and performance reconstruction from minimal flight data. Taken holistically, these contributions aim to improve the accessibility of advanced optimization techniques for industry engineers, making large-scale conceptual multidisciplinary design optimization more practical for real-world systems.",
        "authors": [
            "Peter D. Sharpe"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157809",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Towards Efficient Planning for Navigation using Global Information in Large and Uncertain Environments",
        "abstract": "We would like to enable a team of robots to navigate quickly and efficiently in large and uncertain outdoor environments. We hypothesize that in such environments, global, uncertainty-aware information is necessary to enable high-quality planning. However, most existing systems do not model or plan using global, uncertainty-aware information. For example, many planners assume access to complete global information in the form of full environment maps, or they assume that locally good planning decisions under uncertainty will result in globally good planning outcomes. To enable the use of global information for planning in large and uncertain environments, we must develop models that concisely represent key navigation features of the environment, and build planners that are capable of reasoning efficiently about global information. In this thesis, we design models and planners that use global information in large and uncertain environments to increase the efficiency and quality of planning for navigation. We present four contributions towards using global information for efficient navigation. First, we propose a high-level planning representation that can be learned from previous plans considered in the environment and used online during hierarchical, multi-query robot navigation. Second, we propose a planner for collaborative multiagent navigation in an uncertain environment; the approach uses macro-actions and value function approximations to maintain computational tractability. Third, we develop a robust hierarchical planning system to enable the deployment of the collaborative multiagent planner on a real-world team navigating in a structured, uncertain outdoor environment. Finally, we develop a method for learning uncertainty-aware, single agent value function-based approximations from graph data to increase the efficiency of the collaborative multiagent planner.",
        "authors": [
            "Martina Stadler Kurtz"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157816",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Mechanistic modeling of in vitro transcription incorporating effects of magnesium pyrophosphate crystallization",
        "abstract": "The in vitro transcription (IVT) reaction used in the production of messenger RNA vaccines and therapies remains poorly quantitatively understood. Mechanistic modeling of IVT could inform reaction design, scale‐up, and control. In this work, we develop a mechanistic model of IVT to include nucleation and growth of magnesium pyrophosphate crystals and subsequent agglomeration of crystals and DNA. To help generalize this model to different constructs, a novel quantitative description is included for the rate of transcription as a function of target sequence length, DNA concentration, and T7 RNA polymerase concentration. The model explains previously unexplained trends in IVT data and quantitatively predicts the effect of adding the pyrophosphatase enzyme to the reaction system. The model is validated on additional literature data showing an ability to predict transcription rates as a function of RNA sequence length.",
        "authors": [
            "Nathan Merica Stover",
            "Krystian Ganko",
            "Richard D Braatz"
        ],
        "journal_conference_name": "Biotechnology and Bioengineering",
        "publisher": "Wiley",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157673",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Comment on “Measurement of Gravitational Acceleration Using Bernoulli’s Equation”",
        "abstract": "",
        "authors": [
            "John H. Lienhard",
            "John H. Lienhard"
        ],
        "journal_conference_name": "The Physics Teacher",
        "publisher": "American Association of Physics Teachers",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/156734",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Cyborg Psychology: The Art & Science of Designing Human-AI Systems that Support Human Flourishing",
        "abstract": "As Artificial Intelligence (AI) becomes increasingly integrated into our daily lives, understanding the psychological implications of human-AI interaction is crucial for developing systems that truly support human capabilities. This dissertation introduces “Cyborg Psychology,” an interdisciplinary, human-centered approach to understanding how AI systems influence human psychological processes. Cyborg Psychology also emphasizes applying these insights to design and develop AI systems that support human flourishing. Cyborg Psychology recognizes the complex, non-linear interactions between humans and AI, acknowledging that both can influence and shape each other in dynamic and often unpredictable ways. Informed by human-computer interaction, psychology, and behavioral sciences, this dissertation focuses on understanding AI’s impact on crucial cognitive and behavioral processes, including motivation, critical thinking, self-reflection, confidence, beliefs, biases, and more. In addition, the work presents several AI systems that apply psychological insights to support human cognition and behavior. For example, the “Wearable Reasoner” seeks to enhance human rationality, “Personalized Virtual Characters” aims to support learning motivation, and “Future You” is designed to encourage long-term oriented thinking and behavior. Employing a diverse array of research methodologies, this work proposes a framework for investigating the implications of interaction design choices. The ultimate goal is to empower the development of AI systems that foster human flourishing by nurturing intellectual growth, cultivating motivation, stimulating critical thinking, and preserving individual autonomy in decision-making.",
        "authors": [
            "Pat Pataranutaporn"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157738",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Investigating Interventions in Fine-grained Contexts for Habit Formation",
        "abstract": "Behavior change is important, yet hard to sustain. Habits are automatic responses to specific contextual cues, and can help sustain behavior change. Fine-grained specific contexts are commonly used in habit formation, but interventions in automatically-detected fine-grained contexts have rarely been explored for habit formation. \r\n\r\nWe investigate habit-formation using interventions in fine-grained mobile, physical-world and digital, computer-based contexts, making three key contributions for each: a survey to identify behavior change needs, a prototype system designed to deliver fine-grained context-specific interventions, and a study to investigate habit-formation using interventions in fine-grained contexts, compared to interventions in less fine-grained contexts. We use the Self-report Habit Index (SRHI) and Self-Report Behavioral Automaticity Index (SRBAI) to measure habit formation and habit automaticity, respectively.\r\n\r\nFor mobile, physical-world behavior change, the survey of needs (N=53 participants) indicated that participants want diverse and personalized behavior change support in diverse and specific contexts. We created a wearable device with on-device deep learning for interventions in personalized and privacy-preserving egocentric visual contexts. In a 4-week pilot study (N=10), interventions in egocentric visual contexts led to more percentage increase in average habit formation (SRHI) and automaticity (SRBAI) than interventions in coarse-grained contexts based on time, geolocation, and physical activity. The percentage increase in median habit formation was also more for the fine-grained egocentric context group, whereas the percentage increase in median habit automaticity was similar between the two groups. For both groups, the habits persisted in the post-study evaluations 1 and 10 weeks later, without interventions.\r\n\r\nFor computer-usage behavior change, the survey of needs (N=68) indicated that participants want to reduce excessive/unnecessary use, e.g., social media, and found off-the-screen breaks helpful. We created a Chrome extension to deliver interventions based on specific web activities, and conducted a 6+2-week study (N=31, 6 weeks of interventions and 2 weeks of post-study without interventions). After 6 weeks, interventions in fine-grained website-entry-based contexts led to more percentage increase in mean and median habit formation and automaticity than interventions in coarse-grained interval-based or random contexts. After the additional two-week post-study, without interventions, the website-entry group had the largest percentage increase in mean SRHI/SRBAI, whereas the interval-based group had the largest percentage increase in median SRHI/SRBAI. \r\n\r\nQualitative results from both studies indicated that interventions in fine-grained contexts were delivered at more opportune moments and were less disruptive. We discuss the limitations of our research and present a first step towards investigating interventions in fine-grained contexts for habit formation, potentially for sustainable behavior change, without long-term dependence on technology.",
        "authors": [
            "Mina Khan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157717",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Fluid-fluid displacement in porous-media microfluidics",
        "abstract": "Immiscible fluid-fluid displacement under geometric confinement is a key physical process in large-scale subsurface energy technologies such as geologic carbon sequestration and in small-scale microfluidic techniques. Research over the past few decades has provided improved understanding of the fluid-fluid displacement patterns on the macroscopic scale, which range from compact displacement to fractal pattern. Many questions remain, however, regarding how the macroscopic displacement patterns are controlled by the microscale interactions between the fluid interface and the solid surface in systems under geometric confinement like microfluidic devices and porous media. This fluid-solid interaction—exacerbated by the roughness inherent to all natural and engineered surfaces—introduces a large energy dissipation near the solid boundary that challenges our ability to interpret laboratory experiments and develop mathematical models. In Part I of this Thesis, we study the motion of a fluid-fluid interface at the scale of a single capillary through mathematical modeling and laboratory experiments. We first develop a phase-field model to simulate two-phase flow with moving contact lines in the partial wetting regime. We construct a self-consistent formulation of fluid-solid surface energy which allows prescribing arbitrary static contact angles. We then propose a formulation to account for nonequilibrium conditions near the contact line and demonstrate the ability of our model to simulate dynamic configurations, from spontaneous imbibition to wetting transition and interface pinch-off. We then experimentally study the shape of a moving interface in a capillary tube prewetted with the invading liquid. For viscously favorable displacements (when the invading fluid is more viscous than the defending fluid), we find a universal behavior of the dynamic contact angle—a macroscopic descriptor of interface shape—which increases monotonically with capillary number. In contrast, for viscously unfavorable displacements, we observe a sharp wetting transition where the dynamic contact angle shoots to 180 over a narrow range of flow rates. Above the transition, a trailing film of viscous defending fluid is left behind the displacement front and the invading fluid propagates along the tube center as a finger. We rationalize the emergence of this sharp, trailing-film type of wetting transition by means of a minimal-ingredients hydrodynamic theory that exhibits bifurcated solutions. In Part II of this Thesis, we investigate the role of surface roughness on twophase displacements. We do so in a microfluidic device with a precisely controlled structured surface as an analogue for a rough fracture. In the drainage regime, we show that the roughness induces two types of liquid films entrained on the solid surfaces behind the displacement front: the classical Bretherton “thick film”, and a new type of “thin film” that is confined within the roughness. Each type of liquid film is characterized by distinct stability criteria and dewetting dynamics. In the imbibition regime, we show that surface roughness promotes that the wetting liquid preferentially advances within the roughness layer. The formation of a leading film stabilizes the displacement front as the flow rate increases, which would otherwise— that is, in a smooth confinement—become fractal. In summary, our work sheds light on the microscale physics and macroscopic pattern formation in rough confinement that may control long-term mixing and reactivity in geological systems and lab-on-a-chip applications.",
        "authors": [
            "Yu Qiu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157718",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Structural case on adjuncts",
        "abstract": "This dissertation investigates how case is assigned to nominal adverbials dubbed durative and multiplicative in Korean. These adverbials express the duration of an event, or the number of times an event is repeated. In transitive, unergative, and unaccusative constructions, the adverbial is marked with accusative case. In psychological predicate constructions, the adverbial is marked with nominative case. Interestingly, in passive and inchoative constructions (grouped together under the term nonactive), the adverbial allows both nominative and accusative case.\r\nI derive these patterns from a specific model of Voice, and a model of successive-cyclic Dependent Case. I first argue in favor of a Voice system that treats passive and inchoative constructions as syntactically equivalent: whether a nonactive construction is passive or inchoative is determined by the feature specification on Voice (Kallulli 2007). Furthermore, this nonactive Voice head introduces an implicit agent (for passives) or causer (for inchoatives), which can be optionally realized as a PP. This agent/causer at Spec, VoiceP competes with the theme argument to move to Spec, TP. Hence, there are two different structures that can arise in nonactive constructions. The other constructions that do not show case optionality lack this competition. In transitive, unergative, and unaccusative constructions, there is no implicit agent/causer at Spec, TP to compete with the theme argument. In psychological predicate constructions, the experiencer argument introduced at Spec, ApplP acts as an intervener and blocks the theme argument from competing with the implicit agent/causer.\r\n\r\nMy model of successive-cyclic Dependent Case explains how the different structures result in different case patterns. It is a revised version of Levin’s (2017) original model, whereby case evaluation occurs not only at the end of the syntactic derivation but at the Spell-out of each phase. However, my version of the model involves a more relaxed locality constraint for dependent case assignment. I demonstrate how my model can not only derive the case marking patterns of durative and multiplicative adverbials, but can also account for other case phenomena in Korean such as case stacking and multiple nominative constructions.",
        "authors": [
            "Eunsun Jou"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157869",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Ethics within Metaphysics",
        "abstract": "This dissertation consists of three chapters at the intersection of ethics and metaphysics. In the first chapter, I put forward a new theory of personal identity, give arguments for it, and defend it from objections. In the first part, I argue that the two most prominent theories of personal identity, the psychological theory and the physical theory, do not satisfy some constraints on any acceptable theory: that personal identity be all-or-nothing, determinate, principled, and substantive. I then put forward a new theory, the phenomenal theory, on which personal identity is determined by the uninterrupted continuity of a stream of consciousness. I argue that this theory does satisfy all the desiderata, and is as such a better theory. In the second part, I argue that the phenomenal theory also solves the problem of fission cases, because there are no cases of phenomenal fission. In the third and last part, I consider the objection that, on the phenomenal theory, we do not survive interruptions of consciousness such as sleep; I argue that this objection doesn’t succeed in refuting the theory. In the second chapter, I generalize a debate about laws of nature to the domains of metaphysics and ethics. Patterns in the natural world lead us to the postulation of laws. A metaphysical dispute arises as to whether these laws are mere summaries of the mosaic (as the Humean would have it), or whether they govern the mosaic (as the Anti-Humean would have it). In this paper, I first argue that similarly, patterns in the metaphysical and ethical facts should lead us to the postulation of metaphysical and ethical laws, which are the proper subject of metaphysical and ethical inquiry. Then, I argue that the Humean/Anti-Humean debate also arises when it comes to metaphysical and ethical laws. Finally, I argue in favor of the Anti-Humean conception of metaphysical and ethical laws, both adapting standard arguments used in the debates about laws of nature, and with new arguments specific to metaphysics and ethics. In the third chapter, I investigate conflicts between ethics and metaphysics. Sometimes, a metaphysical theory has revisionary ethical consequences: for example, some have thought that modal realism entails that there are no moral obligations. In these cases, one may be tempted to reject the metaphysical theory on the grounds that it conflicts with commonsensical ethics. This is an ethics-to-metaphysics inference. My claim is that this inference is in general irrational, and that the fact that a metaphysical theory has highly revisionary ethical consequences is no reason at all to reject the theory. I argue for this claim on the basis of general epistemic principles about the transmission of justification, and what makes for a good argument. Furthermore, I argue that my account can explain why a certain narrow class of ethics-to-metaphysics inferences are rational.",
        "authors": [
            "Michele Odissea Impagnatiello"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157866",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Topics in Marma (မာရမာ)",
        "abstract": "Marma¹ an endangered indigenous language of Bangladesh, is spoken by approximately 200,000 Marma individuals residing in Bangladesh’s southern region called the Chittagong Hill Tracts (CHT). Marma language is closely related to Rakhine and Burmese, and many lexical items are almost identical to those in Burmese and Rakhine, “although Marma exhibits a more conservative phonological profile than Burmese in the grammatical particles” Keisuke (2011). This research study analyzed several morphemes and their roles in shaping discourse structures in Marma information structure (topic-focus articulation). Marma has “agglutinative morphology”, meaning words are formed by stringing together morphemes in specific sequences. We observed prefixation, suffixation, and infixation in Marma. We analyzed the multifunctionality of these selective morphemes [“က=ga/ka, ကိ ု =go/ko, စာ=cha,ရာ=ra, ယည်=yi”] within Marma discourse and explored their implications for a better understanding of information structure in Marma language. At the end of this paper, through instrumental analysis, we proposed three tones in Marma (i.High and creaky, ii. low, and iii. falling).\r\n \r\nKey words: Marma, indigenous language, information structure, topic and focus,morphology and tone.\r\n\r\n¹“According to Bradley (1985:180), the Marma group would have first migrated from Arakan to\r\nthe Chittagong Hill Tracts by the early sixteenth century and then after the Burmese conquest in\r\n1785. They live mainly in the Chittagong Hill Tracts where they form one of the main Indigenous\r\ngroups ( Htin, 2015) ”",
        "authors": [
            "Rani Ukhengching (ဦး ချမ်း စိန် မာရမာ) Marma"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157882",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Just Doing My Job: Normative Dimensions of Social Roles",
        "abstract": "“What should I do?” Often, our answers make reference to our social roles: we ask what we should do as lawyers, citizens, or parents. But this confronts us with problems. Consider a would-be whistleblower, a wife challenging the gendered division of household labor, or the conflicted police officer Javert from Les Misérables. These agents feel there is a genuine conflict between morality and the norms of their role. While many philosophers treat social roles as incidental to our moral lives, this dissertation aims to do justice to this experience of roles’ normative force. I argue that doing so prompts revision to orthodox views of role-occupants’ reasons for action, blameworthiness, and responsibility for structural injustice. In Chapter One, I develop a new account of how social roles generate normative reasons for occupants to comply with role norms. I argue that agents’ reasons to comply with their role norms depend on how those norms contribute to functioning social practices. In addition to its claims about the structure of normative reasons, my view delivers a striking upshot in cases of conflict. While popular accounts of role normativity often maintain that moral considerations can cancel roles’ normative force, my project suggests a radically different conclusion: role-occupants have good reasons to comply even with norms that result in conflicts with what they morally ought to do. Social roles generate genuine normative conflicts. While many role-occupants find conflicts between roles and morality distressing, many others seem not to notice that there is a conflict at all. Consider the oft-maligned excuse: “I was just doing my job.” Chapter Two defends an epistemic variant of this excuse. I argue that agents who comply with roles’ deliberative norms may—for good reason—bracket morally relevant considerations. As a result, they may be non-culpably ignorant of wrongdoing. On some views, this can excuse them from blame. But even denying that moral ignorance exculpates is compatible with accepting role-occupants’ excuses. Such views often emphasize being motivated by the right reasons. But because role compliance is often justifiable, ignorance need not be blameworthy indifference to the right reasons. The upshot is a novel position in the debate about moral ignorance as an excuse. We might worry that this unduly lets role-occupants off the hook. If, as I argue, roleoccupants can have good reasons for acting immorally, and they can sometimes be blameless even when they do act wrongly, does that prevent us from taking those wrongs seriously? I grapple with this problem in Chapter Three. Drawing on theories of structural injustice, I argue that roles’ normative character actually generates responsibilities for justice. Because role performance both affirms and instantiates unjust structures, role-occupants bear responsibilities that can only be discharged by changing what their actions mean and do. This vindicates the widespread but philosophically puzzling view that agents ought to direct efforts towards injustices they participate in intimately, even when they could make a greater impact elsewhere. It also means that role-occupants are not off the moral hook. Ultimately, we each bear responsibility to create a more just social world.",
        "authors": [
            "Eliza Wells"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157873",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Site-specific chemical and topological modifications to augment mRNA therapeutic potential",
        "abstract": "Synthetic mRNA has emerged as a promising therapeutic platform for the treatment of a wide variety of diseases. Despite clinical demonstrations of mRNA for SARS-CoV-2 vaccines, mRNAs remain limited in application by their susceptibility to nucleases and overall short expression lifetime in vivo. We investigated the site-specific installation of chemical and topological modifications into therapeutic mRNA to augment their expression in cell cultures and mouse models. We began by developing messenger oligonucleotide-conjugated RNAs\r\n(mocRNAs), which are mRNAs ligated to modified oligonucleotides that contain 3’ nuclease-resistant modifications. We show that mocRNAs are subject to slower deadenylation and enhance therapeutic protein expression in cell lines and primary cell cultures. We expanded on this technology by creating mRNAs with chemically branched poly(A) tails, or multitail mRNAs, which increase the density of modifications at the 3’ end of mocRNA and further stabilize mRNA against deadenylation.\r\n\r\nIn conjunction with increased nuclease resistance at the 3’ terminus, we developed a strategy to enhance translation initiation on circular mRNAs (circRNAs). We developed QRNAs, which are circRNAs that possess an unnaturally-linked inverted 7-methylguanosine (m7G) cap. QRNAs substantially outperform conventional circRNAs, given the low translation initiation efficiency of IRES compared to cap-dependent initiation. Ultimately, our studies exploring the chemical and topological space of mRNA demonstrates the value of site-specific chemical and topological modifications for designing future generations of designer mRNA-based therapeutics.",
        "authors": [
            "Abhishek J. Aditham"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157875",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Precision measurement of the W boson mass with the CMS Experiment in pp collisions at √s = 13 TeV",
        "abstract": "The mass of the W boson, mW, is an important fundamental constant of nature, which\r\nis also potentially sensitive to a plethora of physics beyond the Standard Model. In this\r\nthesis, we discuss the precision measurement of mW with the CMS detector at the LHC in\r\nproton-proton collisions at √s = 13 TeV. The phenomenology of W bosons produced in pp\r\ncollisions, the CMS detector characteristics, and other relevant factors are examined to justify\r\nthe overall strategy to measure mW from the muon transverse momentum and pseudorapidity\r\nspectrum [formula] in the W → µν channel with a part of the 2016 data corresponding\r\nto an integrated luminosity of 16.8 fb⁻¹. Dedicated studies aiming to reduce systematic\r\nuncertainties related to the muon transverse momentum calibration, the muon reconstruction\r\nand background rejection efficiencies, and the modeling of the W boson production and decay\r\nkinematics are presented. A profiled maximum-likelihood fit of MC templates to observed\r\ndata incorporating over 4,000 nuisance parameters is employed to extract the central value\r\nand the total uncertainty on m_W. The result of this measurement is m_W = 80, 360.2 ± 2.4 (stat.) ± 9.6 (syst.) MeV,= 80, 360.2 ± 9.9 MeV, which is consistent with the standard model prediction m(SM/W) = 80, 354.5 ± 5.7 MeV.",
        "authors": [
            "Tianyu Justin Yang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157874",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Essays on attention and creative thought",
        "abstract": "In the mental life of an ordinary person, creative thoughts, as well as other non-rigid forms of thought, such as mind wandering, are both pervasive and important for our cognitive endeavors. The goal of my dissertation is to provide a theory of these non-rigid forms of thought by understanding some of the cognitive mechanisms that underlie them, as well as to understand how these underlying mechanisms contribute to our epistemic lives more generally in all kinds of reasoning. Chapter 1 (based on co-authored work with Azenet Lopez) begins with a puzzle that arises from research on mind wandering: since during mind wandering we plausibly prioritize the information relevant to the concurrent tasks less, why does mind wandering sometimes improve rather than impair concurrent task performance? I resolve the puzzle by rejecting the standard conception of attention, according to which the more focused one’s attention is, the better it is at improving task performance. I instead argue that certain tasks are better performed with a more diffuse rather than focused mode of attention. I offer a conception of \"diffuse attention\" that generalizes from external to internal forms of attention and conceptualize mind wandering as an instance of it. Chapter 2 turns to provide an account of creative thinking, which is closely related to mind wandering. I argue that previous accounts in philosophy about the generation of creative thought are incomplete due to overlooking the role of what I call “memory gists”. Memory gists are memory contents that represent more abstract or qualitative features that are extracted from the specific, surface level features in the memory representations that were initially encoded in memory. I argue that generating and using memory gists in memory search enables highly creative people to form connections between memory contents that are not usually associated with each other by revealing their commonalities shared in their gists. Moreover, I argue that different mechanisms underlie online and offline generation of memory gists: the former involves the mode of diffuse attention that I conceptualized in Chapter 1, while the latter involves memory consolidation during sleep or wakeful rests. The active role that memory plays in creative thinking raises some questions about how to conceptualize the function of memory in our epistemic lives more generally. I explore this topic further in Chapter 3, where I reject the traditional view in epistemology that memory merely functions to preserve previously acquired information, such as information acquired through perception. I argue instead that one of the functions of memory is to improve our understanding of what was represented in the contents that we previously acquired. This is possible thanks to the fact that during memory consolidation, our memory system further processes previously acquired information, and generates representations about relationships between different components of the subject under consideration. My work thus contributes to the ongoing project of understanding memory as an active process instead of merely performing the role of storing information, and highlights understanding as one of the epistemic values that memory generates.",
        "authors": [
            "Jocelyn Yuxing Wang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157884",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A GPU-Enabled Building Block Flow Model for Computational Fluid Dynamics",
        "abstract": "Computational Fluid Dynamics (CFD) is an key tool in the design of aircraft, allowing engineers to predict the performance of a configuration without having to conduct expensive physical tests. However, in order to move to a greater reliance on CFD, the industry requires a high level of accuracy and fast turnaround time, which current methods cannot deliver. In recent years, the rapid development of the GPU industry has led to an explosion of computational power with the GPU architecture. This has allowed wall-modeled large eddy simulation (WMLES), a higher fidelity simulation technique, to become practical for industry use. WMLES requires the use of both a sub-grid scale (SGS) model and a wall model in order to close the system of equations for integration. Although WMLES delivers an improvement over previous methods, classical SGS and wall models do not deliver the accuracy required by the aviation industry. To help close this gap, we introduce a GPU-compatible version of the Building-Block Flow Model (BFM), a machine learning based unified sub-grid scale and wall model for LES introduced in [1]. In this thesis, we discuss the implementation of the BFM for GPU, timing of the BFM versus other closure models for WMLES, and a variety of tests with the BFM designed to evaluate its performance, and possible avenues of improvement.",
        "authors": [
            "Samuel Thomas Costa"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157810",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Technological Innovation and Integration of Whole Brain Imaging, Olfactory Stimulation, and Correlative Microscopy in Larval Zebrafish",
        "abstract": "Achieving a deep understanding of the brain is a cross-disciplinary endeavor that requires the investigator to consider biomolecular, electrical, and sensory interactions across time and space at many scales. This understanding is important because a deeper understanding of the brain precedes advancements in efficient computing, generalizable frameworks for learning, and, of critical importance, the understanding and treatment of neurological diseases. Towards this end, this thesis presents novel approaches and technologies for whole-brain imaging, olfactory stimulation, and correlative imaging---i.e. the utilization and registration of multiple imaging modalities within a single sample. The overall objective of this thesis research is to not just create technologies, but to integrate them to enabler richer and more contextual understandings of the larval zebrafish's brain. \r\nIn this work we show novel light field microscopy algorithms that allow us to reconstruct 3D images from 2D micrographs with improved resolution to enable high-frame-rate recordings of whole-brain neural activity. We describe the designing and building of the first known system for multi-directional olfactory stimulation of larval zebrafish with up to ten separate odor channels. We demonstrate an optimized expansion microscopy-compatible immunostaining protocol for whole-mount zebrafish which preserves registration epitopes to move towards the neuron-level alignment of structural and functional data. And, finally, we showcase a set of proof-of-concept experiments and analyses which demonstrate our ability to integrate olfactory stimulation, whole-brain calcium imaging, behavioral recording, and structural staining in individual larva.",
        "authors": [
            "Corban N. Swain"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157835",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "An Integrated Vehicle, Payload, and Trajectory Optimization Framework for Highly-Coupled Aircraft Systems",
        "abstract": "A class of highly-coupled aircraft systems is identified in Earth observation applications, where the aircraft design couples tightly with the science instrument design and the operation of both the aircraft and science payload. This dissertation identifies an opportunity to simultaneously optimize the aircraft platform, the science payload, and the operational strategy under one system-level objective function to improve the performance of the total aircraft system. This approach extends the field of MDO which demonstrates that simultaneously optimizing all the subsystems within a larger system allows the optimizer to leverage the couplings between disciplines, rather than be subject to them, resulting in better performance outcomes [1]. The inclusion of the instrument and trajectory into the optimization problem introduces additional objectives related to the science mission needs. While many methods for multi-objective optimization exist in the field, these methods are not tractable with the many objectives within these complex systems. A methodology is proposed to explore trade-offs between multiple objectives by sweeping through different combinations of weighting terms in a weighted-sum objective function to find Pareto optimal design points across the design space. These design points are then evaluated within the objective space, a hyperspace where each axis corresponds to a different objective, to understand the performance capabilities with respect to each objective and evaluate trade-offs between objectives. Findings from this objective space exploration can then be communicated to the science stakeholder to find the design that is best capable of meeting the identified science mission needs. This dissertation then applies this methodology to a series of case studies on a representative science mission. The science mission objective of these case studies is to reduce uncertainty in predictions of sea-level rise by understanding ice mechanics that drive ice shelf collapse and destabilize previously grounded glaciers.",
        "authors": [
            "Annick J. Dewald"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157808",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Existence and Analysis of a Rotating Stall Inception Continuum\r\n& Development of Concept Questions in Fluid Dynamics",
        "abstract": "This thesis presents two projects, an analysis of rotating stall inception for axial compressors in turbomachinery, and a description of the creation of Concept Questions for a text on internal flows. The first part of this thesis identifies flow behavior that defines two routes to rotating stall, known as modal and spike type rotating stall inception. It continues previous studies by MIT and the University of Cambridge surrounding unification of these two stall types under a dynamical system framework. Calculations were carried out for an isolated rotor, with a high hub to tip radius ratio, using TBLOCK, a Reynolds Averaged Navier Stokes solver. The results show (i) the dependence of stall inception on the compressor axisymmetric pressure rise characteristic and the characterization of mode and spike stall inception as two paths, located at the ends of a continuum of possible paths to stall. (ii) the effect of blade passage accelerations and asymmetry in the onset process, and (iii) the divergence of stall inception from two-dimensionality as a function of the slope of the total-to-static compressor pressure rise characteristic. The calculations show that compressor pressure rise characteristic slopes, dψ/dϕ, less than 0.3 have a stall cell growth rate, σ, that agrees with two-dimensional theory. The divergence of stall inception from two-dimensionality is suggested as a distinguishing feature of spike type stall inception compared to modal type stall inception. The second part of this thesis encompasses the creation, editing and compilation of Concept Questions for seven book chapters in a new text that describes the use of Concept Questions in teaching (and learning) fluid mechanics. The composition and qualities of a good concept question are defined, and the process of generating and editing questions for the intended audience is discussed.",
        "authors": [
            "Maranda F. Cherry"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157834",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Opportunities and Limitations of Earth Observation Technology for Environmental Justice Advocacy: A Case Study of Toxic Prisons in the U.S.",
        "abstract": "People of color and other socio-economically marginalized groups in the United States experience a disproportionate burden of environmental challenges such as air pollution and extreme heat; the Environmental Justice (EJ) movement aims to combat these burdens and promote collective well being. Earth Observation (EO) technology, such as satellites, can be used to monitor air quality, extreme heat, and other quantities relevant to EJ. However the application of this technology in measuring EJ, or supporting EJ advocacy efforts has not been widely explored. Satellite EO systems also historically have not been designed with EJ end users in mind. This application is increasingly more pressing as space agencies like NASA are seeking information on how their data can be used to support underserved communities. This dissertation brings together EO data science, systems engineering, and community- engagement to elucidate opportunities and limitations of Earth Observation Technology for Environmental Justice Advocacy. The dissertation is organized into three categories of contributions – Description, Evaluation, and Design/Prescription – that are each composed of multiple research efforts.\r\n\r\nIn Description, I apply a three-pronged approach to provide insights on the opportunities and limitations of EO data for EJ. First, along with a team of researchers, I assess peer- reviewed literature on satellite data for environmental justice through a scoping review. The second contribution of this chapter is an interview study with a subset of grassroots EJ actors about how they can use EO data in their domain of EJ activism which contests the exposure of prisons and incarcerated people to environmental hazards. The third contribution of this chapter is a system’s engineering architectural description of NASA’s current satellite EO for EJ ecosystem. Using justice theory as an analytical framework, I reveal limitations of NASA’s current EO for EJ architecture for advancing holistic notions of EJ.\r\n\r\nIn Evaluation, with support from co-authors, I measure spatiotemporal patterns of air pollution burden, and air and land surface temperature extremes in prison landscapes across the U.S. These studies contribute to a nascent literature documenting empirical evidence of environmental hazards in carceral landscapes. It also extends the literature on applications of satellite-derived and modeled geospatial data for EJ. In Design/Prescription, first, supported by 3 years of community engagement with prison EJ activists, I present the Design of a GIS decision support system that features EO data responding to expressed needs of prison EJ activists. Then, I present two essays that Prescribe recommendations for methodological innovations in the design and application of EO technologies and geospatial data for EJ advocacy.\r\n\r\nTogether, these three chapters demonstrate the immediate relevance of EO and geospatial technologies for prison EJ advocacy, and broader implications for the EO community interested in supporting the aims of the EJ movement more holistically.",
        "authors": [
            "Ufuoma Ovienmhada"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157806",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Relationship between synoptic scale meteorology, aircraft\r\nparameters, and observable contrails",
        "abstract": "Long-lasting or \"persistent\" contrails are line-shaped clouds that form when airplanes fly through cold and humid parts of the atmosphere that are ice-supersaturated. Various studies have shown that persistent contrails may be responsible for more than half of aviation’s radiative forcing [1]. Efforts to mitigate persistent contrail formation include operational contrail avoidance. Current research suggests that minor (∼ 2000 ft) deviations in altitude of flights during cruise, in conjunction with advancing engine technologies, have the potential to reduce contrail climate forcing by approximately 90% [2]. Identifying and attributing observed contrails to specific individual flights is necessary to demonstrate the success of flight deviations. Reliable flight attribution, therefore, is critical in verifying large-scale implementation of contrail avoidance strategies. Flight attribution leverages both Earth-observation methods, such as satellite images and weather data, and flight data. However, temporal and spatial \"blindspots\" in satellite instruments, coupled with uncertainties in wind fields, have hindered reliable flight attribution. In this work, we consider eight different probabilistic flight attribution algorithms. All algorithms rely on the use of \"similarity measures\" which we define as the differences in distance, heading, and altitude between a contrail and flight line segment candidates. We define two-dimensional (2D) algorithms as those that use only distance and heading difference measures and the ones that additionally include altitude as three-dimensional (3D) algorithms. The probabilistic aspect of all eight algorithms is intended to account for errors in wind data and relies on the calculation of a Gaussian probability density function for each similarity measure. In an attempt to mitigate wind and positional errors that compound over time, four of the algorithms feature the inclusion of contrails from previous timestamps as potential match candidates. To account for the changes in flight path due to temporal factors, four of the algorithms include the use of time-dependent Gaussian parameters. The inputs to all algorithms include contrail detections, weather data, and flight data. To perform this analysis, a dataset of 180 manually-attributed, unique contrails was created that captures regional (across the continental United States) and diurnal variation. Each contrail was tracked for part of its lifetime, which results in the generation of 1980 total attributions. These attributions were created by seven labelers, with some overlapping scenes. A parameter sweep was performed on the four 2D algorithms to determine locally optimal Gaussian parameters. This sweep was performed on a reduced dataset that consists of 32 unique contrails and 218 total labels. The results of this sweep show that the performance of the algorithms, when using optimal Gaussian parameters, range from 79.7% to 83.6% accuracy. Accuracy is defined as the percentage of contrails that were attributed to the correct flights. These results are solely for the 2D algorithms that were analyzed on the reduced dataset. We then applied the \"locally\" optimal Gaussian parameters from the four 2D algorithms to the respective 3D algorithms and ran all eight algorithms on the remaining 148 contrails (1762 labels). We find that the optimal performance for all eight algorithms ranges from 68.2% to 76.2%. A deeper analysis is also conducted to evaluate the scene conditions that affect algorithm performance.",
        "authors": [
            "Maria Paula Barbosa"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157807",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Effects of Tip Clearance and Surface Roughness on Small-Scale Turbopump Impeller Performance",
        "abstract": "Centimeter-scale turbopump impellers typically used in liquid rocket engines of small launch vehicles suffer from reduced performance due to manufacturing challenges and nonuniform geometric scaling. This thesis aims to characterize the impact of impeller blade tip clearance and surface roughness on the performance of small-scale turbopump impellers by assessing the dominant flow features, quantifying the underlying loss mechanisms, and determining the sensitivity of performance losses to changes in tip clearance and surface roughness. The study identifies the primary flow features governing impeller performance to be blade tip leakage flow and secondary flow. The analysis identified two distinct flow regimes based on tip clearance: above 5% of tip clearance, the losses are predominantly due to blade tip leakage flow, whereas below this threshold, losses are governed by both secondary flow and blade tip leakage flow. For tip clearances above 5% of the blade span, blade tip leakage flow is estimated to contribute more than 80% of total impeller loss. A 1% change in tip clearance is estimated to result in a 0.8% loss in efficiency. The calculations suggest increasing surface roughness reduces the effective tip clearance due to increased viscous effects in the tip gap, but strengthens the secondary flow. This lowers the effective tip clearance that separates the flow regimes. The contribution of blade tip leakage loss to total impeller loss decreases by up to 22% for surface roughness increased from an Rₐ value of 1 µm to 10 µm. The strengthened secondary flow at higher surface roughness increases mixing of the blade tip leakage flow with the blade passage flow, leading to larger regions of blockage. Increasing the surface roughness from an Rₐ value of 1 µm to 10 µm results in a 4% loss in impeller efficiency. This study demonstrates that surface roughness is more impactful on small-scale impeller performance than blade tip clearance, and so manufacturing for smooth surfaces should be prioritized over reducing the blade tip clearance gap.",
        "authors": [
            "Kinjal A. L. Ruecker"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157828",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Limits to extreme event forecasting in chaotic systems",
        "abstract": "Predicting extreme events in chaotic systems, characterized by rare but intensely fluctuating properties, is of great importance due to their impact on the performance and reliability of a wide range of systems. Some examples include weather forecasting, traffic management, power grid operations, and financial market analysis, to name a few. Methods of increasing sophistication have been developed to forecast events in these systems. However, the boundaries that define the maximum accuracy of forecasting tools are still largely unexplored from a theoretical standpoint. Here, we address the question: What is the minimum possible error in the prediction of extreme events in complex, chaotic systems? We derive the minimum probability of error in extreme event forecasting along with its information-theoretic lower and upper bounds. These bounds are universal for a given problem, in that they hold regardless of the modeling approach for extreme event prediction: from traditional linear regressions to sophisticated neural network models. The limits in predictability are obtained from the cost-sensitive Fano’s and Hellman’s inequalities using the Rényi entropy. The results are also connected to Takens’ embedding theorem using the information can’t hurt inequality. Finally, the probability of error for a forecasting model is decomposed into three sources: uncertainty in the initial conditions, hidden variables, and suboptimal modeling assumptions. The latter allows us to assess whether prediction models are operating near their maximum theoretical performance or if further improvements are possible. The bounds are applied to the prediction of extreme events in the Rössler system and the Kolmogorov flow.",
        "authors": [
            "Yuan Yuan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157825",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Constraints on vowel-zero alternations in Hungarian",
        "abstract": "I analyze a large set of Hungarian nominal stems whose last vowel alternates with zero in certain contexts (Vago (1980), Siptár & Törkenczy (2000)): e.g. bokor [bokor], bokr-ok [bokr-ok]. I argue that the mechanism underlying these alternations is syncope, departing in this from earlier work (Vago (1980), Abondolo (1988), J. Jensen & Stong-Jensen (1988, 1989), Törkenczy (1995), Abrusán (2005)) which assumes epenthesis or metathesis. My research focuses on which stems fall into this closed group of vowel-zero alternating stems. I show that there is an interaction between phonological processes that repair phonotactically illicit consonant clusters – like voicing assimilation, gemination, affrication – and vowel-zero alternations. I present a proposal relying on underspecification that correctly predicts that these phonological processes block vowel-zero alternations. The grammar that generates this result includes a ranking schema where the constraint triggering syncope (referred to below as Syncope) is outranked not only by the Markedness constraints that define illicit CC-clusters in Hungarian but also by the faithfulness constraints that are normally violated in the repair of such clusters. The general ranking I will argue for is: (1) Markedness (*CC for various CCs) » Faithfulness to Cs » Syncope » Max V I also present results from a nonce word experiment, which confirms that Hungarian speakers are aware of the systematic restrictions my analysis characterizes. The broad significance of the work is to document a large-scale conspiracy (Kisseberth (1970)) whereby permissible CC clusters emerge in at least two ways: through direct action of repair processes (assimilation or merger of two Cs into one) and through blockage of the syncope process that could yield the inputs to such repairs.",
        "authors": [
            "Dóra Kata Takács"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157881",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Not Function but Function Conquered: Against a Functionalist Theory of Directives",
        "abstract": "Ordering, requesting, and inviting are examples of directive speech acts. Philosophers have offered different accounts of what it is to perform a directive, which they have developed using different theoretical resources. Attitudinal theories of speech acts try to explain what it is to perform a directive in terms of a speaker’s beliefs, desires, and intentions. Nonattitudinal theories of speech acts try to explain directives in terms of something else.\r\n\r\nThis thesis is concerned with functionalism, a nonattitudinal theory of speech acts. According to functionalism, performing a directive is making an utterance with the etiological function of causing hearers to act in response to one’s utterance. I argue that functionalism is false. I develop counterexamples that show functionalism is too permissive about the kinds of causation suitable for generating directives. I argue further that the most plausible way to address these counterexamples is to become more attitudinal: rather than be permissive, functionalism should hold that directives and hearers’ responses to them are caused by specific internal processes.",
        "authors": [
            "John Hill"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157880",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Phonetic Faithfulness in Phonological Opacity",
        "abstract": "This dissertation presents a novel approach to phonological opacity, which is grounded in new findings regarding substantive restrictions on the patterns of opaque interactions. The central thesis posits that phonological opacity functions to preserve the phonetic properties specified in the input of a phonological operation. Specifically, it argues that inputs are enriched with phonetic auditory features, and surface opacity emerges as a result of processing these enriched inputs. This proposal can be detailed as follows. First, processes that become opaque are initially biased by certain phonetic markedness conditions. Second, these phonetic biases, encoded in the phonetically enriched inputs, are mapped onto the nearest phonologically contrastive sounds to satisfy the requirement of phonetic faithfulness, resulting in surface phonological opacity.\r\n \r\nThis hypothesis yields a testable prediction: only phonetically natural processes, which possess an appropriate phonetic markedness condition, can become opaque. The results of typological surveys encompassing 87 counterfeeding and 65 counterbleeding interactions across languages support this prediction, revealing that opacified processes are subject to a narrow range of markedness conditions, such as coarticulatory assimilation (e.g., palatalization) and durational adjustments (e.g., segmental weakening). Other types of phonological processes, particularly non-natural ones, are only rarely, if ever, opacified. This asymmetry in the patterns of phonological opacity underscores that opaque interactions are not independent of phonetic substance. \r\n\r\nIn addition to this main finding, it is also shown that the current proposal offers additional advantages in explaining phonological opacity. First, it successfully accounts for various non-typical opaque interactions such as feeding opacity and stress misapplications, alongside counterfeeding and counterbleeding interactions. The proposal also integrates various phonological phenomena, such as compensatory lengthening, coalescence, and incomplete neutralization, within the framework. Second, learning simulations using a weighted constraint version of the proposed model demonstrate that intermediate hidden structures, such as phonetically enriched inputs, can be learned when the mappings between abstract inputs and surface representations are established.",
        "authors": [
            "Yeong-Joon Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157863",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Preserved functional organization of auditory cortex in two individuals missing one temporal lobe from infancy",
        "abstract": "Human cortical responses to natural sounds, measured with fMRI, can be approximated as the weighted sum of a small number of canonical response patterns (components), each having interpretable functional and anatomical properties. Here, we asked whether this organization is preserved in cases where only one temporal lobe is available due to early brain damage by investigating a unique family: one sibling missing their left temporal lobe from infancy, another missing the right temporal lobe from infancy, and a third anatomically neurotypical. None of the siblings manifested behavioral deficits. We analyzed fMRI responses to diverse natural sounds within the intact hemispheres of these individuals and compared them to 12 neurotypical participants. All siblings manifested typical-like auditory responses in their intact hemispheres. These results suggest that the development of the auditory cortex in each hemisphere does not depend on the existence of the other hemisphere, highlighting the redundancy and equipotentiality of the bilateral auditory system.",
        "authors": [
            "Tamar I Regev",
            "Benjamin Lipkin",
            "Dana Boebinger",
            "Alexander Paunov",
            "Hope Kean",
            "Sam V Norman-Haignere",
            "Evelina Fedorenko"
        ],
        "journal_conference_name": "iScience",
        "publisher": "Elsevier BV",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157742",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Robust assessment of railway vehicle safety risks in operation using a proposed data-driven wheel profile generation approach: Design of computer experiments and surrogate models",
        "abstract": "Worldwide objectives for railway vehicles are increased capacity, faster travels and higher levels of safety. In the vehicle-track complex system, assessing and controlling the interactions between the wheels and the rail track is crucial to these goals. Wheel profiles are specifically designed to steer the vehicle and avoid derailment. Maintenance standards and train operating companies establish safe envelopes for wheel profile geometric parameters. A design of experiments is conducted to model relationships between allowable wheel parameters and expected vehicle safety risks, which is supported by condition monitoring data from operation. Such a robust assessment is missing in the literature. The applied methods consist of: (i) selection of predictors and pre-processing, based on literature, standards and a purely data-driven approach to generate wheel profiles; (ii) space-filling design, using Latin hypercube sampling; (iii) obtaining vehicle responses and post-processing, using a multibody dynamics commercial software and according to standards; (iv) surrogate modelling, using Gaussian processes and linear models; (v) sensitivity analysis, through Sobol indices; (vi) safety assessment, analysing response surfaces. Wheels with large flange height and thickness result in higher flange climb derailment risks. The proposed approach allows quantifying this risk as a function of profile parameters and mitigate it through maintenance actions.",
        "authors": [
            "Joaquim A.P. Braga",
            "João N. Costa",
            "Jorge Ambrósio",
            "Daniel Frey",
            "António R. Andrade"
        ],
        "journal_conference_name": "Reliability Engineering & System Safety",
        "publisher": "Elsevier BV",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/155292",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Protein Folding, Host Cell Proteostasis, and Viral Evolution",
        "abstract": "Pandemics and epidemics caused by pathological RNA viruses, such as the 1918 influenza pandemic, the global AIDS epidemic, and the recent coronavirus pandemic, impose a severe burden on global health and the economy. A major challenge associated with developing effective antiviral strategies is the exceptionally high mutation rate of RNA viruses, which endows them with a remarkable capacity to adapt to selection pressures such as antibodies or antiviral drugs. Hence, it is critical to understand the molecular-level factors that can constrain and potentiate viral evolution. While mutations benefit viruses by generating the diversity required for evolution, they also threaten viral viability because the majority of non-conservative amino acid substitutions cause protein folding defect. Mutations that result in substantial protein folding defects cannot be tolerated, regardless of how adaptively beneficial the resultant protein variant otherwise would be. Importantly, in cells, protein folding is assisted by intricate networks of chaperones and quality control factors, termed proteostasis networks. When a substitution on a protein impedes its proper folding, proteostasis network components can triage the defective protein variant to chaperones for folding assistance, or to quality control factors for timely degradation. Interestingly, virtually all RNA viruses rely on their host’s proteostasis network components for viral protein folding. It follows that the host’s proteostasis network could play a prominent role in defining the sequence space accessible to an evolving viral protein. In this thesis, I address how host proteostasis networks shape viral protein evolution. First, I describe how the composition of the host cell’s proteostasis machineries chapes the accessible sequence space of human immunodeficiency virus envelope protein. Second, I focus on an important immune-escape variant of influenza nucleoprotein whose fitness depends on host chaperones, and reveal the underlying molecular mechanism of the chaperon dependence. Finally, I demonstrate how chaperone machineries can determine the host range of viruses, and provide potential pathways viruses can evolve to overcome this selection pressure. Overall, elucidating how protein folding and host cell proteostasis affect viral protein evolution would substantially improve our ability to accurately predict RNA virus evolution and host-switching, and may enable design of host-targeted therapeutics that reduce the adaptability of RNA viruses.",
        "authors": [
            "Jimin Yoon"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157965",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "System-level Design, Fabrication, and Optimization of Sorbent-based Atmospheric Water Harvesting Devices",
        "abstract": "Sorption-based atmospheric water harvesting (SAWH) has been demonstrated as a promising avenue to addressing the increasing problem of water scarcity, especially in arid inland regions where alternative technologies are limited. However, current sorbent materials are often limited in their applicability due to system integration and device design constraints. In this thesis, we present advancement of atmospheric water harvesting technologies in both the passive and active design space by leveraging a system-level approach to modelling and optimization of devices. First, we discuss SAWH device fundamentals in terms of heat, mass, and fluid transport, and identify key components which impact device performance for both passive (solar) and active (electrical/chemical) systems, as quantified by our proposed performance metrics. Next, we develop a coupled heat and mass transport model of a passive, solar-driven atmospheric water harvesting device and quantify the impact of system variables on device operation. We use this model to fabricate an optimal system that efficiently utilizes a hydrogel-salt composite sorbent for record passive water production in the Atacama Desert. Furthermore, we propose an underlying mechanism for observed system-level degradation of our hydrogel-salt composite and demonstrate successful lifetime elongation of the sorbent in SAWH operation. Additionally, we use our fundamental understanding of SAWH to design an active device for portable use. Highly compact, lightweight, and energy dense, this system operates independent of external environment conditions and produces more than 2 L/day of potable water. Finally, a generalized topology optimization approach is proposed for sorbent scaffolding structures to further improve system water output while reducing power consumption and packing of atmospheric water harvesting devices.",
        "authors": [
            "Chad T. Wilson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158311",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Characterizing Variation in Healthcare across Time and Providers using Machine Learning",
        "abstract": "Modeling healthcare decisions and their outcomes is a complex problem. In addition to being affected by patient characteristics, the prognosis can vary depending on when the patient is receiving care, and treatment decisions can vary depending on who makes the decisions. In this thesis, we consider two axes of variation in healthcare: over time and across providers. For both axes, we focus on identifying when variation exists, characterizing the patients who are affected by such variation, and addressing shifts due to this variation. The solutions we propose draw ideas from causality and dataset shift.\r\n\r\nIn the first part of this thesis, we address these three aspects for variation over time. First, we create an algorithm that can detect when a model is affected by change over time and identify sub-populations where the model is more affected. We use our algorithm to perform a large-scale study of temporal shifts in health insurance claims. We demonstrate changes over time are prevalent in healthcare and examine case studies to better understand the drivers of such changes. Next, we examine how to learn a model that can perform well on current data. As data from the current time period is limited, we consider several methods that can leverage sequences of historical data to learn a good image classification model for the final time step. We build a benchmark for evaluating these methods on sequences constructed from synthetic shifts and validate our conclusions on a real-world dataset.\r\n\r\nIn the second part of this thesis, we address similar questions for variation across providers. First, we create a statistical approach to test whether significant variation exists across providers. Our approach involves learning a model of treatment decisions with provider-specific random effects. We perform a case study on first-line type 2 diabetes treatment and find significant variation exists across providers. Then, we develop an algorithm for identifying regions of patients with the most disagreement between providers. We formalize this as a causal inference problem, where disagreement is defined by the causal effect of the provider on the treatment decision. We illustrate this algorithm on first-line type 2 diabetes and Parkinson's treatment decisions and uncover regions of variation that align with uncertainty in clinical guidelines.\r\n\r\nIn the third part of this thesis, we build a tool for examining the effects of variation over time or across providers for individual patients. We use a large language model built on electronic health record concepts to generate patient trajectories. To enable interventions on time and provider, we introduce new tokenizations for these concepts. We also incorporate a structural causal model for patient visits to allow for generation of interventional and counterfactual trajectories. We hope the model in this part of the thesis can be used to answer additional questions about how patient trajectories would change if they were treated during a different time period or by a different provider.",
        "authors": [
            "Christina X. Ji"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158480",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On the Acquisition of Formal Semantics in Statistical Models of Language",
        "abstract": "The increasingly impressive performance of recent large language models raises a crucial question: to what extent can such models, trained solely on text, develop an understanding of language grounded in the semantics of the underlying domain? Progress on this question carries significant practical and philosophical implications for the relationship between meaning, understanding, and the capacity to exhibit seemingly intelligent behavior.\r\n\r\nThis thesis makes two primary contributions. First, it develops a scientifically rigorous approach to studying what statistical models of language can understand about language based on the formal semantics of programming languages. Specifically, it leverages the probing classifiers framework: training small classifiers to find encodings of program semantics within the model's internal representations. A main insight is that the clean separation between syntax and semantics in this domain allows for greater control in experimental design. It introduces two new techniques. The first, semantic probing interventions, is a general methodology for distinguishing whether the probe's measurements reflect (1) the learned representations of the language model encode semantics or (2) that the probe itself has learned to infer semantics from representations of pure syntax. The second, latent causal probing, is a formal framework for probing that provides a robust empirical methodology for studying whether language models are able to access the latent concepts that underlie the text they observe during training. A key innovation is to create a single structural causal model that unifies (1) the data generation process underlying the text used to train the language model and (2) the steps of a probing experiment. This makes it possible to conduct a causal analysis that intervenes on the data generation process to trace the influence of the latent variables in the training data through the model's internal representations.\r\n\r\nThe second core contribution of this thesis consists of a series of experimental studies. Specifically, we train a language model on a synthetic grid-world navigation task, then probe the model's learned representations for encodings of the unobserved, intermediate world states. By leveraging the techniques we develop, the results deliver strong empirical evidence that statistical models of language are latent concept learners: capable of inducing the latent variables that underlie the generation of their training data, despite being trained only to model a conditional distribution over tokens.",
        "authors": [
            "Charles C. Jin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158503",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Overview of the Neutron Diagnostic Systems for the SPARC Tokamak",
        "abstract": "Neutron measurement is the primary tool in the SPARC tokamak for fusion power (Pfus) monitoring, research on the physics of burning plasmas, validation of the neutronics simulation workflows, and providing feedback for machine protection. A demanding target uncertainty (10% for Pfus) and coverage of a wide dynamic range (>8 orders of magnitude going up to 5x10^19 n/s), coupled with a fast-track timeline for design and deployment, make the development of the SPARC neutron diagnostics challenging. Four subsystems are under design, which exploit the high flux of direct DT and DD plasma neutrons emanating from a shielded opening in a midplane diagnostic port. The systems comprise: a set of ~15 flux monitors mainly ionization chamber and proportional counters for measurement of the neutron yield rate, two independent foil activation systems for measurement of the neutron fluence, a spectrometric radial neutron camera for poloidal profiling of the plasma emissivity, and a high-resolution magnetic proton recoil spectrometer for measurement of the core neutron spectrum. Together, the four systems ensure redundancy of sensors and methods, and aim to provide high resolutions of time (10 ms), space (~7 cm), and energy (<2% at 14 MeV). This paper presents the broader objectives behind the preliminary design of the SPARC neutron diagnostics, and discusses the ongoing studies on neutronics, detector comparisons, prototyping, and integration with the unique infrastructure of SPARC. Engineering details of the four subsystems and the concepts for in-situ neutron calibration are also highlighted.",
        "authors": [
            "P. Raj",
            "J.L. Ball",
            "J. Carmichael",
            "Johan A. Frenje",
            "R. Gocht",
            "G. Gorini",
            "I. Holmes",
            "Maria Gatu Johnson",
            "R. Kennedy",
            "S. Mackie",
            "M. Noncente",
            "E. Panontin",
            "M. Petruzzo",
            "M. Rebai",
            "M. Reinke",
            "John E. Rice",
            "D. Rigamonti",
            "M. Dalla Rosa",
            "A.A. Saltos",
            "M. Tardocchi",
            "R. Alex Tinguely",
            "X. Wang"
        ],
        "journal_conference_name": "Review of Scientific Instruments",
        "publisher": "AIP",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158720",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Technology Platform for Enabling Next-Generation Vacuum Electronic Devices Based on Silicon Field Emitter Arrays",
        "abstract": "As the demand for electronics with better performance and increased functionality continues to escalate, researchers are finding it more and more difficult to surpass the limitations of conventional transistors due to electron transport in solid-state. Nanoscale vacuum-channel transistors, in which the electron transport channel is vacuum instead of solid-state, offer a potential alternative device architecture beyond device scaling. Due to their ballistic transport and higher breakdown field, nanoscale vacuum-channel transistors are expected to show better performance in a wide variety of high-frequency, high-power, or harsh environment applications. Silicon field emitter arrays (FEAs) are a proven and mature technology that can be implemented as vacuum transistors, and they could also be used in vacuum integrated circuits. Many of the challenges regarding uniformity, reliability, and lifetime have been addressed in this technology. However, the scalability of the emission current remains a challenge. \r\n\r\nIn this work, we develop a layout-independent fabrication process for silicon FEAs that improves the scalability of emission current with array size. The fabrication process begins by first fabricating field emitters everywhere across the wafer and then selectively etching field emitters to form individual arrays. Using this process, we present for the first time silicon FEAs with array sizes ranging from 1 μm2 to 1 mm2, and we obtain emission current ranging from 1 nA to 1 mA, which represents a range of six orders of magnitude. In order to facilitate design of future vacuum integrated circuits, we develop a circuit model for silicon FEAs based on measurements of the transfer and output characteristics. The circuit model is used to demonstrate a proof-of-concept inverter based on a silicon FEA and pull-up resistor that could potentially be fabricated as a vacuum integrated circuit. Lastly, we characterize and model the statistical variation in emission current to determine if it is feasible to build vacuum integrated circuits using the layout-independent fabrication process presented in this work.",
        "authors": [
            "Nedeljko Karaulac"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158519",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Utilization and Synthesis of Symbolic World Models for Safe, Generalizable, and Efficient Action",
        "abstract": "Reinforcement learning with neural networks has proven incredibly flexible at learning to act in diverse environments. Model-based RL techniques have helped to ameliorate the dependence on large quantities of data that these models normally have. However, despite their flexibility, neural world models have several drawbacks. Symbolic world models, in comparison, are easier to verify (e.g. for safety concerns), more compatible with domain-independent planning techniques, and able to be learned or adapted with more limited data. In this thesis, I will demonstrate these advantages of symbolic world models in three projects. The first, VSRL, shows how we can use a symbolic world model to ensure that an RL policy is safe during both training and deployment and promote safe exploration. The second, SPARSER, presents a hybrid domain planner which uses world models in a planning domain description language. It showcases how we can exploit the event structure in the world model to enable more efficient planning. In the final project, PWM, I will explore learning a world model directly from observations and actions gathered from interacting with an environment. We combine symbolic and neural synthesis techniques to enable efficient world model synthesis even from visual observations. Together, these projects demonstrate the versatility and value of symbolic world models.",
        "authors": [
            "Nathan Hunt"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158475",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design of a Precision Needle for Injection of Fluid into the Suprachoroidal Space of the Eye for the Treatment of Retinal Detachment",
        "abstract": "Rhegmatogenous retinal detachment (RRD) is a vision-threatening condition that affects 10 to 18 per 100,000 people in the United States annually [1]. The current standard for treatment is pars plana vitrectomy (PPV), which is an invasive and expensive surgical procedure that leaves patients unable to perform usual activities for four to six weeks. In addition, current methods tend to produce distorted vision upon recovery. In-office Suprachoroidal Viscopexy™ (SCVEXY™) is a minimally invasive technique recently developed by Dr. Rajeev Muni for treating rhegmatogenous retinal detachment (RRD) which has been performed on a handful of people [2]. This procedure has the potential to greatly reduce the cost and recovery time of RRD while also improving the quality of the repair. It can be performed with no incision, no tamponade agent, and no patient post-op positioning requirements [2]. SCVEXY works by injecting viscous fluid into the suprachoroidal space, a “potential space” between the sclera and choroid, creating a “bleb” of fluid underneath the tear that pushes the choroid towards the retina and allows it to reattach. However, difficulty in safely injecting into this space at the location of the retinal tear currently limits the widespread utilization of the technique. If this procedure was made reliably safe, it could greatly change how retinal detachments are treated and improve patient outcomes. The primary difficulty arises in precisely locating the suprachoroidal space in order to inject the viscous fluid. The thickness of the sclera varies from patient to patient and between locations on the eye. Additionally, the scleral and choroidal tissues are very thin, leaving little room for positional error. Hemorrhage may occur if the needle punctures through the choroid and into the subretinal space, which could lead to bad outcomes. This work presents a device developed to minimally invasively reach posterior segments of the eye, deploy an injection needle in-situ with high resolution, sense when the needle tip has passed into the suprachoroidal space (SCS), and inject a viscous fluid. Not only will this device be used to treat retinal detachment in a minimally invasive manner, but it could also be used for drug injection or fluid aspiration via the suprachoroidal and subretinal spaces for treatment of a variety of posterior ocular diseases.",
        "authors": [
            "Emma Rutherford"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158203",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Multimodal Representation Learning for Agentic AI Systems",
        "abstract": "Modern artificial intelligence (AI) is poised to transform the scientific process, from ideation and experimentation to peer review. Many researchers posit that emerging generalist AI “agents” will soon no longer be mere tools, but equal partners in scientific exploration. In this work, we contribute to this evolving landscape through converging lines of research focused on developing and evaluating more efficient and interpretable AI systems, spanning both vision and language domains, and their applications to scientific evaluation and review. Our research focuses on three key areas. First, we introduce a novel framework to enhance the efficiency and robustness of cross-modal representation learning methods. Our approach utilizes progressive self-distillation and soft image-text alignments to model the many-to-many correspondences found in noisy web-harvested datasets. Extensive evaluation demonstrates that our method consistently outperforms CLIP across multiple benchmarks, including improved robustness to natural distribution shifts. We extend this framework to zero-shot open vocabulary detection, introducing augmentation, architectural and self-training strategies for improving vision-text feature alignment. Evaluation on long-tail detection benchmarks demonstrates state-of-the-art performance, with competitive performance for unseen classes, as well as superior transfer to additional datasets. Finally, we present the Review Integrated Scientific Evaluation (RISE) benchmark, a novel framework for assessing AI performance in understanding, critiquing, and providing constructive feedback on scientific manuscripts. Our study compares AI-generated reviews against human expert evaluations, revealing both the promising capabilities and current limitations of AI in scientific peer review. The dissertation concludes by proposing future directions for AI-accelerated science, emphasizing the need for collaborative human-AI scientific communities and the development of evaluation methods for higher-level autonomous capabilities in scientific domains. Altogether, this work contributes to the ongoing discourse on AI’s role in scientific research and paves the way for more rigorous integration of AI systems into the scientific process.",
        "authors": [
            "Alexander Andonian"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158506",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Integrated Visible-Light Liquid-Crystal-Based Modulators and\r\nGrating-Based Antennas",
        "abstract": "Current developments in integrated visible-light photonics have led to advancements in applications such as augmented-reality displays and quantum systems. However, the development of crucial integrated-photonics devices such as integrated gratingbased antennas and integrated optical modulators has predominantly focused on the infrared spectrum, leaving a gap in visible-light technologies. This thesis addresses this gap by designing and experimentally demonstrating integrated visible-light liquidcrystal-based (LC-based) modulators and grating-based antennas. First, we provide a thorough design guide for integrated visible-light grating-based antennas and experimentally demonstrate five antennas with varying advanced capabilities, including the first visible-light unidirectionally-emitting grating-based antennas for integrated optical phased arrays (OPAs), facilitating the use of integrated OPAs for new visible-light applications. Second, we discuss the fabrication processes, considerations, and evaluation techniques for successful packaging of integrated LC modulators, supporting the broader integration of LC into silicon-photonics platforms, enabling more compact and efficient on-chip modulation. Third, we experimentally demonstrate the first integrated visible-light LC-based variable-tap amplitude modulators, enabling a compact and low-power solution to integrated visible-light amplitude modulation for high-density integrated visible-light systems. Fourth, we experimentally demonstrate the first 300-mm wafer-scale platform and fabrication process that results in mechanically-flexible photonic wafers and chips, enabling the field of integrated photonics to advance into new application areas that require flexible photonic chips.",
        "authors": [
            "Andres Garcia Coleto"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158513",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Spatially-Adaptive LiDAR and Underwater Communications Using Integrated Optical Phased Arrays",
        "abstract": "Silicon-photonics microsystems have enabled advanced optoelectronic capabilities in applications spanning from sensors to communication systems. In particular, integrated optical-phased-array-based (OPA-based) technologies, such as solid-state LiDAR and free-space optical communications (FSOC) systems, show promise to revolutionize the way we sense and communicate. This thesis enables new integrated-OPA-based solid-state beam-steering capabilities for these existing applications, as well as emerging spatially- and spectrally-demanding applications. First, we develop and experimentally demonstrate a novel multi-beam solid-state OPA-based LiDAR system capable of detecting and ranging multiple targets simultaneously, passively, and without rastering. Through this work, we demonstrate a new spatially-adaptive sensing modality for solid-state LiDAR that promises to reduce the data deluge associated with LiDAR sensing for autonomous systems. Second, we show the first, to the best of our knowledge, spiral integrated OPAs, enabling emission of focusing beams with tunable variable focal heights for the first time. This work introduces a first-of-its-kind integrated OPA architecture and, as such, enables new functionality for emerging applications of OPAs that require focusing operation, such as biophotonic optical tweezers and chip-based 3D printers. Third, we show the first visible-light integrated-OPA-based FSOC transmitter and use it to experimentally demonstrate the first integrated-OPA-based underwater-wireless-optical-communication (UWOC) link. This integrated OPA transmitter chip can reduce the size, weight, and mechanical complexity of apparatus for UWOC systems.",
        "authors": [
            "Daniel Markus DeSantis"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158495",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Guiding Deep Probabilistic Models",
        "abstract": "Deep probabilistic models utilize deep neural networks to learn probability distributions in high-dimensional data spaces. Learning and inference in these models are complicated due to the difficulty of direct evaluation of the differences between the model distribution and the target. This thesis addresses this challenge and develops novel algorithms for learning and inference based on the guidance of complex parameterized distributions towards desired configurations via signals from auxiliary discriminative models.\r\n\r\nIn the first part of the thesis, we develop novel stable training objectives for Generative Adversarial Networks (GANs). We show that under standard unary-discriminator objectives, most of the valid solutions, where the learned distribution is aligned with the target, are unstable. We propose training objectives based on pairwise discriminators that provably preserve distribution alignment and demonstrate improved training stability in image generation tasks.\r\n\r\nIn the second part of the thesis, we introduce distribution support alignment as an alternative to the distribution alignment objective and develop a learning algorithm that guides distributions towards support alignment. We demonstrate the effectiveness of our approach in unsupervised domain adaptation under label distribution shift. Recent works have shown that under cross-domain label distribution shift, optimizing for distribution alignment is excessively restrictive and causes performance degradation. Our algorithm, which is based on support alignment, alleviates this issue.\r\n\r\nIn the third part of the thesis, we develop a novel approach to compositional generation in iterative generative processes: diffusion models and Generative Flow Networks (GFlowNets). Motivated by the growing prominence of generative models pre-trained at scale and the high training costs, we propose composition operations and guidance-based sampling algorithms that enable the combination of multiple pre-trained iterative generative processes. We offer empirical results on image and molecular generation tasks.",
        "authors": [
            "Timur Garipov"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158520",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Interpretable and Automated Bias Detection for AI in Healthcare",
        "abstract": "Biases in artificial intelligence systems and the data they operate over are a major hurdle to their application in clinical and biomedical settings. Such systems have frequently been shown to fail to generalize from their training data to the real world environment and often display differing levels of accuracy over different population subgroups, which has detrimental effects on patients' quality of care and on healthcare equality. Here, we introduce an automated framework for identifying and understanding nontrivial sources of bias in healthcare datasets and AI models. Our framework is data and model agnostic and does not rely on human-developed heuristics or assumptions to uncover bias. We demonstrate its effectiveness by uncovering serious and nontrivial sources of bias in three widely used clinical datasets and one biomedical dataset, over the diverse tasks of diabetes risk prediction, lung cancer risk prediction, and biomolecular toxicity prediction. Our framework is used to uncover biases caused by patient BMI and computed tomography (CT) scanner type in the data used by a cutting-edge lung cancer risk prediction AI model, causing AUC drops on the order of ten percent.",
        "authors": [
            "Christopher Alexiev"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158474",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Safe and Ethical Implementation of Intelligent Systems",
        "abstract": "In the year 2024, the prospect of solving human level tasks using intelligent systems is no longer the subject of science fiction. As these systems play an increasingly critical role in our day-to-day lives, it becomes ever more important to consider the safety and ethics surrounding their implementation. This is a multifaceted challenge spanning multiple disciplines, involving questions at the regulatory, engineering, and theoretical levels. This thesis discusses three projects that span these levels. We first explore the problem of tracing causal influence from training data to outputs of generative models. In our exploration we encounter the phenomenon of unattributability, and consider its scientific and regulatory implications. We next tackle the challenge of designing a high diversity library of therapeutics that is depleted of dangerous off-target binders using intelligent systems, developing a suite of inference and optimization tools along the way. Finally, we derive universal bounds for the robustness of image classifiers that inform us of how safe these intelligent systems can be in theory. Together, these projects present a multilevel overview of the safe and ethical implementation of intelligent systems.",
        "authors": [
            "Zheng Dai"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158497",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Towards Object-based SLAM",
        "abstract": "Simultaneous localization and mapping (SLAM) is a fundamental capability for a robot to perceive its surrounding environment. The research area has developed for more than two decades from the original sparse landmark-based SLAM to dense SLAM, and now there is a demand for semantic understanding of the environment beyond pure geometric understanding. This thesis delves into object-based SLAM where the map consists of a set of objects with their semantic categories recognized and their poses and shapes estimated. Such a map provides vital object-level semantic and geometric perception to applications such as augmented reality (AR), mixed reality (MR), robot manipulation, and self-driving. In order to perform object-based SLAM, the sensor measurements have to undergo a series of processes. First, objects are semantically segmented in the sensor measurements. This step is typically done by a neural network. As robots are often required to bootstrap from some initial labeled datasets and adapt to different environments where labeled data are unavailable, it is important to enable semi-supervised learning to improve the robot’s performance with the unlabeled data collected by the robot itself. Second, after the objects are segmented, measurements for each object across different views have to be associated together for downstream processing. Lastly, the robot must be able to extract the object pose and shape information from the measurements without access to the detailed object CAD models which are commonly unavailable. This thesis studies these three aspects of object-based SLAM, namely semi-supervised learning of semantic segmentation in a robotics context, data association for object-based SLAM, and category-level object pose and shape estimation. The thesis closes with a discussion of how these components can be integrated into a full object-based SLAM system in the future.",
        "authors": [
            "Yihao Zhang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158310",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Intelligent Textiles for Physical Interactions",
        "abstract": "Human-environment interaction is a fundamental aspect of our daily lives, involving the constant use of our sensory and motor systems to extract, process, and communicate information. However, capturing, analyzing, and reproducing these interactions pose significant challenges due to their pervasive, variable, and prolonged nature, as well as their unique character for each individual. Despite these challenges, it is essential to develop systems that can accurately capture and reproduce human-environment interactions for a wide range of applications, including human behavior studies, health monitoring, human-computer interactions, and robot imitation learning. This thesis focuses on developing seamlessly integrated, scalable manufactured sensing and actuating systems, as well as advanced computational pipelines to capture, analyze, and reproduce adaptive ubiquitous physical human-environment interactions.",
        "authors": [
            "Yiyue Luo"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158478",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Learning to Improve Clinical Decisions and AI Safety by Leveraging Structure",
        "abstract": "The availability of large collections of digitized healthcare data along with the increasing power of computation has allowed machine learning (ML) for healthcare to become one of the key applied research domains in ML. ML for health has great potential in providing clinical decision-making support that can improve quality of care and reduce healthcare spending by easing clinical operations. However, the successful development of ML models in healthcare is contingent on data that is complex, noisy, heterogeneous, limited in labels and highly sensitive. In this thesis, we leverage the unique structure present in medical data along with the availability of external knowledge to guide model predictions. Additionally, we develop differentially private (DP) training techniques using gradient structure to mitigate privacy leakage.\r\n\r\nIn this thesis, we develop methods on different medical modalities such as multivariate physiological signals of ICU patients, patient discharge summaries, biomedical scientific articles, radiology reports, chest radiography imaging and spoken utterances. We tackle tasks such as forecasting patient states, relationship extraction, disease prediction, medical report generation and differentially private model training. We begin the thesis by offering open source data processing and modeling frameworks, move towards improved interpretability of model predictions to develop clinician trust and finally investigate differentially private ML techniques to protect user data. \r\n\r\nFirst, we show that the use of aggregated feature representations based on clinical knowledge offers model robustness against evolving hospital systems. Second, we leverage external knowledge in the form of clinical concept extraction to significantly improve relationship extraction. Third, we leverage the rich information from reports associated with chest radiographs to develop highly accurate disease severity prediction models using contrastive learning. Fourth, we showcase that the report generation task offers competitive disease prediction capabilities, label efficiency and improved interpretability. Finally, we introduce novel methods for improved privacy-utility-compute tradeoffs for DP pre-training of large speech models. We highlight DP as an important component of model safety, necessitating its development in conjunction with AI safety approaches that will be pertinent in healthcare and beyond.",
        "authors": [
            "Geeticka Chauhan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158271",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Distributional Private Information Retrieval",
        "abstract": "A private-information-retrieval (PIR) scheme lets a client fetch a record from a remote database without revealing which record it has fetched. Classic PIR schemes treat all database records the same but, in practice, some database records are much more popular (i.e., commonly fetched) than others. We introduce distributional private information retrieval, a new type of PIR that can run faster than classic PIR—both asymptotically and concretely—when the popularity distribution is heavily skewed. Distributional PIR provides exactly the same cryptographic privacy notion as classic PIR. The speedup comes from providing a relaxed form of correctness: distributional PIR guarantees reliable retrieval for PIR queries that follow the popularity distribution, but only “best-effort” retrieval for out-of-distribution queries. We give several constructions of distributional-PIR schemes that make black-box use of existing standard PIR protocols. On a popularity distribution drawn from real-world Twitter data, distributional PIR reduces compute costs by 5.1–77× compared to existing techniques. Finally, we build CrowdSurf, an end-to-end system for privately streaming social-media posts, and show that our PIR schemes reduce the end-to-end server cost by 8×.",
        "authors": [
            "Ryan Lehmkuhl"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158492",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design and Modeling of a Catapulting Magnetic Transmission for Tuning Energy Storage and Release",
        "abstract": "The purpose of this work is to generate design rules and models for a catapulting magnetic leadscrew transmission. These rules and models empower scientists and engineers with the ability to tune energy storage and release, and thereby increase the peak specific power (power/mass) of an actuator. This enables rapid design and development of lightweight (< 0.5 kg), high peak power (>200 W) actuators. This has the potential to impact powered exoskeletons and force-controlled robotics for rehabilitation and strength augmentation of explosive movements such as locomotion, jumping, and throwing. This thesis provided the following scientific contributions: (i) the concept of a catapulting magnetic screw actuator, (ii) experimentally validated models that are useful for the design and optimization of the magnetic leadscrew, considering both magnetic and structural aspects, (iii) experimentally validated models of the catapulting event in a magnetic leadscrew, and (iv) use of these models in the context of a practical application, namely powered exoskeletons that may reduce the metabolic cost of walking. First, the catapulting magnetic screw is introduced. An equation of motion is derived and experimentally validated. The equation of motion demonstrates that the potential wells in the magnetic screw create a ripple in the power as a function of time. Then, despite the equation of motion being a nonlinear differential equation with no closed-form solution, bounds on the ripple magnitude and frequency are derived. This gives the slip force and the lead needed to meet a specified tolerance on power as a function of time.\r\nThen, a model is developed that enables rapid design of a magnetic screw that achieves a desired slip force. This model agrees with finite element analysis to within 10% error across varying each design parameter by multiple orders of magnitude. Then, given a magnetic screw, a structure is needed to be sufficiently stiff to keep the magnets from sticking together. Models of the magnetic stiffness matrix and structural stiffness matrix and simplifications thereof are given to ensure sufficient structural stiffness. Finally, the catapulting event may be too fast for a desired application, so it is shown how nonlinear springs may be used to meet requirements for powered exoskeletons that assist in walking.",
        "authors": [
            "Marcel Adam Craig Thomas"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158320",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Interactive Spin Dynamics in Magnon and Quantum Spin Systems",
        "abstract": "Spintronics utilizes the intrinsic spin of electrons to design next-generation electronic devices, reducing power consumption and enabling innovative computing functions. Over the past decades, significant research interest has been directed toward two types of spin-based systems: collective excitations of spins, known as spin waves or magnons, in magnetic materials, and optically active spin defects as represented by nitrogen-vacancy (NV) centers in diamond, leading to the prosperity of magnonics, quantum sensing, and quantum information processing. As the understanding of dynamics in individual spin systems has deepened, recently there has been an increasing interest in the interactive dynamics within hybrid spin systems. This shift in focus reflects an increasing curiosity about how these complex interactions can be harnessed to further advance their microwave and quantum applications. However, several challenges persist, including the limited coherence length of magnons and the restricted frequency range of NV-based magnetometers, which will be tackled in this thesis. We first leverage the chirality of interlayer magnetic dipolar interactions to introduce an easily implementable system—antiparallel aligned magnetic multilayers—for realizing topological magnonic surface states and low-dissipation spin current transport in a tunable manner. We then expand the frequency window of NV-based magnetometers using nonlinear microwave-spin interactions, offering novel functionalities in quantum state control and sensing. We further exploit nonlinear spin dynamics by hybridizing NV centers with magnonic thin films, which not only amplifies the intensity of nonlinear resonance signals that are intrinsic to NV spins, but also enables novel frequency mixings through parametric pumping and nonlinear magnon scattering effects. We believe our study of interactive spin dynamics in hybrid systems involving magnons, quantum spin defects, and microwave photons help optimize these systems for a wide range of applications in both classical and quantum domains.",
        "authors": [
            "Zhongqiang Hu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158494",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Multiscale design of bioadhesive platforms for next-generation applications in surgery and healthcare",
        "abstract": "Bioadhesives—materials capable of adhering to biological tissues—hold significant promise as transformative tools in healthcare, offering the ability to repair tissues with ease and minimal damage. These materials present numerous opportunities in surgery and human-machine interfaces, creating a broad landscape of applications that has captivated clinical and scientific interest alike. Still, there remain open challenges surrounding their reliability, biocompatibility, usability, and versatility. These include weak adhesion with wet tissues, foreign body response, cumbersome application processes, and limited customizability. This dissertation presents a multiscale framework for addressing these obstacles, encompassing design strategies on the molecular, polymer network architecture, macroscale device, and application process levels. The implementation of this framework is demonstrated through the development of two pioneering bioadhesive platforms: (1) a multifunctional patch for minimally invasive surgery, and (2) a 3D printable bioadhesive for fabricating tunable, application-specific devices. Together, these platforms expand the design space for creating robust and versatile tissue repair solutions and biomedical devices.",
        "authors": [
            "Sarah J. Wu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158323",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Formally Verifying Secure and Leakage-Free Systems: From Application Specification to Circuit-Level Implementation",
        "abstract": "Hardware and software systems are susceptible to bugs and timing side-channel vulnerabilities. Timing leakage is particularly hard to eliminate because leakage is an emergent property that can arise from subtle behaviors or interactions between hardware and software components in the entire system, with root causes such as non-constant-time code, compiler-generated timing variation, and microarchitectural side channels. This thesis contributes a new approach using formal verification to rule out such bugs and build systems that are correct, secure, and leakage-free.\r\n\r\nThis thesis introduces a new theory called information-preserving refinement (IPR) for capturing non-leakage in addition to correctness and security, implements a verification approach for IPR in the Parfait framework, and applies it to verifying hardware security modules (HSMs). Using Parfait, a developer can verify that an HSM implementation leaks no more information than is allowed by a succinct application-level specification of the device's intended behavior, with proofs covering the implementation's hardware and software down to its cycle-precise wire-I/O-level behavior.\r\n\r\nThis thesis uses Parfait to implement and verify several HSMs, including an ECDSA certificate-signing HSM and a password-hashing HSM, on top of Ibex and PicoRV32-based hardware platforms. Parfait provides strong guarantees for these HSMs: for example, it proves that the ECDSA-on-Ibex implementation—2,300 lines of code and 13,500 lines of Verilog—leaks nothing more than what is allowed by a 40-line specification of its behavior.",
        "authors": [
            "Anish Athalye"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158521",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Radiatively Cooled Magnetic Reconnection Experiments Driven by\r\nPulsed Power",
        "abstract": "Magnetic reconnection is a ubiquitous process in astrophysical plasmas, responsible for the explosive conversion of magnetic energy into thermal and kinetic energy. In extreme astrophysical systems, such as black hole coronae and neutron star magnetospheres, radiative cooling modifies the energy partition by rapidly removing internal energy. In this thesis, we perform experimental and computational studies of magnetic reconnection in a radiatively cooled regime, previously unexplored in reconnection studies. The Magnetic Reconnection on Z (MARZ) experiments consist of a dual exploding wire array, driven by a 20 MA peak, 300ns rise time current generated by the Z pulsed-power machine (Sandia National Labs). The load generates oppositely-directed supersonic, super-Alfvénic, collisional plasma flows with anti-parallel magnetic fields, that generate a reconnection layer (Lundquist number SL ∼ 100), in which the total cooling rate far exceeds the Alfvénic transit rate [mathematical notation].\r\n \r\nTwo- and three-dimensional simulations of the MARZ experiments are performed in GORGON, an Eulerian resistive magnetohydrodynamic code. The simulations demonstrate the generation of a reconnection layer, which radiatively collapses, exhibiting a rapid fall in temperature, strong compression, and an increased reconnection rate consistent with theoretical predictions. The reconnection layer is unstable to the plasmoid instability, generating secondary current sheets separated by magnetic islands. High energy X-ray emission is generated predominantly by the plasmoids. The plasmoids also collapse radiatively, and the reconnection layer recovers a laminar large aspect ratio structure, which does not exhibit further plasmoid generation, indicating stabilization of the original plasmoid instability of the current sheet.\r\n \r\nThe experiments confirm numerical predictions by providing evidence of plasmoid formation and strong radiative cooling. Experimental diagnostics directly measure the spatial, temporal, and spectral properties of radiative emission from the reconnecting system. The reconnection layer generates a transient burst of >1 keV X-ray emission, consistent with the formation and subsequent rapid cooling of the layer. Time-gated X-ray images show fast-moving (up to 50 km s−1) hotspots in the layer, consistent with the presence of plasmoids in 3-D resistive magnetohydrodynamic simulations. X-ray spectroscopy shows that these hotspots generate the majority of Al K-shell emission (around 1.6 keV), and exhibit temperatures (170 eV) much greater than that of the plasma inflows and the rest of the reconnection layer.\r\n \r\nThe findings in this thesis are of particular relevance to the generation of radiative emission from reconnection-driven astrophysical events, and to the global dynamics of reconnection in strongly cooled systems. The MARZ experiments also provide a novel platform for investigating radiative effects in high-energy-density and laboratory astrophysics experiments, and for validation of radiation magnetohydrodynamic and atomic spectroscopy codes.",
        "authors": [
            "Rishabh Datta"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158307",
        "group_name": "Cireng Crispy"
    }
]