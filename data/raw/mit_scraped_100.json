[
    {
        "title": "The American Institute for Manufacturing Integrated Photonics: advancing the ecosystem",
        "abstract": "The American Institute for Manufacturing Integrated Photonics (AIM Photonics) is focused on developing an end-to-end integrated photonics ecosystem in the U.S., including domestic foundry access, integrated design tools, automated packaging, assembly and test, and workforce development. This paper describes how the institute has been structured to achieve these goals, with an emphasis on advancing the integrated photonics ecosystem. Additionally, it briefly highlights several of the technological development targets that have been identified to provide enabling advances in the manufacture and application of integrated photonics.",
        "authors": [
            "Thomas L. Koch",
            "Michael Liehr",
            "Douglas Coolbaugh",
            "John E. Bowers",
            "Rod Alferness",
            "Michael Watts",
            "Lionel C Kimerling"
        ],
        "journal_conference_name": "Broadband Access Communication Technologies X",
        "publisher": "SPIE",
        "year": "2106",
        "doi": "http://hdl.handle.net/1721.1/112987",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "How do we interpret the outputs of a neural network trained on classification?",
        "abstract": "Deep neural networks are widely used for classification tasks, but the interpretation of their output activations is often unclear. This tutorial article explains\r\nhow these outputs can be understood as approximations of the Bayesian posterior.\r\nWe showed that, in theory, the loss function for classification tasks – derived by\r\nmaximum likelihood – is minimized by the Bayesian posterior. We conducted\r\nempirical studies training neural networks to classify synthetic data from a known\r\ngenerative model. In a simple classification task, the network closely approximates the theoretically derived posterior. However, a few changes in the task can\r\nmake accurate approximation much more difficult. The ability of the networks to\r\napproximate the posterior depends on multiple factors, such as the complexity of\r\nthe posterior and whether there is sufficient data for learning.",
        "authors": [
            "Yudi Xie"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "International Conference on Learning Representations",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159032",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Geospatial Trucking Industry Decarbonization Explorer (Geo-TIDE): Technical Guide and Methodology",
        "abstract": "Geo-TIDE is a public, interactive decision-support tool developed by the MIT Climate &\r\nSustainability Consortium (MCSC) to help trucking industry stakeholders identify and evaluate early opportunities for fleet and infrastructure decarbonization. By integrating public geospatial datasets such as regional freight flows, policy incentives, and spatially resolved cost and emissions models, Geo-TIDE enables data-driven decisions about where, when, and how to invest in low-carbon technologies. In this technical guide, Danika Eamer (who has led the development of Geo-TIDE) and co-authors Micah Borrero, Brooke Bao, Brilant Kasami, and Helena De Figueiredo Valente detail the tool’s functionality, showcase real-world usage scenarios, and explore the methodology behind its evolution and development.",
        "authors": [
            "Danika Eamer",
            "Micah Borrero",
            "Brooke Bao",
            "Brilant Kasami",
            "Helena De Figueiredo Valente"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159069",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Reducing Proliferation Risks with High-Assay Low-Enriched Uranium Fuels in Reactors with Coated-Particle (TRISO) Fuels",
        "abstract": "The use of graphite-matrix tri-structural-isotropic (TRISO) fuels in high-temperature reactors with high-assay low-enriched uranium (HALEU) can significantly reduce nuclear weapons proliferation risks relative to other fuels and reactor types. The HALEU fuel, with fuels containing 15% to 20% 235U enable used nuclear fuels (UNFs) with thermal neutron–spectrum burnups between 150 000 and 200 000 MWd per ton. At these high burnups, the plutonium isotopics make the direct use for nuclear weapons unattractive and the uranium isotopics unattractive as a feed to a uranium-enrichment plant. On the front end, it would require the theft of ~150 000 pebbles with uranium just under 20% 235U to create the theoretical potential to produce sufficient material for one weapon (1000 kg), which is about a 2-year supply of fuel for these reactors.\r\n\r\nThe chemical and mechanical processing requirements to convert fresh TRISO fuel to uranium metal for use in a nuclear weapon are beyond nonstate actors. Over 10 sequential chemical process steps would be required, plus uranium recovery from waste streams, to avoid large uranium losses in the conversion processes. If a nation-state wanted to make a nuclear weapon starting with HALEU fuel, they would enrich the HALEU from 19.95% to over 90% 235U, which presumes they already possess enrichment capabilities and can use any uranium feedstock. If enriched to weapons-grade 235U, 1 ton of HALEU has sufficient 235U for multiple weapons.\r\n\r\nSeparately, it is not clear if a weapon can actually be built with HALEU fuel. The fuel characteristics also reduce risks from sabotage. Consequently, we conclude that reactor safeguards for fresh HALEU TRISO fuel can be similar to those for low-enriched uranium light water reactor fuel; that is, no requirements for added security or other measures. TRISO UNF safeguards and security can be significantly relaxed relative to the requirements for other types of UNF at the reactor site.",
        "authors": [
            "Charles Forsberg",
            "Andrew Kadak"
        ],
        "journal_conference_name": "Nuclear Technology",
        "publisher": "Taylor & Francis",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159161",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "AGM aquariums and elliptic curves over arbitrary finite fields",
        "abstract": "In this paper, we define a version of the arithmetic-geometric mean (AGM) function for arbitrary finite fields F q , and study the resulting AGM graph with points ( a , b ) ∈ F q × F q and directed edges between points (a, b), ( a + b 2 , ab ) and (a, b), ( a + b 2 , - ab ) . The points in this graph are naturally associated to elliptic curves over F q in Legendre normal form, with the AGM function defining a 2-isogeny between the associated curves. We use this correspondence to prove several results on the structure, size, and multiplicity of the connected components in the AGM graph.",
        "authors": [
            "June Kayath",
            "Connor Lane",
            "Ben Neifeld",
            "Tianyu Ni",
            "Hui Xue"
        ],
        "journal_conference_name": "Research in Number Theory",
        "publisher": "Springer International Publishing",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159072",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Ordering Candidates via Vantage Points",
        "abstract": "Given an n-element set C ⊆ R d and a (sufficiently generic) k-element multiset V ⊆ R d , we can order the points in C by ranking each point c ∈ C according to the sum of the distances from c to the points of V. Let Ψ k ( C ) denote the set of orderings of C that can be obtained in this manner as V varies, and let ψ d , k max ( n ) be the maximum of | Ψ k ( C ) | as C ranges over all n-element subsets of R d . We prove that ψ d , k max ( n ) = Θ d , k ( n 2 d k ) when d ≥ 2 and that ψ 1 , k max ( n ) = Θ k ( n 4 ⌈ k / 2 ⌉ - 2 ) . As a step toward proving this result, we establish a bound on the number of sign patterns determined by a collection of functions that are sums of radicals of nonnegative polynomials; this can be understood as an analogue of a classical theorem of Warren. We also prove several results about the set Ψ ( C ) = ⋃ k ≥ 1 Ψ k ( C ) ; this includes an exact description of Ψ ( C ) when d = 1 and when C is the set of vertices of a vertex-transitive polytope.",
        "authors": [
            "Noga Alon",
            "Colin Defant",
            "Noah Kravitz",
            "Daniel G. Zhu"
        ],
        "journal_conference_name": "Combinatorica",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159170",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Leveraging Seawater Thermal Energy Storage and Heat Pumps for Coupling Electricity and Urban Heating: A Techno-Economic Analysis",
        "abstract": "This paper presents an economic assessment of seawater thermal energy storage (TES) integrated with industrial heat pumps to couple renewable electricity generation with urban district heating networks. Using Amsterdam as a case study, we develop a techno-economic model leveraging real-world data on electricity prices, heat demand, and system costs. Our findings show that large-scale TES using seawater as a storage medium significantly enhances district heating economics through energy arbitrage and operational flexibility. The optimal configuration yields a net present value (NPV) of EUR 466 million over 30 years and a payback period under 6 years. Thermal storage increases NPV by 17% compared to systems without storage, while within-day load shifting further boosts economic value by 23%. Accurate demand and price forecasting is critical, as forecasting errors can reduce NPV by 13.7%. The proposed system is scalable and well suited for coastal cities, offering a sustainable, space-efficient solution for urban decarbonization and addressing renewable energy overproduction.",
        "authors": [
            "Timur Abbiasov",
            "Aldo Bischi",
            "Manfredi Gangi",
            "Andrea Baccioli",
            "Paolo Santi",
            "Carlo Ratti"
        ],
        "journal_conference_name": "Energies",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159075",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Encourage circular practices in the supply chain",
        "abstract": "Every year 300 million tons of plastic waste are produced, and the amount of plastic produced increases with the world population. The more people there are on the planet, the more waste is produced. The concepts of circular economy are gaining popularity. Companies are looking to implement circular strategies to maximize the use of materials, reduce waste and help the environment while improving their corporate image.\r\nSince the coronavirus pandemic, digital transformation has progressed faster and faster, which has boosted digital communication. Social networks began to play a fundamental role in communication since they are an efficient means of interacting with people worldwide in real-time. Due to social networks' social impact, they can be used to influence people's decision-making.\r\nThis study aims to develop a model that encourages people to adopt recycling habits for polyethylene terephthalate (PET) bottles through social networks focused on the population of the United States. This study will use analytics tools such as the Bass Diffusion Model, and an economic analysis of the viability will be carried out to implement the proposed strategies.",
        "authors": [
            "Alejandro Jorge Goitia Polo",
            "Juan Manuel Perez Dovalo"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159025",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Buy, Rent and Sale: Chasing better cash flows",
        "abstract": "This capstone project optimizes the inventory levels of a rental car company and improves the cash-to-cash cycle. The solution approach is a Mixed Integer Linear Program (MILP) model, considering a multiple-period inventory. The model provides purchasing and selling plans and cash and vehicle flow in the system for each quarter and each type of vehicle for five years. The analytical model was created to maximize the company’s gross margin, considering revenues from renting and sales, cost buy, opportunity cost, and general cost (maintenance, holding, and others). Moreover, it considers an initial inventory and helps the company manage these assets in the best way possible to meet the demand. The result shows an optimal solution of 3.3 billion Colombian pesos (COP) for five years in the base case scenario. Afterward, a sensitivity analysis for different perspectives related to renting period, budget, depreciation rate, and exchange rate impact was carried out. From that perspective, it is possible to understand that the primary trigger to create revenue is extending the renting period. Moreover, the sponsor company can interpret how factors in the market affect the total result success and create an action plan to anticipate these risks.",
        "authors": [
            "Ana Patricia Do Couto Selem",
            "Juan Marcelo Oyarzun Rodriguez",
            "Ricardo Leon Monsalve Uribe"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159030",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Reducing food losses by improving the efficiency of the banana supply chain in the Antioquia corridor in Colombia",
        "abstract": "In 2011, the Food and Agricultural Organization of the United Nations (FAO) estimated that one-third of the food produced in the world for human consumption was lost or wasted (FAO, 2021d). Ten years later, a World Wildlife Fund (WWF) study calculated the percentage of food destined for consumption wasted along the entire chain. It reached 1.2 billion tons of food lost on farms and 931 million tons wasted at the retail, food service, and household levels, which accounts for around 40% (WWF-UK, 2021). All this wasted food could feed more than double the number of undernourished people worldwide, estimated to be between 720 and 811 million in 2020 (see figure 1) (FAO, 2021e; World Food Programme, 2020).",
        "authors": [
            "Ananthakumar Sethuramanujam",
            "Laura Natalia Fernandez Cedi"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159027",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Demand Forecasting Analysis for Pharma Retail",
        "abstract": "Demand planning is the connection between marketing, finance, and operations. In an industry like pharma retail, products do not always behave according to a regular stable baseline. In addition, marketing enrichment like promotions or price fluctuations and the impactof government regulations and patient base characteristics increase operational complexity. Moreover, more than thirty percent of changes in the forecast from one cycle to another can lead to overstock or out-of-stock due to the high production and delivery lead times.\r\nThe purpose of this project is to find a proper demand forecasting model for a selected group of stock-keeping units to improve supply processes of the most important stores of the sponsoring company, leading to further benefits such as budget purposes as a top-down analysis. Data analysis is needed for trends, seasonality, stockouts, and demand stability. Followed by the application of various forecasting models, including Machine Learning algorithms, this project provides a comparison of models to define the best baseline as a tool for the planning area to enrich to improve operational KPIs.",
        "authors": [
            "Nestor Andres Moreno Quintero",
            "Mariana Martins de Brito Sousa",
            "Waldo Mauricio Gabriel Flores Trujillo"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159023",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Determining optimal inventory positions in an urban network",
        "abstract": "Supply chain networks are becoming increasingly complex due to the aggressive growth of multiple digital trends, like the rise of e-commerce and the increased customer expectations, which have been enhanced through the pandemic over the last few years. Therefore, this study proposes a model to develop an inventory optimization strategy for a multi-tier supply chain case study in the US market, considering the supply and demand variability for local and international distribution. First, different approaches from the theoretical perspective are analyzed, from traditional inventory management to the new end-to-end perspectives. After that, details of the methodology will be explained, considering the statistical benefits of demand pooling. Finally, real numbers from a case study are applied to the methodology to measure the solution's impact, followed by the conclusions found from the study.",
        "authors": [
            "Juan Pablo Briceño Tipacti",
            "Gabriel Rocha Camargo"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159026",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Reduction of Costs and Emissions in Outbound Transport",
        "abstract": "The global food system accounts for nearly 30% of the total CO2 emissions worldwide. About 19% of that figure is due to transportation-related emissions. The main problem being addressed in this project is to identify the main drivers of CO2 emissions in outbound transportation for a major CPG food company in Antioquia, Colombia, which has declared sustainability as a major driver in their corporate strategy. By indirectly measuring CO2 emissions, a better understanding of the main drivers of emissions can be acquired. The cause-effect relationships on the distribution performance in emissions and cost to serve are in place.\r\nA comprehensive literature review of the state-of-the-art methodologies and techniques to assess CO2 emissions is part of this project, as well as a qualitative evaluation of the challenges of Antioquia’s topography. Two different methodologies have been used throughout this document to estimate CO2 emissions. A fuel-based approach and a distance-weight-based approach use CO2 equivalent units to estimate emissions at different levels of aggregation. Quantitative and spatial analysis allows us to conclude that regions that are harder to reach (low-volume municipalities located in hilly areas, irrespective of distance traveled) have a higher cost to serve and higher emissions due to an increase in transportation costs, fuel usage, difficulties to consolidate cargo and difficulties to increase vehicle usage due to the low volume sales.",
        "authors": [
            "Leonardo Amazonas Machado",
            "Ricardo Chavelas Manzo",
            "Rodrigo Silva Tourinho Nakamura"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159028",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Minimizing last-mile emissions through altitude-aware route optimization",
        "abstract": "This study introduced an exact optimization approach to solve a new special type of Travelling Salesperson Problem. This problem considers time-windows restrictions and a new objective function—the objective regards driver assessment awareness and fuel consumption. The latter is modeled as a function of variable vehicle payload and terrain elevation data. This problem can be stated as the way to find the best route to service a set of customer demands, attempting to deliver within agreed time windows, mimicking paths that are as similar as possible to good routes executed in the past by experienced drivers, allowing small alterations as to reduce duration and fuel consumption. The authors proposed an innovative mixed integer linear programming formulation and a cluster decomposition approach that reduces search space and makes the approach applicable to solving real-world-sized problems. This model was parametrized using a small-sized mockup dataset and had its applicability tested on real data. The latter consisted of a public dataset containing trips executed and evaluated by real drivers of Amazon company. The results show that it was possible to reduce in -5.7% the fuel consumption in the routes of this dataset. Since this variable is directly related to emissions and pollution, this result shows that the suggested approach offers promising prospects for improving efficiency and reducing the carbon footprint of logistics last-mile operations. To the best of the authors' knowledge, this study contributes to the literature in that it is the first to jointly tackle driver assessment awareness and fuel consumption in a route optimization problem. Thus, it is also the first to propose a mathematical formulation and solution approach for this problem.",
        "authors": [
            "Gustavo De Abreu Rodrigues",
            "Gustavo Jimenez Ruan",
            "Jenny Carolina Amores"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159029",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Fulfillment models framework for e-commerce companies",
        "abstract": "E-commerce relevance is increasing, and companies should be prepared to fulfill customers’ expectations and ensure an optimal shopping experience. Online worldwide retail sales generated 70 billion U.S. dollars in 2019, being Mexico and Brazil the main leaders for this type of channel in LATAM (Chevalier, 2020).\r\nWith the objective of being more efficient and differentiate from competitors, it is vital to have an extremely consistent and aligned supply chain that follows the company's business strategy. To achieve this new challenge, the following study aims to generate a framework decision matrix, enabling companies to support decisions of introducing fresh, dry, refrigerated, and frozen product categories based on five major warehousing trends: distribution center, fulfillment center, dark-store, micro-fulfillment center and crowdsourced warehousing solutions. To develop this project a systematic literature review combining case studies, papers, research articles and experts’ validation will be implemented with the objective of establishing a framework that can be used to ensure strategies for the e-commerce retailers, thus they are able to serve and meet customer expectations regarding product quality, optimal price, and delivery time.",
        "authors": [
            "Juan Manuel Bellido",
            "Renata Cabrini Souza e Silva",
            "Dominique Gomez de la Luz"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159022",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Causal inference improving warehouse productivity: zoned storage and killer items",
        "abstract": "E-commerce companies need help with customer service experience: faster and more frequent deliveries. Then, order fulfillment becomes critical to establish a competitive advantage.\r\nThe main objective of this project is to determine whether a new key product assortment, called a class-based scattered storage policy, improves order-picking operations in one of the main warehouses of the sponsor company. This e-commerce firm operates in an emerging market.\r\nAs mentioned earlier, this project addresses the problem by running an A/B quasi-experiment in the warehouse, showing findings directly from a real context for the first time. For this purpose, the warehouse was split into control and treatment sections during peak season when speed is most required. The effect of the proposed storage policy is studied by comparing picking productivity through a two-sample t-test. The samples are chosen using the Coarsened Exact Matching algorithm to have similar data to analyze in observable characteristics.\r\nThe result of this work indicates that the class-based scattered storage policy does not lead to an improvement in picking productivity. It can be attributed to real-context features that are presented and discussed. Additionally, strong recommendations are given to include the findings in future research.",
        "authors": [
            "David Montemurri",
            "Hebe Adriana Herrera",
            "Maria Florencia Ghiglione"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159024",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The State of Supply Chain Sustainability in Brazil",
        "abstract": "As sustainability gains importance for consumers, employees, and investors, Supply Chain Sustainability (SCS) has become an increasingly important topic for companies. The State of Supply Chain Sustainability report, a co-presentation of the MIT Center for Transportation & Logistics and the Council of Supply Chain Management Professionals, provides a clear snapshot of this subject worldwide. Although it has been increasing its range of respondents, including Spanish and Mandarin Chinese translated surveys and the original English survey, Portuguese-speaking countries have not been fully reached. This project aims to understand the state of supply chain sustainability in Brazil, the largest country in Latin America, regarding area, population, and GDP. The State of Supply Chain Sustainability 2022 survey questions were translated to Brazilian Portuguese and applied in a local survey in Brazil with specific questions to capture particularities, such as the impact of regional tax benefits in supply chain-related decision-making. Advanced statistical models were applied to guarantee the quality of the translations and compare the local results with the past results of other countries.",
        "authors": [
            "Leonardo Gonzaga Moreira Sá C Faveret",
            "Marcelo Ikaro Carvalho Mesquita Braga",
            "Rodrigo Junqueira Nogueira"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159031",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Emerging membrane technologies for sustainable lithium extraction from brines and leachates: Innovations, challenges, and industrial scalability",
        "abstract": "This perspective critically examines challenges in advancing membrane-based technologies for lithium extraction from industrial brines, salt lakes, and battery leachates. The rapidly rising deployment of electric vehicles and renewable energy systems has intensified global lithium demand, necessitating sustainable and efficient extraction methods. Traditional techniques like brine evaporation and hard rock mining are environmentally detrimental due to high water usage, ecological disruption, and significant carbon emissions, compounded by geopolitical risks from resource concentration. Emerging membrane technologies, utilizing lithium-selective ligands, biomimetic ion channels, and two-dimensional and porous materials, can potentially realize orders-of-magnitude improvements in lithium selectivity for direct lithium extraction (DLE). However, the effectiveness of DLE membranes is constrained by impurity co-extraction, environmental hazards, lack of scalability and material instability. Conventional lithium brine concentration (LBC) techniques, which complement DLE by concentrating lithium for downstream applications like battery production, face challenges in hypersaline environments, such as fouling and reduced selectivity. Advances in electrodialysis and nanofiltration with surface modifications offer promising solutions to sustain favorable monovalent selectivity under high salinity conditions. Key gaps in the current research landscape include the absence of standardized testing procedures, evaluation metrics poorly suited to hypersaline or multi-ionic environments, scalability challenges in manufacturing, and economic limitations arising from fouling and material degradation. Addressing these issues requires material characterization with representative solution compositions, the development of comprehensive evaluation frameworks, and strategies for co-extracting valuable metals to improve economic viability. A holistic focus on membrane manufacturability, material durability, and process integration is essential to unlock sustainable lithium extraction technologies that can support the global shift to clean energy.",
        "authors": [
            "Zi Hao Foo",
            "John H. Lienhard"
        ],
        "journal_conference_name": "Desalination",
        "publisher": "Elsevier BV",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158972",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Evidence-Based Nutraceuticals Derived from Antrodia cinnamomea",
        "abstract": "Antrodia cinnamomea (A. cinnamomea), a medicinal and edible mushroom endemic to Taiwan, has been traditionally valued as a health tonic. Recent studies have highlighted the diverse specialized metabolites and bioactive potential of this substance, primarily attributed to key secondary metabolites such as benzenoids, maleic and succinic acids, ubiquinone, triterpenoids, and the primary metabolite polysaccharides. These compounds exhibit a broad spectrum of pharmacological properties, including those related to antibacterial, antitumor, anti-inflammation, hepatoprotection, hypoglycaemia, and antioxidant activities, and immunomodulation and gut microbiota regulation. These findings highlight the therapeutic potential of A. cinnamomea and its potential applications in health supplements and functional foods. This review evaluated recent advancements in the cultivation, extraction, and characterization of bioactive compounds from A. cinnamomea, with a particular focus on submerged and solid-state fermentation methods. We hope to provide a comprehensive framework for promoting the efficient and scientific evidence based utilization of A. cinnamomea in novel therapeutic strategies and health-related innovations.",
        "authors": [
            "Chunyuhang Xu",
            "Qingtong Xie",
            "Chien-Liang Kuo",
            "Xin Yang",
            "Dejian Huang"
        ],
        "journal_conference_name": "Foods",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159074",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Evaluating the Impacts of Swapping on the US Decennial Census",
        "abstract": "To meet its dual burdens of providing useful statistics and ensuring privacy of individual respondents, the US Census Bureau has for decades introduced some form of \"noise\" into published statistics. Initially, they used a method known as \"swapping\" (1990-2010). In 2020, they switched to an algorithm called TopDown that ensures a form of Differential Privacy. While the TopDown algorithm has been made public, no implementation of swapping has been released and many details of the deployed swapping methodology deployed have been kept secret. Further, the Bureau has not published (even a synthetic) \"original\" dataset and its swapped version. It is therefore difficult to evaluate the effects of swapping, and to compare these effects to those of other privacy technologies. To address these difficulties we describe and implement a parameterized swapping algorithm based on Census publications, court documents, and informal interviews with Census employees. With this implementation, we characterize the impacts of swapping on a range of statistical quantities of interest. We provide intuition for the types of shifts induced by swapping and compare against those introduced by TopDown. We find that even when swapping and TopDown introduce errors of similar magnitude, the direction in which statistics are biased need not be the same across the two techniques. More broadly, our implementation provides researchers with the tools to analyze and potentially correct for the impacts of disclosure avoidance systems on the quantities they study.",
        "authors": [
            "Mar?a Ballesteros",
            "Cynthia Dwork",
            "Gary King",
            "Conlan Olson",
            "Manish Raghavan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Symposium on Computer Science and Law",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159046",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "What Constitutes a Less Discriminatory Algorithm?",
        "abstract": "Disparate impact doctrine offers an important legal apparatus for targeting discriminatory data-driven algorithmic decisions. A recent body of work has focused on conceptualizing one particular construct from this doctrine: the less discriminatory alternative, an alternative policy that reduces disparities while meeting the same business needs of a status quo or baseline policy. However, attempts to operationalize this construct in the algorithmic setting must grapple with some thorny challenges and ambiguities. In this paper, we attempt to raise and resolve important questions about less discriminatory algorithms (LDAs). How should we formally define LDAs, and how does this interact with different societal goals they might serve? And how feasible is it for firms or plaintiffs to computationally search for candidate LDAs? We find that formal LDA definitions face fundamental challenges when they attempt to evaluate and compare predictive models in the absence of held-out data. As a result, we argue that LDA definitions cannot be purely quantitative, and must rely on standards of \"reasonableness.\" We then raise both mathematical and computational constraints on firms' ability to efficiently conduct a proactive search for LDAs, but we provide evidence that these limits are \"weak\" in a formal sense. By defining LDAs formally, we put forward a framework in which both firms and plaintiffs can search for alternative models that comport with societal goals.",
        "authors": [
            "Benjamin Laufer",
            "Manish Raghavan",
            "Solon Barocas"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Symposium on Computer Science and Law",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159048",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Pluto: Authoring Semantically Aligned Text and Charts for Data-Driven Communication",
        "abstract": "Textual content (including titles, annotations, and captions) plays a central role in helping readers understand a visualization by emphasizing, contextualizing, or summarizing the depicted data. Yet, existing visualization tools provide limited support for jointly authoring the two modalities of text and visuals such that both convey semantically-rich information and are cohesively integrated. In response, we introduce Pluto, a mixed-initiative authoring system that uses features of a chart’s construction (e.g., visual encodings) as well as any textual descriptions a user may have drafted to make suggestions about the content and presentation of the two modalities. For instance, a user can begin to type out a description and interactively brush a region of interest in the chart, and Pluto will generate a relevant auto-completion of the sentence. Similarly, based on a written description, Pluto may suggest lifting a sentence out as an annotation or the visualization’s title, or may suggest applying a data transformation (e.g., sort) to better align the two modalities. A preliminary user study revealed that Pluto’s recommendations were particularly useful for bootstrapping the authoring process and helped identify different strategies participants adopt when jointly authoring text and charts. Based on study feedback, we discuss design implications for integrating interactive verification features between charts and text, offering control over text verbosity and tone, and enhancing the bidirectional flow in unified text and chart authoring tools.",
        "authors": [
            "Arjun Srinivasan",
            "Vidya Setlur",
            "Arvind Satyanarayan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|30th International Conference on Intelligent User Interfaces",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159033",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "MIND (Mixed-Initiative Next-gen Design): Workshop on Blending Agents and Direct Manipulation for Harnessing LLMs",
        "abstract": "Since the 1980s, a key debate in human-centered computing involving machine learning at IUI is between agent-driven systems and direct manipulation. The explosion of Large Language Models (LLMs), particularly auto-regressive as agents serving as chatbots, generative search, and work automation tools, has also brought with it inherent limitations. We posit that efforts to address and alleviate these LLM challenges—hallucinations, unpredictable outputs, lack of transparency, and difficulties in customization—cannot be solved through algorithmic improvements alone but require elevated mixed-initiative interface design at the heart of the IUI community. This workshop aims to bridge the gap between agent-driven automation and direct manipulation by exploring mixed-initiative interaction models that blend the strengths of both paradigms to empower end-users seeking to harness LLMs.",
        "authors": [
            "Karthik Dinakar",
            "Henry Lieberman",
            "Sonia Wu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|30th International Conference on Intelligent User Interfaces Companion",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159042",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "MemPal: Leveraging Multimodal AI and LLMs for Voice-Activated Object Retrieval in Homes of Older Adults",
        "abstract": "Older adults have increasing difficulty with retrospective memory, hindering their abilities to perform daily activities and posing stress on caregivers to ensure their wellbeing. Recent developments in Artificial Intelligence (AI) and large context-aware multimodal models offer an opportunity to create memory support systems that assist older adults with common issues like object finding. This paper discusses the development of an AI-based, wearable memory assistant, MemPal, that helps older adults with a common problem, finding lost objects at home, and presents results from tests of the system in older adults’ own homes. Using visual context from a wearable camera, the multimodal LLM system creates a real-time automated text diary of the person’s activities for memory support purposes, offering object retrieval assistance using a voice-based interface. The system is designed to support additional use cases like context-based proactive safety reminders and recall of past actions. We report on a quantitative and qualitative study with N=15 older adults within their own homes that showed improved performance of object finding with audio-based assistance compared to no aid and positive overall user perceptions on the designed system. We discuss further applications of MemPal’s design as a multi-purpose memory aid and future design guidelines to adapt memory assistants to older adults’ unique needs.",
        "authors": [
            "Natasha Maniar",
            "Samantha Chan",
            "Wazeer Zulfikar",
            "Scott Ren",
            "Christine Xu",
            "Pattie Maes"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|30th International Conference on Intelligent User Interfaces",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159037",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Coalesce: An Accessible Mixed-Initiative System for Designing Community-Centric Questionnaires",
        "abstract": "Effectively incorporating community input into civic decision-making processes is crucial for fostering inclusive governance. However, public officials often face challenges in formulating effective questions to gather meaningful insights due to constraints such as time, resources, and limited experience in questionnaire design. This paper explores the potential of leveraging large language models (LLMs) to address this challenge. We present Coalesce, a novel mixed-initiative system that utilizes LLMs to assist civic leaders in crafting tailored and impactful questions for surveys, interviews, and conversation guides. Guided by best practices in questionnaire design, Coalesce improves question readability, enhances specificity, and reduces bias. To inform our design, we conducted a formative interview study with 30 civic leaders and implemented an iterative human-centered design process involving 14 feedback sessions. We built a fully-functional system before evaluating it through a real-world user study with 16 participants who applied the platform to their own community engagement projects. Our findings show that Coalesce improved participants’ confidence in questionnaire design, supported diverse workflows, and fostered learning while raising important questions about human agency and over-reliance on AI. These insights highlight the potential for intelligent user interfaces to reshape how civic leaders engage with their communities, fostering more informed and inclusive decision-making processes.",
        "authors": [
            "Cassandra Overney",
            "Daniel Kessler",
            "Suyash Fulay",
            "Mahmood Jasim",
            "Deb Roy"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|30th International Conference on Intelligent User Interfaces",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159053",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Scientific Advancements in Gene Therapies: Opportunities for Global Regulatory Convergence",
        "abstract": "On 4 September 2024, the Reagan-Udall Foundation for the FDA (FDA Foundation) in collaboration with the Food and Drug Administration (FDA) and the Gates Foundation hosted a workshop titled “Scientific Advancements in Gene Therapies: Opportunities for Global Regulatory Convergence”. The event brought together a diverse group of experts, including international regulatory bodies, regulated industries, healthcare professionals, patients, academic researchers and global health advocates, to discuss the rapid advancements in gene therapy and the pressing need for equitable access in low-and middle-income countries (LMICs), with sickle cell disease (SCD) serving as the model disorder for the discussions. Although there has been significant progress in gene therapy, such as breakthroughs in clustered regularly interspaced short palindromic repeats (CRISPR)-based technologies and FDA-approved therapies, access to these therapies remain limited in underresourced regions. The workshop addressed critical challenges, including the high cost of therapies, regulatory gaps and barriers and ethical concerns regarding informed consent and public engagement in LMICs. This paper highlights the critical discussion points from the workshop with a focus on exploring strategies for global regulatory convergence, the role of international collaborations and the potential pathways to making gene therapies affordable and accessible to all.",
        "authors": [
            "Jimi Olaghere",
            "David A. Williams",
            "Jeremy Farrar",
            "Hildegard Büning",
            "Cecelia Calhoun",
            "Tony Ho",
            "Maneesha S. Inamdar",
            "David Liu",
            "Julie Makani",
            "Kwasi Nyarko",
            "Sol Ruiz",
            "John Tisdale",
            "Joseph M. McCune",
            "Esther Boadi",
            "Reagan-Udall Foundation for the FDA"
        ],
        "journal_conference_name": "Biomedicines",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159014",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "High-Connectivity Triazolate-Based Metal–Organic Framework for Water Harvesting",
        "abstract": "Increasing the connectivity of structural units presents a potentially valuable approach to improve hydrolytic stability in metal–organic frameworks (MOFs). We herein leverage this strategy by synthesizing the first tritopic benzotriazolate MOF, Zn5(OAc)4(TBTT)2 (H3TBTT = 2,4,6-tris(1H-benzo[d][1,2,3]triazol-5-yl)-1,3,5-triazine), which exhibits open metal sites, high connectivity, high porosity, and significant water uptake capacity. The MOF adopts a previously unknown topology with (3,6,6)-connectivity, which is supported by single-crystal electron diffraction and elemental analysis. The framework undergoes postsynthetic metal and anion exchange with NiCl2, which increases the accessible pore volume and the net hydrophilicity of the framework. With this exchange, the apparent BET surface area increases from 1994 to 3034 m2/g, and the water uptake step shifts from 56 to 33% relative humidity (RH). The high gravimetric capacity of the Ni-rich MOF, 0.98 g/g, translates to a working capacity of 0.64 g/g during a pressure swing cycle between 20 and 40% RH at 25 °C. Combining this performance with a less than 2% loss in working capacity over 100 cycles, the new material rivals the best MOF water sorbents to date.",
        "authors": [
            "Karla Ravin",
            "Patrick Sarver",
            "Bhavish Dinakar",
            "Lukáš Palatinus",
            "Peter Müller",
            "Julius Oppenheim",
            "Mircea Dincă"
        ],
        "journal_conference_name": "Journal of the American Chemical Society",
        "publisher": "American Chemical Society",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158533",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Brain Markers of Resilience to Psychosis in High-Risk Individuals: A Systematic Review and Label-Based Meta-Analysis of Multimodal MRI Studies",
        "abstract": "Background/Objectives: Most individuals who have a familial or clinical risk of developing psychosis remain free from psychopathology. Identifying neural markers of resilience in these at-risk individuals may help clarify underlying mechanisms and yield novel targets for early intervention. However, in contrast to studies on risk biomarkers, studies on neural markers of resilience to psychosis are scarce. The current study aimed to identify potential brain markers of resilience to psychosis. Methods: A systematic review of the literature yielded a total of 43 MRI studies that reported resilience-associated brain changes in individuals with an elevated risk for psychosis. Label-based meta-analysis was used to synthesize findings across MRI modalities. Results: Resilience-associated brain changes were significantly overreported in the default mode and language network, and among highly connected and central brain regions. Conclusions: These findings suggest that the DMN and language-associated areas and central brain hubs may be hotspots for resilience-associated brain changes. These neural systems are thus of key interest as targets of inquiry and, possibly, intervention in at-risk populations.",
        "authors": [
            "Guusje Collin",
            "Joshua E. Goldenberg",
            "Xiao Chang",
            "Zhenghan Qi",
            "Susan Whitfield-Gabrieli",
            "Wiepke Cahn",
            "Jijun Wang",
            "William S. Stone",
            "Matcheri S. Keshavan",
            "Martha E. Shenton"
        ],
        "journal_conference_name": "Brain Sciences",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159013",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Design and Deployment of a Self-Powered, LoRaWAN-Based IoT Environment Sensor Ensemble for Integrated Air Quality Sensing and Simulation",
        "abstract": "The goal of this study is to describe a design architecture for a self-powered IoT (Internet of Things) sensor network that is currently being deployed at various locations throughout the Dallas-Fort Worth metroplex to measure and report on Particulate Matter (PM) concentrations. This system leverages diverse low-cost PM sensors, enhanced by machine learning for sensor calibration, with LoRaWAN connectivity for long-range data transmission. Sensors are GPS-enabled, allowing precise geospatial mapping of collected data, which can be integrated with urban air quality forecasting models and operational forecasting systems. To achieve energy self-sufficiency, the system uses a small-scale solar-powered solution, allowing it to operate independently from the grid, making it both cost-effective and suitable for remote locations. This novel approach leverages multiple operational modes based on power availability to optimize energy efficiency and prevent downtime. By dynamically adjusting system behavior according to power conditions, it ensures continuous operation while conserving energy during periods of reduced supply. This innovative strategy significantly enhances performance and resource management, improving system reliability and sustainability. This IoT network provides localized real-time air quality data, which has significant public health benefits, especially for vulnerable populations in densely populated urban environments. The project demonstrates the synergy between IoT sensor data, machine learning-enhanced calibration, and forecasting methods, contributing to scientific understanding of microenvironments, human exposure, and public health impacts of urban air quality. In addition, this study emphasizes open source design principles, promoting transparency, data quality, and reproducibility by exploring cost-effective sensor calibration techniques and adhering to open data standards. The next iteration of the sensors will include edge processing for short-term air quality forecasts. This work underscores the transformative role of low-cost sensor networks in urban air quality monitoring, advancing equitable policy development and empowering communities to address pollution challenges.",
        "authors": [
            "Lakitha O. H. Wijeratne",
            "Daniel Kiv",
            "John Waczak",
            "Prabuddha Dewage",
            "Gokul Balagopal",
            "Mazhar Iqbal",
            "Adam Aker",
            "Bharana Fernando",
            "Matthew Lary",
            "Vinu Sooriyaarachchi",
            "Rittik Patra",
            "Nora Desmond",
            "Hannah Zabiepour",
            "Darren Xi",
            "Vardhan Agnihotri",
            "Seth Lee",
            "Chris Simmons",
            "David J. Lary"
        ],
        "journal_conference_name": "Air",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159012",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Learning-Augmented Competitive Algorithms for Spatiotemporal Online Allocation with Deadline Constraints",
        "abstract": "We introduce and study spatiotemporal online allocation with deadline constraints (SOAD), a new online problem motivated by emerging challenges in sustainability and energy.  In SOAD, an online player completes a workload by allocating and scheduling it on the points of a metric space $(X, d)$ while subject to a deadline $T$.  At each time step, a service cost function is revealed that represents the cost of servicing the workload at each point, and the player must irrevocably decide the current allocation of work to points.  Whenever the player moves this allocation, they incur a movement cost defined by the distance metric $d(\\cdot, \\ \\cdot)$ that captures, e.g., an overhead cost.  SOAD formalizes the open problem of combining general metrics and deadline constraints in the online algorithms literature, unifying problems such as metrical task systems and online search.  We propose a competitive algorithm for SOAD along with a matching lower bound establishing its optimality.  Our main algorithm, ST-CLIP, is a learning-augmented algorithm that takes advantage of predictions (e.g., forecasts of relevant costs) and achieves an optimal consistency-robustness trade-off.  We evaluate our proposed algorithms in a simulated case study of carbon-aware spatiotemporal workload management, an application in sustainable computing that schedules a delay-tolerant batch compute job on a distributed network of data centers.  In these experiments, we show that ST-CLIP substantially improves on heuristic baseline methods.",
        "authors": [
            "Adam Lechowicz",
            "Nicolas Christianson",
            "Bo Sun",
            "Noman Bashir",
            "Mohammad Hajiesmaili",
            "Adam Wierman",
            "Prashant Shenoy"
        ],
        "journal_conference_name": "Proceedings of the ACM on Measurement and Analysis of Computing Systems",
        "publisher": "ACM",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159050",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Secure finite-time filtering for switched fuzzy systems with scaling attacks and stochastic sensor faults",
        "abstract": "In this study, we introduce a design for robust secure finite-time mixed H ∞ and passivity filter for discrete-time switched fuzzy systems. This design effectively combats both stochastic scaling attacks and sensor failure. To be specific, the sensor signals are represented by stochastic variables with different failure rates. Also, a comprehensive model is presented to characterize the scaling attacks and it is described by the Bernoulli distributed random variable. By designing a suitable Lyapunov functional candidate and leveraging the principles of finite-time theory, we have formulated a new collection of sufficient conditions. These conditions, expressed as linear matrix inequalities, ensure that the augmented fuzzy system maintains robust stochastic finite-time boundedness, along with a predetermined mixed H ∞ and passivity performance index. Ultimately, two numerical demonstrations are provided, incorporating real-world applications from the continuous-time single-link robot arm model and the tunnel diode circuit systems, to highlight the practicality of the proposed secure filter design.",
        "authors": [
            "Murugesan Sathishkumar",
            "Maya Joby",
            "Yong-Ki Ma",
            "Selvaraj M. Anthoni",
            "Srimanta Santra"
        ],
        "journal_conference_name": "Nonlinear Dynamics",
        "publisher": "Springer Netherlands",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159070",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "OpenEarable 2.0: Open-Source Earphone Platform for Physiological Ear Sensing",
        "abstract": "Earphones have evolved from pure audio devices to \"earables\" that are capable of advanced sensing. Bespoke research devices have shown the unique sensing capabilities of the earable platform; however, they are hard to replicate and require expertise to develop in the first place. In this paper, we present OpenEarable 2.0 - an open source, unified platform that integrates a larger number of sensors for conducting comprehensive earable research. OpenEarable 2.0 works as regular binaural Bluetooth earphones and features two ultrasound capable microphones (inward/outward), a 3-axis ear canal accelerometer/bone microphone, a 9-axis head inertial measurement unit, pulse oximeter, optical temperature sensor, ear canal pressure sensor, and microSD card. These capabilities allow for the detection and measurement of 30+ phenomena on the ear that can be used across a wide range of applications in health monitoring, activity tracking, human-computer-interaction and authentication. We describe the design and development of OpenEarable 2.0 which follows best open hardware practices and achieves commercial-level wearability. We provide justification for the selection and placement of integrated sensors and include in-depth descriptions of the extensible, open source firmware and hardware that are implemented using free to use tools and frameworks. For real-time sensor control and data recording we also contribute a web-based dashboard and mobile smartphone app. The wearability and ability to sense different phenomena are validated in four studies which showcases how OpenEarable 2.0 provides accurate measurements in comparison to established gold-standard measurements. We further demonstrate that OpenEarable 2.0 can be assembled by inexperienced users, and that undergraduate students can build applications using the OpenEarable platform.",
        "authors": [
            "Tobias R?ddiger",
            "Michael K?ttner",
            "Philipp Lepold",
            "Tobias King",
            "Dennis Moschina",
            "Oliver Bagge",
            "Joseph Paradiso",
            "Christopher Clarke",
            "Michael Beigl"
        ],
        "journal_conference_name": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "publisher": "ACM",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159051",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Picto: Crafting Remote Tangible Gestures via Recordable, Replayable, and Shareable Motions",
        "abstract": "We introduce Picto, a paired tangible interface that enables intimate dyads to co-create shared kinetic messages, fostering playful remote communication beyond temporal and physical constraints. Picto’s two modular units—a knob for rotational motion and a slider for linear motion—allow users to craft personalized motions and shapes symbolizing their significant other. Presence can be conveyed in real-time or asynchronously through record, replay, and share features. Picto empowers users to express abstract ideas through iconic gestures and non-verbal cues. Using a bistable composite tape-spring structure, we developed a novel mechanism for programming dynamic shape variations and motions. Picto’s control system records, stores, and shares motion-based interactions. A user study with intimate dyads evaluates Picto’s usability and its potential as a remote story-sharing platform and ambient presence media enhanced by metaphorical and beat gestures. The results highlight its potential to enrich and sustain intimate relationships, supporting social presence across distances.",
        "authors": [
            "Kyung Yun Choi",
            "Taehee Jung",
            "Noble Harasha",
            "Hiroshi Ishii"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Nineteenth International Conference on Tangible, Embedded, and Embodied Interaction",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158328",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Performance Analysis for High-Dimensional Bell-State Quantum Illumination",
        "abstract": "Quantum illumination (QI) is an entanglement-based protocol for improving LiDAR/radar detection of unresolved targets beyond what a classical LiDAR/radar of the same average transmitted energy can do. Originally proposed by Seth Lloyd as a discrete-variable quantum LiDAR, it was soon shown that his proposal offered no quantum advantage over its best classical competitor. Continuous-variable, specifically Gaussian-state, QI has been shown to offer a true quantum advantage, both in theory and in table-top experiments. Moreover, despite its considerable drawbacks, the microwave version of Gaussian-state QI continues to attract research attention. A recent QI study by Armanpreet Pannu, Amr Helmy, and Hesham El Gamal (PHE), however, has: (i) combined the entangled state from Lloyd’s QI with the channel models from Gaussian-state QI; (ii) proposed a new positive operator-valued measurement for that composite setup; and (iii) claimed that, unlike Gaussian-state QI, PHE QI achieves the Nair–Gu lower bound on QI target-detection error probability at all noise brightnesses. PHE’s analysis was asymptotic, i.e., it presumed infinite-dimensional entanglement. The current paper works out the finite-dimensional performance of PHE QI. It shows that there is a threshold value for the entangled-state dimensionality below which there is no quantum advantage, and above which the Nair–Gu bound is approached asymptotically. Moreover, with both systems operating with error-probability exponents 1 dB lower than the Nair–Gu bound, PHE QI requires enormously higher entangled-state dimensionality than does Gaussian-state QI to achieve useful error probabilities in both high-brightness (100 photons/mode) and moderate-brightness (1 photon/mode) noise. Furthermore, neither system has an appreciable quantum advantage in low-brightness (much less than 1 photon/mode) noise.",
        "authors": [
            "Jeffrey H. Shapiro"
        ],
        "journal_conference_name": "Physics",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158998",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Choice Vectors: Streamlining Personal AI Alignment Through Binary Selection",
        "abstract": "Value alignment for AI is not “one-size-fits-all”: even polite and friendly models can still fail to represent individual user contexts and preferences, and local cultural norms. This paper presents a modular workflow for personal fine-tuning, synthesizing four core components from our previous research: (1) robust vectorization of user values and preferences, (2) a binary choice user interface (UI) approach to capturing those preferences with minimal cognitive load, (3) contrastive activation methods for steering large language models (LLMs) via difference vectors, and (4) knowledge graph integration for more auditable and structured alignment. Our approach—descended from past research on “Towards an End-to-End Personal Fine-Tuning Framework”—demonstrates how these elements can be combined to create personalized, context-rich alignment solutions. We report on user studies for the forced-choice UI, describe an experimental pipeline for deriving “control vectors”, and propose a “moral graph” method for bridging symbolic and vector-based alignment. Our findings suggest that multi-pronged personalization can significantly reduce user annotation fatigue, improve alignment fidelity, and allow for more flexible, interpretable AI behaviors.",
        "authors": [
            "Eleanor Watson",
            "Minh Nguyen",
            "Sarah Pan",
            "Shujun Zhang"
        ],
        "journal_conference_name": "Multimodal Technologies and Interactions",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158997",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The ultra-thin conception of objecthood",
        "abstract": "In his excellent book Thin Objects, Øystein Linnebo develops a conception of\r\nobjecthood that allows for thin objects: objects whose ‘existence does not\r\nmake a substantial demand on the world’ (p. 4). His proposal is premised on\r\nthe Fregean dictum that to be an object is to be the referent of a possible\r\nsingular term (p. 22). As a result, much of Linnebo’s argumentation is focused\r\non defending a ‘thin’ conception of reference, which is liberal enough to\r\nallow for thin objects. This paper is a critique of Linnebo’s conception of\r\nreference.",
        "authors": [
            "Agustín Rayo"
        ],
        "journal_conference_name": "An Interdisciplinary Journal of Philosophy",
        "publisher": "Taylor & Francis",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159049",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "SySTeC: A Symmetric Sparse Tensor Compiler",
        "abstract": "Symmetric and sparse tensors arise naturally in many domains including linear algebra, statistics, physics, chemistry, and graph theory. Symmetric tensors are equal to their transposes, so in the n-dimensional case we can save up to a factor of n! by avoiding redundant operations. Sparse tensors, on the other hand, are mostly zero, and we can save asymptotically by processing only nonzeros. Unfortunately, specializing for both symmetry and sparsity at the same time is uniquely challenging. Optimizing for symmetry requires consideration of n! transpositions of a triangular kernel, which can be complex and error prone. Considering multiple transposed iteration orders and triangular loop bounds also complicates iteration through intricate sparse tensor formats. Additionally, since each combination of symmetry and sparse tensor formats requires a specialized implementation, this leads to a combinatorial number of cases. A compiler is needed, but existing compilers cannot take advantage of both symmetry and sparsity within the same kernel. In this paper, we describe the first compiler which can automatically generate symmetry-aware code for sparse or structured tensor kernels. We introduce a taxonomy for symmetry in tensor kernels, and show how to target each kind of symmetry. Our implementation demonstrates significant speedups ranging from 1.36x for SSYMV to 30.4x for a 5-dimensional MTTKRP over the non-symmetric state of the art.",
        "authors": [
            "Radha Patel",
            "Willow Ahrens",
            "Saman Amarasinghe"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158438",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Hands-On Quantum Cryptography: Experimentation with the B92 Protocol Using Pulsed Lasers",
        "abstract": "Quantum cryptography continues to be an area of significant research and educational interest. Here, a straightforward and reliable approach to both the experimental and theoretical aspects of quantum key distribution is presented, tailored for senior undergraduate students. Focusing on illustrating the essential concepts of the B92 protocol through a combination of optical experiments and custom-developed computational tools, this work offers a thorough exploration of quantum cryptography according to the principles of the B92 protocol.",
        "authors": [
            "Sara P. Gandelman",
            "Alona Maslennikov",
            "Georgi Gary Rozenman"
        ],
        "journal_conference_name": "Photonics",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158996",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Structurally Similar Mycotoxins Aflatoxin B1 and Sterigmatocystin Trigger Different and Distinctive High-Resolution Mutational Spectra in Mammalian Cells",
        "abstract": "Aflatoxin B1 (AFB1) and sterigmatocystin (ST) are mycotoxins that pose significant threats to human and animal health owing to their mutagenic, carcinogenic, and toxic properties. They are structurally similar and widely believed to exert their biological effects via the generation of DNA-damaging epoxides at their respective terminal furan rings. Despite structural identity in the warhead portion of each toxin, this work shows that distal parts of each molecule are responsible for the distinctive mutational fingerprints seen in gptΔ C57BL/6J mouse embryo fibroblasts (MEFs). The two toxins differ structurally in the puckered cyclopentenone ring of AFB1 and in the planar xanthone functionality of ST. While both toxins mainly induce GC→TA mutations, the aforementioned differences in structure apparently trigger unique patterns of mutations, as revealed by high-resolution duplex sequencing of MEF genomes. AFB1 is more mutagenic than ST and displays its transversion mutations in a pattern with primary and secondary hotspots (underscored) in 5′-CGC-3′ and 5′-CGG-3′ contexts, respectively. ST displays a modest 5′-CGG-3′ hotspot while its other GC→TA transversions are more uniformly distributed in a pattern resembling established oxidative stress mutational spectra. This research delineates the mutational spectra of AFB1 and ST, establishing these patterns as possible early-onset biomarkers of exposure.",
        "authors": [
            "Pennapa Thongararm",
            "Marisa Chancharoen",
            "Nutchapong Suwanwong",
            "Somsak Ruchirawat",
            "Mathuros Ruchirawat",
            "Bogdan I. Fedeles",
            "Robert G. Croy",
            "John M. Essigmann"
        ],
        "journal_conference_name": "Toxins",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158993",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Influence of Religiosity on Muslim Women’s Selection of Fund Providers in Malaysia",
        "abstract": "The purpose of this study is to analyze the factors influencing the attitudes of women investors in the context of Islamic unit trust funds in Malaysia, with a focus on women&rsquo;s religiosity and on the perceived religiosity of fund providers. Using the UTAUT model, the study examines data from a survey of 263 Muslim women in Malaysia and considers seven key factors: risk aversion, religiosity, price sensitivity, and Islamic financial literacy on the side of the investing women and past performance, perceived religiosity, and perceived risk on the side of the fund providers. The findings indicate that the perceived religiosity of a fund provider has a significant and positive impact on attitude, with positive moderating effects on the women&rsquo;s own religiosity and Islamic financial literacy, and a negative moderating effect on the women&rsquo;s price sensitivity. The study also discusses the practical implications of these findings and offers recommendations for fund providers.",
        "authors": [
            "Salim Bouzekouk",
            "Fadillah Mansor"
        ],
        "journal_conference_name": "Journal of Risk and Financial Management",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158992",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Portable High-Resolution Snapshot Multispectral Imaging Device Leveraging Spatial and Spectral Features for Non-Invasive Corn Nitrogen Treatment Classification",
        "abstract": "Spectral imaging has been widely applied in plant phenotyping to assess corn leaf nitrogen status. Recent studies indicate that spatial variations within a single leaf’s multispectral image provide stronger signals for corn nitrogen estimation. However, current technologies for corn multispectral imaging cannot capture a large corn leaf segment with high-resolution and simple operation, limiting their efficiency and accuracy in nitrogen estimation. To address this gap, this study developed a proximal multispectral imaging device that can capture high-resolution snapshot multispectral images of a large segment of a single corn leaf. This device uses airflow to autonomously position and flatten the leaf to minimize the noise in images due to leaf curvature and simplify operation. Moreover, this device adopts a transmittance imaging regime by clamping the corn leaf between the camera and the lighting source to block the environmental lights and supply uniform lighting to capture high-resolution and high-precision leaf images within six seconds. A field assay was conducted to validate the effectiveness of the multispectral images captured by this device in assessing nitrogen status by classifying the nitrogen treatments applied to corn. Six nitrogen treatments were applied to 12 plots of corn fields, and 10 images were collected at each plot. By using the average vegetative index of the whole image, only one treatment was significantly different from the other five treatments, and no significant difference was observed among any other groups. However, by extracting the spatial and spectral features from the images and combining these features, the accuracy of nitrogen treatment classification improved compared to using the average index. In another analysis, by applying spatial–spectral analysis methods to the images, the nitrogen treatment classification accuracy has improved compared to using the average index. These results demonstrated the advantages of this high-resolution and high-throughput imaging device for distinguishing nitrogen treatments by facilitating spatial–spectral combined analysis for more precise classification.",
        "authors": [
            "Xuan Li",
            "Zhongzhong Niu",
            "Ana Gabriela Morales-Ona",
            "Ziling Chen",
            "Tianzhang Zhao",
            "Daniel J. Quinn",
            "Jian Jin"
        ],
        "journal_conference_name": "Sensors",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158525",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Utilization of Classification Learning Algorithms for Upper-Body Non-Cyclic Motion Prediction",
        "abstract": "This study explores two methods of predicting non-cyclic upper-body motions using classification algorithms. Exoskeletons currently face challenges with low fluency, hypothesized to be in part caused by the lag in active control innate in many leader&ndash;follower paradigms seen in today&rsquo;s systems, leading to energetic inefficiencies and discomfort. To address this, we employ k-nearest neighbor (KNN) and deep learning models to predict motion characteristics, such as magnitude and category, from surface electromyography (sEMG) signals. Data were collected from six muscles located around the elbow. The sEMG signals were processed to identify significant activation changes. Two classification approaches were utilized: a KNN algorithm that categorizes motion based on the slopes of processed sEMG signals at change points and a deep neural network employing continuous categorization. Both methods demonstrated the capability to predict future voluntary non-cyclic motions up to and beyond commonly acknowledged electromechanical delay times, with the deep learning model able to predict, with certainty at or beyond 90%, motion characteristics even prior to myoelectric activation of the muscles involved. Our findings indicate that these classification algorithms can be used to predict upper-body non-cyclic motions to potentially increase machine interfacing fluency. Further exploration into regression-based prediction models could enhance the precision of these predictions, and further work could explore their effects on fluency when utilized in a tandem or wearable robotic application.",
        "authors": [
            "Bon H. Koo",
            "Ho Chit Siu",
            "Dava J. Newman",
            "Ellen T. Roche",
            "Lonnie G. Petersen"
        ],
        "journal_conference_name": "Sensors",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158524",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Content Analysis of E-Participation Platforms in Taiwan with Topic Modeling: How to Train and Evaluate Neural Topic Models?",
        "abstract": "E-participation platforms, such as iVoting and Join in Taiwan, provide digital spaces for citizens to engage in deliberation, voting, and oversight. As a forerunner in Asia, Taiwan has implemented these platforms to enhance participatory democracy. However, there is still limited research on the specific content debated on these platforms. Utilising recent advancements in Natural Language Processing, the content of proposals that users have submitted between 2015 and 2025 is explored. In this study, a pipeline for mining text corpora scraped from these platforms in the context of political analysis is proposed. The pipeline is applied to two datasets which have different characteristics. A topic model for each of the two platforms is generated and later evaluated with OCTIS (Optimizing and Comparing Topic Models Is Simple) and compared to different baselines. Our research highlights the trade-offs between model performance and processing time, emphasizing the balance between accuracy and meaningful topic creation. By integrating a translation pipeline from Chinese to English within the text-mining process, our method also demonstrates a solid approach to overcome language barriers. Consequently, our method is adaptable to e-participation platforms in various languages, providing decision-makers with a more comprehensive tool to understand citizens’ needs and enabling the formulation of more informed and effective policies.",
        "authors": [
            "Moritz Sontheimer",
            "Jonas Fahlbusch",
            "Shuo-Yan Chou",
            "Yu-Lin Kuo"
        ],
        "journal_conference_name": "Applied Sciences",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158523",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Diffusion and Percolation: How COVID-19 Spread Through Populations",
        "abstract": "I rely on the key concepts of diffusion and percolation to characterize the sequential but overlapping phases of the spread of infection through entire populations during the first year of the COVID-19 pandemic. Data from Los Angeles County demonstrate an extended initial diffusion phase propelled by radial geographic spread, followed by percolation within hotspots fueled by the presence of multigenerational households. Data from New York City, by contrast, reveal rapid initial diffusion along a unique, extensive subway network. Subsequent percolation within multiple hotspots, similarly powered by a high density of multigenerational households, exerted a positive feedback effect that further enhanced diffusion. Data from Florida counties support the generality of the phenomenon of viral transmission from more mobile, younger individuals to less mobile, older individuals. Data from the South Brooklyn hotspot reveal the limitations of some forms of government regulation in controlling mobility patterns that were critical to the continued percolation of the viral infection. Data from a COVID-19 outbreak at the University of Wisconsin&mdash;Madison demonstrate the critical role of a cluster of off-campus bars as an attractor for the continued percolation of infection. The evidence also demonstrates the efficacy of quarantine as a control strategy when the hotspot is contained and well identified.",
        "authors": [
            "Jeffrey E. Harris"
        ],
        "journal_conference_name": "Populations",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158991",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Point-of-Care No-Specimen Diagnostic Platform Using Machine Learning and Raman Spectroscopy: Proof-of-Concept Studies for Both COVID-19 and Blood Glucose",
        "abstract": "Significance: We describe a novel, specimen-free diagnostic platform that can immediately detect both a metabolite (glucose) or an infection (COVID-19) by non-invasively using Raman spectroscopy and machine learning. Aim: Current diagnostic testing for infections and glucose monitoring requires specimens, disease-specific reagents and processing, and it increases environmental waste. We propose a new hardware&ndash;software paradigm by designing and constructing a finger-scanning hardware device to acquire Raman spectroscopy readouts which, by varying the machine learning algorithm to interpret the data, allows for diverse diagnoses. Approach: A total of 455 patients were enrolled prospectively in the COVID-19 study; 148 tested positive and 307 tested negative through nasal PCR testing conducted concurrently with testing using our viral detector. The tests were performed on both outpatients (N = 382) and inpatients (N = 73) at Holy Name Medical Center in Teaneck, NJ, between June 2021 and August 2022. Patients&rsquo; fingers were scanned using an 830 nm Raman System and then, using machine learning, processed to provide an immediate result. In a separate study between April 2023 and August 2023, measurements using the same device and scanning a finger were used to detect blood glucose levels. Using a Dexcom sensor and an Accu-Chek device as references, a cross-validation-based regression of 205 observations of blood glucose was performed with a machine learning algorithm. Results: In a five-fold cross-validation analysis (including asymptomatic patients), a machine learning classifier using the Raman spectra as input achieved a specificity for COVID-19 of 0.837 at a sensitivity of 0.80 and an area under receiver operating curve (AUROC) of 0.896. However, when the data were split by time, with training data consisting of observations before 1 July 2022 and test data consisting of observations after it, the model achieved an AUROC of 0.67, with 0.863 sensitivity at a specificity of 0.517. This decrease in AUROC may be due to substantial domain shift as the virus evolves. A similar five-fold cross-validation analysis of Raman glucose detection produces an area under precision&ndash;recall curve (AUPR) of 0.58. Conclusions: The combination of Raman spectroscopy, AI/ML, and our patient interface admitting only a patient&rsquo;s finger and using no specimen offers unprecedented flexibility in introducing new diagnostic tests or adapting existing ones. As the ML algorithm can be iteratively re-trained with new data and the software deployed to field devices remotely, it promises to be a valuable tool for detecting rapidly emerging infectious outbreaks and disease-specific biomarkers, such as glucose.",
        "authors": [
            "Allen B. Chefitz",
            "Rohit Singh",
            "Thomas Birch",
            "Yongwu Yang",
            "Arib Hussain",
            "Gabriella Chefitz"
        ],
        "journal_conference_name": "Spectroscopy Journal",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158990",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Computer Science Behind Bars: Lessons Learned from Teaching Incarcerated Students in Prisons and Jails",
        "abstract": "Educational programs for incarcerated individuals, often called \"behind bars\" initiatives, have been shown to improve participants' social and economic outcomes upon release. Since its founding in 2018, MIT's Education Justice Institute (TEJI) has offered accredited classes for incarcerated students, with an increasing focus on computer education. Our courses have been delivered both in person and remotely (e.g., via Zoom). In this poster, we share insights into the challenges present in the incarcerated education environment, and highlight how remote learning offers unique advantages to incarcerated students. We also present preliminary findings from two years of data collected across four recurring computer science courses. This poster aims to foster a dialogue with the broader computer science education community, focusing on: (i) qualitative insights gained from extensive interactions with incarcerated education systems, (ii) preliminary empirical results obtained through IRB-approved surveys, (iii) common challenges faced during data collection, and (iv) an opportunity to seek feedback and pose questions to computer science education experts.",
        "authors": [
            "Andrew Fishberg",
            "Marisa Gaetz",
            "Martin Nisser",
            "Carole Cafferty",
            "Lee Perlman",
            "Raechel Soicher",
            "Joshua Long"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158331",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The d γ / 2 -Variation of Distance Profiles in γ -Liouville Quantum Gravity",
        "abstract": "For Brownian surfaces with boundary and an interior marked point, a natural observable to consider is the distance profile, defined as the process of distances from the marked point to a variable point lying on the boundary. When the boundary is parametrized by the natural length measure on it, this distance profile turns out to be locally absolutely continuous to Brownian motion, and as a result, the boundary length measure itself has a natural interpretation as the quadratic variation process of the distance profile. In this paper, we extend this interpretation to γ -Liouville quantum gravity ( γ -LQG), a one-parameter family of models of random geometry which is known to specialize to the case of Brownian geometry for the case γ = 8 / 3 . With d γ denoting the Hausdorff dimension of γ -LQG, we show that for a γ -LQG surface with boundary, the natural boundary length measure can be interpreted (up to a constant factor) as the d γ / 2 -variation process of the distance profile from an interior point.",
        "authors": [
            "Manan Bhatia"
        ],
        "journal_conference_name": "Communications in Mathematical Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159040",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Systematic Simulations of Structural Stability, Phonon Dispersions, and Thermal Expansion in Zinc-Blende ZnO",
        "abstract": "Zinc oxide (ZnO) has recently gained considerable attention due to its exceptional properties, including higher electron mobility, good thermal conductivity, high breakdown voltage, and a relatively large exciton-binding energy. These characteristics helped engineers to develop low dimensional heterostructures (LDHs)-based advanced flexible/transparent nanoelectronics, which were then integrated into thermal management systems. Coefficients of thermal expansion α(T),\r\n phonon dispersions  ωj(q→)\r\n, and Grüneisen parameters  γj(q→)\r\n can play important roles in evaluating the suitability of materials in such devices. By adopting a realistic rigid-ion model in the quasi-harmonic approximation, this work aims to report the results of a methodical study to comprehend the structural, lattice dynamical, and thermodynamic behavior of zinc-blende (zb) ZnO. Systematic calculations of ωj(q→)\r\n, γj(q→),\r\n and α(T)\r\n have indicated negative thermal expansion (NTE) at low T. Soft transverse acoustic shear mode gammas  γTA\r\n at critical points offered major contributions to NTE. Our results of ωj(q→)\r\n at ambient pressure compare reasonably well with Raman scattering spectroscopy measurements and first-principles calculations. By adjusting the layers of materials with positive and negative thermal expansion, it is possible to create LDHs with near-zero α(T)\r\n. Such a nanostructure might experience a minimal dimensional change with T fluctuations, making it ideal for devices where precise dimensional stability is crucial.",
        "authors": [
            "Devki N. Talwar",
            "Piotr Becla"
        ],
        "journal_conference_name": "Nanomaterials",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158301",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Orbit Plane Rotation Using Aerocapture",
        "abstract": "This study investigates the feasibility of performing orbit plane rotations during aerocapture maneuvers. Three-degrees-of-freedom bounding trajectories at Mars are propagated for a range of vehicle lift-to-drag ratios 𝐿/𝐷\r\n and hyperbolic arrival velocities 𝑣∞\r\n. The results show that the maximum plane rotation achievable increases with vehicle 𝐿/𝐷\r\n and 𝑣∞\r\n. When arriving with 𝑣∞\r\n of 6 km/s, vehicles with 𝐿/𝐷\r\n of 0.25 and 1.0 can achieve plane rotations of up to 11.6 and 45.3 deg, respectively. Heat rate, heat load, and g-loading constraints identified when rotating the orbital plane are not more severe than those observed for two-dimensional aerocapture at a given 𝐿/𝐷\r\n and 𝑣∞\r\n. A direct tradeoff between the maximum plane rotation and entry corridor width exists that will affect the ability of lower 𝐿/𝐷\r\n vehicles to achieve large plane rotations. The proposed maneuver can allow the captured orbit inclination and right ascension of the ascending node to be altered in ways that are not possible using typical interplanetary orbit targeting methods. Further, the maneuver offers the possibility of deploying multiple satellites to different orbits around a target destination using a single launch or approach path.",
        "authors": [
            "Daniel C. Gochenaur",
            "Michael P. Jones",
            "Johannes J. Norheim",
            "Olivier L. de Weck"
        ],
        "journal_conference_name": "Journal of Spacecraft and Rockets",
        "publisher": "American Institute of Aeronautics and Astronautics",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158529",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "\"Why is my code slow?\" Efficiency Bugs in Student Code",
        "abstract": "While prior research has categorized common errors and code quality issues of student programmers, little attention has been paid to researching student efficiency bugs. Qualitative content analysis of 250 slow student submissions across five CS2 assignments yielded over 750 efficiency bugs. Extracting general themes resulted in an efficiency bug taxonomy with three main categories: superfluous computation, suboptimal data structure design, and suboptimal algorithm design, with 12 subcategories. Analysis of specific bug frequencies across the assignments provided insights that may inform content design for programming courses.",
        "authors": [
            "Hope Dargan",
            "Adam Gilbert-Diamond",
            "Adam Hartz",
            "Robert Miller"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158329",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Long-Term Ageing Studies on Eco-Friendly Resistive Plate Chamber Detectors",
        "abstract": "In high-energy physics, resistive plate chamber (RPC) detectors operating in avalanche mode make use of a high-performance gas mixture. Its main component, Tetrafluoroethane (C2H2F4), is classified as a fluorinated greenhouse gas. The RPC EcoGas@GIF++ collaboration is pursuing an intensive R&D on new gas mixtures for RPCs to explore eco-friendly alternatives complying with recent European regulations. The performance of different RPC detectors has been evaluated at the CERN Gamma Irradiation Facility with Tetrafluoropropene (C3H2F4)-CO2-based gas mixtures. A long-term ageing test campaign was launched in 2022, and since 2023, systematic long-term performance studies have been carried out thanks to dedicated beam tests. The results of these studies are discussed together with their future perspectives.",
        "authors": [
            "Marcello Abbrescia",
            "Giulio Aielli",
            "Reham Aly",
            "Maria Cristina Arena",
            "Mapse Barroso Ferreira",
            "Luigi Benussi",
            "Stefano Bianco",
            "Fabio Bordon",
            "Davide Boscherini",
            "Alessia Bruni",
            "Salvatore Buontempo",
            "Mattia Busato",
            "Paolo Camarri",
            "Roberto Cardarelli",
            "Liliana Congedo",
            "Marilisa De Serio",
            "Francesco Debernardis",
            "Anna Di Ciaccio",
            "Luigi Di Stante",
            "Pascal Dupieux"
        ],
        "journal_conference_name": "Particles",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158989",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Practical DB-OS Co-Design with Privileged Kernel Bypass",
        "abstract": "This paper revisits the longstanding challenge of coordinating database systems with general-purpose OS interfaces, such as POSIX, which often lack tailored support for DB requirements. Existing approaches to this DB-OS co-design struggle with limited design space, security risks, and compatibility issues. To overcome these hurdles, we propose a new co-design approach leveraging virtualization to elevate the privilege level of DB processes. Our method enables database systems to fully exploit hardware capabilities via virtualization, while minimizing the need for extensive modifications to the host OS kernel, thereby maintaining compatibility. We demonstrate the effectiveness of our approach through two novel virtual memory mechanisms tailored for database workloads: (1) an efficient snapshotting mechanism that captures memory snapshots at millisecond intervals for in-memory databases and HTAP workloads, and (2) a streamlined in-kernel buffer pool design. We introduce Libdbos, a lightweight guest kernel implementing these mechanisms. Our evaluations highlight significant improvements in latency and efficiency compared to existing snapshotting and buffer pool designs, underscoring the potential of the approach.",
        "authors": [
            "Xinjing Zhou",
            "Viktor Leis",
            "Jinming Hu",
            "Xiangyao Yu",
            "Michael Stonebraker"
        ],
        "journal_conference_name": "Proceedings of the ACM on Management of Data",
        "publisher": "ACM",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158440",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Predicting Mortality in Subarachnoid Hemorrhage Patients Using Big Data and Machine Learning: A Nationwide Study in Türkiye",
        "abstract": "Background/Objective: Subarachnoid hemorrhage (SAH) is associated with high morbidity and mortality rates, necessitating prognostic algorithms to guide decisions. Our study evaluates the use of machine learning (ML) models for predicting 1-month and 1-year mortality among SAH patients using national electronic health records (EHR) system. Methods: Retrospective cohort of 29,274 SAH patients, identified through national EHR system from January 2017 to December 2022, was analyzed, with mortality data obtained from central civil registration system in Türkiye. Variables included (n = 102) pre- (n = 65) and post-admission (n = 37) data, such as patient demographics, clinical presentation, comorbidities, laboratory results, and complications. We employed logistic regression (LR), decision trees (DTs), random forests (RFs), and artificial neural networks (ANN). Model performance was evaluated using area under the curve (AUC), average precision, and accuracy. Feature significance analysis was conducted using LR. Results: The average age was 56.23 ± 16.45 years (47.8% female). The overall mortality rate was 22.8% at 1 month and 33.3% at 1 year. One-month mortality increased from 20.9% to 24.57% (p < 0.001), and 1-year mortality rose from 30.85% to 35.55% (p < 0.001) in the post-COVID period compared to the pre-COVID period. For 1-month mortality prediction, the ANN, LR, RF, and DT models achieved AUCs of 0.946, 0.942, 0.931, and 0.916, with accuracies of 0.905, 0.901, 0.893, and 0.885, respectively. For 1-year mortality, the AUCs were 0.941, 0.927, 0.926, and 0.907, with accuracies of 0.884, 0.875, 0.861, and 0.851, respectively. Key predictors of mortality included age, cardiopulmonary arrest, abnormal laboratory results (such as abnormal glucose and lactate levels) at presentation, and pre-existing comorbidities. Incorporating post-admission features (n = 37) alongside pre-admission features (n = 65) improved model performance for both 1-month and 1-year mortality predictions, with average AUC improvements of 0.093 ± 0.011 and 0.089 ± 0.012, respectively. Conclusions: Our study demonstrates the effectiveness of ML models in predicting mortality in SAH patients using big data. LR models’ robustness, interpretability, and feature significance analysis validate its importance. Including post-admission data significantly improved all models’ performances. Our results demonstrate the utility of big data analytics in population-level health outcomes studies.",
        "authors": [
            "Taghi Khaniyev",
            "Efecan Cekic",
            "Neslihan Nisa Gecici",
            "Sinem Can",
            "Naim Ata",
            "Mustafa Mahir Ulgu",
            "Suayip Birinci",
            "Ahmet Ilkay Isikay",
            "Abdurrahman Bakir",
            "Anil Arat",
            "Sahin Hanalioglu"
        ],
        "journal_conference_name": "Journal of Clinical Medicine",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158299",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Base-Load Nuclear Reactors for Fully Dispatchable Electricity: Nuclear Air-Brayton Combined Cycles, Firebrick Heat Storage, Hydrogen Storage, and Hydrocarbon Biofuels",
        "abstract": "Three partly coupled integrated nuclear energy systems are described. These enable base-load nuclear reactors to provide fully dispatchable electricity without greenhouse-gas emissions, thus replacing gas turbines burning natural gas and batteries storing electricity. These hybrid systems link the industrial sector to the electricity sector. Firstly, electricity-to-high-temperature (1800 &deg;C) gigawatt-hour firebrick heat storage converts low-price electricity to high-temperature stored heat to provide dispatchable heat for industry and power generation. Secondly, Nuclear Air-Brayton Combined Cycles (NACC) with thermodynamic topping cycles using high-temperature stored heat or combustible fuel to provide dispatchable electricity. Peak power output can be two to five times the base-load electricity production. The heat-to-electricity efficiency of the thermodynamic topping cycles exceeds 70%. Thirdly, nuclear hydrogen production for industrial markets enables the production of dispatchable electricity where hydrogen is used for energy storage but not to produce heat and electricity. Base-load nuclear reactors send electricity to the grid and/or electrolyzers for hydrogen production depending upon electricity prices. Low-cost hydrogen storage enables us to meet steady-state industrial hydrogen demands, even though hydrogen and grid electricity production is varied. Hydrogen production for industrial uses (ammonia fertilizer, direct reduction of iron ore to iron replacing coke, cellulosic liquid hydrocarbon biofuels replacing crude oil) may exceed 20% of total energy demand and may be a massive source of dispatchable electricity. The biofuels provide storable energy when heat storage is depleted.",
        "authors": [
            "Charles Forsberg"
        ],
        "journal_conference_name": "Energies",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158300",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Engineering of Genetically Encoded Bright Near-Infrared Fluorescent Voltage Indicator",
        "abstract": "Genetically encoded voltage indicators (GEVIs) allow for the cell-type-specific real-time imaging of neuronal membrane potential dynamics, which is essential to understanding neuronal information processing at both cellular and circuit levels. Among GEVIs, near-infrared-shifted GEVIs offer faster kinetics, better tissue penetration, and compatibility with optogenetic tools, enabling all-optical electrophysiology in complex biological contexts. In our previous work, we employed the directed molecular evolution of microbial rhodopsin Archaerhodopsin-3 (Arch-3) in mammalian cells to develop a voltage sensor called Archon1. Archon1 demonstrated excellent membrane localization, signal-to-noise ratio (SNR), sensitivity, kinetics, and photostability, and full compatibility with optogenetic tools. However, Archon1 suffers from low brightness and requires high illumination intensities, which leads to tissue heating and phototoxicity during prolonged imaging. In this study, we aim to improve the brightness of this voltage sensor. We performed random mutation on a bright Archon derivative and identified a novel variant, monArch, which exhibits satisfactory voltage sensitivity (4~5% ΔF/FAP) and a 9-fold increase in basal brightness compared with Archon1. However, it is hindered by suboptimal membrane localization and compromised voltage sensitivity. These challenges underscore the need for continued optimization to achieve an optimal balance of brightness, stability, and functionality in rhodopsin-based voltage sensors.",
        "authors": [
            "Xian Xiao",
            "Aimei Yang",
            "Hanbin Zhang",
            "Demian Park",
            "Yangdong Wang",
            "Balint Szabo",
            "Edward S. Boyden",
            "Kiryl D. Piatkevich"
        ],
        "journal_conference_name": "Department of Brain and Cognitive Sciences",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158298",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Lessons from COVID-19 patient visitation restrictions: six considerations to help develop ethical patient visitor policies",
        "abstract": "Patient visitor restrictions were implemented in unprecedented ways during the COVID-19 pandemic and included bans on any visitors to dying patients and bans separating mothers from infants. These were implemented without high quality evidence they would be beneficial and the harms to patients, families and medical personnel were often immediately clear. Evidence has also accumulated finding strict visitor restrictions were accompanied by long-term individual and societal consequences. We highlight numerous examples of restrictions that were enacted during the COVID-19 pandemic, including some that continue to be in place today. We outline six specific concerns about the nature and effects of the visitor restrictions seen during the COVID-19 pandemic. These considerations may help provide both an ethical and science-based framework, through which healthcare workers, families and government entities can work towards safeguarding patient and family rights and well-being.",
        "authors": [
            "Tracy B. Høeg",
            "Benjamin Knudsen",
            "Vinay Prasad"
        ],
        "journal_conference_name": "Monash Bioethics Review",
        "publisher": "Springer International Publishing",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158289",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Taxonomy for Social Sustainability in Corporate Communication",
        "abstract": "Sustainability, or environmental, social, and governance (ESG), reports have become ubiquitous among major companies in recent years, often criticized as tools for greenwashing and met with significant backlash. While the environmental aspects of these reports are well-defined, social sustainability remains poorly understood. Through an analysis of narrative sections from six corporate sustainability reports narrative sections, we propose an initial taxonomy of constitutive social sustainability concepts reflected in corporate speech.",
        "authors": [
            "Amelia Dogan",
            "Laura Frye-Levine",
            "Ava Malysa"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158181",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Search for long-lived heavy neutral leptons in proton-proton collision events with a lepton-jet pair associated with a secondary vertex at √s = 13 TeV",
        "abstract": "A search for long-lived heavy neutral leptons (HNLs) using proton-proton collision data corresponding to an integrated luminosity of 138 fb−1 collected at s = 13 TeV with the CMS detector at the CERN LHC is presented. Events are selected with a charged lepton originating from the primary vertex associated with the proton-proton interaction, as well as a second charged lepton and a hadronic jet associated with a secondary vertex that corresponds to the semileptonic decay of a long-lived HNL. No excess of events above the standard model expectation is observed. Exclusion limits at 95% confidence level are evaluated for HNLs that mix with electron and/or muon neutrinos. Limits are presented in the mass range of 1–16.5 GeV, with excluded square mixing parameter values reaching as low as 2 × 10−7. For masses above 11 GeV, the presented limits exceed all previous results in the semileptonic decay channel, and for some of the considered scenarios are the strongest to date.",
        "authors": [
            "A. Hayrapetyan",
            "A. Tumasyan",
            "W. Adam",
            "J. W. Andrejkovic",
            "T. Bergauer",
            "S. Chatterjee",
            "K. Damanakis",
            "M. Dragicevic",
            "P. S. Hussain",
            "M. Jeitler",
            "N. Krammer",
            "A. Li",
            "D. Liko",
            "I. Mikulec",
            "J. Schieck",
            "R. Schöfbeck",
            "D. Schwarz",
            "M. Sonawane",
            "The CMS collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158277",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Protein codes promote selective subcellular compartmentalization",
        "abstract": "Cells have evolved mechanisms to distribute ~10 billion protein molecules to\r\nsubcellular compartments where diverse proteins involved in shared functions must\r\nassemble. Here, we demonstrate that proteins with shared functions share amino\r\nacid sequence codes that guide them to compartment destinations. A protein\r\nlanguage model, ProtGPS, was developed that predicts with high performance the\r\ncompartment localization of human proteins excluded from the training set.\r\nProtGPS successfully guided generation of novel protein sequences that selectively\r\nassemble in the nucleolus. ProtGPS identified pathological mutations that change\r\nthis code and lead to altered subcellular localization of proteins. Our results\r\nindicate that protein sequences contain not only a folding code, but also a\r\npreviously unrecognized code governing their distribution to diverse subcellular\r\ncompartments.",
        "authors": [
            "Henry R. Kilgore",
            "Itamar Chinn",
            "Peter G. Mikhael",
            "Ilan Mitnikov",
            "Catherine Van Dongen",
            "Guy Zylberberg",
            "Lena Afeyan",
            "Salman F. Banani",
            "Susana Wilson-Hawken",
            "Tong Ihn Lee",
            "Regina Barzilay",
            "Richard A. Young"
        ],
        "journal_conference_name": "Science",
        "publisher": "American Association for the Advancement of Science",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158180",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Association Between Medicaid Expansion and Insurance Status, Risk Group, Receipt, and Refusal of Treatment Among Men with Prostate Cancer",
        "abstract": "Simple Summary\r\nWe sought to quantify the impact of Medicaid expansion on insurance status, stage at diagnosis, time to treatment initiation, and refusal of locoregional treatment among patients with prostate cancer, the second leading cause of cancer death among men in the United States. We found that while Medicaid expansion was associated with increased insurance coverage and decreased refusal of radiation therapy, there was no significant association with earlier risk group at diagnosis, treatment within 180 days, nor refusal of locoregional therapy. Similarly, racial minorities experienced no significant changes in time to treatment initiation following Affordable Care Act implementation compared to White patients. Ultimately, more research is needed to understand how Medicaid expansion affects cancer outcomes and whether these effects are borne equitably among different populations.\r\n\r\nAbstract\r\nBackground: Although the Patient Protection and Affordable Care Act (ACA) has been associated with increased Medicaid coverage among prostate cancer patients, the association between Medicaid expansion with risk group at diagnosis, time to treatment initiation (TTI), and the refusal of locoregional treatment (LT) among patients requires further exploration. Methods: Using the National Cancer Database, we performed a retrospective cohort analysis of all patients aged 40 to 64 years diagnosed with localized prostate cancer from 2011 to 2016. Difference-in-difference (DID) analysis was used to compare changes in insurance status, risk group at diagnosis, TTI, and the refusal of LT among patients residing in Medicaid expansion versus non-expansion states. In a secondary analysis, we used DID to compare changes in the above outcomes among racial minorities versus White patients living in expansion states. Results: Of the 112,434 patients with prostate cancer in our analysis, 50,958 patients lived in Medicaid expansion states, and 61,476 patients lived in non-expansion states. In the adjusted analysis, we found that the proportion of uninsured patients (adjusted DID: −0.87%; 95% confidence interval [95% CI]: −1.28 to −0.46) and patients who refused radiation therapy (adjusted DID: −0.71%; 95% CI: −0.95 to −0.47) decreased more in expansion states compared to non-expansion states. Similarly, we observed that the racial disparity of select outcomes in expansion states narrowed, as racial minorities experienced larger absolute decreases in uninsured status and the refusal of radiation therapy (RT) regimens than White patients following ACA implementation (p < 0.01 for all). However, residence in a Medicaid expansion state was not associated with changes in risk group at diagnosis, TTI, nor the refusal of LT (p > 0.01 for all); racial disparities in TTI were also exacerbated in expansion states following ACA implementation. Conclusions: The association between Medicaid expansion and prostate cancer outcomes and disparities remains unclear. While ACA implementation was associated with increased insurance coverage and decreased refusal of RT, there was no significant association with earlier risk group at diagnosis, TTI within 180 days, or refusal of LT. Similarly, racial minorities in expansion states had larger decreases in uninsured status and the refusal of RT regimens, as well as smaller increases in intermediate-/high-risk disease at presentation than White patients following ACA implementation, but experienced no significant changes in TTI. More research is needed to understand how Medicaid expansion affects cancer outcomes and whether these effects are borne equitably among different populations.",
        "authors": [
            "Tej A. Patel",
            "Bhav Jain",
            "Edward Christopher Dee",
            "Khushi Kohli",
            "Sruthi Ranganathan",
            "James Janopaul-Naylor",
            "Brandon A. Mahal",
            "Kosj Yamoah",
            "Sean M. McBride",
            "Paul L. Nguyen",
            "Fumiko Chino",
            "Vinayak Muralidhar",
            "Miranda B. Lam",
            "Neha Vapiwala"
        ],
        "journal_conference_name": "Cancers",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158243",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Recent Progress in Flexible Piezoelectric Tactile Sensors: Materials, Structures, Fabrication, and Application",
        "abstract": "Flexible tactile sensors are widely used in aerospace, medical and health monitoring, electronic skin, human–computer interaction, and other fields due to their unique advantages, thus becoming a research hotspot. The goal is to develop a flexible tactile sensor characterized by outstanding sensitivity, extensive detection range and linearity, elevated spatial resolution, and commendable adaptability. Among several strategies like capacitive, piezoresistive, and triboelectric tactile sensors, etc., we focus on piezoelectric tactile sensors because of their self-powered nature, high sensitivity, and quick response time. These sensors can respond to a wide range of dynamic mechanical stimuli and turn them into measurable electrical signals. This makes it possible to accurately detect objects, including their shapes and textures, and for them to sense touch in real time. This work encapsulates current advancements in flexible piezoelectric tactile sensors, focusing on enhanced material properties, optimized structural design, improved fabrication techniques, and broadened application domains. We outline the challenges facing piezoelectric tactile sensors to provide inspiration and guidance for their future development.",
        "authors": [
            "Jingyao Tang",
            "Yiheng Li",
            "Yirong Yu",
            "Qing Hu",
            "Wenya Du",
            "Dabin Lin"
        ],
        "journal_conference_name": "Sensors",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158242",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Convergence to Bohmian Mechanics in a de Broglie-Like Pilot-Wave System",
        "abstract": "Bohmian mechanics supplements the quantum wavefunction with deterministic particle trajectories, offering an alternate, dynamical language for quantum theory. However, the Bohmian wavefunction evolves independently of these trajectories, and is thus unaffected by the observable properties of the system. While this property is widely assumed necessary to ensure agreement with quantum mechanics, much work has recently been dedicated to understanding classical pilot-wave systems, which feature a two-way coupling between particle and wave. These systems—including the “walking droplet” system of Couder and Fort (Couder and Fort (2006) Phys. Rev. Lett. 97:154101) and its various abstractions (Dagan and Bush (2020) CR Mecanique 348:555–571; Durey and Bush (2020) Front. Phys. 8:300; (2021) Chaos 31:033136; Darrow and Bush (2024) Symmetry 16:149)—allow us to investigate the limits of classical systems and offer a touchstone between quantum and classical dynamics. In this work, we present a general result that bridges Bohmian mechanics with this classical pilot-wave theory. Namely, Darrow and Bush ((2024) Symmetry 16:149) recently introduced a Lagrangian pilot-wave framework to study quantum-like behaviours in classical systems; with a particular choice of particle-wave coupling, they recover key dynamics hypothesised in de Broglie’s early double-solution theory (de Broglie (1970) Foundations Phys. 1:5–15). We here show that, with a different choice of coupling, their de Broglie-like system reduces exactly to single-particle Bohmian mechanics in the non-relativistic limit. Our result clarifies that, while multi-particle entanglement is impossible to replicate in general with local, classical theories, no such restriction exists for single-particle quantum mechanics. Moreover, connecting with the previous work of Darrow and Bush, our work demonstrates that de Broglie’s and Bohm’s theories can be connected naturally within a single Lagrangian framework. Finally, we present an application of the present work in developing a single-particle analogue for position measurement in a de Broglie-like setting.",
        "authors": [
            "David Darrow"
        ],
        "journal_conference_name": "Foundations of Physics",
        "publisher": "Springer US",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158274",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Data Artifacts Glossary: a community-based repository for bias on health datasets",
        "abstract": "Background The deployment of Artificial Intelligence (AI) in healthcare has the potential to transform patient care through improved diagnostics, personalized treatment plans, and more efficient resource management. However, the effectiveness and fairness of AI are critically dependent on the data it learns from. Biased datasets can lead to AI outputs that perpetuate disparities, particularly affecting social minorities and marginalized groups. Objective This paper introduces the “Data Artifacts Glossary”, a dynamic, open-source framework designed to systematically document and update potential biases in healthcare datasets. The aim is to provide a comprehensive tool that enhances the transparency and accuracy of AI applications in healthcare and contributes to understanding and addressing health inequities. Methods Utilizing a methodology inspired by the Delphi method, a diverse team of experts conducted iterative rounds of discussions and literature reviews. The team synthesized insights to develop a comprehensive list of bias categories and designed the glossary’s structure. The Data Artifacts Glossary was piloted using the MIMIC-IV dataset to validate its utility and structure. Results The Data Artifacts Glossary adopts a collaborative approach modeled on successful open-source projects like Linux and Python. Hosted on GitHub, it utilizes robust version control and collaborative features, allowing stakeholders from diverse backgrounds to contribute. Through a rigorous peer review process managed by community members, the glossary ensures the continual refinement and accuracy of its contents. The implementation of the Data Artifacts Glossary with the MIMIC-IV dataset illustrates its utility. It categorizes biases, and facilitates their identification and understanding. Conclusion The Data Artifacts Glossary serves as a vital resource for enhancing the integrity of AI applications in healthcare by providing a mechanism to recognize and mitigate dataset biases before they impact AI outputs. It not only aids in avoiding bias in model development but also contributes to understanding and addressing the root causes of health disparities.",
        "authors": [
            "Rodrigo R. Gameiro",
            "Naira L. Woite",
            "Christopher M. Sauer",
            "Sicheng Hao",
            "Chrystinne O. Fernandes",
            "Anna E. Premo",
            "Alice R. Teixeira",
            "Isabelle Resli",
            "An-Kwok I. Wong",
            "Leo A. Celi"
        ],
        "journal_conference_name": "Journal of Biomedical Science",
        "publisher": "BioMed Central",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158286",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Data Are Power: Addressing the Power Imbalance Around Community Data with the Open-Access Data4HumanRights Curriculum",
        "abstract": "Data4HumanRights’ training materials have been developed as open-source and tailored to limited-resource settings, where community data collectors often live and work. Access to training on data collection, analysis, and visualisation to support the advocacy of vulnerable groups is essential, particularly in the context of increasing human rights challenges such as land rights, adequate housing, conflicts, and climate justice. This paper provides an overview of how the training materials were co-developed with community data collectors in Nigeria and Kenya, offering insights into the fundamental principles (i.e., inclusiveness, adaptive, limited resources, and being gender- and incentive-sensitive) and the structure of the open-access training materials. The development process resulted in 28 modules, each designed to be delivered in a face-to-face format in less than one day by a local trainer. To maximize adaptivity, the training modules can be mixed and matched (e.g., as individual modules or a learning path of several modules around a specific training need). The individual modules cover a range of methods and tools that are useful to human rights work and community advocacy, e.g., documenting evictions, performing rapid needs assessments after acute crises, community profiling, and monitoring community development indicators. The training materials contain instructions for the training facilitator(s) and all necessary training materials. To ensure inclusivity, the training covers both basic and advanced topics, with most modules designed to address basic needs that can be followed using a mobile phone, thereby avoiding the need for computers or printed handouts. The training results in Nigeria and Kenya showcase applications, including mapping waste problems and addressing forced evictions. Trained community groups produced maps of waste piles to prioritize community actions, such as finding space for urban agriculture, and conducted rapid needs assessments during a massive eviction. This approach helps reduce power imbalances and empowers community groups to effectively manage and utilise their own data.",
        "authors": [
            "Monika Kuffer",
            "Dana R. Thomson",
            "Dianne Wakonyo",
            "Nicera Wanjiru Kimani",
            "Divyani Kohli-Poll Jonker",
            "Enyo Okoko",
            "Rasak Toheeb",
            "Bisola Akinmuyiwa",
            "Mohammed Zanna",
            "Dezyno Imole",
            "Andrew Maki"
        ],
        "journal_conference_name": "Department of Urban Studies and Planning",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158297",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploring the Holiday Effect on Elevated Traffic-Related Air Pollution with Hyperlocal Measurements in Chengdu, China",
        "abstract": "Traffic-related air pollutants (TRAPs) pose significant health risks in megacities, yet fixed monitoring sites often fail to capture their complexity. To characterize the TRAP concentrations which fixed sites cannot address, we employed a mobile platform to effectively capture real-time hyperlocal-scale TRAP variations in Chengdu, China. A 17-day sampling campaign was conducted covering the National Holiday of China and collected ~1.2 × 105 1 Hz paired data. We measured particle number concentration (PNC), black carbon (BC), and nitrogen oxides (NOx) across urban and rural freeway environments to assess the impact of reduced heavy-duty diesel vehicles (HDDVs) during the holiday (i.e., holiday effect). No clear impact of wind direction on TRAP concentrations was found in this study. However, substantial differences (two times) were observed when comparing non-holiday to holiday campaigns. Spearman correlations (0.21–0.56) between TRAPs persistently exceeded Pearson correlations (0.14–0.41), indicating non-linear relationships and suggesting the necessity for data transformations (e.g., logarithms) in TRAP analysis. The comparison of the background subtracted TRAPs concentrations between non-holiday and holidays, revealing approximately a 50% reduction in TRAPs across microenvironments. Among the TRAPs, NOx emerged as a reliable indicator of HDDV emissions. The study provides insights into vehicle fleet composition impacts, paving the way for enhanced exposure assessment strategies.",
        "authors": [
            "Sheng Xiang",
            "Jiaojiao Yu",
            "Yu Ting Yu",
            "Pengbo Zhao",
            "Tie Zheng",
            "Jingsong Yue",
            "Yuanyuan Yang",
            "Haobing Liu"
        ],
        "journal_conference_name": "Atmosphere",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158296",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Quantum Computing from Graphs",
        "abstract": "While stabilizer tableaus have proven exceptionally useful as a descriptive tool for additive quantum codes, they otherwise offer little guidance for concrete constructions or coding algorithm analysis. We introduce a representation of stabilizer codes as graphs with certain structures. Specifically, the graphs take a semi-bipartite form wherein input nodes map to output nodes, such that output nodes may connect to each other but input nodes may not. Intuitively, the graph’s input-output edges represent information propagation of the encoding circuit, while output-output edges represent the code’s entanglement structure. We prove that this graph representation is in bijection with tableaus and give an efficient compilation algorithm that transforms tableaus into graphs. We then show that this map is efficiently invertible, which gives a new universal recipe for code construction by way of finding graphs with sufficiently nice properties.\r\n\r\nThe graph representation gives insight into both code construction and algorithms. To the former, we argue that graphs provide a flexible platform for building codes particularly at small non-asymptotic scales. We construct as examples several constant-size codes and several infinite families codes. We also leverage graphs in a probabilistic analysis to extend the quantum Gilbert-Varshamov bound into a three-way distance-rate-weight trade-off. To the latter, we show that key coding algorithms, distance approximation, weight reduction, and decoding, are unified as instances of a single optimization game on a graph. Moreover, key code properties such as distance, weight, and encoding circuit depth, are all controlled by the graph degree. We give efficient algorithms for producing simple encoding circuits whose depths scale as twice the degree and for implementing logical diagonal and certain Clifford gates with non-constant but reduced depth. Finally, we construct a simple efficient decoding algorithm and prove a performance guarantee for a certain classes of graphs. These results give evidence that graphs are generically useful for the study of quantum computing and its practical implementations.",
        "authors": [
            "Andrey Boris Khesin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158853",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Simple Models for Complex Tropical Dynamics",
        "abstract": "Studying Earth's tropics is an essential part of understanding the climate, simulating the Earth system, and predicting the societal impacts of weather. In this thesis, we use a hierarchy of models -- including analytically tractable equations, simplified simulations, and full general circulation models -- to study tropical phenomena including the Hadley Circulation, the Inter-Tropical Convergence Zone (ITCZ), the South Asian monsoon, Pacific and ENSO seasonality, the Walker Circulation, and the modeling of the tropical energy budget. We begin with an examination of tropical SSTs and the ITCZ under warming, finding that the Hadley cells weaken and tropical SST gradients decrease in a warmer climate. The ocean's subtropical cells strengthen and transport more energy in a warmer climate, further flattening SST gradients. The ITCZ, meanwhile, increases in strength with warming because of the exponential relationship between humidity and temperature, and the presence of a dynamic ocean changes a single-ITCZ with a sinusoidal seasonal cycle to a double-ITCZ with a square wave seasonal cycle. Next, we study the ``monsoonal mode,'' an energy and precipitation anomaly triggered by the South Asian Monsoon that moves into the West Pacific during Northern Hemisphere autumn. The monsoonal mode is discussed as a possible underlying cause of the seasonality of the Pacific, i.e., that the West Pacific and ENSO both have seasonalities that favor one season despite being on the equator. To show this, ENSO seasonality is examined using simplified simulations and an energy budget of the Central-Eastern Equatorial Pacific. Similar techniques are then used to study ENSO events in warmer climates, and it is found that the Pacific zonal SST gradient and the Walker circulation, which are the sources of ENSO instability, weaken with warming, decreasing  the magnitude of ENSO events. Lastly, we assess the energy budget of CMIP6 models. It is shown that all CMIP6 models have more energy input to the deep tropics than ERA5 reanalysis, and this bias is bigger in the Southern Hemisphere. The hemispheric asymmetry in this bias can be traced back to radiation absorbed by the atmosphere, which is associated with dust (for shortwave radiation) and total column water (for longwave radiation). As a whole, this thesis demonstrates the utility of studying complex problems with simple models and deepens our understanding of Earth's tropics.",
        "authors": [
            "P.J. Tuckman"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158855",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Electrospray Thrusters in Chemical-Electric Multimode Propulsion for Small Satellites",
        "abstract": "Propulsion for small spacecraft is typically one of two modes, chemical or electric. These modes offer complementary propulsive performance: chemical propulsion provides high thrust and low specific impulse, while electric propulsion provides the inverse. As such, having access to both modes on the same spacecraft (i.e. multimode propulsion) is extremely useful. Unfortunately, the conventional propellants used by chemical and electric thrusters are highly incompatible, making this particularly difficult on small spacecraft that lack the mass, power, and volume to accommodate two separate propulsion systems. However, recent advancements in green monopropellants -- developed as less-toxic alternatives to hydrazine in chemical monopropellant thrusters -- have created a new family of ionic liquids monopropellants, making them the natural propellant for a highly compact form of electric propulsion known as electrospray thrusters. This presents a unique opportunity for a propellant to be shared between two propulsion modes, decreasing required mass and volume to be feasible for small spacecraft. This thesis examines the use of ionic liquid monopropellants in electrospray thrusters for a multimode chemical-electric propulsion system. This thesis focuses particularly on ASCENT, a high-maturity monopropellant with flight heritage in chemical thrusters.\r\n\r\nIn this work, the performance of ASCENT in the MIT ion Electrospray Propulsion System (iEPS) is extensively characterized. Experimental work includes ion plume diagnostics, indirectly and directly obtained performance estimates, temperature-dependent performance estimates, and extended duration firing behavior. Preliminary studies of similar monopropellants are also conducted to assess their use in a multimode system. To support an upcoming technology demonstration flight, a new multimode-compatible iEPS thruster tank is designed, fabricated, and validated. The integration and operation requirements for this thruster in a flight-ready system are defined. Finally, the mission benefits of an ASCENT multimode system for CubeSats are compared against current commercial options using an Earth observation mission case study.\r\n\r\nThis work finds that an iEPS thruster with ASCENT propellant has thrust of 9-15 µN, a specific impulse of 600-750 seconds, and a total efficiency of 18-22%, depending on current setpoint. We find that ASCENT is slightly volatile in high vacuum, which causes time-dependent losses in efficiency and specific impulse from gradual propellant evaporation. This volatility may also increase thruster lifetime by mitigating the risk of thruster failure by emitter flooding. This work also identified a modified version of ASCENT, created when the propellant is exposed to iron. This modified version produces a dramatically higher thrust and thrust-to-power compared to standard ASCENT. Additionally, flight-ready configurations of a multimode system are defined for 6U, 12U, and 27U CubeSats. A case study analysis found that the benefits of a chemical-electrospray multimode system are best realized at the 12U scale and above. Overall, this thesis provides critical insights on the performance, integration, and operation of electrospray thrusters with ionic liquid monopropellants. These results can then be used to enable a multimode propulsion system for small satellites.",
        "authors": [
            "Amelia R. Bruno"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158790",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Fast methods for full-wave electromagnetic solvers in MRI",
        "abstract": "High static field ( 3T) MR scanners can produce human tissue images of astounding clarity, but rely on high frequency ( 123MHz) electromagnetic radiation that generates complex in-tissue field patterns that are patient-specific and potentially harmful. Many such scanners use multiple transmitters to better control field patterns, but then adjust the transmitters based on general guidelines rather than optimizing for the specific patient, mostly because computing patient-specific fields was presumed far too slow. It was recently demonstrated that the combination of fast low-resolution tissue mapping and fast voxel-based field simulation can be used to perform a rapid patient-specific MR safety check. However, the field simulation still required several minutes, making it too slow to perform the dozens of simulations that would be needed for patient-specific optimization. In this work, we develop a set of numerical acceleration techniques that facilitate fast field simulations that bridge the gap between the performance of current state-of-art full-wave electromagnetic packages and time requirements dictated by real-time patient-specific field optimization in a clinical setting. These techniques cater to a large range of body sizes and complex coil geometries.",
        "authors": [
            "Georgy D. Guryev"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158943",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Detection and Localization of Pressure Transients in Water Distribution Systems",
        "abstract": "Water distribution systems are critical to urban water supply, but as they age they become increasingly vulnerable to bursts and leaks, leading to significant economic, social, and environmental consequences. The complexity and inaccessibility of underground pipelines present substantial challenges for their maintenance. As a result, the development of real-time monitoring systems for these systems is essential to reduce water waste and minimize adverse impacts to consumers and surrounding infrastructures. This thesis investigates the effectiveness of continuous pressure monitoring systems in detecting pipe bursts and transient events within water distribution systems. Using PTSNet, a parallel transient simulation Python package, we simulate pipe burst events at each node in a real-world system and examine the pressure-time response at all other nodes. By adding Gaussian noise to the simulation results to mimic real-world background noise, we assess the detection success of pressure signals at each node using a modified CUSUM algorithm. The correlations between detection success and three spatial metrics between the source and sensor are calculated. We show that a spatial metric, the effective number of magnitude-changing junctions along the fastest path, (NJFP), has a stronger correlation with detection success than the shortest travel path or the shortest distance. By comparing detection performance for networks with differing topologies (gridded, looped, and branched) and pipe characteristics, we discover that multiple shortest paths (MSP; where pressure waves from different paths arrive almost simultaneously at the sensor) amplify the signal due to transient interference phenomenon and enhance the detectability of transients. This effect is particularly pronounced in gridded networks. We investigate the capabilities of monitoring, from a network of fixed stations, to achieve unique localization of pressure transient events using a time-reversal back-propagation algorithm. This algorithm identifies the event source by matching the theoretical and detected arrival time differences at the sensors. A novel time differences space is constructed, representing the independent shortest time differences from locations along all the pipes to the sensors, based on network information and sensor locations. Pipe sections with unique shortest time differences are identified as uniquely localizable pipes. Effective-NJFP-based probabilities of transient detection with accurate arrival times (error < 0.1s) are derived from these simulation results. The localization performance of the sensor network is evaluated by the probability-weighted total lengths of the pipes that can be uniquely localized.\r\nWe consider sensor placement strategies aimed at maximizing the detection and localization performance of pressure monitoring sensor networks. Detection performance is defined as the total weighted pipe lengths in the network, where the weight of each pipe corresponds to its detection probability. Two problems are addressed: In order to maximize transient event detection performance when only a limited number of sensors are available, we formulate a mixed-integer programming (MIP) optimization model and employ a genetic algorithm to find solutions. The second problem involves determining the minimum number of sensors and their optimal locations to detect transient events across the entire network without a constraint on the number of available sensors. This is formulated as a minimum set cover problem, and an optimal solution is obtained using a mixed-integer linear programming solver. We focus on maximizing transient localization performance with a limited number of sensors. A genetic algorithm is applied to determine sensor locations, and the solutions obtained by this method provide significantly better localization performance than other approaches. We show differences in sensor placements for detection and localization: sensors are more evenly distributed throughout the network for detection purposes, while for localization, they are more concentrated in areas with longer pipes and simpler network structures. Finally, we present an analysis of two pressure monitoring datasets collected from a real-world water distribution system (SLG network). The first dataset consists of data from 28 sensors with a 100 Hz sampling frequency, collected over 7 to 30 days. We propose a method to identify and analyze noise levels and distributions at each sensor. Using a modified CUSUM algorithm, we detect transients and correlate them across sensors to identify events detected by multiple sensors. A transient-magnitude-based clustering method is then employed to group events based on their magnitudes, followed by a localization approach that utilizes the arrival time differences of transients between sensors. The findings indicate that noise levels in real-world monitoring data vary both spatially and temporally and are not independently normally distributed. Additionally, the arrival times detected by the modified CUSUM algorithm may not always accurately reflect the true transient arrival times due to mismatches between the signal characteristics and tuning of model parameters. Accurately identification of transient arrival time is particularly challenging for slowly changing pressure wave fronts. The second dataset includes pressure monitoring data from 7 sensors, during which 14 active transients with known source locations, times, and magnitudes were generated. We apply the modified CUSUM algorithm to detect transients at the sensors and correlate detection success with spatial metrics. The analysis confirms that the effective NJFP has the highest correlation with detection success, consistent with the simulation results. Additionally, the transient magnitude ratios between sensors and the source are found to be similar to the ratios calculated based on theoretical transmission coefficients when the source and sensor are in close proximity, suggesting that transmission coefficients can be used to estimate transient magnitudes in real networks.",
        "authors": [
            "Shiqing Liu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158880",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Information-centric Algorithms for Feature Extraction in High-Dimensional Sequential Data",
        "abstract": "Hidden Markov Models (HMMs) are a cornerstone of sequential data analysis, offering a robust framework for modeling observable events influenced by hidden internal states. With applications spanning speech recognition, video analysis, bioinformatics, and financial time series, HMMs enable the prediction and classification of raw data by leveraging their dual-layer stochastic structure: hidden Markov states and observable outputs. However, as real-world data grows increasingly high-dimensional, extracting meaningful features from observations becomes critical to reduce computational complexity while retaining relevant information.\r\n\r\nThis thesis addresses key challenges in feature extraction for high-dimensional HMMs. Current methods, such as neural networks (NNs), are widely used for nonlinear feature learning but lack mechanisms to prioritize useful features or incorporate known structural constraints. To bridge this gap, this work proposes novel algorithms to decouple representation learning from task-specific objectives and extract features aligned with predefined constraints.\r\n\r\nThe theoretical foundation, including local information geometry and Hirschfeld-Gebelein-Rényi (HGR) maximal correlation, is introduced in Chapter 2. Chapter 3 details three innovative feature extraction algorithms and their corresponding neural network architectures, highlighting their strengths and limitations. Convergence analyses and tail bounds for these methods are presented in Chapter 4. Numerical simulations validating the efficacy of the proposed approaches are provided in Chapter 5, while Chapter 6 concludes with a summary of contributions and potential future research directions.\r\n\r\nThis thesis advances the field by offering structured, constraint-aware feature extraction techniques tailored for high-dimensional sequential data, setting the stage for more effective and interpretable inference in HMMs.",
        "authors": [
            "Jiejun Jin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158923",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "System-Technology Co-Optimization of Scaled Electronics\r\nBased on Two-Dimensional Materials",
        "abstract": "Over the past 60 years, the semiconductor industry has focused on developing highly scaled electronic devices and high-density integrated circuits. However, bottlenecks have arisen recently as transistor dimensions approach the physical limits, and integration density is constrained. This thesis addresses these issues with two-dimensional (2D) materials, which includes inventing a low-temperature (< 300 °C) metal-organic chemical vapor deposition (MOCVD) method for 2D materials on 8-inch wafers, investigating extreme device scaling and multi-channel transistors. Design-Technology Co-Optimization (DTCO) and SystemTechnology Co-Optimization (STCO) are employed to rapidly model, evaluate and optimize device and circuit performance. Moreover, heterogeneous integration and monolithic 3D integration techniques are investigated, addressing challenges in integrating 2D materials with silicon complementary-metal-oxide-semiconductor (CMOS) circuits and flexible substrates. This research aims to advance high-density, high-performance electronics with low-power consumption for next-generation integrated systems.",
        "authors": [
            "Jiadi Zhu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158961",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Improved Complexity Analysis for the Proximal Bundle Algorithm Under a Novel Perspective",
        "abstract": "The proximal bundle algorithm (PBA) is a fundamental and computationally effective algorithm for solving optimization problems with non-smooth components. We investigate its convergence rate in two settings. We first focus on a composite setting where one function is smooth and the other is piecewise linear. We interpret a sequence of null steps of the PBA as a Frank-Wolfe algorithm on the Moreau envelope of the dual problem. In light of this correspondence, we first extend the linear convergence of Kelley's method on convex piecewise linear functions from the positive homogeneous to the general case. Building on this result, we propose a novel complexity analysis of PBA and derive a O (epsilon^-4/5) iteration complexity, improving upon the best known O (epsilon^-2) guarantee. This approach also unveils new insights on bundle management. We then present the first variant of the PBA for smooth objectives, achieving an accelerated convergence rate of O(epsilon^-1/2 log(epsilon^-1)), where epsilon is the desired accuracy. Our approach addresses an open question regarding the convergence guarantee of the PBA, which was previously posed in two recent papers. We interpret the PBA as a proximal point algorithm and base our proposed algorithm on an accelerated inexact proximal point scheme. Our variant introduces a novel null step test and oracle while maintaining the core structure of the original algorithm. The newly proposed oracle substitutes the traditional cutting planes with a smooth lower approximation of the true function. We show that this smooth interpolating lower model can be computed as a convex quadratic program. We finally show that Nesterov acceleration can be effectively applied when the objective is the sum of a smooth function and a piecewise linear one.",
        "authors": [
            "David Fersztand"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158820",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Towards an Artificial Neuroscience: Analytics for Language Model Interpretability",
        "abstract": "The growing deployment of neural language models demands greater understanding of their internal mechanisms. The goal of this thesis is to make progress on understanding the latent computations within large language models (LLMs) to lay the groundwork for monitoring, controlling, and aligning future powerful AI systems. We explore four areas using open source language models: concept encoding across neurons, universality of learned features and components across model initializations, presence of spatial and temporal representations, and basic dynamical systems modeling.\r\n\r\nIn Chapter 2, we adapt optimal sparse classification methods to neural network probing, allowing us to study how concepts are represented across multiple neurons. This sparse probing technique reveals both monosemantic neurons (dedicated to single concepts) and polysemantic neurons (representing multiple concepts in superposition) in full-scale LLMs confirming predictions from toy models. In Chapter 3, we identify and exhaustively catalog universal neurons across different model initializations by computing pairwise correlations of neuron activations over large datasets. Our findings show that 1-5\\% of neurons are universal, often with clear interpretations, and we taxonomize them into distinct neuron families.\r\n\r\nTo investigate spatial and temporal representations, we analyze LLM activations on carefully curated datasets of real-world entities in Chapter 4. We discover that models learn linear representations of space and time across multiple scales, which are robust to prompting variations and unified across different entity types. We identify individual \"space neurons\" and \"time neurons\" that reliably encode spatial and temporal coordinates. In Chapter 5, we use optimal sprase regression techniques to improve the sparse identification of nonlinear dynamics (SINDy) framework, demonstrating improved sample efficiency and support recovery in canonical differential systems. We then leverage this improvement to study the ability of LLMs to in-context learn dynamical systems and find internal representations which track the underlying system state.",
        "authors": [
            "Robert Wesley Gurnee"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158869",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Leveraging Structure for Efficient and Dexterous Contact-Rich Manipulation",
        "abstract": "Contact-rich manipulation has proved challenging due to the need to consider multiple combinatoric possibilities of making or breaking contact with the surrounding environment. As a result, existing methods have often resorted to combinatorial optimization that utilizes dynamics structure but considers all possibilities exhaustively, or compute-heavy and inefficient sampling methods that utilize blackbox optimization such as Reinforcement Learning (RL). In this thesis, I aim to show that by combining structured contact smoothing in conjunction with local gradient-based control and sampling-based motion planning, we can bypass the combinatorial explosion of contact modes while still leveraging structure and achieve highly efficient contact-rich manipulation. To achieve this capability, I first shed light on how RL abstracts contact modes and optimizes difficult landscapes by combining stochastic smoothing and zeroth-order optimization; yet, I show how following a similar stochastic strategy while using gradients suffers from several drawbacks such as empirical bias and high variance. To leverage structure in a more helpful manner, I propose a method for smoothing contact dynamics without relying on stochastic smoothing, bypassing these drawbacks. Using this smoothing scheme, I present a highly efficient and performant local control based on gradient-based trajectory optimization and model predictive control. Finally, I connect these local control capabilities with global sampling-based motion planners to achieve long-horizon global plans. The proposed method achieves contact-rich plans such as dexterous in-hand reorientation and whole-body manipulation much more efficiently than RL while being highly scalable compared to methods that explicitly reason about contact modes. These results achieve a reduction of contact-rich manipulation to kinodynamic motion planning, and exposes the true difficulty of contact-rich manipulation from combinatorial explosion in contact modes to combinatorial and highly non-local decisions over motion planning behaviors.",
        "authors": [
            "Hyung Ju Terry Suh"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158946",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Laboratory Astrophysics Studies of Magnetized Collisionless Shock Precursors and the ³He³He Proton Spectrum at the OMEGA Laser Facility",
        "abstract": "Laboratory astrophysics enables the study of astrophysical systems in the lab. There are broadly two types of laboratory astrophysics experiments: macrophysics and microphysics. Macrophysics experiments study a scaled down version of an astrophysical system while microphysics experiments create a small volume of matter with the same conditions as an astrophysical system. This thesis details work related to both macrophysics and microphysics laboratory astrophysics experiments. For the macrophysics contribution, collisionless shocks experiments were conducted at the OMEGA laser facility using the new gas jet platform. Collisionless shocks are shock waves formed through plasma processes when particle collisions are negligible. These shocks can form as bow shocks in the interaction between the solar wind and planetary ionospheres and can accelerate charge particles to high energies. In the experiment, a CH plasma flow collides with a hydrogen gas jet plasma to create a forming magnetized collisionless shock. Different diagnostics show a moving density jump, strong magnetic fields, and the acceleration of electrons. These observations coupled with magnetohydrodynamics and kinetic particle-in-cell simulations paint a complete physical picture of the forming shock in a configuration similar to the bow shock of Venus. Late time proton radiographs show a complicated structure which is studied for magnetic turbulence. Turbulence is important in several astrophysical systems, especially collisionless shocks where it dissipates shock kinetic energy and is essential for accelerating charged particles to cosmic ray energies. Magnetic power spectra extracted from proton radiography data show a break in the spectrum between the ion Larmor radius and the ion skin depth for high plasma β, a sign of kinetic turbulence. Large scale particle-in-cell simulations of high β turbulence also have this feature showing that the experimental data are consistent with high β kinetic turbulence. For the microphysics contribution, a new proton spectrometer is designed for measurements of the ³He³He proton spectrum. The ³He³He fusion reaction is the last step of the proton-proton I chain which produces the majority of the sun’s power. Previous experiments were not able to measure the ³He³He proton spectrum below 6 MeV. A new proton step range filter (SRF) spectrometer with a larger energy range is designed using a Monte Carlo tool. This tool uses Geant4 and is able to self-consistently apply the instrument response function. The new SRF design is validated and a method for analyzing experimental data using the Monte Carlo code is presented.",
        "authors": [
            "Timothy Mark Johnson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158838",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Enabling AI Copilots for Engineering Design With Parametric, Graph, And Component Inputs",
        "abstract": "Engineering design demands the synthesis of multimodal and often incomplete data—ranging from detailed parametric specifications, assembly graphs, visual references, and textual descriptions. Despite growing interest in generative models for design ideation and exploration, state-of-the-art approaches struggle with incomplete inputs, lack of support for modalities other than text and image, and limited controllability. This thesis addresses these gaps by unifying two complementary advances:\r\n\r\nFirst, we introduce a graph-guided diffusion approach for parametric data completion. By coupling Graph Attention Networks with a diffusion-based imputation mechanism, our method acts as a highly accurate and creative design auto-completion system for incomplete partial designs. On a dataset of 12,500 bicycles, this design imputation framework achieves a root mean square error (RMSE) of approximately 0.92 on numerical features and an error rate of around 0.18 for categorical attributes, outperforming both classical imputation methods such as MissForest, hotDeck, PPCA and advanced diffusion-based baselines such as TabCSDI. Moreover, it achives a Diversity Score of 3.10, surpassing all baselines, illustrating that the imputation process transforms incomplete data into multiple creative designs.\r\n\r\nSecond, we develop a multimodal control architecture that can extend foundation models to condition their generation processes with all or a subset of parametric inputs, assembly graphs, component images, and textual constraints. This model tremendously enhances both the controllability and precision of the generation process of foundational generative models, enabling controlling modalities that were not possible before. We first show that our model excels at tasks that state-of-the-art models struggle on. We further validate the performance of our model with surrogate models that investigate individual features. Our model achieves 95% or greater R^2 scores on different continuous parameters. Further, we show that our model is able to generate creative and novel designs while maintaining a high level of precision. This enables engineers to guide generative outputs along precise dimensional, aesthetic, and functional targets. Across numerous trials of different settings, we observe that our pipeline robustly fuses tabular parametric information, assembly graphs, and reference component images to produce results aligned with both specification precision and creativity. \r\n\r\nTogether, these contributions establish a coherent framework for AI-augmented design exploration. By viewing missing parameters as an opportunity for data-driven design autocompletion and by tightly integrating multimodal control over foundation models, this work elevates generative AI from a niche conceptual tool to a reliable design copilot. The implications of this thesis are profound: we show the possibilities and the pathways to AI copilot systems that can reduce data bottlenecks, broaden design spaces, and offer more thorough, constraint-adherent design candidates. As engineering problems grow in complexity and scale, the synergy of high-fidelity parametric imputation and multimodal control promises to accelerate innovation, cut development cycles, and guide human designers toward more inventive and manufacturable solutions.",
        "authors": [
            "Rui Zhou"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158805",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Radium and Mercury Dynamics in the Arctic: Investigating Terrestrial Inputs, Groundwater Discharge, and Chemical Cycling in a Changing Climate",
        "abstract": "The Arctic Ocean is distinctive due to its extreme seasonal variations in temperature and significant terrestrial inputs, including freshwater, carbon, nutrients, and toxins. Of particular concern is mercury (Hg) in its neurotoxic form, methylmercury (MeHg), which is already beginning to adversely affect Arctic human populations and wildlife. However, the region’s harsh conditions and remoteness have made conducting seasonal chemical and hydrological studies challenging. Tracers of boundary inputs, such as the radium (Ra) isotope quartet, offer potential for tracking and quantifying riverine and submarine groundwater discharge (SGD) of species like Hg into the Arctic Ocean. This thesis employs seasonal data and laboratory experiments to investigate the factors influencing terrestrial Ra inputs to the Arctic Ocean, quantifies SGD and associated Hg inputs to an Arctic coastal lagoon, and elucidates the chemical and geological factors influencing Hg cycling in Arctic groundwater.\r\n\r\nUsing historical and unpublished datasets combined with new laboratory investigations, differences in inputs of riverine Ra isotopes between the North American and Eurasian land masses were identified. The findings revealed higher Ra fluxes from the North American continent, attributed to greater sediment loads and lower organic matter in rivers compared to those on the Eurasian land mass. Subsequently, Ra data from five extensive field campaigns to Simpson Lagoon, Alaska, provided insights into Ra cycling on a more localized scale. These campaigns offered the first seasonal perspective on supra-permafrost SGD along an Arctic coastline, suggesting that SGD fluxes may rival those of rivers along the Beaufort Sea coast. Concurrently collected Hg groundwater concentrations allowed for the development of the first estimates of Hg fluxes from groundwater to the Arctic Ocean. If these estimates hold true along the rest of the Pan-Arctic coastline, they could significantly alter our understanding of microbial MeHg uptake in the Arctic Ocean. Finally, sediment cores from Simpson Lagoon and two other locations along the Beaufort Sea coast were used to examine how changing groundwater conditions, such as changing salinity, temperature, and redox conditions, influence Hg cycling. These experiments, alongside findings from Simpson Lagoon groundwater, indicate that Hg cycling in recently thawed permafrost sediments involves a complex interplay between organic material, metal oxides, and sulfide species, with groundwater conditions and soil carbon content playing crucial roles in Hg mobilization.",
        "authors": [
            "Emma Jacqueline Bullock"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158870",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Development of Dual Extruder Biomaterial 3D Printer",
        "abstract": "This research presents the design and fabrication of a novel dual-extruder biotic 3D printer for the precise deposition of natural biocomposites using organic materials such as pectin, chitosan, and cellulose. Unlike traditional FDM printers that rely on thermoplastic extrusion, this printer employs a syringe-based mechanical extruder capable of depositing viscous biomaterial hydrogels. The integration of a first-of-its-kind dual-extruder system enables the fabrication of multi-material prints and the exploration of biomaterial composites and complex geometric structures, thereby advancing sustainable, bio-inspired manufacturing.\r\nThis thesis emphasizes the machine engineering aspects of the printer's development, including project motivation, systematic design methodology, component design and fabrication, testing, and exploration of future work. Notable features of the system include user-friendly operation for non-experts, open-source accessibility, and compatibility with a wide range of biomaterials. By addressing existing limitations in biomaterial 3D printing technology, this work provides a robust platform to support future research in biomaterials, sustainable additive manufacturing, and bio-inspired design. Furthermore, the open-source nature of the printer fosters innovation and collaboration, accelerating the adoption of sustainable materials and manufacturing methods.",
        "authors": [
            "Jesse P. de Alva"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158914",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design of a cam and follower linear actuator for satellite optical systems",
        "abstract": "Optical systems for satellites are used to image and track the physical environment of earth from space. Where the optical system images can be controlled through the rotation and movement of the optical system. Optical alignment is achieved though linear actuators, which constrain different degrees of freedom of the optical system. Optical systems require precise alignment, meaning the linear actuators that align them must have precise resolutions. During satellite launch, the satellite experiences both high acceleration and large magnitude vibrations, \r\nwhich can damage equipment. Common precision actuation methods cannot meet the high stiffness required for these satellite linear actuators. A cam and follower linear actuator was \r\ndesigned to fulfill these stiffness and precision requirements. Through modeling the dynamic and kinematic interactions between the cam and follower, a cam shape was designed, and necessary materials were chosen. Next through analysis of process capabilities of available \r\nfabrication tools, manufacturing methods for different parts were selected. Finally, using components designed for testing, kinematic tests were conducted on the linear actuator. Testing \r\nof the actuator demonstrated it was capable of actuating with a precision of 9.15 microns. More testing is needed to understand the stiffness of the device.",
        "authors": [
            "Darrell Brown"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158847",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Uncovering the link between twin-twin interactions and damage nucleation in an (α+β) Ti alloy",
        "abstract": "Recently, a (α+β) Ti alloy was developed with an outstanding combination of both high strength and high ductility; however, the plasticity micromechanisms that lead to damage nucleation for this alloy had not yet been investigated in detail. In this work, post-mortem analysis and an in-situ SEM-EBSD tensile experiment were conducted to determine where damage was nucleating most frequently in the microstructure, and what deformation modes were associated with damage nucleation. Damage within primary α grains was found to be the most common, with most of these damage incidents occurring along {10̅12} twin-twin boundaries with a ~60° misorientation. The {10̅12} twinning mode is only activated in the localized neck, and twin activation is strongly dependent on initial crystallographic texture. The twinned domains are rotated such that prismatic slip is easier to activate, but prismatic slip transfer is unlikely across ~60° twin-twin boundaries due to geometric incompatibilities. The in-situ test revealed that a crack formed along a ~60° twin-twin boundary where slip was blocked. These findings provide new insights into how twin-twin interactions in Ti alloys can lead to damage nucleation and impact overall ductility.",
        "authors": [
            "Megan F. L. Cooper"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158903",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Forecasting the lift of a randomly maneuvering airfoil\r\nunder dynamic stall conditions, Re ∼ 10⁵",
        "abstract": "Dynamic stall is the abrupt flow separation from airfoils rapidly changing their orientation. This phenomenon, characterized by a delayed stall followed by a sharp drop in lift, has prompted efforts to prevent or delay it. This study aims to predict the lift of an airfoil randomly maneuvering under dynamic stall conditions by utilizing sparse surface pressure measurements, which we believe can maximize the effectiveness of various dynamic stall suppression techniques. Using data from large eddy simulations, we demonstrate that a long short-term memory network, fed with raw surface pressures, delivers accurate predictions. Also, a new method introduced here, IdDM, conclusively links the characteristic frequency range of pressure fluctuations that emerges during the dynamic stall to the chord-lengthscale vortex dynamics. However, further analysis suggests that the forecast predominantly relies on the lower frequency components tied to the airfoil motion, possibly because the vortex dynamics are dependent on and sensitive to the airfoil motion. Meanwhile, specific sensor locations are proven to be more informative than others in this random, unsteady flow, and we show that optimal sensor placement can be quickly determined using mutual information alone. It reveals that two pressure sensors positioned near the leading edge, one on each side of the airfoil, capture most of the information needed to predict lift. The lift can be predicted with sparse sensors because surface pressures are strongly correlated across the airfoil, with large-scale flow structures dominating the forces.",
        "authors": [
            "Donghyun Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158900",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Reasoning over Hierarchical Abstractions for Long-Horizon Planning in Robotics",
        "abstract": "We aim to enable robots to act intelligently in complex environments not explicitly designed around them. In order to do so, robots can simplify decision making by forming hierarchical abstractions of their world, and planning within those representations. However, in reality, the types of abstractions robots are able to build are often poorly aligned with the planning problems they must solve, which limits how useful those abstractions can be in efficient decision making. For example, autonomous agents struggle in many real world scenarios, particularly when their environments are large, cluttered with obstructions, or beset by uncertainty. These factors often imply that decisions made at higher levels of abstraction may not be easily refined to low level plans, leading to backtracking during either search or execution. In this thesis, we consider contributions which improve the efficiency and quality of long-horizon hierarchical planning in robotics. Specifically, we propose approaches which explicitly reason about the imperfections of the abstractions available to robots during planning, and show how those methods can improve performance on a variety of tasks and environments.\r\n\r\nThere are three primary settings for which we make contributions in this thesis. First, we will consider the problem of solving tasks in partially revealed environments, wherein our abstract plans cannot be known to be feasible until we attempt execution because the world is not fully known at planning time. To solve this problem, we first develop a high level planning representation which recognizes that actions that enter unknown space can either succeed or fail with some probability. The first contribution of this work is then to learn to predict the feasibility and cost of actions within that abstraction from visual input. We also describe a method for planning which uses these predictions, and we are able to show that our approach can generate plans that are significantly faster at completing tasks in unknown environments experimentally when compared with heuristic driven baselines. Next, we will discuss work in Task and Motion Planning (TAMP), where the world is fully known, but the problems require complex interaction with the environment to the point that we must intelligently guide search in order to find plans efficiently. We build upon our work in the first setting by once again learning to predict the outcome and cost of different sub-tasks within a TAMP abstraction. We further contribute a novel method to guide search in this setting for plans which minimize cost given our learned predictions, and demonstrate the ability to find faster plans than established TAMP approaches both in simulation, and on real world robots. In our final problem setting, we consider attempting to solve TAMP problems in real world, large-scale environments. To do this, we define an approach for constructing tractable planning abstractions from real perception using hierarchical scene graphs, ensuring that when we refine our abstract plans within these representations, the low-level trajectories still satisfy the given task’s constraints. A major contribution of this work is an approach for planning efficiently in these domains by pruning provably superfluous information from the world model. The unifying aim of the work in this thesis is to develop approaches which enable robots to solve complex tasks in large-scale, real world environments without human intervention. To that end, across all contributions, we demonstrate experimentally on real robots the importance of accounting for imperfections in hierarchical abstraction during planning.",
        "authors": [
            "Christopher P. Bradley"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158796",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Causal Inference with Survival Outcomes via Orthogonal Statistical Learning",
        "abstract": "The field of causal inference has recently made great strides in incorporating machine learning into confounding adjustment and estimation of heterogeneous treatment effects (HTE). However, there were some gaps regarding survival outcomes.\r\n\r\nFirst, overlap-weighted effect estimators based on machine learning nuisance models were not available for such outcomes. Thus, researchers wishing to mitigate bias and variance from poor overlap had to accept potential bias from nuisance model misspecification in its place. In Chapter 2, we fill this gap by proposing a class of one-step cross-fitted double/debiased machine learning estimators for cumulative weighted average treatment effects for both survival outcomes and competing risk outcomes. Our approach combines importance sampling, semiparametric theory, and Neyman orthogonality to resolve both model misspecification and lack of covariate overlap between treatment arms in observational studies with censored outcomes. We give regularity conditions for the consistency, asymptotic linearity, and semiparametric efficiency bounds of the proposed estimators. Through simulation, it is shown that the proposed estimators do not require oracle parametric nuisance models. We apply the proposed estimators to compare the effects of two first-line anti-diabetic drugs on cancer outcomes.\r\n\r\nSecond, a wide range of machine learning methods (or ”learners”) for estimating heterogeneous treatment effects were not applicable to estimating effects on survival outcomes, particularly in the presence of competing risks. In Chapter 3, we fill this gap by developing several once-for-all (orthogonal) censoring unbiased transformations which convert time-to-event data into continuous outcomes, such that all HTE learners and oracle rates for continuous outcomes can be borrowed. Our approach not only reduces the pressing need to develop various HTE learners for censored outcomes and especially competing risks, but also fully leverage the state of the art of existing schemes. Through direct application of HTE learners to these transformed continuous outcomes, we obtain consistent estimates of heterogeneous cumulative incidence effects, total effects, and separable direct effects. We provide generic model-free learner-specific oracle inequalities bounding the finite-sample excess risk. The oracle efficiency results depend on the oracle selector and estimated nuisance functions from all steps involved in the transformation. We demonstrate the empirical performance of the proposed methods in simulation studies.\r\n\r\nAn important application area for causal inference methods, and one which originally motivated my interest in the field, is drug repurposing. In Chapter 4, we apply the methods of Chapter 2 to investigate whether metformin, a diabetes medication, might also have unexpected beneficial effects on cancer. The analysis encountered three major challenges: poor overlap between treatment groups, model misspecification, and pre-cancer death as competing risks for cancer incidence. To resolve these issues simultaneously, we take balancingweighted total cause-specific effects, controlled direct effect, and separable effects as causal estimands and develop balancing-weighted double/debiased machine learning estimators for both cumulative incidence functions and restricted mean time lost, with all estimators satisfying Neyman orthogonality. Using the Clinical Practice Research Datalink (CPRD) data, we find that metformin revealed a preventive direct effect on cancer incidence over sulfonylureas. The results also demonstrate the advantage of choosing the average treatment effect for the overlap population as the target quantity.\r\n\r\nFinally, just as machine learning helps to automate nuisance model estimation for confounding adjustment and modeling effect heterogeneity, causally informed artificial intelligence (AI) and large language models (LLMs) might help to automate hypothesis generation for drug repurposing and surveillance opportunities. In Chapter 5, we explore this potential by developing a high-throughput screening approach to evaluate available drugs across multiple diseases. The screening methodology aims to identify drug-disease pairs with significant positive signals that could represent promising repurposing candidates, while also detecting pairs with negative signals that might indicate potential safety concerns–both being critical aspects for pharmacoepidemiology research. This systematic approach leverages the convergence of expanding healthcare data sources and modern data science advances to establish a data-driven framework for drug repurposing discovery and pharmacovigilance.\r\n\r\nTo conclude, we discuss the limitations of the proposed methods and provide possible future research directions.",
        "authors": [
            "Shenbo Xu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158798",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Examining the placenta’s role in neurodevelopment in the context of maternal obesity",
        "abstract": "The placenta is a key organ determining fetal development and likely contributes to programming of long-term offspring health, in particular neurodevelopment. Various maternal exposures, such as psychosocial stress, diabetes, infection, and high body mass index (BMI) are associated with higher risks of impaired neurodevelopment in the offspring. One third of women in the United States are affected by maternal obesity (MO) during pregnancy, making it one of the most common exposures.\r\nWe profiled the term placental transcriptome in humans using single-nucleus RNA-seq, comparing expression profiles in MO versus lean conditions, in each of the two faces of the placenta separately. On both sides of the placenta across several cell types, MO was associated with upregulation of hypoxia response genes. On only the maternal-facing side, hypoxia gene expression was associated with offspring neurodevelopment outcomes measured at multiple time-points, in the Genetics of Glucose regulation in Gestation and Growth (Gen3G) cohort, an independent pre-birth cohort with bulk RNA-seq from placental tissue. We leveraged Gen3G to determine genes that correlated with impaired neurodevelopment and found these genes to be most highly expressed in extravillous trophoblasts (EVTs). EVTs further showed the strongest correlation between neurodevelopment impairment gene scores (NDIGSs) and the hypoxia gene score. We validated these findings in EVTs in an independent single-cell RNA-seq cohort from second trimester placenta, and found that cultured EVTs have increased NDIGSs in response to exposure to hypoxia. These data suggest that hypoxia in EVTs may be a key process in the neurodevelopmental programming of fetal exposure to MO. Our work opens up new directions of research, such as exploring applications of antioxidants to potentially mitigate some of the offspring consequences associated with MO.",
        "authors": [
            "Fatima M. Gunter-Rahman"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158860",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Marketplace Multiculturalism",
        "abstract": "Picture Texas. No longer simply cowboys, footballs, and firearms, this land today is sustained by a daily choreography of cross-border commerce, managed by entertainment media turned handheld surveillance, and peppered with enclaves of immigrants from the world over. A contact zone where logistical and legislative apparati warp to serve consumer comfort, Texas today is the world tomorrow: forget the Alamo, it’s highways, tax-incentives, and backyard barbecue on the 21st century frontier. This thesis responds to a call for roadside service stations along a planned international tourist corridor in the Texas-Mexico borderlands with six interventions: a panoramic viewing tower disguised as a billboard, a sunken stadium for athletic agonism, a photovoltaic drive-in charging cinema, an international culinary incubator, a showroom for automated fulfilment, and a customs and border patrol welcome center. These structures are testing grounds for modes of relation and value exchange that edge beyond the outdated positivisms of globalization. They ask how architecture might produce new possibilities and publics by working within and taking advantage of contemporary systems of control. As tourist destinations, the stops suggest the nation’s true mythos lies not in static symbols but in choreographies of transaction and contact. Articulating in built form the dynamic processes that define a territory of sprawl, this proposal suggests that Texas’s most authentic monuments are the stops we make along the way.",
        "authors": [
            "Harris Chowdhary"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158823",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Predicting Flood Risks to City Infrastructure Systems\r\nUtilizing Scalable, Time Sensitive Modeling",
        "abstract": "Flooding is emerging as the most expensive and frequent natural hazard around the world. Floods are highly dynamic in nature and cause physical damage to our built environment, loss of life, economic damage, and major impacts to society. An example of this is the at-ground road system, which comprises 30-60% of a city’s area in the US, is highly susceptible to flood damage, while still needs to act as evacuation routes for local residents. Similarly, the underground built system is extremely vulnerable to flooding damage as well as life risk to anyone within it. With urban landscapes constantly evolving, accurately predicting flood propagation and extent is imperative to mitigate these risks, especially as floods worsen due to climate change.\r\n\r\nHistorically, the focus of flood risk assessment through industry and academia has been on the coastal urban environment, assessing the impact of fluvial flooding. This resulted in many risk assessment tools that mostly caters to the infinite amount of flood water identified from a riverine or coastal fluvial flooding. As for the rain-driven impact, the common practice simply changed the flood modeling to pluvial oriented, keeping the rest of the risk tool components identical for the different flood mechanisms. For pluvial flooding, existing urban flood modeling tools such as SWMM and PC-SWMM are limited by their catchment-based approach, neglecting surface runoff dynamics and spatial-temporal flood impacts. Consequently, these tools fail to capture the full extent of rain-driven floods, underestimating their severity and impact on urban environments.\r\n\r\nAddressing this gap requires sophisticated simulations that account for rain event characteristics and city morphology, yet such simulations are computationally demanding and require detailed urban data. Currently, flood impact analysis tools lack specificity for pluvial flood risks and do not address the risks to various city systems beyond building damage. As a result, the contribution of pluvial floods to overall flood risks is underestimated, compromising infrastructure resilience. As flood model results are a critical component in flood risk assessments, the accuracy of spatial temporal urban flood results will allow the pluvial flood impact assessment to be simplified and the flood damage to the different urban systems will be quantified.\r\n\r\nThis research aims to develop a scalable and streamlined method to accurately quantify the risks of rain-driven floods to urban infrastructure systems. It addresses three key questions: (1) To what extent does current practice underestimate pluvial flood impacts? (2) What are the impacts of pluvial flooding on pavement systems when incorporating spatial-temporal modeling? (3) What is the significance of modeling pluvial floods using urban underground spaces? Using advanced flood modeling and numerical soil-water infiltration techniques, this research will quantify damages and lifecycle impacts to pavement and underground spaces systems. The method will provide information on the spatial and temporal distribution of flood damage and will enable scaling up single-element assessments to system-wide impacts. This holistic approach will improve urban flood risk management, supporting informed decision-making and the development of resilient infrastructure systems.",
        "authors": [
            "Katerina Boukin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158861",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "(De)fluorination of Organic Substrates Mediated by Nontrigonal Phosphorus Triamide",
        "abstract": "Due to its high electronegativity and small size, fluorine atoms form the strongest single bond to carbon, and impart unique physical, chemical, and physiological properties to organic compounds. Therefore, the number of industrially synthesized products containing fluorine has seen a substantial increase in recent decades. The strategies to access organofluorine compounds include two opposite approaches: 1) (nucleophilic, electrophilic, or radical) fluorination, and 2) selective defluorination of polyfluorinated substrates. Both creating and breaking C−F bonds in selective manners are of great importance, and present challenges of their own. The work herein describes chemical transformations incorporating the cleavage or formation of the C−F bonds mediated by nontrigonal phosphorus triamide. Thanks to the enhanced biphilicity resulting from geometric deformation, the Cs-symmetric tricoordinate phosphorus compound can activate strong covalent bonds.\r\n\r\nAt the outset, Chapter 1 reviews the existing literature on (de)fluorinative chemical transformations focused on deoxyfluorination and hydrodefluorination, as well as examples of nontrigonal tricoordinate phosphorus compounds and their characteristic reactivity. Combining the two approaches, Chapters 2 and 3 introduce method development for accessing organofluorine compounds using a butterfly-shaped phosphorus triamide as a bond activator. In Chapter 2, the method for deoxyfluorination of aliphatic alcohol substrates via O−H activation by phosphorus, catalyzed by borane Lewis acids, is detailed. The scope of the method covers tertiary alkyl fluorides, which are generally challenging targets, selectively yielding stereoinversion products for chiral substrates. Chapter 3 reports a closed P(III)/P(V) synthetic cycle for the hydrodefluorination of polyfluoroarene substrates that consists of C−F oxidative addition, F-to-H ligand metathesis, and C−H reductive elimination. The overall sequence is analogous to transition metal-catalyzed aryl cross-coupling reactions. Taken together, the methods described in this dissertation highlight the potential of nontrigonal phosphorus compounds as a mediator for the manipulation of strong covalent bonds, useful in the development of synthetic methods that complement existing ones.",
        "authors": [
            "Soohyun Lim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158937",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design and Engineering of Protected Superconducting\r\nQubits",
        "abstract": "Building extensible quantum information processors becomes increasingly promising as the qubits exhibit longer coherence times. To this end, realizing protected qubits, whose Hamiltonians are inherently resilient to both relaxation and dephasing, has attracted strong interest. In this thesis, we primarily explore the soft 0 − π qubit, a leading candidate for implementing superconducting qubit protection with current fabrication techniques. To enhance protection, the soft 0 − π qubit requires its two major modes, the charge-mode (θ) and the flux-mode (ϕ), to satisfy an asymmetric condition: maximizing charge-mode capacitance while minimizing flux-mode capacitance. The main challenge is therefore reducing stray capacitance from the large charge-mode capacitor, which hinders the reduction of flux-mode capacitance. To address this challenge, we depart from the conventional coplanar interdigitated capacitor design and use parallel-plate capacitors (PPC) with small footprints, achieving the desired large charge-mode capacitance while reducing unwanted stray capacitances. By reducing the capacitor area by a factor of approximately 50, the PPC 0−π qubit has achieved an estimated Eᵠ_C /Eᶿ_C ratio of 30–50, placing it among the highest reported. Additionally, we propose enhanced mode-selective control of the soft 0−π qubit using these parallel-plate capacitors. Finally, we discuss the remaining challenges of the soft 0−π qubit and introduce alternative parameter regimes that can potentially improve Raman-based control and qubit readout.",
        "authors": [
            "Junghyun Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158919",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Freight Distribution During Disasters: Measuring and Improving Operational Performance of Critical Systems",
        "abstract": "The frequency and intensity of weather-related natural disasters have increased in the last őve decades. Moreover, the US faces more than a third of the disaster-related economic losses globally, majority of which are from storms. As the demand for distribution of essential freight increases during disasters, the physical and operational constraints decrease the capacity of the freight distribution systems. Accordingly, public and private-sector stakeholders seek disaster preparedness and response interventions to ensure timely and economic distribution of vital freight to the population in need. The goal of this thesis is to facilitate better strategic and tactical planning that results in higher operational performance of essential freight distribution systems during disasters. We study two critical freight distribution systems, namely, downstream fuel distribution and full truckload transportation of general freight. Truckload transportation plays a vital role in distributing relief supplies during emergencies, and fuel is required for humanitarian operations such as running generators, moving emergency response crews, and evacuation of the affected population. We collaborate with The US Federal Emergency Management Agency in response to multiple North-Atlantic storms and measure the operational performance of these systems under regular and disaster conditions, as well as identify public and private-sector interventions to make the performance better during future disasters. Our research contributes to the bodies of disaster modeling and management, fuel distribution, service procurement, and truckload procurement literature by, i) creating system level understanding of multi-server tandem cyclic queues with time-limited customers, ii) studying process improvement interventions for disasters, iii) quantifying the magnitude, geographical extent, timing, and duration of the causal effects of disaster conditions and consequent disaster relief activities on transportation procurement prices, iv) using datadriven analysis to design ŕexible truckload contracts that consider uncertainty in demand, and v) modeling dynamic-pricing where the buyer offers the price to service providers. In this thesis, we provide several actionable insights for public and private-sector stakeholders to manage freight distribution during future disasters. We identify which process improvement interventions are best suited for which type of downstream fuel distribution system, and which storage terminals should be prioritized under limited budget. We also measure how private-sector shippers should account for changes in truckload spot procurement prices during disaster episodes to manage their budgets and operational decisions. Moreover, we offer an alternate dynamic-priced truckload contract solution for public-sector shippers that deal with uncertain episodic demand in response to disasters. We demonstrate the impact of our research by implementing it to multiple real-life case studies in the US. Furthermore, our methodologies and results are generalizable to other geographical regions as well as other disaster conditions. Thus, we hope that they are used by public and private-sector actors to better manage essential freight distribution moving forward.",
        "authors": [
            "Shraddha Rana"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158803",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design of Lewis Acidic Pnictenium Ions Using Carbone and Capping Arene Ligands for Bond Functionalization",
        "abstract": "Interest in the chemistry of antimony and bismuth is rapidly growing due to isolation of low coordinate, subvalent or Lewis acidic compounds that can mediate reactivity traditionally reserved for their d block counterparts. Ligand strategies play a key role in the isolation of such species. Anionic ligands with large steric profiles, as well as carbenes, have been widely implemented to stabilize subvalent heavy group 15 element compounds. However, synthetic strategies to prepare Lewis acidic antimony and bismuth complexes remain underexplored. Cationization is one of the most common methods used to enhance the Lewis acidity of heavy group 15 elements by creating a vacant p orbital on the pnictogen atom. Lewis acids are also employed in frustrated Lewis pair (FLP) chemistry to enable intra- and intermolecular reactivity. Carbone ligands, which are neutral, 4 electron donor ligands, offer a unique ability to support highly electrophilic main-group elements. This dissertation investigates the stabilization of heavy pnictenium ions using neutral donor ligands, such as carbodicarbenes and capping arene ligands, and explores their potential in Lewis acid-mediated chemistry. In Chapter Two, the synthesis and characterization of a series carbodicarbene-pnictenium ions is described. The utilization of strongly donating carbodicarbene ligands enables the isolation of mono-, di- and tri-cationic antimony and bismuth cations. These ions have multiple bond character between carbon and antimony/bismuth, representing some of the first examples of stibaalkene and bismaalkene cationic compounds. The Lewis acidity of these ions was assessed using the Gutmann-Beckett method and computationally derived fluoride ion affinities, the latter of which indicates Lewis superacidity for the bis(pyridyl)carbodicarbene-pnictenium trications. In Chapter Three, the reactivity of the bis(pyridyl)-carbodicarbene stibenium trication toward C(sp³)–H and C(sp)–H bonds is demonstrated. The Lewis superacidic antimony cation mimics the chemistry of frustrated Lewis pairs in the presence of the sterically encumbered base 2,6-di-tert-butylpyridine to enable C–H bond breaking of acetonitrile and a set of terminal alkynes. Kinetic analyses, in conjunction with density functional theory, support a mechanism by which acetonitrile coordinates to antimony, acidifying the C–H bonds, which can be subsequently deprotonated by the base in solution. The resulting stiba-methylene nitrile and stiba-alkynyl adducts undergo reactivity with elemental iodine to generate iodoacetonitrile and 1-iodoalkynes while reforming a stibenium trication. In Chapter Four, capping arene ligands are coordinated to antimony and bismuth tribromide to afford a series of κ²-bound complexes. Bromide abstraction from these neutral adducts affords ionic compounds. Both the neutral and ionic species have distinctive Menschutkin interactions, whereby the lone pair on the pnictogen atom is oriented toward the π system of the pendant arene. Shortening of the distances between the pyridyl nitrogen atoms and pnictogen atom are observed upon cationization from the neutral adducts. The Lewis acidity of these complexes was assessed using the Gutmann-Beckett method. Notably, acceptor numbers as high as 111 are observed for these ions.",
        "authors": [
            "Levi Warring"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158947",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Underwater Semantic Simultaneous Localization and\r\nMapping",
        "abstract": "Building semantically meaningful object level maps of underwater environments is crucial for enabling higher-level autonomy, fostering human-robot collaboration, and providing compressed map representations for bandwidth-constrained underwater communications, while localizing against such maps can improve the positioning accuracy of underwater vehicles by correcting for odometric drift. However, underwater semantic simultaneous localization and mapping (SLAM) has lagged behind analogous terrestrial and aerial semantic SLAM techniques largely due to the lack of large labeled underwater datasets and the challenging sensor modalities specific to underwater environments. To address these shortcomings, this thesis develops a range of methodologies to advance underwater semantic SLAM capabilities. \r\n\r\nFirst, self-supervised learning and visual foundation models are leveraged to detect and segment underwater objects in an open-set manner, i.e., objects need not be present in the training dataset to be detected. The machinery of the open-set object detection technique breaks several assumptions made by existing closed-set semantic SLAM methods. Thus, new methods for object representation and data association are proposed and demonstrated. A method to localize underwater objects is then developed through an analysis of the geometry of underwater monocular cameras and multibeam sonars. \r\n\r\nFinally, a formulation of open-set object-level place recognition as a graph matching problem is introduced. The formulation includes a method for calculating and tracking semantic uncertainty for open-set object detections. Experimental results on both underwater and terrestrial datasets demonstrate that the proposed formulation can be used for real-time accurate open-set object-based place recognition. \r\n\r\nIn summary, techniques for underwater object detection, localization, and data association are introduced and integrated with probabilistic graphical models for open-set semantic SLAM. The proposed techniques are tested across a wide variety of scenarios, and are shown to generalize to terrestrial settings as well.",
        "authors": [
            "Kurran Singh"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158852",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "CO₂ Capture with Lithium Oxide in Molten Salt Media : A Case Study of CO₂ Capture via Electrochemically Produced Metal Oxide",
        "abstract": "As the unprecedented temperature rise originating from anthropogenic carbon dioxide (CO₂) emission intensifies, the development of post-combustion carbon capture technologies has been urged. Although its maturity, conventional thermal swing processes using aqueous amines, suffer from significant limitations, including high energy requirements and sorbent degradation. Electrochemical CO₂ capture technologies, which use electrical energy instead of thermal energy, have emerged as an energy efficient way to capture CO₂. This shift not only improves energy efficiency but also reduces reliance on fossil fuels, further contributing to reduction in CO₂ emissions. This work explored the potential of electrochemical metal oxide formation for CO₂ capture, a promising alternative to amine-based systems due to its exceptional sorbent (i.e., metal oxide) stability. Li₂O in eutectic mixture of potassium nitrate (KNO₃) and lithium nitrate (LiNO₃) was chosen as a case study due to the relatively well-understood chemistry of the system and the potential synergistic effects between metal oxide and the molten salt. Primarily, we investigated the synergistic effect of Li₂O in nitrate molten salt via thermal gravimetric analysis. Next, electrochemically produced Li₂O by reduction of oxygen gas was tested as a CO₂ sorbent while investigating parameters affecting its conversion to lithium carbonate (Li₂CO₃). Through this study, we suggested dissolution model as a crucial pathway for conversion. Lastly, we explored the effect of adding nitrite ion (NO₂⁻) to the molten salt. Irreversible side reaction between NO₂⁻ and CO₂ was confirmed with X-ray diffraction and NOₓ measurement. This thesis demonstrates the feasibility of electrochemical metal oxide-based CO₂ capture, highlighting some considerations in the capture step.",
        "authors": [
            "Gi Hyun Byun"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158875",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Learning-Based Complex Terrain Navigation Under Uncertainty",
        "abstract": "In complex off-road environments, accurately identifying traversable terrain is crucial for achieving fast and reliable navigation. Existing methods learn terrain properties directly from data via self-supervision to automatically penalize trajectories moving through undesirable terrain. However, challenges remain in properly quantifying and mitigating risk due to uncertainty in learned models and improving model generalization in novel environments. To address these challenges, this thesis presents a unified framework to learn uncertainty-aware, physics-informed traversability models and achieve risk-aware navigation in both indistribution and out-of-distribution terrain. First, the proposed method efficiently quantifies both aleatoric and epistemic uncertainty by learning discrete traversability distributions and probability densities of the traversability predictor’s latent features. Leveraging evidential deep learning, this work parameterizes Dirichlet distributions with network outputs and proposes a novel uncertainty-aware squared Earth Mover’s distance loss with a closed-form expression that enhances learning accuracy and navigation performance. Second, the proposed method achieves risk-aware navigation by simulating state trajectories with the worst-case expected traversability values to handle aleatoric uncertainty and by penalizing trajectories moving through novel terrain with high epistemic uncertainty. Third, the proposed method improves model generalization by embedding physics priors directly into the mathematical formulation of evidential neural networks and implicitly aligning learned models with physics models through a physics-informed training loss. Finally, through extensive simulation and real-world experiments on wheeled and quadruped robots, it is demonstrated that this work leads to faster navigation with higher success rates when compared to existing risk-aware approaches, even in environments with significant distribution shifts.",
        "authors": [
            "Xiaoyi Cai"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158876",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design-technology Co-optimization for Sub-2 nm Technology Node Based on 2D Materials",
        "abstract": "Emerging disruptive technologies such as Artificial Intelligence (AI) and 6G communications have driven stringent demands for hardware components that enable faster and more energy-efficient computation. With the diminishing returns of traditional silicon-based scaling and the escalating complexity of advanced semiconductor processes, two-dimensional (2D) materials offer promising opportunities when developed through Design-Technology Co-Optimization (DTCO). This thesis presents a comprehensive study of DTCO with a novel framework tailored for 2D material-based electronics that addresses critical challenges in material synthesis, device design, and circuit integration. In this framework, experimental material and device data are integrated into the design and optimization of MoS₂-based multichannel transistors (MCTs). With the help of DTCO, we have achieved record performance for double-gate, single-channel MoS₂ transistors as well as the first demonstration of high-performance, functional double channel MoS₂ transistors. Based on the results of MCTs, a Process Design Kit (PDK) is developed to facilitate circuit-level integration. These advancements constitute a promising foundation for the development of next-generation electronics beyond sub-2 nm technology node.",
        "authors": [
            "Aijia Yao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158931",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On Hing Travel Agency Fictional Archive of Disappearing Hong Kong",
        "abstract": "Hong Kong, shaped by rapid transformation and precarious land ownership, is a city where erasure defines its urban landscape. Amid this flux, a place I once called home was demolished, prompting the question: “How can one return to a place that no longer exists?” This thesis explores the transformative potential of disappearance, reframing it as a generative force that creates space for imagination, resistance, and continuity. Through On Hing Travel Agency (OHTA), demolished buildings \"travel\" into fictional worlds, becoming vessels of memory and imagination. Rooted in Hong Kong’s literary tradition—where fiction resists erasure and archives aspirations—the project employs fiction as both a tool of preservation and a site for belonging. Fictional destinations, inspired by Hong Kong novels, such as The Permanent City (1959), The Floating City (1986), and The Vanished Cities (2010), reflect pivotal historical moments while offering pathways to reconcile personal loss and master alternative spatial logics. The project culminates in the Lost Traveler’s Guide to Hong Kong, a publication curating maps, brochures, and layered narratives to immerse travelers in speculative thinking. By bridging the past and future, real and imagined, OHTA is a attempt to demonstrates how fiction can reclaim agency within the politics of disappearance, transforming loss into a catalyst for new narratives and creative engagement. Even in absence, Hong Kong’s disappearing spaces retain their resonance, generating new narratives and underscoring the creative potential of loss.",
        "authors": [
            "Ina Wu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158840",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Investigating the Atmospheric and Oceanic Drivers of Atlantic Multidecadal Variability and Predictability",
        "abstract": "Despite its numerous impacts across the Earth system, the relative importance of ocean and atmospheric dynamics in generating Atlantic Multidecadal Variability (AMV) remains an open question. This thesis presents three pathways to understanding how oceanic and atmospheric processes generate key spatio-temporal signatures of AMV through a combination of processed-based and data-driven approaches. Part 1 (Chapter 2) takes a \"bottom up\" approach, building a hierarchy of stochastic models to identify the contributions of vertical entrainment and seasonality in local upper-ocean processes to sea surface temperature (SST) variability. Through this hierarchy, I highlight unrealistic features present in slab ocean models widely used to isolate atmospheric contributions to AMV. On the opposite end of the spectrum, Part 2 (Chapter 3) utilizes a \"top-down\" data-driven approach where deep neural networks are trained to predict the North Atlantic SST Index in both the Community Earth System Model 1 Large Ensemble (CESM1) and observation-based datasets using atmospheric and oceanic predictors. I apply explainable artificial intelligence techniques to highlight a significant source of multidecadal predictability over the Transition Zone in oceanic predictors such as sea surface salinity (SSS) and sea surface height in the presence of external forcings. Part 3 (Chapter 4) returns to the process-based hierarchy, but applies this to understanding SSS variability. The stochastic salinity model is used to investigate the role of mixed-layer re-emergence, subsurface ocean damping and SST-evaporation feedback in shaping the pattern and amplitude of AMV.",
        "authors": [
            "Glenn Yu-zu Liu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158897",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Weak Shock Waves on a Chip: Generation and Applications",
        "abstract": "In conventional laser-shock experiments in solid media, shock waves are typically excited from the ablation of a photoacoustic transducer layer deposited onto the sample of interest. Unavoidably, the target materials are damaged. This leads to the necessity of changing targets after each exposure, likely lowering the shot-to-shot reproducibility and data quality, while lowering the throughput of the experiment. Motivated by the need to generate large-amplitude transient strain waves at a high repetition rate, this thesis introduces a novel platform for the non-destructive generation and amplification of acoustic waves with associated strain levels in the percent range — up to the formation of shock waves. The acoustic amplification scheme is first described. Then, owing to the capabilities of the technique to repeatedly load a material with finite-amplitude strain waves, a demonstration of the use of the platform for microscale fatigue testing is made. Finally, the strain localization of surface acoustic waves is leveraged by transiently modulating a monolayer of a transition metal dichalcogenide deposited on a substrate.",
        "authors": [
            "Jude Deschamps"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158942",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Planning Beyond Crisis: The Promise of Insurgent Planning in Post-Disaster Mocoa",
        "abstract": "During the evening of March 31, 2017, a catastrophic landslide engulfed the Colombian city of Mocoa, killing at least 335 people in roughly thirty minutes. Seventy others disappeared, and over one hundred people were reported injured across 48 neighborhoods, and roughly 1,500 housing units were destroyed. With a total of 22,000 people impacted, this catastrophe was the deadliest disaster affecting Colombia in recent decades. Yet, despite an alignment of major national political commitments, international cooperation, and a multi-million-dollar humanitarian budget, reconstruction plans have not been completed seven years later. Why? As the first comprehensive analysis of the landslide and its aftermath, this dissertation is a novel investigation into the competing forces that ultimately canceled the central reconstruction plan, demonstrating that the kind of disruption caused by the disaster mobilized new actors and new forms of agency. In contrast to the popular perception that this kind of lack of remediation suggests the failure of urban governance, the dissertation speaks to the success of activists who have neutralized the government’s reconstruction plan, which activists perceived as worsening the circumstances leading up to both the catastrophe and recovery. Distinguishing between the “landslide” and the “larger disaster,” the dissertation further explains the government’s proposed reconstruction plan within a history of violent extraction, dispossession and displacement. Framing an original case consisting of fifteen planning vignettes to trace actions, reactions and counteractions, I expose the reduction of the planning process as crisis urbanism. My research contributes to our understanding of variability among insurgent planning actors and their invented spaces for engagement in the context of disaster, by defining technocratic resistance as a valid form of dissent inside the government, and by proposing a new device for the study of insurgent planning called transformative spaces enabling local community’s right to plan. Drawing on contemporary debates on anti-crisis, risk and decolonial thought, the dissertation imagines an alternative paradigm for planning beyond crisis that enables radical community action through dissenting grassroots leadership. \r\n\r\nKeywords: crisis urbanism, technocratic resistance, insurgent planning, regenerative planning, anti-crisis, risk, decolonial thought",
        "authors": [
            "Juan Camilo Osorio Botero"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158822",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Dynamics of Diversity, Equity, and Inclusion Practice Adoption",
        "abstract": "Despite the widespread adoption of Diversity, Equity, and Inclusion (DEI) initiatives in corporate America, significant disparities persist in the representation, compensation, and treatment of women and racial minorities. This paper investigates why well-intentioned DEI efforts often fail to achieve their intended outcomes and identifies managerial barriers to progress. This research employs a qualitative dynamic modeling approach to analyze the complexities of DEI practice implementation within organizations. I conducted a scoping review, focusing on longitudinal and experimental designs to identify key mechanisms influencing the outcomes of DEI practices. The interplay between organizational processes and individual cognitive and behavioral responses can be illustrated via reinforcing and balancing feedback loops that I map onto a causal loop diagram, which reveals how DEI initiatives interact with existing organizational processes and cultural dynamics. This paper introduces a dynamic perspective on DEI practice implementation, highlighting the feedback mechanisms that can either hinder or facilitate progress toward diversity goals. The model reveals that certain DEI practices may inadvertently trigger reinforcing loops that perpetuate inequality. By mapping DEI practices and their effects, this study provides a framework for understanding how DEI outcomes can diverge significantly depending on different implementation strategies. It underscores the importance of considering the endogenous feedback effects of DEI initiatives and offers insights into strategic interventions that can disrupt undesirable reinforcing cycles and promote progress toward organizational diversity, equity, and inclusion.",
        "authors": [
            "Aishwarya Pandey Yadama"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158834",
        "group_name": "Cireng Crispy"
    }
]