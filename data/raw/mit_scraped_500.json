[
    {
        "title": "The American Institute for Manufacturing Integrated Photonics: advancing the ecosystem",
        "abstract": "The American Institute for Manufacturing Integrated Photonics (AIM Photonics) is focused on developing an end-to-end integrated photonics ecosystem in the U.S., including domestic foundry access, integrated design tools, automated packaging, assembly and test, and workforce development. This paper describes how the institute has been structured to achieve these goals, with an emphasis on advancing the integrated photonics ecosystem. Additionally, it briefly highlights several of the technological development targets that have been identified to provide enabling advances in the manufacture and application of integrated photonics.",
        "authors": [
            "Thomas L. Koch",
            "Michael Liehr",
            "Douglas Coolbaugh",
            "John E. Bowers",
            "Rod Alferness",
            "Michael Watts",
            "Lionel C Kimerling"
        ],
        "journal_conference_name": "Broadband Access Communication Technologies X",
        "publisher": "SPIE",
        "year": "2106",
        "doi": "http://hdl.handle.net/1721.1/112987",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "How do we interpret the outputs of a neural network trained on classification?",
        "abstract": "Deep neural networks are widely used for classification tasks, but the interpretation of their output activations is often unclear. This tutorial article explains\r\nhow these outputs can be understood as approximations of the Bayesian posterior.\r\nWe showed that, in theory, the loss function for classification tasks – derived by\r\nmaximum likelihood – is minimized by the Bayesian posterior. We conducted\r\nempirical studies training neural networks to classify synthetic data from a known\r\ngenerative model. In a simple classification task, the network closely approximates the theoretically derived posterior. However, a few changes in the task can\r\nmake accurate approximation much more difficult. The ability of the networks to\r\napproximate the posterior depends on multiple factors, such as the complexity of\r\nthe posterior and whether there is sufficient data for learning.",
        "authors": [
            "Yudi Xie"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "International Conference on Learning Representations",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159032",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Half-Space Intersection Properties for Minimal Hypersurfaces",
        "abstract": "We prove “half-space” intersection properties in three settings: the hemisphere, half-geodesic balls in space forms, and certain subsets of Gaussian space. For instance, any two embedded minimal hypersurfaces in the sphere must intersect in every closed hemisphere. Two approaches are developed: one using classifications of stable minimal hypersurfaces, and the second using conformal change and comparison geometry for α -Bakry-Émery-Ricci curvature. Our methods yield the analogous intersection properties for free boundary minimal hypersurfaces in space form balls, even when the interior or boundary curvature may be negative. Finally, Colding and Minicozzi recently showed that any two embedded shrinkers of dimension n must intersect in a large enough Euclidean ball of radius R(n). We show that R ( n ) ≤ 2 n.",
        "authors": [
            "Keaton Naff",
            "Jonathan J. Zhu"
        ],
        "journal_conference_name": "The Journal of Geometric Analysis",
        "publisher": "Springer US",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159178",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Pooling solvent mixtures for solvation free energy predictions",
        "abstract": "Solvation free energy is an important design parameter in reaction kinetics and separation processes, making it a critical property to predict during process development. In previous research, directed message passing neural networks (D-MPNN) have successfully been used to predict solvation free energies and enthalpies in organic solvents. However, solvent mixtures provide greater flexibility for optimizing solvent interactions than monosolvents. This work aims to extend our previous models to mixtures. To handle mixtures in a permutation invariant manner we propose a pooling function; MolPool. With this pooling function, the machine learning models can learn and predict solvation energy and enthalpy for an arbitrary number of molecules in the mixed solvent. The novel SolProp-mix software that applies MolPool to D-MPNN was compared to state-of-the-art architectures for predicting mixture properties and validated with our new database of COSMOtherm calculations; BinarySolv-QM. To improve predictions towards experimental accuracy, the network was then fine-tuned on experimental data in monosolvents. To demonstrate the benefit of this transfer learning methodology, experimental datasets of solvation free energies in binary (BinarySolv-Exp) and ternary (TernarySolv-Exp) solvent mixtures were compiled from data on vapor–liquid equilibria and activity coefficients. The neural network performed comparable in accuracy to the benchmark of COSMOtherm calculations with an MAE of 0.29 kcal/mol and an RMSE of 0.45 kcal/mol for binary mixed solvents. Additionally, the ability to capture trends for a varying mixture composition was validated successfully. Our model’s ability to accurately predict mixture properties from the combination of in silico data and pure component experimental data is promising given the scarcity of experimental data for mixtures in many fields.",
        "authors": [
            "Roel J. Leenhouts",
            "Nathan Morgan",
            "Emad Al Ibrahim",
            "William H. Green",
            "Florence H. Vermeire"
        ],
        "journal_conference_name": "Chemical Engineering Journal",
        "publisher": "Elsevier BV",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159176",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Geospatial Trucking Industry Decarbonization Explorer (Geo-TIDE): Technical Guide and Methodology",
        "abstract": "Geo-TIDE is a public, interactive decision-support tool developed by the MIT Climate &\r\nSustainability Consortium (MCSC) to help trucking industry stakeholders identify and evaluate early opportunities for fleet and infrastructure decarbonization. By integrating public geospatial datasets such as regional freight flows, policy incentives, and spatially resolved cost and emissions models, Geo-TIDE enables data-driven decisions about where, when, and how to invest in low-carbon technologies. In this technical guide, Danika Eamer (who has led the development of Geo-TIDE) and co-authors Micah Borrero, Brooke Bao, Brilant Kasami, and Helena De Figueiredo Valente detail the tool’s functionality, showcase real-world usage scenarios, and explore the methodology behind its evolution and development.",
        "authors": [
            "Danika Eamer",
            "Micah Borrero",
            "Brooke Bao",
            "Brilant Kasami",
            "Helena De Figueiredo Valente"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159069",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Reducing Proliferation Risks with High-Assay Low-Enriched Uranium Fuels in Reactors with Coated-Particle (TRISO) Fuels",
        "abstract": "The use of graphite-matrix tri-structural-isotropic (TRISO) fuels in high-temperature reactors with high-assay low-enriched uranium (HALEU) can significantly reduce nuclear weapons proliferation risks relative to other fuels and reactor types. The HALEU fuel, with fuels containing 15% to 20% 235U enable used nuclear fuels (UNFs) with thermal neutron–spectrum burnups between 150 000 and 200 000 MWd per ton. At these high burnups, the plutonium isotopics make the direct use for nuclear weapons unattractive and the uranium isotopics unattractive as a feed to a uranium-enrichment plant. On the front end, it would require the theft of ~150 000 pebbles with uranium just under 20% 235U to create the theoretical potential to produce sufficient material for one weapon (1000 kg), which is about a 2-year supply of fuel for these reactors.\r\n\r\nThe chemical and mechanical processing requirements to convert fresh TRISO fuel to uranium metal for use in a nuclear weapon are beyond nonstate actors. Over 10 sequential chemical process steps would be required, plus uranium recovery from waste streams, to avoid large uranium losses in the conversion processes. If a nation-state wanted to make a nuclear weapon starting with HALEU fuel, they would enrich the HALEU from 19.95% to over 90% 235U, which presumes they already possess enrichment capabilities and can use any uranium feedstock. If enriched to weapons-grade 235U, 1 ton of HALEU has sufficient 235U for multiple weapons.\r\n\r\nSeparately, it is not clear if a weapon can actually be built with HALEU fuel. The fuel characteristics also reduce risks from sabotage. Consequently, we conclude that reactor safeguards for fresh HALEU TRISO fuel can be similar to those for low-enriched uranium light water reactor fuel; that is, no requirements for added security or other measures. TRISO UNF safeguards and security can be significantly relaxed relative to the requirements for other types of UNF at the reactor site.",
        "authors": [
            "Charles Forsberg",
            "Andrew Kadak"
        ],
        "journal_conference_name": "Nuclear Technology",
        "publisher": "Taylor & Francis",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159161",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "AGM aquariums and elliptic curves over arbitrary finite fields",
        "abstract": "In this paper, we define a version of the arithmetic-geometric mean (AGM) function for arbitrary finite fields F q , and study the resulting AGM graph with points ( a , b ) ∈ F q × F q and directed edges between points (a, b), ( a + b 2 , ab ) and (a, b), ( a + b 2 , - ab ) . The points in this graph are naturally associated to elliptic curves over F q in Legendre normal form, with the AGM function defining a 2-isogeny between the associated curves. We use this correspondence to prove several results on the structure, size, and multiplicity of the connected components in the AGM graph.",
        "authors": [
            "June Kayath",
            "Connor Lane",
            "Ben Neifeld",
            "Tianyu Ni",
            "Hui Xue"
        ],
        "journal_conference_name": "Research in Number Theory",
        "publisher": "Springer International Publishing",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159072",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Ordering Candidates via Vantage Points",
        "abstract": "Given an n-element set C ⊆ R d and a (sufficiently generic) k-element multiset V ⊆ R d , we can order the points in C by ranking each point c ∈ C according to the sum of the distances from c to the points of V. Let Ψ k ( C ) denote the set of orderings of C that can be obtained in this manner as V varies, and let ψ d , k max ( n ) be the maximum of | Ψ k ( C ) | as C ranges over all n-element subsets of R d . We prove that ψ d , k max ( n ) = Θ d , k ( n 2 d k ) when d ≥ 2 and that ψ 1 , k max ( n ) = Θ k ( n 4 ⌈ k / 2 ⌉ - 2 ) . As a step toward proving this result, we establish a bound on the number of sign patterns determined by a collection of functions that are sums of radicals of nonnegative polynomials; this can be understood as an analogue of a classical theorem of Warren. We also prove several results about the set Ψ ( C ) = ⋃ k ≥ 1 Ψ k ( C ) ; this includes an exact description of Ψ ( C ) when d = 1 and when C is the set of vertices of a vertex-transitive polytope.",
        "authors": [
            "Noga Alon",
            "Colin Defant",
            "Noah Kravitz",
            "Daniel G. Zhu"
        ],
        "journal_conference_name": "Combinatorica",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159170",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Leveraging Seawater Thermal Energy Storage and Heat Pumps for Coupling Electricity and Urban Heating: A Techno-Economic Analysis",
        "abstract": "This paper presents an economic assessment of seawater thermal energy storage (TES) integrated with industrial heat pumps to couple renewable electricity generation with urban district heating networks. Using Amsterdam as a case study, we develop a techno-economic model leveraging real-world data on electricity prices, heat demand, and system costs. Our findings show that large-scale TES using seawater as a storage medium significantly enhances district heating economics through energy arbitrage and operational flexibility. The optimal configuration yields a net present value (NPV) of EUR 466 million over 30 years and a payback period under 6 years. Thermal storage increases NPV by 17% compared to systems without storage, while within-day load shifting further boosts economic value by 23%. Accurate demand and price forecasting is critical, as forecasting errors can reduce NPV by 13.7%. The proposed system is scalable and well suited for coastal cities, offering a sustainable, space-efficient solution for urban decarbonization and addressing renewable energy overproduction.",
        "authors": [
            "Timur Abbiasov",
            "Aldo Bischi",
            "Manfredi Gangi",
            "Andrea Baccioli",
            "Paolo Santi",
            "Carlo Ratti"
        ],
        "journal_conference_name": "Energies",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159075",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Encourage circular practices in the supply chain",
        "abstract": "Every year 300 million tons of plastic waste are produced, and the amount of plastic produced increases with the world population. The more people there are on the planet, the more waste is produced. The concepts of circular economy are gaining popularity. Companies are looking to implement circular strategies to maximize the use of materials, reduce waste and help the environment while improving their corporate image.\r\nSince the coronavirus pandemic, digital transformation has progressed faster and faster, which has boosted digital communication. Social networks began to play a fundamental role in communication since they are an efficient means of interacting with people worldwide in real-time. Due to social networks' social impact, they can be used to influence people's decision-making.\r\nThis study aims to develop a model that encourages people to adopt recycling habits for polyethylene terephthalate (PET) bottles through social networks focused on the population of the United States. This study will use analytics tools such as the Bass Diffusion Model, and an economic analysis of the viability will be carried out to implement the proposed strategies.",
        "authors": [
            "Alejandro Jorge Goitia Polo",
            "Juan Manuel Perez Dovalo"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159025",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Buy, Rent and Sale: Chasing better cash flows",
        "abstract": "This capstone project optimizes the inventory levels of a rental car company and improves the cash-to-cash cycle. The solution approach is a Mixed Integer Linear Program (MILP) model, considering a multiple-period inventory. The model provides purchasing and selling plans and cash and vehicle flow in the system for each quarter and each type of vehicle for five years. The analytical model was created to maximize the company’s gross margin, considering revenues from renting and sales, cost buy, opportunity cost, and general cost (maintenance, holding, and others). Moreover, it considers an initial inventory and helps the company manage these assets in the best way possible to meet the demand. The result shows an optimal solution of 3.3 billion Colombian pesos (COP) for five years in the base case scenario. Afterward, a sensitivity analysis for different perspectives related to renting period, budget, depreciation rate, and exchange rate impact was carried out. From that perspective, it is possible to understand that the primary trigger to create revenue is extending the renting period. Moreover, the sponsor company can interpret how factors in the market affect the total result success and create an action plan to anticipate these risks.",
        "authors": [
            "Ana Patricia Do Couto Selem",
            "Juan Marcelo Oyarzun Rodriguez",
            "Ricardo Leon Monsalve Uribe"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159030",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Reduction of Costs and Emissions in Outbound Transport",
        "abstract": "The global food system accounts for nearly 30% of the total CO2 emissions worldwide. About 19% of that figure is due to transportation-related emissions. The main problem being addressed in this project is to identify the main drivers of CO2 emissions in outbound transportation for a major CPG food company in Antioquia, Colombia, which has declared sustainability as a major driver in their corporate strategy. By indirectly measuring CO2 emissions, a better understanding of the main drivers of emissions can be acquired. The cause-effect relationships on the distribution performance in emissions and cost to serve are in place.\r\nA comprehensive literature review of the state-of-the-art methodologies and techniques to assess CO2 emissions is part of this project, as well as a qualitative evaluation of the challenges of Antioquia’s topography. Two different methodologies have been used throughout this document to estimate CO2 emissions. A fuel-based approach and a distance-weight-based approach use CO2 equivalent units to estimate emissions at different levels of aggregation. Quantitative and spatial analysis allows us to conclude that regions that are harder to reach (low-volume municipalities located in hilly areas, irrespective of distance traveled) have a higher cost to serve and higher emissions due to an increase in transportation costs, fuel usage, difficulties to consolidate cargo and difficulties to increase vehicle usage due to the low volume sales.",
        "authors": [
            "Leonardo Amazonas Machado",
            "Ricardo Chavelas Manzo",
            "Rodrigo Silva Tourinho Nakamura"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159028",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Minimizing last-mile emissions through altitude-aware route optimization",
        "abstract": "This study introduced an exact optimization approach to solve a new special type of Travelling Salesperson Problem. This problem considers time-windows restrictions and a new objective function—the objective regards driver assessment awareness and fuel consumption. The latter is modeled as a function of variable vehicle payload and terrain elevation data. This problem can be stated as the way to find the best route to service a set of customer demands, attempting to deliver within agreed time windows, mimicking paths that are as similar as possible to good routes executed in the past by experienced drivers, allowing small alterations as to reduce duration and fuel consumption. The authors proposed an innovative mixed integer linear programming formulation and a cluster decomposition approach that reduces search space and makes the approach applicable to solving real-world-sized problems. This model was parametrized using a small-sized mockup dataset and had its applicability tested on real data. The latter consisted of a public dataset containing trips executed and evaluated by real drivers of Amazon company. The results show that it was possible to reduce in -5.7% the fuel consumption in the routes of this dataset. Since this variable is directly related to emissions and pollution, this result shows that the suggested approach offers promising prospects for improving efficiency and reducing the carbon footprint of logistics last-mile operations. To the best of the authors' knowledge, this study contributes to the literature in that it is the first to jointly tackle driver assessment awareness and fuel consumption in a route optimization problem. Thus, it is also the first to propose a mathematical formulation and solution approach for this problem.",
        "authors": [
            "Gustavo De Abreu Rodrigues",
            "Gustavo Jimenez Ruan",
            "Jenny Carolina Amores"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159029",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Fulfillment models framework for e-commerce companies",
        "abstract": "E-commerce relevance is increasing, and companies should be prepared to fulfill customers’ expectations and ensure an optimal shopping experience. Online worldwide retail sales generated 70 billion U.S. dollars in 2019, being Mexico and Brazil the main leaders for this type of channel in LATAM (Chevalier, 2020).\r\nWith the objective of being more efficient and differentiate from competitors, it is vital to have an extremely consistent and aligned supply chain that follows the company's business strategy. To achieve this new challenge, the following study aims to generate a framework decision matrix, enabling companies to support decisions of introducing fresh, dry, refrigerated, and frozen product categories based on five major warehousing trends: distribution center, fulfillment center, dark-store, micro-fulfillment center and crowdsourced warehousing solutions. To develop this project a systematic literature review combining case studies, papers, research articles and experts’ validation will be implemented with the objective of establishing a framework that can be used to ensure strategies for the e-commerce retailers, thus they are able to serve and meet customer expectations regarding product quality, optimal price, and delivery time.",
        "authors": [
            "Juan Manuel Bellido",
            "Renata Cabrini Souza e Silva",
            "Dominique Gomez de la Luz"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159022",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Causal inference improving warehouse productivity: zoned storage and killer items",
        "abstract": "E-commerce companies need help with customer service experience: faster and more frequent deliveries. Then, order fulfillment becomes critical to establish a competitive advantage.\r\nThe main objective of this project is to determine whether a new key product assortment, called a class-based scattered storage policy, improves order-picking operations in one of the main warehouses of the sponsor company. This e-commerce firm operates in an emerging market.\r\nAs mentioned earlier, this project addresses the problem by running an A/B quasi-experiment in the warehouse, showing findings directly from a real context for the first time. For this purpose, the warehouse was split into control and treatment sections during peak season when speed is most required. The effect of the proposed storage policy is studied by comparing picking productivity through a two-sample t-test. The samples are chosen using the Coarsened Exact Matching algorithm to have similar data to analyze in observable characteristics.\r\nThe result of this work indicates that the class-based scattered storage policy does not lead to an improvement in picking productivity. It can be attributed to real-context features that are presented and discussed. Additionally, strong recommendations are given to include the findings in future research.",
        "authors": [
            "David Montemurri",
            "Hebe Adriana Herrera",
            "Maria Florencia Ghiglione"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159024",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The State of Supply Chain Sustainability in Brazil",
        "abstract": "As sustainability gains importance for consumers, employees, and investors, Supply Chain Sustainability (SCS) has become an increasingly important topic for companies. The State of Supply Chain Sustainability report, a co-presentation of the MIT Center for Transportation & Logistics and the Council of Supply Chain Management Professionals, provides a clear snapshot of this subject worldwide. Although it has been increasing its range of respondents, including Spanish and Mandarin Chinese translated surveys and the original English survey, Portuguese-speaking countries have not been fully reached. This project aims to understand the state of supply chain sustainability in Brazil, the largest country in Latin America, regarding area, population, and GDP. The State of Supply Chain Sustainability 2022 survey questions were translated to Brazilian Portuguese and applied in a local survey in Brazil with specific questions to capture particularities, such as the impact of regional tax benefits in supply chain-related decision-making. Advanced statistical models were applied to guarantee the quality of the translations and compare the local results with the past results of other countries.",
        "authors": [
            "Leonardo Gonzaga Moreira Sá C Faveret",
            "Marcelo Ikaro Carvalho Mesquita Braga",
            "Rodrigo Junqueira Nogueira"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159031",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Emerging membrane technologies for sustainable lithium extraction from brines and leachates: Innovations, challenges, and industrial scalability",
        "abstract": "This perspective critically examines challenges in advancing membrane-based technologies for lithium extraction from industrial brines, salt lakes, and battery leachates. The rapidly rising deployment of electric vehicles and renewable energy systems has intensified global lithium demand, necessitating sustainable and efficient extraction methods. Traditional techniques like brine evaporation and hard rock mining are environmentally detrimental due to high water usage, ecological disruption, and significant carbon emissions, compounded by geopolitical risks from resource concentration. Emerging membrane technologies, utilizing lithium-selective ligands, biomimetic ion channels, and two-dimensional and porous materials, can potentially realize orders-of-magnitude improvements in lithium selectivity for direct lithium extraction (DLE). However, the effectiveness of DLE membranes is constrained by impurity co-extraction, environmental hazards, lack of scalability and material instability. Conventional lithium brine concentration (LBC) techniques, which complement DLE by concentrating lithium for downstream applications like battery production, face challenges in hypersaline environments, such as fouling and reduced selectivity. Advances in electrodialysis and nanofiltration with surface modifications offer promising solutions to sustain favorable monovalent selectivity under high salinity conditions. Key gaps in the current research landscape include the absence of standardized testing procedures, evaluation metrics poorly suited to hypersaline or multi-ionic environments, scalability challenges in manufacturing, and economic limitations arising from fouling and material degradation. Addressing these issues requires material characterization with representative solution compositions, the development of comprehensive evaluation frameworks, and strategies for co-extracting valuable metals to improve economic viability. A holistic focus on membrane manufacturability, material durability, and process integration is essential to unlock sustainable lithium extraction technologies that can support the global shift to clean energy.",
        "authors": [
            "Zi Hao Foo",
            "John H. Lienhard"
        ],
        "journal_conference_name": "Desalination",
        "publisher": "Elsevier BV",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158972",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Evidence-Based Nutraceuticals Derived from Antrodia cinnamomea",
        "abstract": "Antrodia cinnamomea (A. cinnamomea), a medicinal and edible mushroom endemic to Taiwan, has been traditionally valued as a health tonic. Recent studies have highlighted the diverse specialized metabolites and bioactive potential of this substance, primarily attributed to key secondary metabolites such as benzenoids, maleic and succinic acids, ubiquinone, triterpenoids, and the primary metabolite polysaccharides. These compounds exhibit a broad spectrum of pharmacological properties, including those related to antibacterial, antitumor, anti-inflammation, hepatoprotection, hypoglycaemia, and antioxidant activities, and immunomodulation and gut microbiota regulation. These findings highlight the therapeutic potential of A. cinnamomea and its potential applications in health supplements and functional foods. This review evaluated recent advancements in the cultivation, extraction, and characterization of bioactive compounds from A. cinnamomea, with a particular focus on submerged and solid-state fermentation methods. We hope to provide a comprehensive framework for promoting the efficient and scientific evidence based utilization of A. cinnamomea in novel therapeutic strategies and health-related innovations.",
        "authors": [
            "Chunyuhang Xu",
            "Qingtong Xie",
            "Chien-Liang Kuo",
            "Xin Yang",
            "Dejian Huang"
        ],
        "journal_conference_name": "Foods",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159074",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Menin: from molecular insights to clinical impact",
        "abstract": "Menin, the protein product of the MEN1 gene, is essential for development and has been implicated in multiple different cancer types. These include leukemias and several different solid tumors, including neuroendocrine tumors. Menin interacts with many different protein partners and genomic loci in a context-dependent manner, implicating it in numerous cellular processes. The role of Menin varies across tumor types as well, acting as a tumor suppressor in some tissues and an oncogenic co-factor in others. Given the role of Menin in cancer, and particularly its oncogenic role in acute myeloid leukemia, the development of Menin inhibitors has been an expanding field over the past 10-15 years. Many inhibitors have been in clinical trials and one has recently received approval from the Food and Drug Administration (FDA). In this review, we explore the role of Menin in multiple cancer types, the development of Menin inhibitors and their clinical applications and what the focus of the field should be in the next 5-10 years to expand the use and efficacy of these drugs.",
        "authors": [
            "Margaret R Brown",
            "Yadira M Soto-Feliciano"
        ],
        "journal_conference_name": "Epigenomics",
        "publisher": "Informa UK Limited",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159177",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Evaluating the Impacts of Swapping on the US Decennial Census",
        "abstract": "To meet its dual burdens of providing useful statistics and ensuring privacy of individual respondents, the US Census Bureau has for decades introduced some form of \"noise\" into published statistics. Initially, they used a method known as \"swapping\" (1990-2010). In 2020, they switched to an algorithm called TopDown that ensures a form of Differential Privacy. While the TopDown algorithm has been made public, no implementation of swapping has been released and many details of the deployed swapping methodology deployed have been kept secret. Further, the Bureau has not published (even a synthetic) \"original\" dataset and its swapped version. It is therefore difficult to evaluate the effects of swapping, and to compare these effects to those of other privacy technologies. To address these difficulties we describe and implement a parameterized swapping algorithm based on Census publications, court documents, and informal interviews with Census employees. With this implementation, we characterize the impacts of swapping on a range of statistical quantities of interest. We provide intuition for the types of shifts induced by swapping and compare against those introduced by TopDown. We find that even when swapping and TopDown introduce errors of similar magnitude, the direction in which statistics are biased need not be the same across the two techniques. More broadly, our implementation provides researchers with the tools to analyze and potentially correct for the impacts of disclosure avoidance systems on the quantities they study.",
        "authors": [
            "Mar?a Ballesteros",
            "Cynthia Dwork",
            "Gary King",
            "Conlan Olson",
            "Manish Raghavan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Symposium on Computer Science and Law",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159046",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "What Constitutes a Less Discriminatory Algorithm?",
        "abstract": "Disparate impact doctrine offers an important legal apparatus for targeting discriminatory data-driven algorithmic decisions. A recent body of work has focused on conceptualizing one particular construct from this doctrine: the less discriminatory alternative, an alternative policy that reduces disparities while meeting the same business needs of a status quo or baseline policy. However, attempts to operationalize this construct in the algorithmic setting must grapple with some thorny challenges and ambiguities. In this paper, we attempt to raise and resolve important questions about less discriminatory algorithms (LDAs). How should we formally define LDAs, and how does this interact with different societal goals they might serve? And how feasible is it for firms or plaintiffs to computationally search for candidate LDAs? We find that formal LDA definitions face fundamental challenges when they attempt to evaluate and compare predictive models in the absence of held-out data. As a result, we argue that LDA definitions cannot be purely quantitative, and must rely on standards of \"reasonableness.\" We then raise both mathematical and computational constraints on firms' ability to efficiently conduct a proactive search for LDAs, but we provide evidence that these limits are \"weak\" in a formal sense. By defining LDAs formally, we put forward a framework in which both firms and plaintiffs can search for alternative models that comport with societal goals.",
        "authors": [
            "Benjamin Laufer",
            "Manish Raghavan",
            "Solon Barocas"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Symposium on Computer Science and Law",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159048",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Pluto: Authoring Semantically Aligned Text and Charts for Data-Driven Communication",
        "abstract": "Textual content (including titles, annotations, and captions) plays a central role in helping readers understand a visualization by emphasizing, contextualizing, or summarizing the depicted data. Yet, existing visualization tools provide limited support for jointly authoring the two modalities of text and visuals such that both convey semantically-rich information and are cohesively integrated. In response, we introduce Pluto, a mixed-initiative authoring system that uses features of a chart’s construction (e.g., visual encodings) as well as any textual descriptions a user may have drafted to make suggestions about the content and presentation of the two modalities. For instance, a user can begin to type out a description and interactively brush a region of interest in the chart, and Pluto will generate a relevant auto-completion of the sentence. Similarly, based on a written description, Pluto may suggest lifting a sentence out as an annotation or the visualization’s title, or may suggest applying a data transformation (e.g., sort) to better align the two modalities. A preliminary user study revealed that Pluto’s recommendations were particularly useful for bootstrapping the authoring process and helped identify different strategies participants adopt when jointly authoring text and charts. Based on study feedback, we discuss design implications for integrating interactive verification features between charts and text, offering control over text verbosity and tone, and enhancing the bidirectional flow in unified text and chart authoring tools.",
        "authors": [
            "Arjun Srinivasan",
            "Vidya Setlur",
            "Arvind Satyanarayan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|30th International Conference on Intelligent User Interfaces",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159033",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "MIND (Mixed-Initiative Next-gen Design): Workshop on Blending Agents and Direct Manipulation for Harnessing LLMs",
        "abstract": "Since the 1980s, a key debate in human-centered computing involving machine learning at IUI is between agent-driven systems and direct manipulation. The explosion of Large Language Models (LLMs), particularly auto-regressive as agents serving as chatbots, generative search, and work automation tools, has also brought with it inherent limitations. We posit that efforts to address and alleviate these LLM challenges—hallucinations, unpredictable outputs, lack of transparency, and difficulties in customization—cannot be solved through algorithmic improvements alone but require elevated mixed-initiative interface design at the heart of the IUI community. This workshop aims to bridge the gap between agent-driven automation and direct manipulation by exploring mixed-initiative interaction models that blend the strengths of both paradigms to empower end-users seeking to harness LLMs.",
        "authors": [
            "Karthik Dinakar",
            "Henry Lieberman",
            "Sonia Wu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|30th International Conference on Intelligent User Interfaces Companion",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159042",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "MemPal: Leveraging Multimodal AI and LLMs for Voice-Activated Object Retrieval in Homes of Older Adults",
        "abstract": "Older adults have increasing difficulty with retrospective memory, hindering their abilities to perform daily activities and posing stress on caregivers to ensure their wellbeing. Recent developments in Artificial Intelligence (AI) and large context-aware multimodal models offer an opportunity to create memory support systems that assist older adults with common issues like object finding. This paper discusses the development of an AI-based, wearable memory assistant, MemPal, that helps older adults with a common problem, finding lost objects at home, and presents results from tests of the system in older adults’ own homes. Using visual context from a wearable camera, the multimodal LLM system creates a real-time automated text diary of the person’s activities for memory support purposes, offering object retrieval assistance using a voice-based interface. The system is designed to support additional use cases like context-based proactive safety reminders and recall of past actions. We report on a quantitative and qualitative study with N=15 older adults within their own homes that showed improved performance of object finding with audio-based assistance compared to no aid and positive overall user perceptions on the designed system. We discuss further applications of MemPal’s design as a multi-purpose memory aid and future design guidelines to adapt memory assistants to older adults’ unique needs.",
        "authors": [
            "Natasha Maniar",
            "Samantha Chan",
            "Wazeer Zulfikar",
            "Scott Ren",
            "Christine Xu",
            "Pattie Maes"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|30th International Conference on Intelligent User Interfaces",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159037",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Coalesce: An Accessible Mixed-Initiative System for Designing Community-Centric Questionnaires",
        "abstract": "Effectively incorporating community input into civic decision-making processes is crucial for fostering inclusive governance. However, public officials often face challenges in formulating effective questions to gather meaningful insights due to constraints such as time, resources, and limited experience in questionnaire design. This paper explores the potential of leveraging large language models (LLMs) to address this challenge. We present Coalesce, a novel mixed-initiative system that utilizes LLMs to assist civic leaders in crafting tailored and impactful questions for surveys, interviews, and conversation guides. Guided by best practices in questionnaire design, Coalesce improves question readability, enhances specificity, and reduces bias. To inform our design, we conducted a formative interview study with 30 civic leaders and implemented an iterative human-centered design process involving 14 feedback sessions. We built a fully-functional system before evaluating it through a real-world user study with 16 participants who applied the platform to their own community engagement projects. Our findings show that Coalesce improved participants’ confidence in questionnaire design, supported diverse workflows, and fostered learning while raising important questions about human agency and over-reliance on AI. These insights highlight the potential for intelligent user interfaces to reshape how civic leaders engage with their communities, fostering more informed and inclusive decision-making processes.",
        "authors": [
            "Cassandra Overney",
            "Daniel Kessler",
            "Suyash Fulay",
            "Mahmood Jasim",
            "Deb Roy"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|30th International Conference on Intelligent User Interfaces",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159053",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Scientific Advancements in Gene Therapies: Opportunities for Global Regulatory Convergence",
        "abstract": "On 4 September 2024, the Reagan-Udall Foundation for the FDA (FDA Foundation) in collaboration with the Food and Drug Administration (FDA) and the Gates Foundation hosted a workshop titled “Scientific Advancements in Gene Therapies: Opportunities for Global Regulatory Convergence”. The event brought together a diverse group of experts, including international regulatory bodies, regulated industries, healthcare professionals, patients, academic researchers and global health advocates, to discuss the rapid advancements in gene therapy and the pressing need for equitable access in low-and middle-income countries (LMICs), with sickle cell disease (SCD) serving as the model disorder for the discussions. Although there has been significant progress in gene therapy, such as breakthroughs in clustered regularly interspaced short palindromic repeats (CRISPR)-based technologies and FDA-approved therapies, access to these therapies remain limited in underresourced regions. The workshop addressed critical challenges, including the high cost of therapies, regulatory gaps and barriers and ethical concerns regarding informed consent and public engagement in LMICs. This paper highlights the critical discussion points from the workshop with a focus on exploring strategies for global regulatory convergence, the role of international collaborations and the potential pathways to making gene therapies affordable and accessible to all.",
        "authors": [
            "Jimi Olaghere",
            "David A. Williams",
            "Jeremy Farrar",
            "Hildegard Büning",
            "Cecelia Calhoun",
            "Tony Ho",
            "Maneesha S. Inamdar",
            "David Liu",
            "Julie Makani",
            "Kwasi Nyarko",
            "Sol Ruiz",
            "John Tisdale",
            "Joseph M. McCune",
            "Esther Boadi",
            "Reagan-Udall Foundation for the FDA"
        ],
        "journal_conference_name": "Biomedicines",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159014",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "High-Connectivity Triazolate-Based Metal–Organic Framework for Water Harvesting",
        "abstract": "Increasing the connectivity of structural units presents a potentially valuable approach to improve hydrolytic stability in metal–organic frameworks (MOFs). We herein leverage this strategy by synthesizing the first tritopic benzotriazolate MOF, Zn5(OAc)4(TBTT)2 (H3TBTT = 2,4,6-tris(1H-benzo[d][1,2,3]triazol-5-yl)-1,3,5-triazine), which exhibits open metal sites, high connectivity, high porosity, and significant water uptake capacity. The MOF adopts a previously unknown topology with (3,6,6)-connectivity, which is supported by single-crystal electron diffraction and elemental analysis. The framework undergoes postsynthetic metal and anion exchange with NiCl2, which increases the accessible pore volume and the net hydrophilicity of the framework. With this exchange, the apparent BET surface area increases from 1994 to 3034 m2/g, and the water uptake step shifts from 56 to 33% relative humidity (RH). The high gravimetric capacity of the Ni-rich MOF, 0.98 g/g, translates to a working capacity of 0.64 g/g during a pressure swing cycle between 20 and 40% RH at 25 °C. Combining this performance with a less than 2% loss in working capacity over 100 cycles, the new material rivals the best MOF water sorbents to date.",
        "authors": [
            "Karla Ravin",
            "Patrick Sarver",
            "Bhavish Dinakar",
            "Lukáš Palatinus",
            "Peter Müller",
            "Julius Oppenheim",
            "Mircea Dincă"
        ],
        "journal_conference_name": "Journal of the American Chemical Society",
        "publisher": "American Chemical Society",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158533",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Brain Markers of Resilience to Psychosis in High-Risk Individuals: A Systematic Review and Label-Based Meta-Analysis of Multimodal MRI Studies",
        "abstract": "Background/Objectives: Most individuals who have a familial or clinical risk of developing psychosis remain free from psychopathology. Identifying neural markers of resilience in these at-risk individuals may help clarify underlying mechanisms and yield novel targets for early intervention. However, in contrast to studies on risk biomarkers, studies on neural markers of resilience to psychosis are scarce. The current study aimed to identify potential brain markers of resilience to psychosis. Methods: A systematic review of the literature yielded a total of 43 MRI studies that reported resilience-associated brain changes in individuals with an elevated risk for psychosis. Label-based meta-analysis was used to synthesize findings across MRI modalities. Results: Resilience-associated brain changes were significantly overreported in the default mode and language network, and among highly connected and central brain regions. Conclusions: These findings suggest that the DMN and language-associated areas and central brain hubs may be hotspots for resilience-associated brain changes. These neural systems are thus of key interest as targets of inquiry and, possibly, intervention in at-risk populations.",
        "authors": [
            "Guusje Collin",
            "Joshua E. Goldenberg",
            "Xiao Chang",
            "Zhenghan Qi",
            "Susan Whitfield-Gabrieli",
            "Wiepke Cahn",
            "Jijun Wang",
            "William S. Stone",
            "Matcheri S. Keshavan",
            "Martha E. Shenton"
        ],
        "journal_conference_name": "Brain Sciences",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159013",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Design and Deployment of a Self-Powered, LoRaWAN-Based IoT Environment Sensor Ensemble for Integrated Air Quality Sensing and Simulation",
        "abstract": "The goal of this study is to describe a design architecture for a self-powered IoT (Internet of Things) sensor network that is currently being deployed at various locations throughout the Dallas-Fort Worth metroplex to measure and report on Particulate Matter (PM) concentrations. This system leverages diverse low-cost PM sensors, enhanced by machine learning for sensor calibration, with LoRaWAN connectivity for long-range data transmission. Sensors are GPS-enabled, allowing precise geospatial mapping of collected data, which can be integrated with urban air quality forecasting models and operational forecasting systems. To achieve energy self-sufficiency, the system uses a small-scale solar-powered solution, allowing it to operate independently from the grid, making it both cost-effective and suitable for remote locations. This novel approach leverages multiple operational modes based on power availability to optimize energy efficiency and prevent downtime. By dynamically adjusting system behavior according to power conditions, it ensures continuous operation while conserving energy during periods of reduced supply. This innovative strategy significantly enhances performance and resource management, improving system reliability and sustainability. This IoT network provides localized real-time air quality data, which has significant public health benefits, especially for vulnerable populations in densely populated urban environments. The project demonstrates the synergy between IoT sensor data, machine learning-enhanced calibration, and forecasting methods, contributing to scientific understanding of microenvironments, human exposure, and public health impacts of urban air quality. In addition, this study emphasizes open source design principles, promoting transparency, data quality, and reproducibility by exploring cost-effective sensor calibration techniques and adhering to open data standards. The next iteration of the sensors will include edge processing for short-term air quality forecasts. This work underscores the transformative role of low-cost sensor networks in urban air quality monitoring, advancing equitable policy development and empowering communities to address pollution challenges.",
        "authors": [
            "Lakitha O. H. Wijeratne",
            "Daniel Kiv",
            "John Waczak",
            "Prabuddha Dewage",
            "Gokul Balagopal",
            "Mazhar Iqbal",
            "Adam Aker",
            "Bharana Fernando",
            "Matthew Lary",
            "Vinu Sooriyaarachchi",
            "Rittik Patra",
            "Nora Desmond",
            "Hannah Zabiepour",
            "Darren Xi",
            "Vardhan Agnihotri",
            "Seth Lee",
            "Chris Simmons",
            "David J. Lary"
        ],
        "journal_conference_name": "Air",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159012",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Learning-Augmented Competitive Algorithms for Spatiotemporal Online Allocation with Deadline Constraints",
        "abstract": "We introduce and study spatiotemporal online allocation with deadline constraints (SOAD), a new online problem motivated by emerging challenges in sustainability and energy.  In SOAD, an online player completes a workload by allocating and scheduling it on the points of a metric space $(X, d)$ while subject to a deadline $T$.  At each time step, a service cost function is revealed that represents the cost of servicing the workload at each point, and the player must irrevocably decide the current allocation of work to points.  Whenever the player moves this allocation, they incur a movement cost defined by the distance metric $d(\\cdot, \\ \\cdot)$ that captures, e.g., an overhead cost.  SOAD formalizes the open problem of combining general metrics and deadline constraints in the online algorithms literature, unifying problems such as metrical task systems and online search.  We propose a competitive algorithm for SOAD along with a matching lower bound establishing its optimality.  Our main algorithm, ST-CLIP, is a learning-augmented algorithm that takes advantage of predictions (e.g., forecasts of relevant costs) and achieves an optimal consistency-robustness trade-off.  We evaluate our proposed algorithms in a simulated case study of carbon-aware spatiotemporal workload management, an application in sustainable computing that schedules a delay-tolerant batch compute job on a distributed network of data centers.  In these experiments, we show that ST-CLIP substantially improves on heuristic baseline methods.",
        "authors": [
            "Adam Lechowicz",
            "Nicolas Christianson",
            "Bo Sun",
            "Noman Bashir",
            "Mohammad Hajiesmaili",
            "Adam Wierman",
            "Prashant Shenoy"
        ],
        "journal_conference_name": "Proceedings of the ACM on Measurement and Analysis of Computing Systems",
        "publisher": "ACM",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159050",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Secure finite-time filtering for switched fuzzy systems with scaling attacks and stochastic sensor faults",
        "abstract": "In this study, we introduce a design for robust secure finite-time mixed H ∞ and passivity filter for discrete-time switched fuzzy systems. This design effectively combats both stochastic scaling attacks and sensor failure. To be specific, the sensor signals are represented by stochastic variables with different failure rates. Also, a comprehensive model is presented to characterize the scaling attacks and it is described by the Bernoulli distributed random variable. By designing a suitable Lyapunov functional candidate and leveraging the principles of finite-time theory, we have formulated a new collection of sufficient conditions. These conditions, expressed as linear matrix inequalities, ensure that the augmented fuzzy system maintains robust stochastic finite-time boundedness, along with a predetermined mixed H ∞ and passivity performance index. Ultimately, two numerical demonstrations are provided, incorporating real-world applications from the continuous-time single-link robot arm model and the tunnel diode circuit systems, to highlight the practicality of the proposed secure filter design.",
        "authors": [
            "Murugesan Sathishkumar",
            "Maya Joby",
            "Yong-Ki Ma",
            "Selvaraj M. Anthoni",
            "Srimanta Santra"
        ],
        "journal_conference_name": "Nonlinear Dynamics",
        "publisher": "Springer Netherlands",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159070",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "OpenEarable 2.0: Open-Source Earphone Platform for Physiological Ear Sensing",
        "abstract": "Earphones have evolved from pure audio devices to \"earables\" that are capable of advanced sensing. Bespoke research devices have shown the unique sensing capabilities of the earable platform; however, they are hard to replicate and require expertise to develop in the first place. In this paper, we present OpenEarable 2.0 - an open source, unified platform that integrates a larger number of sensors for conducting comprehensive earable research. OpenEarable 2.0 works as regular binaural Bluetooth earphones and features two ultrasound capable microphones (inward/outward), a 3-axis ear canal accelerometer/bone microphone, a 9-axis head inertial measurement unit, pulse oximeter, optical temperature sensor, ear canal pressure sensor, and microSD card. These capabilities allow for the detection and measurement of 30+ phenomena on the ear that can be used across a wide range of applications in health monitoring, activity tracking, human-computer-interaction and authentication. We describe the design and development of OpenEarable 2.0 which follows best open hardware practices and achieves commercial-level wearability. We provide justification for the selection and placement of integrated sensors and include in-depth descriptions of the extensible, open source firmware and hardware that are implemented using free to use tools and frameworks. For real-time sensor control and data recording we also contribute a web-based dashboard and mobile smartphone app. The wearability and ability to sense different phenomena are validated in four studies which showcases how OpenEarable 2.0 provides accurate measurements in comparison to established gold-standard measurements. We further demonstrate that OpenEarable 2.0 can be assembled by inexperienced users, and that undergraduate students can build applications using the OpenEarable platform.",
        "authors": [
            "Tobias R?ddiger",
            "Michael K?ttner",
            "Philipp Lepold",
            "Tobias King",
            "Dennis Moschina",
            "Oliver Bagge",
            "Joseph Paradiso",
            "Christopher Clarke",
            "Michael Beigl"
        ],
        "journal_conference_name": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "publisher": "ACM",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159051",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Picto: Crafting Remote Tangible Gestures via Recordable, Replayable, and Shareable Motions",
        "abstract": "We introduce Picto, a paired tangible interface that enables intimate dyads to co-create shared kinetic messages, fostering playful remote communication beyond temporal and physical constraints. Picto’s two modular units—a knob for rotational motion and a slider for linear motion—allow users to craft personalized motions and shapes symbolizing their significant other. Presence can be conveyed in real-time or asynchronously through record, replay, and share features. Picto empowers users to express abstract ideas through iconic gestures and non-verbal cues. Using a bistable composite tape-spring structure, we developed a novel mechanism for programming dynamic shape variations and motions. Picto’s control system records, stores, and shares motion-based interactions. A user study with intimate dyads evaluates Picto’s usability and its potential as a remote story-sharing platform and ambient presence media enhanced by metaphorical and beat gestures. The results highlight its potential to enrich and sustain intimate relationships, supporting social presence across distances.",
        "authors": [
            "Kyung Yun Choi",
            "Taehee Jung",
            "Noble Harasha",
            "Hiroshi Ishii"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Nineteenth International Conference on Tangible, Embedded, and Embodied Interaction",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158328",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Performance Analysis for High-Dimensional Bell-State Quantum Illumination",
        "abstract": "Quantum illumination (QI) is an entanglement-based protocol for improving LiDAR/radar detection of unresolved targets beyond what a classical LiDAR/radar of the same average transmitted energy can do. Originally proposed by Seth Lloyd as a discrete-variable quantum LiDAR, it was soon shown that his proposal offered no quantum advantage over its best classical competitor. Continuous-variable, specifically Gaussian-state, QI has been shown to offer a true quantum advantage, both in theory and in table-top experiments. Moreover, despite its considerable drawbacks, the microwave version of Gaussian-state QI continues to attract research attention. A recent QI study by Armanpreet Pannu, Amr Helmy, and Hesham El Gamal (PHE), however, has: (i) combined the entangled state from Lloyd’s QI with the channel models from Gaussian-state QI; (ii) proposed a new positive operator-valued measurement for that composite setup; and (iii) claimed that, unlike Gaussian-state QI, PHE QI achieves the Nair–Gu lower bound on QI target-detection error probability at all noise brightnesses. PHE’s analysis was asymptotic, i.e., it presumed infinite-dimensional entanglement. The current paper works out the finite-dimensional performance of PHE QI. It shows that there is a threshold value for the entangled-state dimensionality below which there is no quantum advantage, and above which the Nair–Gu bound is approached asymptotically. Moreover, with both systems operating with error-probability exponents 1 dB lower than the Nair–Gu bound, PHE QI requires enormously higher entangled-state dimensionality than does Gaussian-state QI to achieve useful error probabilities in both high-brightness (100 photons/mode) and moderate-brightness (1 photon/mode) noise. Furthermore, neither system has an appreciable quantum advantage in low-brightness (much less than 1 photon/mode) noise.",
        "authors": [
            "Jeffrey H. Shapiro"
        ],
        "journal_conference_name": "Physics",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158998",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Choice Vectors: Streamlining Personal AI Alignment Through Binary Selection",
        "abstract": "Value alignment for AI is not “one-size-fits-all”: even polite and friendly models can still fail to represent individual user contexts and preferences, and local cultural norms. This paper presents a modular workflow for personal fine-tuning, synthesizing four core components from our previous research: (1) robust vectorization of user values and preferences, (2) a binary choice user interface (UI) approach to capturing those preferences with minimal cognitive load, (3) contrastive activation methods for steering large language models (LLMs) via difference vectors, and (4) knowledge graph integration for more auditable and structured alignment. Our approach—descended from past research on “Towards an End-to-End Personal Fine-Tuning Framework”—demonstrates how these elements can be combined to create personalized, context-rich alignment solutions. We report on user studies for the forced-choice UI, describe an experimental pipeline for deriving “control vectors”, and propose a “moral graph” method for bridging symbolic and vector-based alignment. Our findings suggest that multi-pronged personalization can significantly reduce user annotation fatigue, improve alignment fidelity, and allow for more flexible, interpretable AI behaviors.",
        "authors": [
            "Eleanor Watson",
            "Minh Nguyen",
            "Sarah Pan",
            "Shujun Zhang"
        ],
        "journal_conference_name": "Multimodal Technologies and Interactions",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158997",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The ultra-thin conception of objecthood",
        "abstract": "In his excellent book Thin Objects, Øystein Linnebo develops a conception of\r\nobjecthood that allows for thin objects: objects whose ‘existence does not\r\nmake a substantial demand on the world’ (p. 4). His proposal is premised on\r\nthe Fregean dictum that to be an object is to be the referent of a possible\r\nsingular term (p. 22). As a result, much of Linnebo’s argumentation is focused\r\non defending a ‘thin’ conception of reference, which is liberal enough to\r\nallow for thin objects. This paper is a critique of Linnebo’s conception of\r\nreference.",
        "authors": [
            "Agustín Rayo"
        ],
        "journal_conference_name": "An Interdisciplinary Journal of Philosophy",
        "publisher": "Taylor & Francis",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159049",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "SySTeC: A Symmetric Sparse Tensor Compiler",
        "abstract": "Symmetric and sparse tensors arise naturally in many domains including linear algebra, statistics, physics, chemistry, and graph theory. Symmetric tensors are equal to their transposes, so in the n-dimensional case we can save up to a factor of n! by avoiding redundant operations. Sparse tensors, on the other hand, are mostly zero, and we can save asymptotically by processing only nonzeros. Unfortunately, specializing for both symmetry and sparsity at the same time is uniquely challenging. Optimizing for symmetry requires consideration of n! transpositions of a triangular kernel, which can be complex and error prone. Considering multiple transposed iteration orders and triangular loop bounds also complicates iteration through intricate sparse tensor formats. Additionally, since each combination of symmetry and sparse tensor formats requires a specialized implementation, this leads to a combinatorial number of cases. A compiler is needed, but existing compilers cannot take advantage of both symmetry and sparsity within the same kernel. In this paper, we describe the first compiler which can automatically generate symmetry-aware code for sparse or structured tensor kernels. We introduce a taxonomy for symmetry in tensor kernels, and show how to target each kind of symmetry. Our implementation demonstrates significant speedups ranging from 1.36x for SSYMV to 30.4x for a 5-dimensional MTTKRP over the non-symmetric state of the art.",
        "authors": [
            "Radha Patel",
            "Willow Ahrens",
            "Saman Amarasinghe"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158438",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Hands-On Quantum Cryptography: Experimentation with the B92 Protocol Using Pulsed Lasers",
        "abstract": "Quantum cryptography continues to be an area of significant research and educational interest. Here, a straightforward and reliable approach to both the experimental and theoretical aspects of quantum key distribution is presented, tailored for senior undergraduate students. Focusing on illustrating the essential concepts of the B92 protocol through a combination of optical experiments and custom-developed computational tools, this work offers a thorough exploration of quantum cryptography according to the principles of the B92 protocol.",
        "authors": [
            "Sara P. Gandelman",
            "Alona Maslennikov",
            "Georgi Gary Rozenman"
        ],
        "journal_conference_name": "Photonics",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158996",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Structurally Similar Mycotoxins Aflatoxin B1 and Sterigmatocystin Trigger Different and Distinctive High-Resolution Mutational Spectra in Mammalian Cells",
        "abstract": "Aflatoxin B1 (AFB1) and sterigmatocystin (ST) are mycotoxins that pose significant threats to human and animal health owing to their mutagenic, carcinogenic, and toxic properties. They are structurally similar and widely believed to exert their biological effects via the generation of DNA-damaging epoxides at their respective terminal furan rings. Despite structural identity in the warhead portion of each toxin, this work shows that distal parts of each molecule are responsible for the distinctive mutational fingerprints seen in gptΔ C57BL/6J mouse embryo fibroblasts (MEFs). The two toxins differ structurally in the puckered cyclopentenone ring of AFB1 and in the planar xanthone functionality of ST. While both toxins mainly induce GC→TA mutations, the aforementioned differences in structure apparently trigger unique patterns of mutations, as revealed by high-resolution duplex sequencing of MEF genomes. AFB1 is more mutagenic than ST and displays its transversion mutations in a pattern with primary and secondary hotspots (underscored) in 5′-CGC-3′ and 5′-CGG-3′ contexts, respectively. ST displays a modest 5′-CGG-3′ hotspot while its other GC→TA transversions are more uniformly distributed in a pattern resembling established oxidative stress mutational spectra. This research delineates the mutational spectra of AFB1 and ST, establishing these patterns as possible early-onset biomarkers of exposure.",
        "authors": [
            "Pennapa Thongararm",
            "Marisa Chancharoen",
            "Nutchapong Suwanwong",
            "Somsak Ruchirawat",
            "Mathuros Ruchirawat",
            "Bogdan I. Fedeles",
            "Robert G. Croy",
            "John M. Essigmann"
        ],
        "journal_conference_name": "Toxins",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158993",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Influence of Religiosity on Muslim Women’s Selection of Fund Providers in Malaysia",
        "abstract": "The purpose of this study is to analyze the factors influencing the attitudes of women investors in the context of Islamic unit trust funds in Malaysia, with a focus on women&rsquo;s religiosity and on the perceived religiosity of fund providers. Using the UTAUT model, the study examines data from a survey of 263 Muslim women in Malaysia and considers seven key factors: risk aversion, religiosity, price sensitivity, and Islamic financial literacy on the side of the investing women and past performance, perceived religiosity, and perceived risk on the side of the fund providers. The findings indicate that the perceived religiosity of a fund provider has a significant and positive impact on attitude, with positive moderating effects on the women&rsquo;s own religiosity and Islamic financial literacy, and a negative moderating effect on the women&rsquo;s price sensitivity. The study also discusses the practical implications of these findings and offers recommendations for fund providers.",
        "authors": [
            "Salim Bouzekouk",
            "Fadillah Mansor"
        ],
        "journal_conference_name": "Journal of Risk and Financial Management",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158992",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Portable High-Resolution Snapshot Multispectral Imaging Device Leveraging Spatial and Spectral Features for Non-Invasive Corn Nitrogen Treatment Classification",
        "abstract": "Spectral imaging has been widely applied in plant phenotyping to assess corn leaf nitrogen status. Recent studies indicate that spatial variations within a single leaf’s multispectral image provide stronger signals for corn nitrogen estimation. However, current technologies for corn multispectral imaging cannot capture a large corn leaf segment with high-resolution and simple operation, limiting their efficiency and accuracy in nitrogen estimation. To address this gap, this study developed a proximal multispectral imaging device that can capture high-resolution snapshot multispectral images of a large segment of a single corn leaf. This device uses airflow to autonomously position and flatten the leaf to minimize the noise in images due to leaf curvature and simplify operation. Moreover, this device adopts a transmittance imaging regime by clamping the corn leaf between the camera and the lighting source to block the environmental lights and supply uniform lighting to capture high-resolution and high-precision leaf images within six seconds. A field assay was conducted to validate the effectiveness of the multispectral images captured by this device in assessing nitrogen status by classifying the nitrogen treatments applied to corn. Six nitrogen treatments were applied to 12 plots of corn fields, and 10 images were collected at each plot. By using the average vegetative index of the whole image, only one treatment was significantly different from the other five treatments, and no significant difference was observed among any other groups. However, by extracting the spatial and spectral features from the images and combining these features, the accuracy of nitrogen treatment classification improved compared to using the average index. In another analysis, by applying spatial–spectral analysis methods to the images, the nitrogen treatment classification accuracy has improved compared to using the average index. These results demonstrated the advantages of this high-resolution and high-throughput imaging device for distinguishing nitrogen treatments by facilitating spatial–spectral combined analysis for more precise classification.",
        "authors": [
            "Xuan Li",
            "Zhongzhong Niu",
            "Ana Gabriela Morales-Ona",
            "Ziling Chen",
            "Tianzhang Zhao",
            "Daniel J. Quinn",
            "Jian Jin"
        ],
        "journal_conference_name": "Sensors",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158525",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Utilization of Classification Learning Algorithms for Upper-Body Non-Cyclic Motion Prediction",
        "abstract": "This study explores two methods of predicting non-cyclic upper-body motions using classification algorithms. Exoskeletons currently face challenges with low fluency, hypothesized to be in part caused by the lag in active control innate in many leader&ndash;follower paradigms seen in today&rsquo;s systems, leading to energetic inefficiencies and discomfort. To address this, we employ k-nearest neighbor (KNN) and deep learning models to predict motion characteristics, such as magnitude and category, from surface electromyography (sEMG) signals. Data were collected from six muscles located around the elbow. The sEMG signals were processed to identify significant activation changes. Two classification approaches were utilized: a KNN algorithm that categorizes motion based on the slopes of processed sEMG signals at change points and a deep neural network employing continuous categorization. Both methods demonstrated the capability to predict future voluntary non-cyclic motions up to and beyond commonly acknowledged electromechanical delay times, with the deep learning model able to predict, with certainty at or beyond 90%, motion characteristics even prior to myoelectric activation of the muscles involved. Our findings indicate that these classification algorithms can be used to predict upper-body non-cyclic motions to potentially increase machine interfacing fluency. Further exploration into regression-based prediction models could enhance the precision of these predictions, and further work could explore their effects on fluency when utilized in a tandem or wearable robotic application.",
        "authors": [
            "Bon H. Koo",
            "Ho Chit Siu",
            "Dava J. Newman",
            "Ellen T. Roche",
            "Lonnie G. Petersen"
        ],
        "journal_conference_name": "Sensors",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158524",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Content Analysis of E-Participation Platforms in Taiwan with Topic Modeling: How to Train and Evaluate Neural Topic Models?",
        "abstract": "E-participation platforms, such as iVoting and Join in Taiwan, provide digital spaces for citizens to engage in deliberation, voting, and oversight. As a forerunner in Asia, Taiwan has implemented these platforms to enhance participatory democracy. However, there is still limited research on the specific content debated on these platforms. Utilising recent advancements in Natural Language Processing, the content of proposals that users have submitted between 2015 and 2025 is explored. In this study, a pipeline for mining text corpora scraped from these platforms in the context of political analysis is proposed. The pipeline is applied to two datasets which have different characteristics. A topic model for each of the two platforms is generated and later evaluated with OCTIS (Optimizing and Comparing Topic Models Is Simple) and compared to different baselines. Our research highlights the trade-offs between model performance and processing time, emphasizing the balance between accuracy and meaningful topic creation. By integrating a translation pipeline from Chinese to English within the text-mining process, our method also demonstrates a solid approach to overcome language barriers. Consequently, our method is adaptable to e-participation platforms in various languages, providing decision-makers with a more comprehensive tool to understand citizens’ needs and enabling the formulation of more informed and effective policies.",
        "authors": [
            "Moritz Sontheimer",
            "Jonas Fahlbusch",
            "Shuo-Yan Chou",
            "Yu-Lin Kuo"
        ],
        "journal_conference_name": "Applied Sciences",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158523",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Diffusion and Percolation: How COVID-19 Spread Through Populations",
        "abstract": "I rely on the key concepts of diffusion and percolation to characterize the sequential but overlapping phases of the spread of infection through entire populations during the first year of the COVID-19 pandemic. Data from Los Angeles County demonstrate an extended initial diffusion phase propelled by radial geographic spread, followed by percolation within hotspots fueled by the presence of multigenerational households. Data from New York City, by contrast, reveal rapid initial diffusion along a unique, extensive subway network. Subsequent percolation within multiple hotspots, similarly powered by a high density of multigenerational households, exerted a positive feedback effect that further enhanced diffusion. Data from Florida counties support the generality of the phenomenon of viral transmission from more mobile, younger individuals to less mobile, older individuals. Data from the South Brooklyn hotspot reveal the limitations of some forms of government regulation in controlling mobility patterns that were critical to the continued percolation of the viral infection. Data from a COVID-19 outbreak at the University of Wisconsin&mdash;Madison demonstrate the critical role of a cluster of off-campus bars as an attractor for the continued percolation of infection. The evidence also demonstrates the efficacy of quarantine as a control strategy when the hotspot is contained and well identified.",
        "authors": [
            "Jeffrey E. Harris"
        ],
        "journal_conference_name": "Populations",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158991",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Point-of-Care No-Specimen Diagnostic Platform Using Machine Learning and Raman Spectroscopy: Proof-of-Concept Studies for Both COVID-19 and Blood Glucose",
        "abstract": "Significance: We describe a novel, specimen-free diagnostic platform that can immediately detect both a metabolite (glucose) or an infection (COVID-19) by non-invasively using Raman spectroscopy and machine learning. Aim: Current diagnostic testing for infections and glucose monitoring requires specimens, disease-specific reagents and processing, and it increases environmental waste. We propose a new hardware&ndash;software paradigm by designing and constructing a finger-scanning hardware device to acquire Raman spectroscopy readouts which, by varying the machine learning algorithm to interpret the data, allows for diverse diagnoses. Approach: A total of 455 patients were enrolled prospectively in the COVID-19 study; 148 tested positive and 307 tested negative through nasal PCR testing conducted concurrently with testing using our viral detector. The tests were performed on both outpatients (N = 382) and inpatients (N = 73) at Holy Name Medical Center in Teaneck, NJ, between June 2021 and August 2022. Patients&rsquo; fingers were scanned using an 830 nm Raman System and then, using machine learning, processed to provide an immediate result. In a separate study between April 2023 and August 2023, measurements using the same device and scanning a finger were used to detect blood glucose levels. Using a Dexcom sensor and an Accu-Chek device as references, a cross-validation-based regression of 205 observations of blood glucose was performed with a machine learning algorithm. Results: In a five-fold cross-validation analysis (including asymptomatic patients), a machine learning classifier using the Raman spectra as input achieved a specificity for COVID-19 of 0.837 at a sensitivity of 0.80 and an area under receiver operating curve (AUROC) of 0.896. However, when the data were split by time, with training data consisting of observations before 1 July 2022 and test data consisting of observations after it, the model achieved an AUROC of 0.67, with 0.863 sensitivity at a specificity of 0.517. This decrease in AUROC may be due to substantial domain shift as the virus evolves. A similar five-fold cross-validation analysis of Raman glucose detection produces an area under precision&ndash;recall curve (AUPR) of 0.58. Conclusions: The combination of Raman spectroscopy, AI/ML, and our patient interface admitting only a patient&rsquo;s finger and using no specimen offers unprecedented flexibility in introducing new diagnostic tests or adapting existing ones. As the ML algorithm can be iteratively re-trained with new data and the software deployed to field devices remotely, it promises to be a valuable tool for detecting rapidly emerging infectious outbreaks and disease-specific biomarkers, such as glucose.",
        "authors": [
            "Allen B. Chefitz",
            "Rohit Singh",
            "Thomas Birch",
            "Yongwu Yang",
            "Arib Hussain",
            "Gabriella Chefitz"
        ],
        "journal_conference_name": "Spectroscopy Journal",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158990",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Computer Science Behind Bars: Lessons Learned from Teaching Incarcerated Students in Prisons and Jails",
        "abstract": "Educational programs for incarcerated individuals, often called \"behind bars\" initiatives, have been shown to improve participants' social and economic outcomes upon release. Since its founding in 2018, MIT's Education Justice Institute (TEJI) has offered accredited classes for incarcerated students, with an increasing focus on computer education. Our courses have been delivered both in person and remotely (e.g., via Zoom). In this poster, we share insights into the challenges present in the incarcerated education environment, and highlight how remote learning offers unique advantages to incarcerated students. We also present preliminary findings from two years of data collected across four recurring computer science courses. This poster aims to foster a dialogue with the broader computer science education community, focusing on: (i) qualitative insights gained from extensive interactions with incarcerated education systems, (ii) preliminary empirical results obtained through IRB-approved surveys, (iii) common challenges faced during data collection, and (iv) an opportunity to seek feedback and pose questions to computer science education experts.",
        "authors": [
            "Andrew Fishberg",
            "Marisa Gaetz",
            "Martin Nisser",
            "Carole Cafferty",
            "Lee Perlman",
            "Raechel Soicher",
            "Joshua Long"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158331",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The d γ / 2 -Variation of Distance Profiles in γ -Liouville Quantum Gravity",
        "abstract": "For Brownian surfaces with boundary and an interior marked point, a natural observable to consider is the distance profile, defined as the process of distances from the marked point to a variable point lying on the boundary. When the boundary is parametrized by the natural length measure on it, this distance profile turns out to be locally absolutely continuous to Brownian motion, and as a result, the boundary length measure itself has a natural interpretation as the quadratic variation process of the distance profile. In this paper, we extend this interpretation to γ -Liouville quantum gravity ( γ -LQG), a one-parameter family of models of random geometry which is known to specialize to the case of Brownian geometry for the case γ = 8 / 3 . With d γ denoting the Hausdorff dimension of γ -LQG, we show that for a γ -LQG surface with boundary, the natural boundary length measure can be interpreted (up to a constant factor) as the d γ / 2 -variation process of the distance profile from an interior point.",
        "authors": [
            "Manan Bhatia"
        ],
        "journal_conference_name": "Communications in Mathematical Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159040",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Systematic Simulations of Structural Stability, Phonon Dispersions, and Thermal Expansion in Zinc-Blende ZnO",
        "abstract": "Zinc oxide (ZnO) has recently gained considerable attention due to its exceptional properties, including higher electron mobility, good thermal conductivity, high breakdown voltage, and a relatively large exciton-binding energy. These characteristics helped engineers to develop low dimensional heterostructures (LDHs)-based advanced flexible/transparent nanoelectronics, which were then integrated into thermal management systems. Coefficients of thermal expansion α(T),\r\n phonon dispersions  ωj(q→)\r\n, and Grüneisen parameters  γj(q→)\r\n can play important roles in evaluating the suitability of materials in such devices. By adopting a realistic rigid-ion model in the quasi-harmonic approximation, this work aims to report the results of a methodical study to comprehend the structural, lattice dynamical, and thermodynamic behavior of zinc-blende (zb) ZnO. Systematic calculations of ωj(q→)\r\n, γj(q→),\r\n and α(T)\r\n have indicated negative thermal expansion (NTE) at low T. Soft transverse acoustic shear mode gammas  γTA\r\n at critical points offered major contributions to NTE. Our results of ωj(q→)\r\n at ambient pressure compare reasonably well with Raman scattering spectroscopy measurements and first-principles calculations. By adjusting the layers of materials with positive and negative thermal expansion, it is possible to create LDHs with near-zero α(T)\r\n. Such a nanostructure might experience a minimal dimensional change with T fluctuations, making it ideal for devices where precise dimensional stability is crucial.",
        "authors": [
            "Devki N. Talwar",
            "Piotr Becla"
        ],
        "journal_conference_name": "Nanomaterials",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158301",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Orbit Plane Rotation Using Aerocapture",
        "abstract": "This study investigates the feasibility of performing orbit plane rotations during aerocapture maneuvers. Three-degrees-of-freedom bounding trajectories at Mars are propagated for a range of vehicle lift-to-drag ratios 𝐿/𝐷\r\n and hyperbolic arrival velocities 𝑣∞\r\n. The results show that the maximum plane rotation achievable increases with vehicle 𝐿/𝐷\r\n and 𝑣∞\r\n. When arriving with 𝑣∞\r\n of 6 km/s, vehicles with 𝐿/𝐷\r\n of 0.25 and 1.0 can achieve plane rotations of up to 11.6 and 45.3 deg, respectively. Heat rate, heat load, and g-loading constraints identified when rotating the orbital plane are not more severe than those observed for two-dimensional aerocapture at a given 𝐿/𝐷\r\n and 𝑣∞\r\n. A direct tradeoff between the maximum plane rotation and entry corridor width exists that will affect the ability of lower 𝐿/𝐷\r\n vehicles to achieve large plane rotations. The proposed maneuver can allow the captured orbit inclination and right ascension of the ascending node to be altered in ways that are not possible using typical interplanetary orbit targeting methods. Further, the maneuver offers the possibility of deploying multiple satellites to different orbits around a target destination using a single launch or approach path.",
        "authors": [
            "Daniel C. Gochenaur",
            "Michael P. Jones",
            "Johannes J. Norheim",
            "Olivier L. de Weck"
        ],
        "journal_conference_name": "Journal of Spacecraft and Rockets",
        "publisher": "American Institute of Aeronautics and Astronautics",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158529",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "\"Why is my code slow?\" Efficiency Bugs in Student Code",
        "abstract": "While prior research has categorized common errors and code quality issues of student programmers, little attention has been paid to researching student efficiency bugs. Qualitative content analysis of 250 slow student submissions across five CS2 assignments yielded over 750 efficiency bugs. Extracting general themes resulted in an efficiency bug taxonomy with three main categories: superfluous computation, suboptimal data structure design, and suboptimal algorithm design, with 12 subcategories. Analysis of specific bug frequencies across the assignments provided insights that may inform content design for programming courses.",
        "authors": [
            "Hope Dargan",
            "Adam Gilbert-Diamond",
            "Adam Hartz",
            "Robert Miller"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158329",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Long-Term Ageing Studies on Eco-Friendly Resistive Plate Chamber Detectors",
        "abstract": "In high-energy physics, resistive plate chamber (RPC) detectors operating in avalanche mode make use of a high-performance gas mixture. Its main component, Tetrafluoroethane (C2H2F4), is classified as a fluorinated greenhouse gas. The RPC EcoGas@GIF++ collaboration is pursuing an intensive R&D on new gas mixtures for RPCs to explore eco-friendly alternatives complying with recent European regulations. The performance of different RPC detectors has been evaluated at the CERN Gamma Irradiation Facility with Tetrafluoropropene (C3H2F4)-CO2-based gas mixtures. A long-term ageing test campaign was launched in 2022, and since 2023, systematic long-term performance studies have been carried out thanks to dedicated beam tests. The results of these studies are discussed together with their future perspectives.",
        "authors": [
            "Marcello Abbrescia",
            "Giulio Aielli",
            "Reham Aly",
            "Maria Cristina Arena",
            "Mapse Barroso Ferreira",
            "Luigi Benussi",
            "Stefano Bianco",
            "Fabio Bordon",
            "Davide Boscherini",
            "Alessia Bruni",
            "Salvatore Buontempo",
            "Mattia Busato",
            "Paolo Camarri",
            "Roberto Cardarelli",
            "Liliana Congedo",
            "Marilisa De Serio",
            "Francesco Debernardis",
            "Anna Di Ciaccio",
            "Luigi Di Stante",
            "Pascal Dupieux"
        ],
        "journal_conference_name": "Particles",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158989",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Practical DB-OS Co-Design with Privileged Kernel Bypass",
        "abstract": "This paper revisits the longstanding challenge of coordinating database systems with general-purpose OS interfaces, such as POSIX, which often lack tailored support for DB requirements. Existing approaches to this DB-OS co-design struggle with limited design space, security risks, and compatibility issues. To overcome these hurdles, we propose a new co-design approach leveraging virtualization to elevate the privilege level of DB processes. Our method enables database systems to fully exploit hardware capabilities via virtualization, while minimizing the need for extensive modifications to the host OS kernel, thereby maintaining compatibility. We demonstrate the effectiveness of our approach through two novel virtual memory mechanisms tailored for database workloads: (1) an efficient snapshotting mechanism that captures memory snapshots at millisecond intervals for in-memory databases and HTAP workloads, and (2) a streamlined in-kernel buffer pool design. We introduce Libdbos, a lightweight guest kernel implementing these mechanisms. Our evaluations highlight significant improvements in latency and efficiency compared to existing snapshotting and buffer pool designs, underscoring the potential of the approach.",
        "authors": [
            "Xinjing Zhou",
            "Viktor Leis",
            "Jinming Hu",
            "Xiangyao Yu",
            "Michael Stonebraker"
        ],
        "journal_conference_name": "Proceedings of the ACM on Management of Data",
        "publisher": "ACM",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158440",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Predicting Mortality in Subarachnoid Hemorrhage Patients Using Big Data and Machine Learning: A Nationwide Study in Türkiye",
        "abstract": "Background/Objective: Subarachnoid hemorrhage (SAH) is associated with high morbidity and mortality rates, necessitating prognostic algorithms to guide decisions. Our study evaluates the use of machine learning (ML) models for predicting 1-month and 1-year mortality among SAH patients using national electronic health records (EHR) system. Methods: Retrospective cohort of 29,274 SAH patients, identified through national EHR system from January 2017 to December 2022, was analyzed, with mortality data obtained from central civil registration system in Türkiye. Variables included (n = 102) pre- (n = 65) and post-admission (n = 37) data, such as patient demographics, clinical presentation, comorbidities, laboratory results, and complications. We employed logistic regression (LR), decision trees (DTs), random forests (RFs), and artificial neural networks (ANN). Model performance was evaluated using area under the curve (AUC), average precision, and accuracy. Feature significance analysis was conducted using LR. Results: The average age was 56.23 ± 16.45 years (47.8% female). The overall mortality rate was 22.8% at 1 month and 33.3% at 1 year. One-month mortality increased from 20.9% to 24.57% (p < 0.001), and 1-year mortality rose from 30.85% to 35.55% (p < 0.001) in the post-COVID period compared to the pre-COVID period. For 1-month mortality prediction, the ANN, LR, RF, and DT models achieved AUCs of 0.946, 0.942, 0.931, and 0.916, with accuracies of 0.905, 0.901, 0.893, and 0.885, respectively. For 1-year mortality, the AUCs were 0.941, 0.927, 0.926, and 0.907, with accuracies of 0.884, 0.875, 0.861, and 0.851, respectively. Key predictors of mortality included age, cardiopulmonary arrest, abnormal laboratory results (such as abnormal glucose and lactate levels) at presentation, and pre-existing comorbidities. Incorporating post-admission features (n = 37) alongside pre-admission features (n = 65) improved model performance for both 1-month and 1-year mortality predictions, with average AUC improvements of 0.093 ± 0.011 and 0.089 ± 0.012, respectively. Conclusions: Our study demonstrates the effectiveness of ML models in predicting mortality in SAH patients using big data. LR models’ robustness, interpretability, and feature significance analysis validate its importance. Including post-admission data significantly improved all models’ performances. Our results demonstrate the utility of big data analytics in population-level health outcomes studies.",
        "authors": [
            "Taghi Khaniyev",
            "Efecan Cekic",
            "Neslihan Nisa Gecici",
            "Sinem Can",
            "Naim Ata",
            "Mustafa Mahir Ulgu",
            "Suayip Birinci",
            "Ahmet Ilkay Isikay",
            "Abdurrahman Bakir",
            "Anil Arat",
            "Sahin Hanalioglu"
        ],
        "journal_conference_name": "Journal of Clinical Medicine",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158299",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Base-Load Nuclear Reactors for Fully Dispatchable Electricity: Nuclear Air-Brayton Combined Cycles, Firebrick Heat Storage, Hydrogen Storage, and Hydrocarbon Biofuels",
        "abstract": "Three partly coupled integrated nuclear energy systems are described. These enable base-load nuclear reactors to provide fully dispatchable electricity without greenhouse-gas emissions, thus replacing gas turbines burning natural gas and batteries storing electricity. These hybrid systems link the industrial sector to the electricity sector. Firstly, electricity-to-high-temperature (1800 &deg;C) gigawatt-hour firebrick heat storage converts low-price electricity to high-temperature stored heat to provide dispatchable heat for industry and power generation. Secondly, Nuclear Air-Brayton Combined Cycles (NACC) with thermodynamic topping cycles using high-temperature stored heat or combustible fuel to provide dispatchable electricity. Peak power output can be two to five times the base-load electricity production. The heat-to-electricity efficiency of the thermodynamic topping cycles exceeds 70%. Thirdly, nuclear hydrogen production for industrial markets enables the production of dispatchable electricity where hydrogen is used for energy storage but not to produce heat and electricity. Base-load nuclear reactors send electricity to the grid and/or electrolyzers for hydrogen production depending upon electricity prices. Low-cost hydrogen storage enables us to meet steady-state industrial hydrogen demands, even though hydrogen and grid electricity production is varied. Hydrogen production for industrial uses (ammonia fertilizer, direct reduction of iron ore to iron replacing coke, cellulosic liquid hydrocarbon biofuels replacing crude oil) may exceed 20% of total energy demand and may be a massive source of dispatchable electricity. The biofuels provide storable energy when heat storage is depleted.",
        "authors": [
            "Charles Forsberg"
        ],
        "journal_conference_name": "Energies",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158300",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Engineering of Genetically Encoded Bright Near-Infrared Fluorescent Voltage Indicator",
        "abstract": "Genetically encoded voltage indicators (GEVIs) allow for the cell-type-specific real-time imaging of neuronal membrane potential dynamics, which is essential to understanding neuronal information processing at both cellular and circuit levels. Among GEVIs, near-infrared-shifted GEVIs offer faster kinetics, better tissue penetration, and compatibility with optogenetic tools, enabling all-optical electrophysiology in complex biological contexts. In our previous work, we employed the directed molecular evolution of microbial rhodopsin Archaerhodopsin-3 (Arch-3) in mammalian cells to develop a voltage sensor called Archon1. Archon1 demonstrated excellent membrane localization, signal-to-noise ratio (SNR), sensitivity, kinetics, and photostability, and full compatibility with optogenetic tools. However, Archon1 suffers from low brightness and requires high illumination intensities, which leads to tissue heating and phototoxicity during prolonged imaging. In this study, we aim to improve the brightness of this voltage sensor. We performed random mutation on a bright Archon derivative and identified a novel variant, monArch, which exhibits satisfactory voltage sensitivity (4~5% ΔF/FAP) and a 9-fold increase in basal brightness compared with Archon1. However, it is hindered by suboptimal membrane localization and compromised voltage sensitivity. These challenges underscore the need for continued optimization to achieve an optimal balance of brightness, stability, and functionality in rhodopsin-based voltage sensors.",
        "authors": [
            "Xian Xiao",
            "Aimei Yang",
            "Hanbin Zhang",
            "Demian Park",
            "Yangdong Wang",
            "Balint Szabo",
            "Edward S. Boyden",
            "Kiryl D. Piatkevich"
        ],
        "journal_conference_name": "Department of Brain and Cognitive Sciences",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158298",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Lessons from COVID-19 patient visitation restrictions: six considerations to help develop ethical patient visitor policies",
        "abstract": "Patient visitor restrictions were implemented in unprecedented ways during the COVID-19 pandemic and included bans on any visitors to dying patients and bans separating mothers from infants. These were implemented without high quality evidence they would be beneficial and the harms to patients, families and medical personnel were often immediately clear. Evidence has also accumulated finding strict visitor restrictions were accompanied by long-term individual and societal consequences. We highlight numerous examples of restrictions that were enacted during the COVID-19 pandemic, including some that continue to be in place today. We outline six specific concerns about the nature and effects of the visitor restrictions seen during the COVID-19 pandemic. These considerations may help provide both an ethical and science-based framework, through which healthcare workers, families and government entities can work towards safeguarding patient and family rights and well-being.",
        "authors": [
            "Tracy B. Høeg",
            "Benjamin Knudsen",
            "Vinay Prasad"
        ],
        "journal_conference_name": "Monash Bioethics Review",
        "publisher": "Springer International Publishing",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158289",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Taxonomy for Social Sustainability in Corporate Communication",
        "abstract": "Sustainability, or environmental, social, and governance (ESG), reports have become ubiquitous among major companies in recent years, often criticized as tools for greenwashing and met with significant backlash. While the environmental aspects of these reports are well-defined, social sustainability remains poorly understood. Through an analysis of narrative sections from six corporate sustainability reports narrative sections, we propose an initial taxonomy of constitutive social sustainability concepts reflected in corporate speech.",
        "authors": [
            "Amelia Dogan",
            "Laura Frye-Levine",
            "Ava Malysa"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158181",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Search for long-lived heavy neutral leptons in proton-proton collision events with a lepton-jet pair associated with a secondary vertex at √s = 13 TeV",
        "abstract": "A search for long-lived heavy neutral leptons (HNLs) using proton-proton collision data corresponding to an integrated luminosity of 138 fb−1 collected at s = 13 TeV with the CMS detector at the CERN LHC is presented. Events are selected with a charged lepton originating from the primary vertex associated with the proton-proton interaction, as well as a second charged lepton and a hadronic jet associated with a secondary vertex that corresponds to the semileptonic decay of a long-lived HNL. No excess of events above the standard model expectation is observed. Exclusion limits at 95% confidence level are evaluated for HNLs that mix with electron and/or muon neutrinos. Limits are presented in the mass range of 1–16.5 GeV, with excluded square mixing parameter values reaching as low as 2 × 10−7. For masses above 11 GeV, the presented limits exceed all previous results in the semileptonic decay channel, and for some of the considered scenarios are the strongest to date.",
        "authors": [
            "A. Hayrapetyan",
            "A. Tumasyan",
            "W. Adam",
            "J. W. Andrejkovic",
            "T. Bergauer",
            "S. Chatterjee",
            "K. Damanakis",
            "M. Dragicevic",
            "P. S. Hussain",
            "M. Jeitler",
            "N. Krammer",
            "A. Li",
            "D. Liko",
            "I. Mikulec",
            "J. Schieck",
            "R. Schöfbeck",
            "D. Schwarz",
            "M. Sonawane",
            "The CMS collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158277",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Protein codes promote selective subcellular compartmentalization",
        "abstract": "Cells have evolved mechanisms to distribute ~10 billion protein molecules to\r\nsubcellular compartments where diverse proteins involved in shared functions must\r\nassemble. Here, we demonstrate that proteins with shared functions share amino\r\nacid sequence codes that guide them to compartment destinations. A protein\r\nlanguage model, ProtGPS, was developed that predicts with high performance the\r\ncompartment localization of human proteins excluded from the training set.\r\nProtGPS successfully guided generation of novel protein sequences that selectively\r\nassemble in the nucleolus. ProtGPS identified pathological mutations that change\r\nthis code and lead to altered subcellular localization of proteins. Our results\r\nindicate that protein sequences contain not only a folding code, but also a\r\npreviously unrecognized code governing their distribution to diverse subcellular\r\ncompartments.",
        "authors": [
            "Henry R. Kilgore",
            "Itamar Chinn",
            "Peter G. Mikhael",
            "Ilan Mitnikov",
            "Catherine Van Dongen",
            "Guy Zylberberg",
            "Lena Afeyan",
            "Salman F. Banani",
            "Susana Wilson-Hawken",
            "Tong Ihn Lee",
            "Regina Barzilay",
            "Richard A. Young"
        ],
        "journal_conference_name": "Science",
        "publisher": "American Association for the Advancement of Science",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158180",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Association Between Medicaid Expansion and Insurance Status, Risk Group, Receipt, and Refusal of Treatment Among Men with Prostate Cancer",
        "abstract": "Simple Summary\r\nWe sought to quantify the impact of Medicaid expansion on insurance status, stage at diagnosis, time to treatment initiation, and refusal of locoregional treatment among patients with prostate cancer, the second leading cause of cancer death among men in the United States. We found that while Medicaid expansion was associated with increased insurance coverage and decreased refusal of radiation therapy, there was no significant association with earlier risk group at diagnosis, treatment within 180 days, nor refusal of locoregional therapy. Similarly, racial minorities experienced no significant changes in time to treatment initiation following Affordable Care Act implementation compared to White patients. Ultimately, more research is needed to understand how Medicaid expansion affects cancer outcomes and whether these effects are borne equitably among different populations.\r\n\r\nAbstract\r\nBackground: Although the Patient Protection and Affordable Care Act (ACA) has been associated with increased Medicaid coverage among prostate cancer patients, the association between Medicaid expansion with risk group at diagnosis, time to treatment initiation (TTI), and the refusal of locoregional treatment (LT) among patients requires further exploration. Methods: Using the National Cancer Database, we performed a retrospective cohort analysis of all patients aged 40 to 64 years diagnosed with localized prostate cancer from 2011 to 2016. Difference-in-difference (DID) analysis was used to compare changes in insurance status, risk group at diagnosis, TTI, and the refusal of LT among patients residing in Medicaid expansion versus non-expansion states. In a secondary analysis, we used DID to compare changes in the above outcomes among racial minorities versus White patients living in expansion states. Results: Of the 112,434 patients with prostate cancer in our analysis, 50,958 patients lived in Medicaid expansion states, and 61,476 patients lived in non-expansion states. In the adjusted analysis, we found that the proportion of uninsured patients (adjusted DID: −0.87%; 95% confidence interval [95% CI]: −1.28 to −0.46) and patients who refused radiation therapy (adjusted DID: −0.71%; 95% CI: −0.95 to −0.47) decreased more in expansion states compared to non-expansion states. Similarly, we observed that the racial disparity of select outcomes in expansion states narrowed, as racial minorities experienced larger absolute decreases in uninsured status and the refusal of radiation therapy (RT) regimens than White patients following ACA implementation (p < 0.01 for all). However, residence in a Medicaid expansion state was not associated with changes in risk group at diagnosis, TTI, nor the refusal of LT (p > 0.01 for all); racial disparities in TTI were also exacerbated in expansion states following ACA implementation. Conclusions: The association between Medicaid expansion and prostate cancer outcomes and disparities remains unclear. While ACA implementation was associated with increased insurance coverage and decreased refusal of RT, there was no significant association with earlier risk group at diagnosis, TTI within 180 days, or refusal of LT. Similarly, racial minorities in expansion states had larger decreases in uninsured status and the refusal of RT regimens, as well as smaller increases in intermediate-/high-risk disease at presentation than White patients following ACA implementation, but experienced no significant changes in TTI. More research is needed to understand how Medicaid expansion affects cancer outcomes and whether these effects are borne equitably among different populations.",
        "authors": [
            "Tej A. Patel",
            "Bhav Jain",
            "Edward Christopher Dee",
            "Khushi Kohli",
            "Sruthi Ranganathan",
            "James Janopaul-Naylor",
            "Brandon A. Mahal",
            "Kosj Yamoah",
            "Sean M. McBride",
            "Paul L. Nguyen",
            "Fumiko Chino",
            "Vinayak Muralidhar",
            "Miranda B. Lam",
            "Neha Vapiwala"
        ],
        "journal_conference_name": "Cancers",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158243",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Recent Progress in Flexible Piezoelectric Tactile Sensors: Materials, Structures, Fabrication, and Application",
        "abstract": "Flexible tactile sensors are widely used in aerospace, medical and health monitoring, electronic skin, human–computer interaction, and other fields due to their unique advantages, thus becoming a research hotspot. The goal is to develop a flexible tactile sensor characterized by outstanding sensitivity, extensive detection range and linearity, elevated spatial resolution, and commendable adaptability. Among several strategies like capacitive, piezoresistive, and triboelectric tactile sensors, etc., we focus on piezoelectric tactile sensors because of their self-powered nature, high sensitivity, and quick response time. These sensors can respond to a wide range of dynamic mechanical stimuli and turn them into measurable electrical signals. This makes it possible to accurately detect objects, including their shapes and textures, and for them to sense touch in real time. This work encapsulates current advancements in flexible piezoelectric tactile sensors, focusing on enhanced material properties, optimized structural design, improved fabrication techniques, and broadened application domains. We outline the challenges facing piezoelectric tactile sensors to provide inspiration and guidance for their future development.",
        "authors": [
            "Jingyao Tang",
            "Yiheng Li",
            "Yirong Yu",
            "Qing Hu",
            "Wenya Du",
            "Dabin Lin"
        ],
        "journal_conference_name": "Sensors",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158242",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Convergence to Bohmian Mechanics in a de Broglie-Like Pilot-Wave System",
        "abstract": "Bohmian mechanics supplements the quantum wavefunction with deterministic particle trajectories, offering an alternate, dynamical language for quantum theory. However, the Bohmian wavefunction evolves independently of these trajectories, and is thus unaffected by the observable properties of the system. While this property is widely assumed necessary to ensure agreement with quantum mechanics, much work has recently been dedicated to understanding classical pilot-wave systems, which feature a two-way coupling between particle and wave. These systems—including the “walking droplet” system of Couder and Fort (Couder and Fort (2006) Phys. Rev. Lett. 97:154101) and its various abstractions (Dagan and Bush (2020) CR Mecanique 348:555–571; Durey and Bush (2020) Front. Phys. 8:300; (2021) Chaos 31:033136; Darrow and Bush (2024) Symmetry 16:149)—allow us to investigate the limits of classical systems and offer a touchstone between quantum and classical dynamics. In this work, we present a general result that bridges Bohmian mechanics with this classical pilot-wave theory. Namely, Darrow and Bush ((2024) Symmetry 16:149) recently introduced a Lagrangian pilot-wave framework to study quantum-like behaviours in classical systems; with a particular choice of particle-wave coupling, they recover key dynamics hypothesised in de Broglie’s early double-solution theory (de Broglie (1970) Foundations Phys. 1:5–15). We here show that, with a different choice of coupling, their de Broglie-like system reduces exactly to single-particle Bohmian mechanics in the non-relativistic limit. Our result clarifies that, while multi-particle entanglement is impossible to replicate in general with local, classical theories, no such restriction exists for single-particle quantum mechanics. Moreover, connecting with the previous work of Darrow and Bush, our work demonstrates that de Broglie’s and Bohm’s theories can be connected naturally within a single Lagrangian framework. Finally, we present an application of the present work in developing a single-particle analogue for position measurement in a de Broglie-like setting.",
        "authors": [
            "David Darrow"
        ],
        "journal_conference_name": "Foundations of Physics",
        "publisher": "Springer US",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158274",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Data Artifacts Glossary: a community-based repository for bias on health datasets",
        "abstract": "Background The deployment of Artificial Intelligence (AI) in healthcare has the potential to transform patient care through improved diagnostics, personalized treatment plans, and more efficient resource management. However, the effectiveness and fairness of AI are critically dependent on the data it learns from. Biased datasets can lead to AI outputs that perpetuate disparities, particularly affecting social minorities and marginalized groups. Objective This paper introduces the “Data Artifacts Glossary”, a dynamic, open-source framework designed to systematically document and update potential biases in healthcare datasets. The aim is to provide a comprehensive tool that enhances the transparency and accuracy of AI applications in healthcare and contributes to understanding and addressing health inequities. Methods Utilizing a methodology inspired by the Delphi method, a diverse team of experts conducted iterative rounds of discussions and literature reviews. The team synthesized insights to develop a comprehensive list of bias categories and designed the glossary’s structure. The Data Artifacts Glossary was piloted using the MIMIC-IV dataset to validate its utility and structure. Results The Data Artifacts Glossary adopts a collaborative approach modeled on successful open-source projects like Linux and Python. Hosted on GitHub, it utilizes robust version control and collaborative features, allowing stakeholders from diverse backgrounds to contribute. Through a rigorous peer review process managed by community members, the glossary ensures the continual refinement and accuracy of its contents. The implementation of the Data Artifacts Glossary with the MIMIC-IV dataset illustrates its utility. It categorizes biases, and facilitates their identification and understanding. Conclusion The Data Artifacts Glossary serves as a vital resource for enhancing the integrity of AI applications in healthcare by providing a mechanism to recognize and mitigate dataset biases before they impact AI outputs. It not only aids in avoiding bias in model development but also contributes to understanding and addressing the root causes of health disparities.",
        "authors": [
            "Rodrigo R. Gameiro",
            "Naira L. Woite",
            "Christopher M. Sauer",
            "Sicheng Hao",
            "Chrystinne O. Fernandes",
            "Anna E. Premo",
            "Alice R. Teixeira",
            "Isabelle Resli",
            "An-Kwok I. Wong",
            "Leo A. Celi"
        ],
        "journal_conference_name": "Journal of Biomedical Science",
        "publisher": "BioMed Central",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158286",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Data Are Power: Addressing the Power Imbalance Around Community Data with the Open-Access Data4HumanRights Curriculum",
        "abstract": "Data4HumanRights’ training materials have been developed as open-source and tailored to limited-resource settings, where community data collectors often live and work. Access to training on data collection, analysis, and visualisation to support the advocacy of vulnerable groups is essential, particularly in the context of increasing human rights challenges such as land rights, adequate housing, conflicts, and climate justice. This paper provides an overview of how the training materials were co-developed with community data collectors in Nigeria and Kenya, offering insights into the fundamental principles (i.e., inclusiveness, adaptive, limited resources, and being gender- and incentive-sensitive) and the structure of the open-access training materials. The development process resulted in 28 modules, each designed to be delivered in a face-to-face format in less than one day by a local trainer. To maximize adaptivity, the training modules can be mixed and matched (e.g., as individual modules or a learning path of several modules around a specific training need). The individual modules cover a range of methods and tools that are useful to human rights work and community advocacy, e.g., documenting evictions, performing rapid needs assessments after acute crises, community profiling, and monitoring community development indicators. The training materials contain instructions for the training facilitator(s) and all necessary training materials. To ensure inclusivity, the training covers both basic and advanced topics, with most modules designed to address basic needs that can be followed using a mobile phone, thereby avoiding the need for computers or printed handouts. The training results in Nigeria and Kenya showcase applications, including mapping waste problems and addressing forced evictions. Trained community groups produced maps of waste piles to prioritize community actions, such as finding space for urban agriculture, and conducted rapid needs assessments during a massive eviction. This approach helps reduce power imbalances and empowers community groups to effectively manage and utilise their own data.",
        "authors": [
            "Monika Kuffer",
            "Dana R. Thomson",
            "Dianne Wakonyo",
            "Nicera Wanjiru Kimani",
            "Divyani Kohli-Poll Jonker",
            "Enyo Okoko",
            "Rasak Toheeb",
            "Bisola Akinmuyiwa",
            "Mohammed Zanna",
            "Dezyno Imole",
            "Andrew Maki"
        ],
        "journal_conference_name": "Department of Urban Studies and Planning",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158297",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploring the Holiday Effect on Elevated Traffic-Related Air Pollution with Hyperlocal Measurements in Chengdu, China",
        "abstract": "Traffic-related air pollutants (TRAPs) pose significant health risks in megacities, yet fixed monitoring sites often fail to capture their complexity. To characterize the TRAP concentrations which fixed sites cannot address, we employed a mobile platform to effectively capture real-time hyperlocal-scale TRAP variations in Chengdu, China. A 17-day sampling campaign was conducted covering the National Holiday of China and collected ~1.2 × 105 1 Hz paired data. We measured particle number concentration (PNC), black carbon (BC), and nitrogen oxides (NOx) across urban and rural freeway environments to assess the impact of reduced heavy-duty diesel vehicles (HDDVs) during the holiday (i.e., holiday effect). No clear impact of wind direction on TRAP concentrations was found in this study. However, substantial differences (two times) were observed when comparing non-holiday to holiday campaigns. Spearman correlations (0.21–0.56) between TRAPs persistently exceeded Pearson correlations (0.14–0.41), indicating non-linear relationships and suggesting the necessity for data transformations (e.g., logarithms) in TRAP analysis. The comparison of the background subtracted TRAPs concentrations between non-holiday and holidays, revealing approximately a 50% reduction in TRAPs across microenvironments. Among the TRAPs, NOx emerged as a reliable indicator of HDDV emissions. The study provides insights into vehicle fleet composition impacts, paving the way for enhanced exposure assessment strategies.",
        "authors": [
            "Sheng Xiang",
            "Jiaojiao Yu",
            "Yu Ting Yu",
            "Pengbo Zhao",
            "Tie Zheng",
            "Jingsong Yue",
            "Yuanyuan Yang",
            "Haobing Liu"
        ],
        "journal_conference_name": "Atmosphere",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158296",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Quantum Computing from Graphs",
        "abstract": "While stabilizer tableaus have proven exceptionally useful as a descriptive tool for additive quantum codes, they otherwise offer little guidance for concrete constructions or coding algorithm analysis. We introduce a representation of stabilizer codes as graphs with certain structures. Specifically, the graphs take a semi-bipartite form wherein input nodes map to output nodes, such that output nodes may connect to each other but input nodes may not. Intuitively, the graph’s input-output edges represent information propagation of the encoding circuit, while output-output edges represent the code’s entanglement structure. We prove that this graph representation is in bijection with tableaus and give an efficient compilation algorithm that transforms tableaus into graphs. We then show that this map is efficiently invertible, which gives a new universal recipe for code construction by way of finding graphs with sufficiently nice properties.\r\n\r\nThe graph representation gives insight into both code construction and algorithms. To the former, we argue that graphs provide a flexible platform for building codes particularly at small non-asymptotic scales. We construct as examples several constant-size codes and several infinite families codes. We also leverage graphs in a probabilistic analysis to extend the quantum Gilbert-Varshamov bound into a three-way distance-rate-weight trade-off. To the latter, we show that key coding algorithms, distance approximation, weight reduction, and decoding, are unified as instances of a single optimization game on a graph. Moreover, key code properties such as distance, weight, and encoding circuit depth, are all controlled by the graph degree. We give efficient algorithms for producing simple encoding circuits whose depths scale as twice the degree and for implementing logical diagonal and certain Clifford gates with non-constant but reduced depth. Finally, we construct a simple efficient decoding algorithm and prove a performance guarantee for a certain classes of graphs. These results give evidence that graphs are generically useful for the study of quantum computing and its practical implementations.",
        "authors": [
            "Andrey Boris Khesin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158853",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Simple Models for Complex Tropical Dynamics",
        "abstract": "Studying Earth's tropics is an essential part of understanding the climate, simulating the Earth system, and predicting the societal impacts of weather. In this thesis, we use a hierarchy of models -- including analytically tractable equations, simplified simulations, and full general circulation models -- to study tropical phenomena including the Hadley Circulation, the Inter-Tropical Convergence Zone (ITCZ), the South Asian monsoon, Pacific and ENSO seasonality, the Walker Circulation, and the modeling of the tropical energy budget. We begin with an examination of tropical SSTs and the ITCZ under warming, finding that the Hadley cells weaken and tropical SST gradients decrease in a warmer climate. The ocean's subtropical cells strengthen and transport more energy in a warmer climate, further flattening SST gradients. The ITCZ, meanwhile, increases in strength with warming because of the exponential relationship between humidity and temperature, and the presence of a dynamic ocean changes a single-ITCZ with a sinusoidal seasonal cycle to a double-ITCZ with a square wave seasonal cycle. Next, we study the ``monsoonal mode,'' an energy and precipitation anomaly triggered by the South Asian Monsoon that moves into the West Pacific during Northern Hemisphere autumn. The monsoonal mode is discussed as a possible underlying cause of the seasonality of the Pacific, i.e., that the West Pacific and ENSO both have seasonalities that favor one season despite being on the equator. To show this, ENSO seasonality is examined using simplified simulations and an energy budget of the Central-Eastern Equatorial Pacific. Similar techniques are then used to study ENSO events in warmer climates, and it is found that the Pacific zonal SST gradient and the Walker circulation, which are the sources of ENSO instability, weaken with warming, decreasing  the magnitude of ENSO events. Lastly, we assess the energy budget of CMIP6 models. It is shown that all CMIP6 models have more energy input to the deep tropics than ERA5 reanalysis, and this bias is bigger in the Southern Hemisphere. The hemispheric asymmetry in this bias can be traced back to radiation absorbed by the atmosphere, which is associated with dust (for shortwave radiation) and total column water (for longwave radiation). As a whole, this thesis demonstrates the utility of studying complex problems with simple models and deepens our understanding of Earth's tropics.",
        "authors": [
            "P.J. Tuckman"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158855",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Electrospray Thrusters in Chemical-Electric Multimode Propulsion for Small Satellites",
        "abstract": "Propulsion for small spacecraft is typically one of two modes, chemical or electric. These modes offer complementary propulsive performance: chemical propulsion provides high thrust and low specific impulse, while electric propulsion provides the inverse. As such, having access to both modes on the same spacecraft (i.e. multimode propulsion) is extremely useful. Unfortunately, the conventional propellants used by chemical and electric thrusters are highly incompatible, making this particularly difficult on small spacecraft that lack the mass, power, and volume to accommodate two separate propulsion systems. However, recent advancements in green monopropellants -- developed as less-toxic alternatives to hydrazine in chemical monopropellant thrusters -- have created a new family of ionic liquids monopropellants, making them the natural propellant for a highly compact form of electric propulsion known as electrospray thrusters. This presents a unique opportunity for a propellant to be shared between two propulsion modes, decreasing required mass and volume to be feasible for small spacecraft. This thesis examines the use of ionic liquid monopropellants in electrospray thrusters for a multimode chemical-electric propulsion system. This thesis focuses particularly on ASCENT, a high-maturity monopropellant with flight heritage in chemical thrusters.\r\n\r\nIn this work, the performance of ASCENT in the MIT ion Electrospray Propulsion System (iEPS) is extensively characterized. Experimental work includes ion plume diagnostics, indirectly and directly obtained performance estimates, temperature-dependent performance estimates, and extended duration firing behavior. Preliminary studies of similar monopropellants are also conducted to assess their use in a multimode system. To support an upcoming technology demonstration flight, a new multimode-compatible iEPS thruster tank is designed, fabricated, and validated. The integration and operation requirements for this thruster in a flight-ready system are defined. Finally, the mission benefits of an ASCENT multimode system for CubeSats are compared against current commercial options using an Earth observation mission case study.\r\n\r\nThis work finds that an iEPS thruster with ASCENT propellant has thrust of 9-15 µN, a specific impulse of 600-750 seconds, and a total efficiency of 18-22%, depending on current setpoint. We find that ASCENT is slightly volatile in high vacuum, which causes time-dependent losses in efficiency and specific impulse from gradual propellant evaporation. This volatility may also increase thruster lifetime by mitigating the risk of thruster failure by emitter flooding. This work also identified a modified version of ASCENT, created when the propellant is exposed to iron. This modified version produces a dramatically higher thrust and thrust-to-power compared to standard ASCENT. Additionally, flight-ready configurations of a multimode system are defined for 6U, 12U, and 27U CubeSats. A case study analysis found that the benefits of a chemical-electrospray multimode system are best realized at the 12U scale and above. Overall, this thesis provides critical insights on the performance, integration, and operation of electrospray thrusters with ionic liquid monopropellants. These results can then be used to enable a multimode propulsion system for small satellites.",
        "authors": [
            "Amelia R. Bruno"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158790",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Fast methods for full-wave electromagnetic solvers in MRI",
        "abstract": "High static field ( 3T) MR scanners can produce human tissue images of astounding clarity, but rely on high frequency ( 123MHz) electromagnetic radiation that generates complex in-tissue field patterns that are patient-specific and potentially harmful. Many such scanners use multiple transmitters to better control field patterns, but then adjust the transmitters based on general guidelines rather than optimizing for the specific patient, mostly because computing patient-specific fields was presumed far too slow. It was recently demonstrated that the combination of fast low-resolution tissue mapping and fast voxel-based field simulation can be used to perform a rapid patient-specific MR safety check. However, the field simulation still required several minutes, making it too slow to perform the dozens of simulations that would be needed for patient-specific optimization. In this work, we develop a set of numerical acceleration techniques that facilitate fast field simulations that bridge the gap between the performance of current state-of-art full-wave electromagnetic packages and time requirements dictated by real-time patient-specific field optimization in a clinical setting. These techniques cater to a large range of body sizes and complex coil geometries.",
        "authors": [
            "Georgy D. Guryev"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158943",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Detection and Localization of Pressure Transients in Water Distribution Systems",
        "abstract": "Water distribution systems are critical to urban water supply, but as they age they become increasingly vulnerable to bursts and leaks, leading to significant economic, social, and environmental consequences. The complexity and inaccessibility of underground pipelines present substantial challenges for their maintenance. As a result, the development of real-time monitoring systems for these systems is essential to reduce water waste and minimize adverse impacts to consumers and surrounding infrastructures. This thesis investigates the effectiveness of continuous pressure monitoring systems in detecting pipe bursts and transient events within water distribution systems. Using PTSNet, a parallel transient simulation Python package, we simulate pipe burst events at each node in a real-world system and examine the pressure-time response at all other nodes. By adding Gaussian noise to the simulation results to mimic real-world background noise, we assess the detection success of pressure signals at each node using a modified CUSUM algorithm. The correlations between detection success and three spatial metrics between the source and sensor are calculated. We show that a spatial metric, the effective number of magnitude-changing junctions along the fastest path, (NJFP), has a stronger correlation with detection success than the shortest travel path or the shortest distance. By comparing detection performance for networks with differing topologies (gridded, looped, and branched) and pipe characteristics, we discover that multiple shortest paths (MSP; where pressure waves from different paths arrive almost simultaneously at the sensor) amplify the signal due to transient interference phenomenon and enhance the detectability of transients. This effect is particularly pronounced in gridded networks. We investigate the capabilities of monitoring, from a network of fixed stations, to achieve unique localization of pressure transient events using a time-reversal back-propagation algorithm. This algorithm identifies the event source by matching the theoretical and detected arrival time differences at the sensors. A novel time differences space is constructed, representing the independent shortest time differences from locations along all the pipes to the sensors, based on network information and sensor locations. Pipe sections with unique shortest time differences are identified as uniquely localizable pipes. Effective-NJFP-based probabilities of transient detection with accurate arrival times (error < 0.1s) are derived from these simulation results. The localization performance of the sensor network is evaluated by the probability-weighted total lengths of the pipes that can be uniquely localized.\r\nWe consider sensor placement strategies aimed at maximizing the detection and localization performance of pressure monitoring sensor networks. Detection performance is defined as the total weighted pipe lengths in the network, where the weight of each pipe corresponds to its detection probability. Two problems are addressed: In order to maximize transient event detection performance when only a limited number of sensors are available, we formulate a mixed-integer programming (MIP) optimization model and employ a genetic algorithm to find solutions. The second problem involves determining the minimum number of sensors and their optimal locations to detect transient events across the entire network without a constraint on the number of available sensors. This is formulated as a minimum set cover problem, and an optimal solution is obtained using a mixed-integer linear programming solver. We focus on maximizing transient localization performance with a limited number of sensors. A genetic algorithm is applied to determine sensor locations, and the solutions obtained by this method provide significantly better localization performance than other approaches. We show differences in sensor placements for detection and localization: sensors are more evenly distributed throughout the network for detection purposes, while for localization, they are more concentrated in areas with longer pipes and simpler network structures. Finally, we present an analysis of two pressure monitoring datasets collected from a real-world water distribution system (SLG network). The first dataset consists of data from 28 sensors with a 100 Hz sampling frequency, collected over 7 to 30 days. We propose a method to identify and analyze noise levels and distributions at each sensor. Using a modified CUSUM algorithm, we detect transients and correlate them across sensors to identify events detected by multiple sensors. A transient-magnitude-based clustering method is then employed to group events based on their magnitudes, followed by a localization approach that utilizes the arrival time differences of transients between sensors. The findings indicate that noise levels in real-world monitoring data vary both spatially and temporally and are not independently normally distributed. Additionally, the arrival times detected by the modified CUSUM algorithm may not always accurately reflect the true transient arrival times due to mismatches between the signal characteristics and tuning of model parameters. Accurately identification of transient arrival time is particularly challenging for slowly changing pressure wave fronts. The second dataset includes pressure monitoring data from 7 sensors, during which 14 active transients with known source locations, times, and magnitudes were generated. We apply the modified CUSUM algorithm to detect transients at the sensors and correlate detection success with spatial metrics. The analysis confirms that the effective NJFP has the highest correlation with detection success, consistent with the simulation results. Additionally, the transient magnitude ratios between sensors and the source are found to be similar to the ratios calculated based on theoretical transmission coefficients when the source and sensor are in close proximity, suggesting that transmission coefficients can be used to estimate transient magnitudes in real networks.",
        "authors": [
            "Shiqing Liu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158880",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Information-centric Algorithms for Feature Extraction in High-Dimensional Sequential Data",
        "abstract": "Hidden Markov Models (HMMs) are a cornerstone of sequential data analysis, offering a robust framework for modeling observable events influenced by hidden internal states. With applications spanning speech recognition, video analysis, bioinformatics, and financial time series, HMMs enable the prediction and classification of raw data by leveraging their dual-layer stochastic structure: hidden Markov states and observable outputs. However, as real-world data grows increasingly high-dimensional, extracting meaningful features from observations becomes critical to reduce computational complexity while retaining relevant information.\r\n\r\nThis thesis addresses key challenges in feature extraction for high-dimensional HMMs. Current methods, such as neural networks (NNs), are widely used for nonlinear feature learning but lack mechanisms to prioritize useful features or incorporate known structural constraints. To bridge this gap, this work proposes novel algorithms to decouple representation learning from task-specific objectives and extract features aligned with predefined constraints.\r\n\r\nThe theoretical foundation, including local information geometry and Hirschfeld-Gebelein-Rényi (HGR) maximal correlation, is introduced in Chapter 2. Chapter 3 details three innovative feature extraction algorithms and their corresponding neural network architectures, highlighting their strengths and limitations. Convergence analyses and tail bounds for these methods are presented in Chapter 4. Numerical simulations validating the efficacy of the proposed approaches are provided in Chapter 5, while Chapter 6 concludes with a summary of contributions and potential future research directions.\r\n\r\nThis thesis advances the field by offering structured, constraint-aware feature extraction techniques tailored for high-dimensional sequential data, setting the stage for more effective and interpretable inference in HMMs.",
        "authors": [
            "Jiejun Jin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158923",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "System-Technology Co-Optimization of Scaled Electronics\r\nBased on Two-Dimensional Materials",
        "abstract": "Over the past 60 years, the semiconductor industry has focused on developing highly scaled electronic devices and high-density integrated circuits. However, bottlenecks have arisen recently as transistor dimensions approach the physical limits, and integration density is constrained. This thesis addresses these issues with two-dimensional (2D) materials, which includes inventing a low-temperature (< 300 °C) metal-organic chemical vapor deposition (MOCVD) method for 2D materials on 8-inch wafers, investigating extreme device scaling and multi-channel transistors. Design-Technology Co-Optimization (DTCO) and SystemTechnology Co-Optimization (STCO) are employed to rapidly model, evaluate and optimize device and circuit performance. Moreover, heterogeneous integration and monolithic 3D integration techniques are investigated, addressing challenges in integrating 2D materials with silicon complementary-metal-oxide-semiconductor (CMOS) circuits and flexible substrates. This research aims to advance high-density, high-performance electronics with low-power consumption for next-generation integrated systems.",
        "authors": [
            "Jiadi Zhu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158961",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Improved Complexity Analysis for the Proximal Bundle Algorithm Under a Novel Perspective",
        "abstract": "The proximal bundle algorithm (PBA) is a fundamental and computationally effective algorithm for solving optimization problems with non-smooth components. We investigate its convergence rate in two settings. We first focus on a composite setting where one function is smooth and the other is piecewise linear. We interpret a sequence of null steps of the PBA as a Frank-Wolfe algorithm on the Moreau envelope of the dual problem. In light of this correspondence, we first extend the linear convergence of Kelley's method on convex piecewise linear functions from the positive homogeneous to the general case. Building on this result, we propose a novel complexity analysis of PBA and derive a O (epsilon^-4/5) iteration complexity, improving upon the best known O (epsilon^-2) guarantee. This approach also unveils new insights on bundle management. We then present the first variant of the PBA for smooth objectives, achieving an accelerated convergence rate of O(epsilon^-1/2 log(epsilon^-1)), where epsilon is the desired accuracy. Our approach addresses an open question regarding the convergence guarantee of the PBA, which was previously posed in two recent papers. We interpret the PBA as a proximal point algorithm and base our proposed algorithm on an accelerated inexact proximal point scheme. Our variant introduces a novel null step test and oracle while maintaining the core structure of the original algorithm. The newly proposed oracle substitutes the traditional cutting planes with a smooth lower approximation of the true function. We show that this smooth interpolating lower model can be computed as a convex quadratic program. We finally show that Nesterov acceleration can be effectively applied when the objective is the sum of a smooth function and a piecewise linear one.",
        "authors": [
            "David Fersztand"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158820",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Towards an Artificial Neuroscience: Analytics for Language Model Interpretability",
        "abstract": "The growing deployment of neural language models demands greater understanding of their internal mechanisms. The goal of this thesis is to make progress on understanding the latent computations within large language models (LLMs) to lay the groundwork for monitoring, controlling, and aligning future powerful AI systems. We explore four areas using open source language models: concept encoding across neurons, universality of learned features and components across model initializations, presence of spatial and temporal representations, and basic dynamical systems modeling.\r\n\r\nIn Chapter 2, we adapt optimal sparse classification methods to neural network probing, allowing us to study how concepts are represented across multiple neurons. This sparse probing technique reveals both monosemantic neurons (dedicated to single concepts) and polysemantic neurons (representing multiple concepts in superposition) in full-scale LLMs confirming predictions from toy models. In Chapter 3, we identify and exhaustively catalog universal neurons across different model initializations by computing pairwise correlations of neuron activations over large datasets. Our findings show that 1-5\\% of neurons are universal, often with clear interpretations, and we taxonomize them into distinct neuron families.\r\n\r\nTo investigate spatial and temporal representations, we analyze LLM activations on carefully curated datasets of real-world entities in Chapter 4. We discover that models learn linear representations of space and time across multiple scales, which are robust to prompting variations and unified across different entity types. We identify individual \"space neurons\" and \"time neurons\" that reliably encode spatial and temporal coordinates. In Chapter 5, we use optimal sprase regression techniques to improve the sparse identification of nonlinear dynamics (SINDy) framework, demonstrating improved sample efficiency and support recovery in canonical differential systems. We then leverage this improvement to study the ability of LLMs to in-context learn dynamical systems and find internal representations which track the underlying system state.",
        "authors": [
            "Robert Wesley Gurnee"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158869",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Leveraging Structure for Efficient and Dexterous Contact-Rich Manipulation",
        "abstract": "Contact-rich manipulation has proved challenging due to the need to consider multiple combinatoric possibilities of making or breaking contact with the surrounding environment. As a result, existing methods have often resorted to combinatorial optimization that utilizes dynamics structure but considers all possibilities exhaustively, or compute-heavy and inefficient sampling methods that utilize blackbox optimization such as Reinforcement Learning (RL). In this thesis, I aim to show that by combining structured contact smoothing in conjunction with local gradient-based control and sampling-based motion planning, we can bypass the combinatorial explosion of contact modes while still leveraging structure and achieve highly efficient contact-rich manipulation. To achieve this capability, I first shed light on how RL abstracts contact modes and optimizes difficult landscapes by combining stochastic smoothing and zeroth-order optimization; yet, I show how following a similar stochastic strategy while using gradients suffers from several drawbacks such as empirical bias and high variance. To leverage structure in a more helpful manner, I propose a method for smoothing contact dynamics without relying on stochastic smoothing, bypassing these drawbacks. Using this smoothing scheme, I present a highly efficient and performant local control based on gradient-based trajectory optimization and model predictive control. Finally, I connect these local control capabilities with global sampling-based motion planners to achieve long-horizon global plans. The proposed method achieves contact-rich plans such as dexterous in-hand reorientation and whole-body manipulation much more efficiently than RL while being highly scalable compared to methods that explicitly reason about contact modes. These results achieve a reduction of contact-rich manipulation to kinodynamic motion planning, and exposes the true difficulty of contact-rich manipulation from combinatorial explosion in contact modes to combinatorial and highly non-local decisions over motion planning behaviors.",
        "authors": [
            "Hyung Ju Terry Suh"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158946",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Laboratory Astrophysics Studies of Magnetized Collisionless Shock Precursors and the ³He³He Proton Spectrum at the OMEGA Laser Facility",
        "abstract": "Laboratory astrophysics enables the study of astrophysical systems in the lab. There are broadly two types of laboratory astrophysics experiments: macrophysics and microphysics. Macrophysics experiments study a scaled down version of an astrophysical system while microphysics experiments create a small volume of matter with the same conditions as an astrophysical system. This thesis details work related to both macrophysics and microphysics laboratory astrophysics experiments. For the macrophysics contribution, collisionless shocks experiments were conducted at the OMEGA laser facility using the new gas jet platform. Collisionless shocks are shock waves formed through plasma processes when particle collisions are negligible. These shocks can form as bow shocks in the interaction between the solar wind and planetary ionospheres and can accelerate charge particles to high energies. In the experiment, a CH plasma flow collides with a hydrogen gas jet plasma to create a forming magnetized collisionless shock. Different diagnostics show a moving density jump, strong magnetic fields, and the acceleration of electrons. These observations coupled with magnetohydrodynamics and kinetic particle-in-cell simulations paint a complete physical picture of the forming shock in a configuration similar to the bow shock of Venus. Late time proton radiographs show a complicated structure which is studied for magnetic turbulence. Turbulence is important in several astrophysical systems, especially collisionless shocks where it dissipates shock kinetic energy and is essential for accelerating charged particles to cosmic ray energies. Magnetic power spectra extracted from proton radiography data show a break in the spectrum between the ion Larmor radius and the ion skin depth for high plasma β, a sign of kinetic turbulence. Large scale particle-in-cell simulations of high β turbulence also have this feature showing that the experimental data are consistent with high β kinetic turbulence. For the microphysics contribution, a new proton spectrometer is designed for measurements of the ³He³He proton spectrum. The ³He³He fusion reaction is the last step of the proton-proton I chain which produces the majority of the sun’s power. Previous experiments were not able to measure the ³He³He proton spectrum below 6 MeV. A new proton step range filter (SRF) spectrometer with a larger energy range is designed using a Monte Carlo tool. This tool uses Geant4 and is able to self-consistently apply the instrument response function. The new SRF design is validated and a method for analyzing experimental data using the Monte Carlo code is presented.",
        "authors": [
            "Timothy Mark Johnson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158838",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Enabling AI Copilots for Engineering Design With Parametric, Graph, And Component Inputs",
        "abstract": "Engineering design demands the synthesis of multimodal and often incomplete data—ranging from detailed parametric specifications, assembly graphs, visual references, and textual descriptions. Despite growing interest in generative models for design ideation and exploration, state-of-the-art approaches struggle with incomplete inputs, lack of support for modalities other than text and image, and limited controllability. This thesis addresses these gaps by unifying two complementary advances:\r\n\r\nFirst, we introduce a graph-guided diffusion approach for parametric data completion. By coupling Graph Attention Networks with a diffusion-based imputation mechanism, our method acts as a highly accurate and creative design auto-completion system for incomplete partial designs. On a dataset of 12,500 bicycles, this design imputation framework achieves a root mean square error (RMSE) of approximately 0.92 on numerical features and an error rate of around 0.18 for categorical attributes, outperforming both classical imputation methods such as MissForest, hotDeck, PPCA and advanced diffusion-based baselines such as TabCSDI. Moreover, it achives a Diversity Score of 3.10, surpassing all baselines, illustrating that the imputation process transforms incomplete data into multiple creative designs.\r\n\r\nSecond, we develop a multimodal control architecture that can extend foundation models to condition their generation processes with all or a subset of parametric inputs, assembly graphs, component images, and textual constraints. This model tremendously enhances both the controllability and precision of the generation process of foundational generative models, enabling controlling modalities that were not possible before. We first show that our model excels at tasks that state-of-the-art models struggle on. We further validate the performance of our model with surrogate models that investigate individual features. Our model achieves 95% or greater R^2 scores on different continuous parameters. Further, we show that our model is able to generate creative and novel designs while maintaining a high level of precision. This enables engineers to guide generative outputs along precise dimensional, aesthetic, and functional targets. Across numerous trials of different settings, we observe that our pipeline robustly fuses tabular parametric information, assembly graphs, and reference component images to produce results aligned with both specification precision and creativity. \r\n\r\nTogether, these contributions establish a coherent framework for AI-augmented design exploration. By viewing missing parameters as an opportunity for data-driven design autocompletion and by tightly integrating multimodal control over foundation models, this work elevates generative AI from a niche conceptual tool to a reliable design copilot. The implications of this thesis are profound: we show the possibilities and the pathways to AI copilot systems that can reduce data bottlenecks, broaden design spaces, and offer more thorough, constraint-adherent design candidates. As engineering problems grow in complexity and scale, the synergy of high-fidelity parametric imputation and multimodal control promises to accelerate innovation, cut development cycles, and guide human designers toward more inventive and manufacturable solutions.",
        "authors": [
            "Rui Zhou"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158805",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Radium and Mercury Dynamics in the Arctic: Investigating Terrestrial Inputs, Groundwater Discharge, and Chemical Cycling in a Changing Climate",
        "abstract": "The Arctic Ocean is distinctive due to its extreme seasonal variations in temperature and significant terrestrial inputs, including freshwater, carbon, nutrients, and toxins. Of particular concern is mercury (Hg) in its neurotoxic form, methylmercury (MeHg), which is already beginning to adversely affect Arctic human populations and wildlife. However, the region’s harsh conditions and remoteness have made conducting seasonal chemical and hydrological studies challenging. Tracers of boundary inputs, such as the radium (Ra) isotope quartet, offer potential for tracking and quantifying riverine and submarine groundwater discharge (SGD) of species like Hg into the Arctic Ocean. This thesis employs seasonal data and laboratory experiments to investigate the factors influencing terrestrial Ra inputs to the Arctic Ocean, quantifies SGD and associated Hg inputs to an Arctic coastal lagoon, and elucidates the chemical and geological factors influencing Hg cycling in Arctic groundwater.\r\n\r\nUsing historical and unpublished datasets combined with new laboratory investigations, differences in inputs of riverine Ra isotopes between the North American and Eurasian land masses were identified. The findings revealed higher Ra fluxes from the North American continent, attributed to greater sediment loads and lower organic matter in rivers compared to those on the Eurasian land mass. Subsequently, Ra data from five extensive field campaigns to Simpson Lagoon, Alaska, provided insights into Ra cycling on a more localized scale. These campaigns offered the first seasonal perspective on supra-permafrost SGD along an Arctic coastline, suggesting that SGD fluxes may rival those of rivers along the Beaufort Sea coast. Concurrently collected Hg groundwater concentrations allowed for the development of the first estimates of Hg fluxes from groundwater to the Arctic Ocean. If these estimates hold true along the rest of the Pan-Arctic coastline, they could significantly alter our understanding of microbial MeHg uptake in the Arctic Ocean. Finally, sediment cores from Simpson Lagoon and two other locations along the Beaufort Sea coast were used to examine how changing groundwater conditions, such as changing salinity, temperature, and redox conditions, influence Hg cycling. These experiments, alongside findings from Simpson Lagoon groundwater, indicate that Hg cycling in recently thawed permafrost sediments involves a complex interplay between organic material, metal oxides, and sulfide species, with groundwater conditions and soil carbon content playing crucial roles in Hg mobilization.",
        "authors": [
            "Emma Jacqueline Bullock"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158870",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Development of Dual Extruder Biomaterial 3D Printer",
        "abstract": "This research presents the design and fabrication of a novel dual-extruder biotic 3D printer for the precise deposition of natural biocomposites using organic materials such as pectin, chitosan, and cellulose. Unlike traditional FDM printers that rely on thermoplastic extrusion, this printer employs a syringe-based mechanical extruder capable of depositing viscous biomaterial hydrogels. The integration of a first-of-its-kind dual-extruder system enables the fabrication of multi-material prints and the exploration of biomaterial composites and complex geometric structures, thereby advancing sustainable, bio-inspired manufacturing.\r\nThis thesis emphasizes the machine engineering aspects of the printer's development, including project motivation, systematic design methodology, component design and fabrication, testing, and exploration of future work. Notable features of the system include user-friendly operation for non-experts, open-source accessibility, and compatibility with a wide range of biomaterials. By addressing existing limitations in biomaterial 3D printing technology, this work provides a robust platform to support future research in biomaterials, sustainable additive manufacturing, and bio-inspired design. Furthermore, the open-source nature of the printer fosters innovation and collaboration, accelerating the adoption of sustainable materials and manufacturing methods.",
        "authors": [
            "Jesse P. de Alva"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158914",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design of a cam and follower linear actuator for satellite optical systems",
        "abstract": "Optical systems for satellites are used to image and track the physical environment of earth from space. Where the optical system images can be controlled through the rotation and movement of the optical system. Optical alignment is achieved though linear actuators, which constrain different degrees of freedom of the optical system. Optical systems require precise alignment, meaning the linear actuators that align them must have precise resolutions. During satellite launch, the satellite experiences both high acceleration and large magnitude vibrations, \r\nwhich can damage equipment. Common precision actuation methods cannot meet the high stiffness required for these satellite linear actuators. A cam and follower linear actuator was \r\ndesigned to fulfill these stiffness and precision requirements. Through modeling the dynamic and kinematic interactions between the cam and follower, a cam shape was designed, and necessary materials were chosen. Next through analysis of process capabilities of available \r\nfabrication tools, manufacturing methods for different parts were selected. Finally, using components designed for testing, kinematic tests were conducted on the linear actuator. Testing \r\nof the actuator demonstrated it was capable of actuating with a precision of 9.15 microns. More testing is needed to understand the stiffness of the device.",
        "authors": [
            "Darrell Brown"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158847",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Uncovering the link between twin-twin interactions and damage nucleation in an (α+β) Ti alloy",
        "abstract": "Recently, a (α+β) Ti alloy was developed with an outstanding combination of both high strength and high ductility; however, the plasticity micromechanisms that lead to damage nucleation for this alloy had not yet been investigated in detail. In this work, post-mortem analysis and an in-situ SEM-EBSD tensile experiment were conducted to determine where damage was nucleating most frequently in the microstructure, and what deformation modes were associated with damage nucleation. Damage within primary α grains was found to be the most common, with most of these damage incidents occurring along {10̅12} twin-twin boundaries with a ~60° misorientation. The {10̅12} twinning mode is only activated in the localized neck, and twin activation is strongly dependent on initial crystallographic texture. The twinned domains are rotated such that prismatic slip is easier to activate, but prismatic slip transfer is unlikely across ~60° twin-twin boundaries due to geometric incompatibilities. The in-situ test revealed that a crack formed along a ~60° twin-twin boundary where slip was blocked. These findings provide new insights into how twin-twin interactions in Ti alloys can lead to damage nucleation and impact overall ductility.",
        "authors": [
            "Megan F. L. Cooper"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158903",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Forecasting the lift of a randomly maneuvering airfoil\r\nunder dynamic stall conditions, Re ∼ 10⁵",
        "abstract": "Dynamic stall is the abrupt flow separation from airfoils rapidly changing their orientation. This phenomenon, characterized by a delayed stall followed by a sharp drop in lift, has prompted efforts to prevent or delay it. This study aims to predict the lift of an airfoil randomly maneuvering under dynamic stall conditions by utilizing sparse surface pressure measurements, which we believe can maximize the effectiveness of various dynamic stall suppression techniques. Using data from large eddy simulations, we demonstrate that a long short-term memory network, fed with raw surface pressures, delivers accurate predictions. Also, a new method introduced here, IdDM, conclusively links the characteristic frequency range of pressure fluctuations that emerges during the dynamic stall to the chord-lengthscale vortex dynamics. However, further analysis suggests that the forecast predominantly relies on the lower frequency components tied to the airfoil motion, possibly because the vortex dynamics are dependent on and sensitive to the airfoil motion. Meanwhile, specific sensor locations are proven to be more informative than others in this random, unsteady flow, and we show that optimal sensor placement can be quickly determined using mutual information alone. It reveals that two pressure sensors positioned near the leading edge, one on each side of the airfoil, capture most of the information needed to predict lift. The lift can be predicted with sparse sensors because surface pressures are strongly correlated across the airfoil, with large-scale flow structures dominating the forces.",
        "authors": [
            "Donghyun Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158900",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Reasoning over Hierarchical Abstractions for Long-Horizon Planning in Robotics",
        "abstract": "We aim to enable robots to act intelligently in complex environments not explicitly designed around them. In order to do so, robots can simplify decision making by forming hierarchical abstractions of their world, and planning within those representations. However, in reality, the types of abstractions robots are able to build are often poorly aligned with the planning problems they must solve, which limits how useful those abstractions can be in efficient decision making. For example, autonomous agents struggle in many real world scenarios, particularly when their environments are large, cluttered with obstructions, or beset by uncertainty. These factors often imply that decisions made at higher levels of abstraction may not be easily refined to low level plans, leading to backtracking during either search or execution. In this thesis, we consider contributions which improve the efficiency and quality of long-horizon hierarchical planning in robotics. Specifically, we propose approaches which explicitly reason about the imperfections of the abstractions available to robots during planning, and show how those methods can improve performance on a variety of tasks and environments.\r\n\r\nThere are three primary settings for which we make contributions in this thesis. First, we will consider the problem of solving tasks in partially revealed environments, wherein our abstract plans cannot be known to be feasible until we attempt execution because the world is not fully known at planning time. To solve this problem, we first develop a high level planning representation which recognizes that actions that enter unknown space can either succeed or fail with some probability. The first contribution of this work is then to learn to predict the feasibility and cost of actions within that abstraction from visual input. We also describe a method for planning which uses these predictions, and we are able to show that our approach can generate plans that are significantly faster at completing tasks in unknown environments experimentally when compared with heuristic driven baselines. Next, we will discuss work in Task and Motion Planning (TAMP), where the world is fully known, but the problems require complex interaction with the environment to the point that we must intelligently guide search in order to find plans efficiently. We build upon our work in the first setting by once again learning to predict the outcome and cost of different sub-tasks within a TAMP abstraction. We further contribute a novel method to guide search in this setting for plans which minimize cost given our learned predictions, and demonstrate the ability to find faster plans than established TAMP approaches both in simulation, and on real world robots. In our final problem setting, we consider attempting to solve TAMP problems in real world, large-scale environments. To do this, we define an approach for constructing tractable planning abstractions from real perception using hierarchical scene graphs, ensuring that when we refine our abstract plans within these representations, the low-level trajectories still satisfy the given task’s constraints. A major contribution of this work is an approach for planning efficiently in these domains by pruning provably superfluous information from the world model. The unifying aim of the work in this thesis is to develop approaches which enable robots to solve complex tasks in large-scale, real world environments without human intervention. To that end, across all contributions, we demonstrate experimentally on real robots the importance of accounting for imperfections in hierarchical abstraction during planning.",
        "authors": [
            "Christopher P. Bradley"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158796",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Causal Inference with Survival Outcomes via Orthogonal Statistical Learning",
        "abstract": "The field of causal inference has recently made great strides in incorporating machine learning into confounding adjustment and estimation of heterogeneous treatment effects (HTE). However, there were some gaps regarding survival outcomes.\r\n\r\nFirst, overlap-weighted effect estimators based on machine learning nuisance models were not available for such outcomes. Thus, researchers wishing to mitigate bias and variance from poor overlap had to accept potential bias from nuisance model misspecification in its place. In Chapter 2, we fill this gap by proposing a class of one-step cross-fitted double/debiased machine learning estimators for cumulative weighted average treatment effects for both survival outcomes and competing risk outcomes. Our approach combines importance sampling, semiparametric theory, and Neyman orthogonality to resolve both model misspecification and lack of covariate overlap between treatment arms in observational studies with censored outcomes. We give regularity conditions for the consistency, asymptotic linearity, and semiparametric efficiency bounds of the proposed estimators. Through simulation, it is shown that the proposed estimators do not require oracle parametric nuisance models. We apply the proposed estimators to compare the effects of two first-line anti-diabetic drugs on cancer outcomes.\r\n\r\nSecond, a wide range of machine learning methods (or ”learners”) for estimating heterogeneous treatment effects were not applicable to estimating effects on survival outcomes, particularly in the presence of competing risks. In Chapter 3, we fill this gap by developing several once-for-all (orthogonal) censoring unbiased transformations which convert time-to-event data into continuous outcomes, such that all HTE learners and oracle rates for continuous outcomes can be borrowed. Our approach not only reduces the pressing need to develop various HTE learners for censored outcomes and especially competing risks, but also fully leverage the state of the art of existing schemes. Through direct application of HTE learners to these transformed continuous outcomes, we obtain consistent estimates of heterogeneous cumulative incidence effects, total effects, and separable direct effects. We provide generic model-free learner-specific oracle inequalities bounding the finite-sample excess risk. The oracle efficiency results depend on the oracle selector and estimated nuisance functions from all steps involved in the transformation. We demonstrate the empirical performance of the proposed methods in simulation studies.\r\n\r\nAn important application area for causal inference methods, and one which originally motivated my interest in the field, is drug repurposing. In Chapter 4, we apply the methods of Chapter 2 to investigate whether metformin, a diabetes medication, might also have unexpected beneficial effects on cancer. The analysis encountered three major challenges: poor overlap between treatment groups, model misspecification, and pre-cancer death as competing risks for cancer incidence. To resolve these issues simultaneously, we take balancingweighted total cause-specific effects, controlled direct effect, and separable effects as causal estimands and develop balancing-weighted double/debiased machine learning estimators for both cumulative incidence functions and restricted mean time lost, with all estimators satisfying Neyman orthogonality. Using the Clinical Practice Research Datalink (CPRD) data, we find that metformin revealed a preventive direct effect on cancer incidence over sulfonylureas. The results also demonstrate the advantage of choosing the average treatment effect for the overlap population as the target quantity.\r\n\r\nFinally, just as machine learning helps to automate nuisance model estimation for confounding adjustment and modeling effect heterogeneity, causally informed artificial intelligence (AI) and large language models (LLMs) might help to automate hypothesis generation for drug repurposing and surveillance opportunities. In Chapter 5, we explore this potential by developing a high-throughput screening approach to evaluate available drugs across multiple diseases. The screening methodology aims to identify drug-disease pairs with significant positive signals that could represent promising repurposing candidates, while also detecting pairs with negative signals that might indicate potential safety concerns–both being critical aspects for pharmacoepidemiology research. This systematic approach leverages the convergence of expanding healthcare data sources and modern data science advances to establish a data-driven framework for drug repurposing discovery and pharmacovigilance.\r\n\r\nTo conclude, we discuss the limitations of the proposed methods and provide possible future research directions.",
        "authors": [
            "Shenbo Xu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158798",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Examining the placenta’s role in neurodevelopment in the context of maternal obesity",
        "abstract": "The placenta is a key organ determining fetal development and likely contributes to programming of long-term offspring health, in particular neurodevelopment. Various maternal exposures, such as psychosocial stress, diabetes, infection, and high body mass index (BMI) are associated with higher risks of impaired neurodevelopment in the offspring. One third of women in the United States are affected by maternal obesity (MO) during pregnancy, making it one of the most common exposures.\r\nWe profiled the term placental transcriptome in humans using single-nucleus RNA-seq, comparing expression profiles in MO versus lean conditions, in each of the two faces of the placenta separately. On both sides of the placenta across several cell types, MO was associated with upregulation of hypoxia response genes. On only the maternal-facing side, hypoxia gene expression was associated with offspring neurodevelopment outcomes measured at multiple time-points, in the Genetics of Glucose regulation in Gestation and Growth (Gen3G) cohort, an independent pre-birth cohort with bulk RNA-seq from placental tissue. We leveraged Gen3G to determine genes that correlated with impaired neurodevelopment and found these genes to be most highly expressed in extravillous trophoblasts (EVTs). EVTs further showed the strongest correlation between neurodevelopment impairment gene scores (NDIGSs) and the hypoxia gene score. We validated these findings in EVTs in an independent single-cell RNA-seq cohort from second trimester placenta, and found that cultured EVTs have increased NDIGSs in response to exposure to hypoxia. These data suggest that hypoxia in EVTs may be a key process in the neurodevelopmental programming of fetal exposure to MO. Our work opens up new directions of research, such as exploring applications of antioxidants to potentially mitigate some of the offspring consequences associated with MO.",
        "authors": [
            "Fatima M. Gunter-Rahman"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158860",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Marketplace Multiculturalism",
        "abstract": "Picture Texas. No longer simply cowboys, footballs, and firearms, this land today is sustained by a daily choreography of cross-border commerce, managed by entertainment media turned handheld surveillance, and peppered with enclaves of immigrants from the world over. A contact zone where logistical and legislative apparati warp to serve consumer comfort, Texas today is the world tomorrow: forget the Alamo, it’s highways, tax-incentives, and backyard barbecue on the 21st century frontier. This thesis responds to a call for roadside service stations along a planned international tourist corridor in the Texas-Mexico borderlands with six interventions: a panoramic viewing tower disguised as a billboard, a sunken stadium for athletic agonism, a photovoltaic drive-in charging cinema, an international culinary incubator, a showroom for automated fulfilment, and a customs and border patrol welcome center. These structures are testing grounds for modes of relation and value exchange that edge beyond the outdated positivisms of globalization. They ask how architecture might produce new possibilities and publics by working within and taking advantage of contemporary systems of control. As tourist destinations, the stops suggest the nation’s true mythos lies not in static symbols but in choreographies of transaction and contact. Articulating in built form the dynamic processes that define a territory of sprawl, this proposal suggests that Texas’s most authentic monuments are the stops we make along the way.",
        "authors": [
            "Harris Chowdhary"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158823",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Predicting Flood Risks to City Infrastructure Systems\r\nUtilizing Scalable, Time Sensitive Modeling",
        "abstract": "Flooding is emerging as the most expensive and frequent natural hazard around the world. Floods are highly dynamic in nature and cause physical damage to our built environment, loss of life, economic damage, and major impacts to society. An example of this is the at-ground road system, which comprises 30-60% of a city’s area in the US, is highly susceptible to flood damage, while still needs to act as evacuation routes for local residents. Similarly, the underground built system is extremely vulnerable to flooding damage as well as life risk to anyone within it. With urban landscapes constantly evolving, accurately predicting flood propagation and extent is imperative to mitigate these risks, especially as floods worsen due to climate change.\r\n\r\nHistorically, the focus of flood risk assessment through industry and academia has been on the coastal urban environment, assessing the impact of fluvial flooding. This resulted in many risk assessment tools that mostly caters to the infinite amount of flood water identified from a riverine or coastal fluvial flooding. As for the rain-driven impact, the common practice simply changed the flood modeling to pluvial oriented, keeping the rest of the risk tool components identical for the different flood mechanisms. For pluvial flooding, existing urban flood modeling tools such as SWMM and PC-SWMM are limited by their catchment-based approach, neglecting surface runoff dynamics and spatial-temporal flood impacts. Consequently, these tools fail to capture the full extent of rain-driven floods, underestimating their severity and impact on urban environments.\r\n\r\nAddressing this gap requires sophisticated simulations that account for rain event characteristics and city morphology, yet such simulations are computationally demanding and require detailed urban data. Currently, flood impact analysis tools lack specificity for pluvial flood risks and do not address the risks to various city systems beyond building damage. As a result, the contribution of pluvial floods to overall flood risks is underestimated, compromising infrastructure resilience. As flood model results are a critical component in flood risk assessments, the accuracy of spatial temporal urban flood results will allow the pluvial flood impact assessment to be simplified and the flood damage to the different urban systems will be quantified.\r\n\r\nThis research aims to develop a scalable and streamlined method to accurately quantify the risks of rain-driven floods to urban infrastructure systems. It addresses three key questions: (1) To what extent does current practice underestimate pluvial flood impacts? (2) What are the impacts of pluvial flooding on pavement systems when incorporating spatial-temporal modeling? (3) What is the significance of modeling pluvial floods using urban underground spaces? Using advanced flood modeling and numerical soil-water infiltration techniques, this research will quantify damages and lifecycle impacts to pavement and underground spaces systems. The method will provide information on the spatial and temporal distribution of flood damage and will enable scaling up single-element assessments to system-wide impacts. This holistic approach will improve urban flood risk management, supporting informed decision-making and the development of resilient infrastructure systems.",
        "authors": [
            "Katerina Boukin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158861",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "(De)fluorination of Organic Substrates Mediated by Nontrigonal Phosphorus Triamide",
        "abstract": "Due to its high electronegativity and small size, fluorine atoms form the strongest single bond to carbon, and impart unique physical, chemical, and physiological properties to organic compounds. Therefore, the number of industrially synthesized products containing fluorine has seen a substantial increase in recent decades. The strategies to access organofluorine compounds include two opposite approaches: 1) (nucleophilic, electrophilic, or radical) fluorination, and 2) selective defluorination of polyfluorinated substrates. Both creating and breaking C−F bonds in selective manners are of great importance, and present challenges of their own. The work herein describes chemical transformations incorporating the cleavage or formation of the C−F bonds mediated by nontrigonal phosphorus triamide. Thanks to the enhanced biphilicity resulting from geometric deformation, the Cs-symmetric tricoordinate phosphorus compound can activate strong covalent bonds.\r\n\r\nAt the outset, Chapter 1 reviews the existing literature on (de)fluorinative chemical transformations focused on deoxyfluorination and hydrodefluorination, as well as examples of nontrigonal tricoordinate phosphorus compounds and their characteristic reactivity. Combining the two approaches, Chapters 2 and 3 introduce method development for accessing organofluorine compounds using a butterfly-shaped phosphorus triamide as a bond activator. In Chapter 2, the method for deoxyfluorination of aliphatic alcohol substrates via O−H activation by phosphorus, catalyzed by borane Lewis acids, is detailed. The scope of the method covers tertiary alkyl fluorides, which are generally challenging targets, selectively yielding stereoinversion products for chiral substrates. Chapter 3 reports a closed P(III)/P(V) synthetic cycle for the hydrodefluorination of polyfluoroarene substrates that consists of C−F oxidative addition, F-to-H ligand metathesis, and C−H reductive elimination. The overall sequence is analogous to transition metal-catalyzed aryl cross-coupling reactions. Taken together, the methods described in this dissertation highlight the potential of nontrigonal phosphorus compounds as a mediator for the manipulation of strong covalent bonds, useful in the development of synthetic methods that complement existing ones.",
        "authors": [
            "Soohyun Lim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158937",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design and Engineering of Protected Superconducting\r\nQubits",
        "abstract": "Building extensible quantum information processors becomes increasingly promising as the qubits exhibit longer coherence times. To this end, realizing protected qubits, whose Hamiltonians are inherently resilient to both relaxation and dephasing, has attracted strong interest. In this thesis, we primarily explore the soft 0 − π qubit, a leading candidate for implementing superconducting qubit protection with current fabrication techniques. To enhance protection, the soft 0 − π qubit requires its two major modes, the charge-mode (θ) and the flux-mode (ϕ), to satisfy an asymmetric condition: maximizing charge-mode capacitance while minimizing flux-mode capacitance. The main challenge is therefore reducing stray capacitance from the large charge-mode capacitor, which hinders the reduction of flux-mode capacitance. To address this challenge, we depart from the conventional coplanar interdigitated capacitor design and use parallel-plate capacitors (PPC) with small footprints, achieving the desired large charge-mode capacitance while reducing unwanted stray capacitances. By reducing the capacitor area by a factor of approximately 50, the PPC 0−π qubit has achieved an estimated Eᵠ_C /Eᶿ_C ratio of 30–50, placing it among the highest reported. Additionally, we propose enhanced mode-selective control of the soft 0−π qubit using these parallel-plate capacitors. Finally, we discuss the remaining challenges of the soft 0−π qubit and introduce alternative parameter regimes that can potentially improve Raman-based control and qubit readout.",
        "authors": [
            "Junghyun Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158919",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Freight Distribution During Disasters: Measuring and Improving Operational Performance of Critical Systems",
        "abstract": "The frequency and intensity of weather-related natural disasters have increased in the last őve decades. Moreover, the US faces more than a third of the disaster-related economic losses globally, majority of which are from storms. As the demand for distribution of essential freight increases during disasters, the physical and operational constraints decrease the capacity of the freight distribution systems. Accordingly, public and private-sector stakeholders seek disaster preparedness and response interventions to ensure timely and economic distribution of vital freight to the population in need. The goal of this thesis is to facilitate better strategic and tactical planning that results in higher operational performance of essential freight distribution systems during disasters. We study two critical freight distribution systems, namely, downstream fuel distribution and full truckload transportation of general freight. Truckload transportation plays a vital role in distributing relief supplies during emergencies, and fuel is required for humanitarian operations such as running generators, moving emergency response crews, and evacuation of the affected population. We collaborate with The US Federal Emergency Management Agency in response to multiple North-Atlantic storms and measure the operational performance of these systems under regular and disaster conditions, as well as identify public and private-sector interventions to make the performance better during future disasters. Our research contributes to the bodies of disaster modeling and management, fuel distribution, service procurement, and truckload procurement literature by, i) creating system level understanding of multi-server tandem cyclic queues with time-limited customers, ii) studying process improvement interventions for disasters, iii) quantifying the magnitude, geographical extent, timing, and duration of the causal effects of disaster conditions and consequent disaster relief activities on transportation procurement prices, iv) using datadriven analysis to design ŕexible truckload contracts that consider uncertainty in demand, and v) modeling dynamic-pricing where the buyer offers the price to service providers. In this thesis, we provide several actionable insights for public and private-sector stakeholders to manage freight distribution during future disasters. We identify which process improvement interventions are best suited for which type of downstream fuel distribution system, and which storage terminals should be prioritized under limited budget. We also measure how private-sector shippers should account for changes in truckload spot procurement prices during disaster episodes to manage their budgets and operational decisions. Moreover, we offer an alternate dynamic-priced truckload contract solution for public-sector shippers that deal with uncertain episodic demand in response to disasters. We demonstrate the impact of our research by implementing it to multiple real-life case studies in the US. Furthermore, our methodologies and results are generalizable to other geographical regions as well as other disaster conditions. Thus, we hope that they are used by public and private-sector actors to better manage essential freight distribution moving forward.",
        "authors": [
            "Shraddha Rana"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158803",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design of Lewis Acidic Pnictenium Ions Using Carbone and Capping Arene Ligands for Bond Functionalization",
        "abstract": "Interest in the chemistry of antimony and bismuth is rapidly growing due to isolation of low coordinate, subvalent or Lewis acidic compounds that can mediate reactivity traditionally reserved for their d block counterparts. Ligand strategies play a key role in the isolation of such species. Anionic ligands with large steric profiles, as well as carbenes, have been widely implemented to stabilize subvalent heavy group 15 element compounds. However, synthetic strategies to prepare Lewis acidic antimony and bismuth complexes remain underexplored. Cationization is one of the most common methods used to enhance the Lewis acidity of heavy group 15 elements by creating a vacant p orbital on the pnictogen atom. Lewis acids are also employed in frustrated Lewis pair (FLP) chemistry to enable intra- and intermolecular reactivity. Carbone ligands, which are neutral, 4 electron donor ligands, offer a unique ability to support highly electrophilic main-group elements. This dissertation investigates the stabilization of heavy pnictenium ions using neutral donor ligands, such as carbodicarbenes and capping arene ligands, and explores their potential in Lewis acid-mediated chemistry. In Chapter Two, the synthesis and characterization of a series carbodicarbene-pnictenium ions is described. The utilization of strongly donating carbodicarbene ligands enables the isolation of mono-, di- and tri-cationic antimony and bismuth cations. These ions have multiple bond character between carbon and antimony/bismuth, representing some of the first examples of stibaalkene and bismaalkene cationic compounds. The Lewis acidity of these ions was assessed using the Gutmann-Beckett method and computationally derived fluoride ion affinities, the latter of which indicates Lewis superacidity for the bis(pyridyl)carbodicarbene-pnictenium trications. In Chapter Three, the reactivity of the bis(pyridyl)-carbodicarbene stibenium trication toward C(sp³)–H and C(sp)–H bonds is demonstrated. The Lewis superacidic antimony cation mimics the chemistry of frustrated Lewis pairs in the presence of the sterically encumbered base 2,6-di-tert-butylpyridine to enable C–H bond breaking of acetonitrile and a set of terminal alkynes. Kinetic analyses, in conjunction with density functional theory, support a mechanism by which acetonitrile coordinates to antimony, acidifying the C–H bonds, which can be subsequently deprotonated by the base in solution. The resulting stiba-methylene nitrile and stiba-alkynyl adducts undergo reactivity with elemental iodine to generate iodoacetonitrile and 1-iodoalkynes while reforming a stibenium trication. In Chapter Four, capping arene ligands are coordinated to antimony and bismuth tribromide to afford a series of κ²-bound complexes. Bromide abstraction from these neutral adducts affords ionic compounds. Both the neutral and ionic species have distinctive Menschutkin interactions, whereby the lone pair on the pnictogen atom is oriented toward the π system of the pendant arene. Shortening of the distances between the pyridyl nitrogen atoms and pnictogen atom are observed upon cationization from the neutral adducts. The Lewis acidity of these complexes was assessed using the Gutmann-Beckett method. Notably, acceptor numbers as high as 111 are observed for these ions.",
        "authors": [
            "Levi Warring"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158947",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Underwater Semantic Simultaneous Localization and\r\nMapping",
        "abstract": "Building semantically meaningful object level maps of underwater environments is crucial for enabling higher-level autonomy, fostering human-robot collaboration, and providing compressed map representations for bandwidth-constrained underwater communications, while localizing against such maps can improve the positioning accuracy of underwater vehicles by correcting for odometric drift. However, underwater semantic simultaneous localization and mapping (SLAM) has lagged behind analogous terrestrial and aerial semantic SLAM techniques largely due to the lack of large labeled underwater datasets and the challenging sensor modalities specific to underwater environments. To address these shortcomings, this thesis develops a range of methodologies to advance underwater semantic SLAM capabilities. \r\n\r\nFirst, self-supervised learning and visual foundation models are leveraged to detect and segment underwater objects in an open-set manner, i.e., objects need not be present in the training dataset to be detected. The machinery of the open-set object detection technique breaks several assumptions made by existing closed-set semantic SLAM methods. Thus, new methods for object representation and data association are proposed and demonstrated. A method to localize underwater objects is then developed through an analysis of the geometry of underwater monocular cameras and multibeam sonars. \r\n\r\nFinally, a formulation of open-set object-level place recognition as a graph matching problem is introduced. The formulation includes a method for calculating and tracking semantic uncertainty for open-set object detections. Experimental results on both underwater and terrestrial datasets demonstrate that the proposed formulation can be used for real-time accurate open-set object-based place recognition. \r\n\r\nIn summary, techniques for underwater object detection, localization, and data association are introduced and integrated with probabilistic graphical models for open-set semantic SLAM. The proposed techniques are tested across a wide variety of scenarios, and are shown to generalize to terrestrial settings as well.",
        "authors": [
            "Kurran Singh"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158852",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "CO₂ Capture with Lithium Oxide in Molten Salt Media : A Case Study of CO₂ Capture via Electrochemically Produced Metal Oxide",
        "abstract": "As the unprecedented temperature rise originating from anthropogenic carbon dioxide (CO₂) emission intensifies, the development of post-combustion carbon capture technologies has been urged. Although its maturity, conventional thermal swing processes using aqueous amines, suffer from significant limitations, including high energy requirements and sorbent degradation. Electrochemical CO₂ capture technologies, which use electrical energy instead of thermal energy, have emerged as an energy efficient way to capture CO₂. This shift not only improves energy efficiency but also reduces reliance on fossil fuels, further contributing to reduction in CO₂ emissions. This work explored the potential of electrochemical metal oxide formation for CO₂ capture, a promising alternative to amine-based systems due to its exceptional sorbent (i.e., metal oxide) stability. Li₂O in eutectic mixture of potassium nitrate (KNO₃) and lithium nitrate (LiNO₃) was chosen as a case study due to the relatively well-understood chemistry of the system and the potential synergistic effects between metal oxide and the molten salt. Primarily, we investigated the synergistic effect of Li₂O in nitrate molten salt via thermal gravimetric analysis. Next, electrochemically produced Li₂O by reduction of oxygen gas was tested as a CO₂ sorbent while investigating parameters affecting its conversion to lithium carbonate (Li₂CO₃). Through this study, we suggested dissolution model as a crucial pathway for conversion. Lastly, we explored the effect of adding nitrite ion (NO₂⁻) to the molten salt. Irreversible side reaction between NO₂⁻ and CO₂ was confirmed with X-ray diffraction and NOₓ measurement. This thesis demonstrates the feasibility of electrochemical metal oxide-based CO₂ capture, highlighting some considerations in the capture step.",
        "authors": [
            "Gi Hyun Byun"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158875",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Learning-Based Complex Terrain Navigation Under Uncertainty",
        "abstract": "In complex off-road environments, accurately identifying traversable terrain is crucial for achieving fast and reliable navigation. Existing methods learn terrain properties directly from data via self-supervision to automatically penalize trajectories moving through undesirable terrain. However, challenges remain in properly quantifying and mitigating risk due to uncertainty in learned models and improving model generalization in novel environments. To address these challenges, this thesis presents a unified framework to learn uncertainty-aware, physics-informed traversability models and achieve risk-aware navigation in both indistribution and out-of-distribution terrain. First, the proposed method efficiently quantifies both aleatoric and epistemic uncertainty by learning discrete traversability distributions and probability densities of the traversability predictor’s latent features. Leveraging evidential deep learning, this work parameterizes Dirichlet distributions with network outputs and proposes a novel uncertainty-aware squared Earth Mover’s distance loss with a closed-form expression that enhances learning accuracy and navigation performance. Second, the proposed method achieves risk-aware navigation by simulating state trajectories with the worst-case expected traversability values to handle aleatoric uncertainty and by penalizing trajectories moving through novel terrain with high epistemic uncertainty. Third, the proposed method improves model generalization by embedding physics priors directly into the mathematical formulation of evidential neural networks and implicitly aligning learned models with physics models through a physics-informed training loss. Finally, through extensive simulation and real-world experiments on wheeled and quadruped robots, it is demonstrated that this work leads to faster navigation with higher success rates when compared to existing risk-aware approaches, even in environments with significant distribution shifts.",
        "authors": [
            "Xiaoyi Cai"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158876",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design-technology Co-optimization for Sub-2 nm Technology Node Based on 2D Materials",
        "abstract": "Emerging disruptive technologies such as Artificial Intelligence (AI) and 6G communications have driven stringent demands for hardware components that enable faster and more energy-efficient computation. With the diminishing returns of traditional silicon-based scaling and the escalating complexity of advanced semiconductor processes, two-dimensional (2D) materials offer promising opportunities when developed through Design-Technology Co-Optimization (DTCO). This thesis presents a comprehensive study of DTCO with a novel framework tailored for 2D material-based electronics that addresses critical challenges in material synthesis, device design, and circuit integration. In this framework, experimental material and device data are integrated into the design and optimization of MoS₂-based multichannel transistors (MCTs). With the help of DTCO, we have achieved record performance for double-gate, single-channel MoS₂ transistors as well as the first demonstration of high-performance, functional double channel MoS₂ transistors. Based on the results of MCTs, a Process Design Kit (PDK) is developed to facilitate circuit-level integration. These advancements constitute a promising foundation for the development of next-generation electronics beyond sub-2 nm technology node.",
        "authors": [
            "Aijia Yao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158931",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On Hing Travel Agency Fictional Archive of Disappearing Hong Kong",
        "abstract": "Hong Kong, shaped by rapid transformation and precarious land ownership, is a city where erasure defines its urban landscape. Amid this flux, a place I once called home was demolished, prompting the question: “How can one return to a place that no longer exists?” This thesis explores the transformative potential of disappearance, reframing it as a generative force that creates space for imagination, resistance, and continuity. Through On Hing Travel Agency (OHTA), demolished buildings \"travel\" into fictional worlds, becoming vessels of memory and imagination. Rooted in Hong Kong’s literary tradition—where fiction resists erasure and archives aspirations—the project employs fiction as both a tool of preservation and a site for belonging. Fictional destinations, inspired by Hong Kong novels, such as The Permanent City (1959), The Floating City (1986), and The Vanished Cities (2010), reflect pivotal historical moments while offering pathways to reconcile personal loss and master alternative spatial logics. The project culminates in the Lost Traveler’s Guide to Hong Kong, a publication curating maps, brochures, and layered narratives to immerse travelers in speculative thinking. By bridging the past and future, real and imagined, OHTA is a attempt to demonstrates how fiction can reclaim agency within the politics of disappearance, transforming loss into a catalyst for new narratives and creative engagement. Even in absence, Hong Kong’s disappearing spaces retain their resonance, generating new narratives and underscoring the creative potential of loss.",
        "authors": [
            "Ina Wu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158840",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Investigating the Atmospheric and Oceanic Drivers of Atlantic Multidecadal Variability and Predictability",
        "abstract": "Despite its numerous impacts across the Earth system, the relative importance of ocean and atmospheric dynamics in generating Atlantic Multidecadal Variability (AMV) remains an open question. This thesis presents three pathways to understanding how oceanic and atmospheric processes generate key spatio-temporal signatures of AMV through a combination of processed-based and data-driven approaches. Part 1 (Chapter 2) takes a \"bottom up\" approach, building a hierarchy of stochastic models to identify the contributions of vertical entrainment and seasonality in local upper-ocean processes to sea surface temperature (SST) variability. Through this hierarchy, I highlight unrealistic features present in slab ocean models widely used to isolate atmospheric contributions to AMV. On the opposite end of the spectrum, Part 2 (Chapter 3) utilizes a \"top-down\" data-driven approach where deep neural networks are trained to predict the North Atlantic SST Index in both the Community Earth System Model 1 Large Ensemble (CESM1) and observation-based datasets using atmospheric and oceanic predictors. I apply explainable artificial intelligence techniques to highlight a significant source of multidecadal predictability over the Transition Zone in oceanic predictors such as sea surface salinity (SSS) and sea surface height in the presence of external forcings. Part 3 (Chapter 4) returns to the process-based hierarchy, but applies this to understanding SSS variability. The stochastic salinity model is used to investigate the role of mixed-layer re-emergence, subsurface ocean damping and SST-evaporation feedback in shaping the pattern and amplitude of AMV.",
        "authors": [
            "Glenn Yu-zu Liu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158897",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Weak Shock Waves on a Chip: Generation and Applications",
        "abstract": "In conventional laser-shock experiments in solid media, shock waves are typically excited from the ablation of a photoacoustic transducer layer deposited onto the sample of interest. Unavoidably, the target materials are damaged. This leads to the necessity of changing targets after each exposure, likely lowering the shot-to-shot reproducibility and data quality, while lowering the throughput of the experiment. Motivated by the need to generate large-amplitude transient strain waves at a high repetition rate, this thesis introduces a novel platform for the non-destructive generation and amplification of acoustic waves with associated strain levels in the percent range — up to the formation of shock waves. The acoustic amplification scheme is first described. Then, owing to the capabilities of the technique to repeatedly load a material with finite-amplitude strain waves, a demonstration of the use of the platform for microscale fatigue testing is made. Finally, the strain localization of surface acoustic waves is leveraged by transiently modulating a monolayer of a transition metal dichalcogenide deposited on a substrate.",
        "authors": [
            "Jude Deschamps"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158942",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Planning Beyond Crisis: The Promise of Insurgent Planning in Post-Disaster Mocoa",
        "abstract": "During the evening of March 31, 2017, a catastrophic landslide engulfed the Colombian city of Mocoa, killing at least 335 people in roughly thirty minutes. Seventy others disappeared, and over one hundred people were reported injured across 48 neighborhoods, and roughly 1,500 housing units were destroyed. With a total of 22,000 people impacted, this catastrophe was the deadliest disaster affecting Colombia in recent decades. Yet, despite an alignment of major national political commitments, international cooperation, and a multi-million-dollar humanitarian budget, reconstruction plans have not been completed seven years later. Why? As the first comprehensive analysis of the landslide and its aftermath, this dissertation is a novel investigation into the competing forces that ultimately canceled the central reconstruction plan, demonstrating that the kind of disruption caused by the disaster mobilized new actors and new forms of agency. In contrast to the popular perception that this kind of lack of remediation suggests the failure of urban governance, the dissertation speaks to the success of activists who have neutralized the government’s reconstruction plan, which activists perceived as worsening the circumstances leading up to both the catastrophe and recovery. Distinguishing between the “landslide” and the “larger disaster,” the dissertation further explains the government’s proposed reconstruction plan within a history of violent extraction, dispossession and displacement. Framing an original case consisting of fifteen planning vignettes to trace actions, reactions and counteractions, I expose the reduction of the planning process as crisis urbanism. My research contributes to our understanding of variability among insurgent planning actors and their invented spaces for engagement in the context of disaster, by defining technocratic resistance as a valid form of dissent inside the government, and by proposing a new device for the study of insurgent planning called transformative spaces enabling local community’s right to plan. Drawing on contemporary debates on anti-crisis, risk and decolonial thought, the dissertation imagines an alternative paradigm for planning beyond crisis that enables radical community action through dissenting grassroots leadership. \r\n\r\nKeywords: crisis urbanism, technocratic resistance, insurgent planning, regenerative planning, anti-crisis, risk, decolonial thought",
        "authors": [
            "Juan Camilo Osorio Botero"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158822",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Dynamics of Diversity, Equity, and Inclusion Practice Adoption",
        "abstract": "Despite the widespread adoption of Diversity, Equity, and Inclusion (DEI) initiatives in corporate America, significant disparities persist in the representation, compensation, and treatment of women and racial minorities. This paper investigates why well-intentioned DEI efforts often fail to achieve their intended outcomes and identifies managerial barriers to progress. This research employs a qualitative dynamic modeling approach to analyze the complexities of DEI practice implementation within organizations. I conducted a scoping review, focusing on longitudinal and experimental designs to identify key mechanisms influencing the outcomes of DEI practices. The interplay between organizational processes and individual cognitive and behavioral responses can be illustrated via reinforcing and balancing feedback loops that I map onto a causal loop diagram, which reveals how DEI initiatives interact with existing organizational processes and cultural dynamics. This paper introduces a dynamic perspective on DEI practice implementation, highlighting the feedback mechanisms that can either hinder or facilitate progress toward diversity goals. The model reveals that certain DEI practices may inadvertently trigger reinforcing loops that perpetuate inequality. By mapping DEI practices and their effects, this study provides a framework for understanding how DEI outcomes can diverge significantly depending on different implementation strategies. It underscores the importance of considering the endogenous feedback effects of DEI initiatives and offers insights into strategic interventions that can disrupt undesirable reinforcing cycles and promote progress toward organizational diversity, equity, and inclusion.",
        "authors": [
            "Aishwarya Pandey Yadama"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158834",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Modeling and Analysis of Voltage Feasibility Problems for\r\nCost-Effective Microgrids",
        "abstract": "Global efforts to mitigate climate change have led to a significant increase in the integration of renewable energy resources into the electricity grid. This transition not only necessitates the adoption of renewable energy technologies but also requires rethinking and redesigning existing power grid infrastructures to accommodate the unique characteristics of these resources. This research focuses on modeling techniques which can assist in analyzing the feasibility of microgrid topologies. Microgrids have emerged as a flexible and efficient approach to implementing novel grid topologies that support higher levels of renewable energy penetration. They also support the integration of distributed energy resources (DERs), such as photovoltaic (PV) systems, thereby promoting a more sustainable and efficient energy grid design. This thesis utilized sanitized load and system topology data from a real world microgrid located in Illinois to test the feasibility of increasing the number of PV units the system can utilize for reactive power support. \r\n\r\nIn these systems, ensuring feasibility is a crucial concern due to power mismatches caused by the inherent variability of renewable resources. This work focuses of maintaining voltage within the constraints while increasing PV penetration on the system. We simulate the implementation of microgrids with PV generation using Alternating Current Optimal Power Flow (AC-OPF). The results of this thesis show the limits of feasible reactive power support from distributed PV units on a utility disconnected microgrid based on our voltage constraints. The study shows that there exists a limit to reactive power support provided by distributed PV units. Beyond this limit we see voltage collapse shown as infeasibility of power flow solutions. In order to avoid this problem we optimize the reactive power support from PV so that a solution exists within the constraints. The lesson learned for practical use of this result is that operators should use AC-OPF to compensate for reactive power using PV. Future research will explore the challenges and opportunities associated with the widespread adoption of microgrids, such as dynamic voltage instabilities that can occur with high levels of PV integration and complexities in inverter control strategies.",
        "authors": [
            "Aaron Jerome Jones"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158920",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Ending Well, Making the Harvest-Paths of Our Values",
        "abstract": "Any single story shrinks all others. In a place historically cultivated for the cocoa cash crop, this thesis proposes reorienting architectural practice towards a plural valuing of land and its constituent spirits. The journey begins in 2022 with my acquisition of a 99-year lease for a 5-acre land in Ghana. Prior to the conception of an academic proposal, this was to preserve and grow ecological and financial value through time.\r\nLocated on a hill-cluster in the Eastern Region, this place is crucial as the birthplace of Ghana’s cocoa industry, which became the world’s largest exporter by 1911. Spurred by economic and colonial incentives, farmer-settlers acquired and cultivated forest land including the one I presently steward. They forged communities that live on despite a subsequent decline of cocoa production in the region. Five centuries of colonial influence in West Africa reduced a plural landscape into singular extractive narratives, creating place-names like the Gold Coast, renamed Ghana after independence. The capitalist framework of monocultural extraction, one reliant on a colonial government and its land survey department, continues under contemporary African states. Architecture and planning—a practice historically tied to power and capital—remains instrumental in this system, often overlooking other ways of valuing land.\r\nThis thesis confronts the dispositions of an inherited profession by foregrounding the practices and materials of a socio-cultural paradigm. It is epitomized by the tree called Newbouldia laevis (African boundary tree) and its plural meanings in West Africa. It follows a cocoa harvest-path from a community named after a farmer-settler, Yaa-Aso, and ascends the hills, crossing the land limits of 7 farmers. It ends on the land I hold, with a lease ending in CE 2122.\r\nIn July 2024, I led a convocation of the farmers along the path in the defunct cocoa distribution building, toward framing futures based on other values apart from capital. 3 languages were spoken in that gathering - Twi, Anlo-Eʋe and English. It resulted in a 7-foot expansion of the path, and the pacification of a seasonal spirit-stream that crosses it. They set the context for imagining a series of 5 moments, herein recorded, that explore a value system of things spiritual and communal, offered by the transgressions of a widened path and the land I hold at its end.",
        "authors": [
            "Courage Dzidula Kwaku Kpodo"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158886",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Responsible Computational Text Generation: AI Content Classification and Policy Framework",
        "abstract": "Recent advances in generative AI, particularly in producing human-like text, have blurred the lines between human and AI authorship. Since these AI tools rely on stochastic generation rather than traditional scientific reasoning, concerns about misinformation and reliability have emerged, highlighting the need for AI detection tools and policy guidelines. In response, this study proposes a dual approach: (1) the application of adaptive thresholds to improve the use of AI text detectors and (2) an AI policy framework based on user patterns and opinions. To enhance detector performance, we present a threshold optimization algorithm that adapts to diverse subgroups, such as those based on text lengths and stylistic features, thereby reducing discrepancies in error rates. The commonly used method relies on a single universal threshold, which has led to inconsistent results across various text types because of different probability distributions. Our approach addresses these shortcomings by tailoring thresholds to the specific characteristics of each group. In parallel, the study examines the pressing need for comprehensive AI guidelines, given the rise of misinformation and academic integrity issues. While a few institutions have introduced comprehensive policies, many institutes lack approaches grounded in user patterns and opinions. To remedy this problem, we propose a policy framework based on a user study. The findings of this research will provide practical solutions for more effective AI text classification and a reliable framework for the necessity of AI writing policies.",
        "authors": [
            "Minseok Jung"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158904",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "High Order Immersed Finite Difference Methods for Complex Domains with Moving Boundaries and Interfaces",
        "abstract": "Moving domain boundaries and material interfaces are a hallmark of multiphysics systems such as fluid-structure interaction, alloy solidification, and multiphase flows. Simulating moving interfaces with traditional techniques requires a moving mesh that continuously adapts to the interface, which is costly and places restrictions on the interface motion. Immersed methods avoid these challenges by simulating moving geometries on a stationary Cartesian grid, locally altering the numerical method to account for boundaries and interfaces that are not grid-aligned. Most existing immersed methods have low-order spatial accuracy, requiring fine grids to generate accurate results. High order immersed methods can produce more accurate results at lower resolution, making them a promising tool for 3D simulations with tight error tolerances. However, the majority of available high order immersed methods have been numerical experiments developed for stationary 2D geometries and simple PDEs. In this thesis we demonstrate that high order immersed methods can be extended to complex nonlinear PDEs and moving 3D geometries, both of which are necessary to simulate practical engineering problems. We begin by introducing a boundary treatment that locally approximates PDE solutions with high order accuracy using a weighted least-squares fit, and show that the procedure remains valid for smooth 2D or 3D geometries satisfying a local curvature constraint. This boundary treatment is combined with a high order finite difference method to discretize the Poisson equation with up to sixth order accuracy. We then expand the scope of the method to include PDEs with immersed material interfaces, spatially-variable coefficients, vector-valued unknowns, cross-derivative terms, and nonlinearities. These techniques are applied to generate a sixth-order discretization of 2D nonlinear elasticity, demonstrating the applicability of high order immersed methods to complex PDE systems relevant in mechanical engineering. In the second half, we focus on large-scale 3D simulations with moving boundaries. We construct a third order immersed advection discretization with provable stability in one dimension, and show experimentally that the scheme remains stable in 2D and 3D domains. To treat moving boundaries, we introduce a general framework that allows high order immersed methods to maintain their accuracy in both space and time when paired with any explicit Runge-Kutta time integrator. We conclude by presenting results from massively-parallel high order simulations of the 3D advection-diffusion equation with moving boundaries on a multiresolution grid. Taken together, these results demonstrate that high order immersed methods can achieve the scale and complexity necessary to enable practical simulations that are difficult or impossible with traditional mesh-based techniques.",
        "authors": [
            "James Gabbard"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158843",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Persian Lessons: Islamic Art in America, circa 1876–1925",
        "abstract": "This dissertation investigates the prehistory of academic Islamic art history in the United States through the lens of American cultural history. It shows that between the US Centennial in 1876 and the inauguration of the Pahlavi dynasty in Iran in 1925, aesthetic theory and American citizenship were debated in the United States through objects identified, regardless of actual provenance, as “Persian.” This cultural phenomenon coincided with the acceleration of the transnational market for Islamic art, including architectural tiles, single-page paintings, and hand-knotted pile carpets. Examining instances of collecting, classifying, displaying, and otherwise handling and beholding Islamic art within different scales of home (family, nation, and international Christianity) and spaces of pedagogy (the living room, commercial gallery, advertisement, schoolroom, voluntary association, museum, and world’s fair), \"Persian Lessons\" reveals that notions of Persian art were instrumentalized in the service of competing American identities and ideologies in the late nineteenth and early twentieth centuries. \r\n\r\nThrough an analysis of published writings, museum archives, and government documents, the study shows how the art critic S. G. W. Benjamin, who also served as the first US diplomat to Iran in 1883–85, constructed an ideal of the Persian artist to champion liberal individualism and public art education. An investigation into the presence of Muslim prayer carpets in American Christian homes reveals that Sarkis Nahigian and other diasporic entrepreneurs from the Ottoman Empire became partners to middle-class women, who jointly turned the Oriental carpet into a symbol of obligation to the American nation. Lastly, an examination of visual and textual evidence recasts a collection of more than 20,000 objects—given to the Museum of Fine Arts, Boston, and William Hayes Fogg Art Museum of Harvard University by design pedagogue and museum patron-administrator Denman Waldo Ross between 1888 and 1935—as a tool of “training for citizenship.” Ross regarded Persian textiles and single-page paintings as value-neutral objects for the design education that he believed bolstered participatory democracy. \r\n\r\nThe fifty-year history that this dissertation covers concludes in the late 1920s and '30s with the establishment of the first official positions in Islamic art history at universities and museums in the United States. \"Persian Lessons\" thus shows that the founding of Islamic art history as an academic discipline was not simply imported from Europe. Professionalization stabilized a half century of domestic engagement with Persian art as a polysemic guiding light for American culture and society.",
        "authors": [
            "Roxanne Goldberg"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158801",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Synthesis and perception of sounds from physical interactions reveals auditory intuitive physics",
        "abstract": "Object interactions – collisions, scraping and rolling – create many of the sounds that we hear in the world around us. These sounds are generated via lawful physical dynamics. Anecdotally, humans possess some intuitive knowledge of the physical generative processes underlying sound production, but little is known about the extent and nature of this knowledge. This thesis characterizes the auditory perception of physical object interactions, making three main contributions. First, we develop realistic contact sound synthesis tools, in part via large-scale measurements of object acoustics. Second, we show that humans solve ill-posed problem of inferring of object mass and damping by using internalized knowledge of the distribution of object resonances. Third, we provide evidence for “auditory intuitive physics” in which human listeners derive physical information through sound, maintain it over time in object representations, and compare it across sensory modalities.",
        "authors": [
            "Vinayak Agarwal"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158825",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Archean origin of assimilatory sulfate metabolisms provides novel insight into redox conditions of early Earth environments",
        "abstract": "Dissimilatory sulfur metabolisms recording differing biological isotopic fractionation are well studied, important components of sulfur cycling (Mateos et al., 2023). Assimilatory sulfur metabolisms and genes across life provide a complementary window into sulfur biogeochemistry with individual pathways having specific isotopic fractionations acting on distinct redox states (e.g. sulfate, sulfide, sulfite) for anabolism (Liu et al., 2012). An assimilation pathway exists, which starts with sulfate adenylyltransferase (sat/ATP sulfurylase) catalyzing a reaction of adenosine triphosphate (ATP) and sulfate (SO42-) resulting in adenosine 5’-phosphosulfate (APS), and incorporation of more reduced sulfur into biomolecules. This sat/ATP sulfurylase enzyme represents the first step required by life to incorporate sulfate and informs our understanding of biological processes performing this fundamental chemical reaction. A phylogenetic and molecular clock analysis of the sat/ATP sulfurylase protein family (E.C. 2.7.7.4) was performed to determine the age of sulfate assimilation proteins. Extant diversity of sat proteins was estimated to have a last common ancestor ~3.24 Ga (95% CI 3.52–3.06 Ga) using relaxed molecular clocks calibrated with eukaryotic and cyanobacteria age ranges from previously published fossil calibrated investigations. These results suggest sulfate cycling in Paleoarchean environments, despite extensive evidence of low marine sulfate concentrations (Crowe & Canfield et al., 2014). Archean sulfate biogeochemical cycling could result from microbial sulfur oxidation and sources could include abiotic oxidation of volcanic sulfur, hydrothermal processes or pyrite (Canfield, 2001, Lyons et al., 2024). This phylogenomic evidence of sulfate during Archean times provides an independent complement to geochemical records and indicates that sulfur redox chemistry during the Archean was likely more complex than previously described.",
        "authors": [
            "Jack G. Payette"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158898",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Geographies of Selective Surveillance: Analyzing the Lived Experiences of Street-Level Trans Sex Workers and Muslims in India through the Matrix of Domination",
        "abstract": "In this paper, I present a study of public and private CCTV surveillance of urban public spaces in India, which I term as ‘geographies of selective surveillance’ — areas where state power is discretionarily exercised and abused, and the presence of the state is experienced principally through police pickets and everyday violence unleashed on marginal occupants, rather than by access to civic amenities and systems of justice. I analyze these experiences of surveillance from the standpoint (Harding, 1992) of minoritized communities of street-level trans sex workers in Kolkata and Muslims in Mumbai. I then situate these experiences within the Matrix of Domination (Collins, 1990), a theoretical framework that explains how systems of power are configured. Defining empowerment as the power to gain control of and/or benefit from a scenario by weakening the Matrix of Domination, I analyze the structural determinants that make surveillance empowering or disempowering for these communities. I find that on the one hand, surveillance can be an empowering tool for minoritized communities as evidence of harm and innocence in cases of false accusations or when police officials typically refuse to believe their experiences due to discriminatory attitudes. On the other hand, surveillance also offers new opportunities for the private exploitation of the instruments of state power through corruption as well as community-based moral policing to be done with greater success and efficiency. I argue that what ultimately determines how surveillance is experienced is not laws and policies, but rather how power is discretionarily exercised on the ground, refracted through the influence of cultural and political beliefs, and discourse.",
        "authors": [
            "Radhika Radhakrishnan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158809",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design and Manufacture of a Modular Continuous Unit Dose Pharmaceutical Lyophilizer",
        "abstract": "Pharmaceutical lyophilization (freeze-drying) enables long term storage and simplified transportation for aqueous vaccines and protein formulations. Modern industrial pharmaceutical freeze-driers rely on large batch and open loop formulation processing, limiting supply chains and resulting in variable quality products. This work describes the design and manufacture of a modular continuous lyophilization machine for pharmaceutical production. Additionally, the scaling and design methodology outlined in this work enables the development of both smaller systems for laboratory testing and larger machines to fit the needs and requirements of individual facilities. This machine introduces three new technologies to the pharmaceutical freeze-drying process. The first innovation is a continuous flow lyophilization topology which separates the lyophilization steps spatially rather than temporally. This layout allows product to travel through the system in smaller batches for increased product uniformity and quality control. The second innovation is a weight-based sensor for monitoring residual water content. This sensor enables in-situ monitoring of product during sublimation, and it resolves mass measurements as small as 5mg. The third innovation is the implementation of a thermal shock method of inducing controlled nucleation. The convective cooling and spatial non-uniformity within the machine allow vials to experience a 40°C temperature drop in less than 30 seconds. This nucleation front starts on the vial walls, rather than at the top surface of the solution in the vial, potentially increasing the water sublimation rate during drying compared to current nucleation methods. The machine designed and built for this work integrates into modern factory processes and can be scaled from the lab bench to a production line. The manufactured prototype demonstrates improvements on the production rate, flexibility, and quality of existing machines.",
        "authors": [
            "Steven Burcat"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158826",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Precision Pointing for the CubeSat Laser Infrared CrosslinK (CLICK) Mission",
        "abstract": "Advances in Free Space Optical Communications have led to numerous missions that have demonstrated optical space-to-ground links, however, fewer missions have demonstrated optical space-to-space links. NASA’s CubeSat Laser Infrared CrosslinK (CLICK) Mission aims to be the first to demonstrate optical space-to-space communication on a CubeSat scale using Commercial Off the Shelf (COTS) components that include a micro electromechanical system (MEMS) fine steering mirror for precision pointing. The first phase of the CLICK mission, CLICK-A, launched in September 2022 to demonstrate optical downlink. The second phase, CLICK-B/C, aims to demonstrate optical crosslink between two spacecraft: CLICK-B and CLICK-C. Optical crosslink communication requires precision pointing for both spacecraft to close the link. The development of the CLICK-B/C Fine Pointing, Acquisition, and Tracking (PAT) is presented in this thesis, as well as the analysis of disturbance rejection and evaluation of expected spacecraft disturbances. This thesis also asses the slewing required for differential drag control which is used to maintain the crosslink range between the two CubeSats. Preliminary results are presented from the CLICK-B/C flight hardware integration and testing phases, as well as findings from simulation of the lasercom payload’s performance.",
        "authors": [
            "Paige Forester"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158924",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Impact of Introducing Technical Design Elements in Makerspace\r\nTrainings",
        "abstract": "Makerspaces are used as a tool in higher education to support curricular, hands-on projects and encourage student extracurricular and personal projects. Because access to making is more self-driven, there is a gap between what makerspace trainings teach students and what students are expected to know by the time they reach capstone courses in engineering. To test the effects of introducing a technical makerspace training to students, several steps were taken. First, known barriers to making were explored and organized into categories. Second, Design Expertise was defined as a means to combat these barriers: it is a combination of (1) knowledge, (2) skill, (3) perspective, and (4) motivation. Third, a rigorous framework, the Design-Fabrication-Performance (DFP) matrix was created to break down design expertise into manageable chunks. Next, existing makerspace trainings at MIT were characterized using the DFP matrix. Afterwards, the DFP matrix was used to design a new, experimental training which would incorporate engineering design thinking and expertise with the typical makerspace machine training structure. Finally, 23 student participants were recruited, surveyed using a Likert scale (1 = strongly disagree, 5 = strongly agree), and interviewed to understand the impact of the training on participant perspectives, engineering identity, and maker motivation. Initial results suggest that student self-efficacy increases as a result of the training, This outcome is shown by the highest average differential of all survey responses (M = 0.78, SD = 0.85) for question 15: “I am confident in my ability to use GIR level knowledge to design and make things that perform as intended”. The maker training reinforced the motivation to make things for a majority of students, with the average score for the associated question being 4.48 (SD: 0.85). The training also positively impacted some traditionally marginalized groups in STEM. For the statement \"I feel comfortable in engineering at MIT\", women averaged 3.27 and men 3.90 before the training. The average differentials in the post- and pretraining scores to this question for these groups were 0.4 and 0.91 respectively. The training also appears to level playing field for students with less advanced backgrounds in engineering and science. For the question “I am confident in my ability to solve GIR level problems on my own”, students with parents with graduate degrees or higher averaged 4.44 before the training, while those with parents with undergraduate degrees or lower averaged 3.57. The average differentials are 0.22 and 0.64 respectively. Although students saw the value in modeling systems before design and fabrication, several questions demonstrated that students found modeling to be tedious and preferred to test and iterate on their designs in the makerspace; further work is needed to eliminate barriers to sustain student interest and participation in the long term. A longitudinal study following these students would also be needed to reveal long term outcomes such as STEM retention and long-term makerspace usage.",
        "authors": [
            "Layal A. Barakat"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158817",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Essays on Spatial Constraints and Gender Equality: the Impact of COVID-19 Lockdowns on Work-from-Anywhere Dynamics and Gender Equality in Job Searches",
        "abstract": "This dissertation explores the intersection of spatial constraints and gender equality by leveraging the COVID-19 lockdowns as a natural experiment to study the impact of work-from-anywhere (WFA) dynamics on job search behaviors. The introduction of mandatory lockdowns drastically shifted the labor market landscape, prompting an increase in the demand for flexible work formats. Utilizing data from over one million job seekers on an online employment platform, this research examines how the sudden wide availability of remote work options influenced job search activities differently across genders. Using unique data from a large online job platform, a comparison of pre- and post-COVID-19 lockdown data shows that women significantly increased their engagement with geographically flexible job postings, reacting more strongly than men to the rise in remote job opportunities at both the job viewing and application stages. This shift also resulted in a narrowing of the wage gap in positions viewed and applied for during the post-lockdown period compared to pre-lockdown benchmarks. Notably, the study identifies variations in job search behavior among those likely constrained by domestic responsibilities. While differences in job posting views suggest an initial differential impact, such differences vanish at the application stage. Collectively, these results indicate that the pandemic-induced shift towards remote work has contributed to a gender-equalizing effect in the job market, including those navigating domestic labor constraints. This research not only highlights the transformative potential of WFA arrangements in promoting gender equality but also provides insights into the mechanisms that drive these changes within the labor market. \r\n\r\nKeywords: organizational studies, gender inequality, flexible working arrangements, hiring, applications processes, decision making, digital platforms.",
        "authors": [
            "Tatiana Labuzova"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158794",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Cooling with less: Design and simulation of multifunctional building components for a material-efficient, heat-resilient architecture",
        "abstract": "As temperatures rise globally and the demand for housing intensifies, designing affordable buildings for heat resilience and with low carbon emissions becomes crucial. Conventional air conditioning (AC) systems, although often an effective and accessible cooling solution, are energy-intensive and typically fail to consider local climatic and urban contexts. This work alternatively focuses on the opportunity behind designing building components (such as slabs, blocks, roofs, or footings) for multifunctionality, integrating passive strategies and low-energy cooling systems within them in a material-efficient manner. Collapsing multiple functions into a single building component is typically regarded as a strategy that leads to better overall performance and reduced costs compared to implementing each function separately. However, the effectiveness of this strategy in cooling-dominated climates and in the context of the current climate crisis remains underexplored. \r\n\r\nThe dissertation proposes new designs and evaluation methods for three multifunctional building components: multi-hollowed blocks (ceramic blocks with interior air pockets), shaped chilled slabs (shaped concrete slabs with embedded radiant ceiling systems), and integrated heat sinks (thermally activated concrete footings and roofs). Each component is designed to optimize a specific cooling strategy based on its context within the building and intrinsic material properties - thermal mass, radiant cooling, and ground/radiative cooling. Chapter 2 demonstrates how shape-optimized ceramic blocks can double the heat capacity of existing commercial solutions without additional material or reduce their weight by 33% while increasing the heat capacity by 23%. Chapter 3 presents slab geometries that achieve embodied carbon reductions of up to 50% relative to conventional prismatic floors while reducing operational carbon by 12-14%. Chapter 4 finds that buildings in temperate climates with a Floor Area Ratio (FAR) of up to 4.5 can meet 100% of the cooling demand exclusively through heat dissipation systems integrated into the building’s foundations and roof.  Methodologically, this research puts together heat transfer theory and analytical models with state-of-the-art shape optimization methods; this effort results in a fast and accurate multi-objective simulation framework tailored for early design stages.\r\n\r\nThis thesis provides, for the first time, validated methods and quantitative results that support the viability of multifunctional building components in cooling-dominated climates, optimizing the shape of walls, blocks, foundations, and roofs to improve their structural and thermal performance simultaneously, reducing their weight and improving buildings’ resilience to heat.  From a climate adaptation perspective, this approach ensures that buildings are ready for extreme heat even when active systems are unavailable due to, for example, a power outage. From a carbon mitigation perspective, the presented results highlight the potential to reduce the whole-life carbon of buildings by shape-optimizing components for enhanced thermal performance and material efficiency.",
        "authors": [
            "Eduardo Gascón Alvarez"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158791",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Accelerating the Discovery of Novel Metal Organic Chalcogenolates: A Computational and Machine Learning-Driven Approach",
        "abstract": "Metal Organic Chalcogenolates (MOChas) are a class of robust, self-assembling, and hybrid materials featuring inorganic metalo-chalcogen frameworks that are scaffolded by organic ligands. These low-dimensional structures exhibit tunable optoelectronic properties, making them promising candidates for various applications, including optical sensors and nanotechnology. This tunable relationship between MOCha structural arrangements and targeted properties opens up a vast yet challenging search space for novel MOCha structures. Density Functional Theory (DFT) can predict properties of materials with good accuracy, making it a powerful choice for even hypothetical materials. However, the discovery of novel MOChas structures is constrained by poor scalability of DFT relaxation times for large systems and a lack of high-throughput design methods that can capture the complex geometries of MOChas. In this work, we employ DFT calculations to investigate the energetic and electronic properties of various MOChas, and provide insight into the optical behavior and kinetic favorability of such structures. To address the computational bottlenecks of high-throughput design and DFT workloads, we discuss the use of machine-learned interatomic potentials and various generative models that can enable rapid prototyping of novel MOCha structures.",
        "authors": [
            "Adriana J. Ladera"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158916",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Crafting Cannabinoid Capitalism: Health, Sustainability, and Regeneration in the United States",
        "abstract": "This dissertation offers a critical exploration of cannabis legalization through an ethnographic study of small-scale \"legacy\" cannabis farmers in Humboldt County, California, as they navigate a complex transition from prohibition to commodity capitalism. I focus on their collective efforts to envision and practice “regenerative agriculture\" as a response to both the historical injustices of prohibition and the compounding challenges of climate change. Drawing on history, STS, and the anthropology of food, agriculture, and medicine, I show how the logics of the war on drugs— rooted in carcerality, settler colonialism, and plantation agriculture—structurally and affectively persist in the socalled “post-prohibition” era, frustrating farmers’ efforts to resist monopolization and dispossession. Throughout, I attend to how the pervasive notions of “health,” “sustainability,” and “regeneration” are actively negotiated, modified, and put to use as material and symbolic tools in crafting medicinal, agricultural, and ecological futures. The Introduction weaves a tapestry of themes, histories, and theories that set the stage for the main ethnography. Through a blend of personal narrative, ethnographic vignette, and critical theory, it works to situate cannabis as a fluid and multifaceted object, highlighting people’s ambivalent hopes and cynicisms towards legalization. From alternative farming to molecularized biocapital, it articulates the intersecting influences of climate change, racial capitalism, and Indigenous sovereignties in ongoing projects to commercialize and legalize cannabis in a globally connected United States. Chapter One outlines my research methods and provides a social and narrative history of the study’s fieldsite, grappling with the anthropological complexities and complicities of studying working landscapes in a settler colonial “frontier ecology.” Chapter Two unpacks the shifting and embodied subjectivities of both farmers and workers as they reconfigure themselves in service of licensed production, highlighting sociocultural tensions and contradictions, the structural challenges of regenerative gardening, and the labor dynamics that shape these processes. Chapter Three analyzes how the inchoate and social nature of cannabis regulation both hinders and supports regenerative farming, emphasizing financial strain, and the ever-pervasive role that surveillance technologies are playing in cannabis governance. Chapter Four shifts to the harvest season, exploring farmers’ collective efforts to market their products through the concept of “drug terroir,” unpacking how their values and practices entangled with regional efforts to address wildfires and remediate leftover drug war infrastructures. Chapter Five moves off the farm and onto the topic of consumption as it historicizes the growing scientific literature about cannabis and pregnancy, demonstrating how carcerality continues to infiltrate maternal-fetal health science and conceptions of reproduction and health. The dissertation ultimately explores the ways in which American cannabis legalization often regenerates, rather than resolves, the legacies of prohibition and settler colonialism, while at the same time illuminating alternative and promising practices that might challenge these enduring forces.",
        "authors": [
            "Alexander Nicholas Rewegan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158789",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Missing Megawatts Problem: Improving Modelling Practices to Prepare for an Uncertain Future",
        "abstract": "Long-term energy system planning is one of the most pressing challenges for the power sector, which must maintain reliability while decarbonizing. Currently, no unified regulatory, modelling, or market framework exists in the United States to facilitate planning in pursuit of a clean and reliable grid. Variable renewable energy (VRE) generation can produce cheap power but they increase the grids exposure to interannual variability in demand and VRE generation. This raises questions about how grid planners will value VRE and clean firm power (such as nuclear power). This thesis evaluates the importance of considering interannual variability and clean firm power in long-term energy system planning. I use GenX, an open-source capacity expansion model, to model the U.S. New England region in 2050 assuming a high degree of electrification and various technology availability and emissions reduction pathways. I find that clean firm power will reduce the cost of decarbonizing the New England grid but that grid planners must consider decades of weather and demand data if they are to make appropriate investments. I also present a novel outputs-based timeseries clustering method which allows models like GenX to optimize grids using longer timeseries of weather and demand data. Based on my work, I recommend that policymakers, grid operators, and market designers establish rigorous standards around energy modelling for long-term planning that includes multiple scenarios and appropriately values technologies such as firm power.",
        "authors": [
            "Nirmal K. Bhatt"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158844",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "VoxelPrompt: A Vision-Language Agent for Grounded Medical Image Analysis",
        "abstract": "We present VoxelPrompt, an agent-driven vision-language framework that tackles diverse radiological tasks through joint modeling of natural language, image volumes, and analytical metrics. VoxelPrompt is multi-modal and versatile, leveraging the flexibility of language interaction while providing quantitatively-grounded image analysis. Given a variable number of 3D medical volumes, such as MRI and CT scans, VoxelPrompt employs a language agent that iteratively predicts executable instructions to solve a task specified by a natural language input prompt. These instructions communicate with a vision network to encode image features and generate volumetric outputs (e.g., segmentations). VoxelPrompt interprets the results of intermediate instructions and plans further actions to compute discrete measures (e.g., tumor growth across a series of scans) and present relevant outputs to the user. We evaluate this framework on diverse neuroimaging tasks and show that the single VoxelPrompt model can delineate hundreds of anatomical and pathological features, measure many complex morphological properties, and perform open-language analysis of lesion characteristics. VoxelPrompt carries out these objectives with accuracy similar to that of fine-tuned, single-task models for segmentation and question-answering, while facilitating a large range of tasks.",
        "authors": [
            "Andrew Hoopes"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158933",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Polymer Deconstructability and Recyclability via Introduction of Cleavable Si−O Bonds",
        "abstract": "The synthesis of a new polysilylether via entropy-driven ring-opening metathesis polymerization (ED-ROMP) of cyclic bifunctional silyl ether-based monomers is reported. High molecular weight polymers (up to 100 k) with narrow dispersities were achieved at modest temperature. These polymers display excellent thermal stability and ultra-low T_g (–88 ºC). The polymers are both rapidly deconstructable via the cleavage of the labile silicon-oxygen linkages with either acid or fluoride triggers and partially depolymerizable by the addition of exogenous metathesis catalyst. Analysis of the deconstructed polymer products provided insight into the polymer microstructure, showing that the ED-ROMP process was regiorandom. Altogether, this work offers a new class of deconstructable polymers with a range of potential applications. Incorporation of these bifunctional silyl ether-based monomers into copolymers could aid in the triggered deconstruction of otherwise nondegradable hydrocarbon backbones.",
        "authors": [
            "Alayna Johnson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158921",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Sweating Details: Labor of “Los Constructores del Valle”",
        "abstract": "“You should always be grateful for the work you can find, so make sure you prove you deserve it.”- Commonly heard growing up amongst the Builders of the Valley in Orange, NJ. The necessary attitude that fuels the built environment.\r\n\r\nThis thesis proposes a dialogical method of tectonics through exploring the embodied experiences of those who physically build the city and its architecture, positioning architectural design as fundamentally tied to the labor that makes buildings possible. It centers on two primary questions: “Who builds this architecture?” and “How does this design impact a builder’s occupational livelihood?”\r\n\r\nTo challenge professional standards that perpetuate a disconnection between designers and builders, this thesis reconnects me, as a designer, with my educators from Orange, NJ. These individuals—professional construction workers—shaped my earliest understanding of the built environment and how to navigate it socially and professionally. Through this process, learning more about who they are, how they entered construction, and how the work has affected them over the years.\r\n\r\nThis education with ongoing dialogue pushes towards future opportunities of working together, focusing on designing better for the act of building by prioritizing the physical, mental, and financial longevity of my Educators. The culmination of this research and communication is materialized through four architectural details within a workspace, designed to showcase my Educator’s expertise and affinities as professionals. These details reimagine occupational choreography, opening up for future workflows that think through both lessening and healing the musculoskeletal disorders that many builders face after years of laboring across the tristate area.",
        "authors": [
            "Gabriel Andrade"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158839",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Thermally Hardened RF GaN HEMTs in Extreme Environments",
        "abstract": "Traditional, room temperature electronics based on silicon has truly changed the world around us over the past 70+ years. However, many more applications still exist that are limited by the temperature performance of silicon devices (<250◦C). This area of high temperature (HT) electronics is an increasingly growing field with critical future applications in geothermal energy, space exploration, hypersonic aircraft, and deep gas/oil drilling, among others. Gallium Nitride (GaN) high electron mobility transistors (HEMTs) are especially well suited for high temperature electronic applications due to their low intrinsic carrier concentration and excellent electrical properties. Despite great progress in HT GaN technology, most demonstrations target logic or mixed-signal applications, and the performance of radio-frequency (RF) GaN devices remains lacking at high temperatures despite the critical need for wireless communication systems and high-speed electronics for these high-temperature applications. In this thesis, we investigate the physics of GaN HEMT devices at high temperatures and design RF transistors that demonstrate record performance at these temperatures.",
        "authors": [
            "John Niroula"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158912",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Mining Multifaceted Customer Opinions from Online Reviews",
        "abstract": "Online reviews are a valuable source for studying customer needs and preferences. Previous studies focus on extracting a set of a priori defined constructs such as product attribute perception or explicit customer needs from reviews. Such a priori focus circumvents the limitations of certain natural language processing algorithms but discards valuable information in reviews that are not in the scope of the predefined construct. This study proposes a new method of extracting customer opinions and opinion targets from reviews with the Aspect Sentiment Triplet Extraction (ASTE) algorithm and then identifying theoretical constructs critical for product development with a posteriori interpretation method. We demonstrate the value of our proposed method by identifying granular opinion targets and expressions to find infrequent but important phenomena such as user innovations and delights.",
        "authors": [
            "Chengfeng Mao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158810",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Sub-Bottom Profiling Using an Autonomous Underwater Vehicle Equipped With a Sound Source and Towed Hydrophone Array",
        "abstract": "Sub-bottom profiling using an autonomous underwater vehicle equipped with a source and a towed array is an excellent method to finely survey large areas of the ocean bottom with minimal interference from the water column. This approach has the benefit of being able to determine the range dependence of the sub-bottom on a meter-by-meter scale rather than assuming constant sub-bottom properties over a large range. This thesis conducts theoretical and experimental studies to investigate the feasibility of using the arrival times of acoustic signals from an autonomous underwater vehicle source to a short, 16-element towed hydrophone array to determine the sound speed and layer thickness of the seabed through Bayesian geoacoustic inversion. This method provides range-dependent geoacoustic parameters with a resolution on the order of 10 meters. Numerical studies indicate that, for timing data with low variance, arrival times can be used to accurately estimate seabed properties. However, the performance of the Bayesian inversion model deteriorates as the variance of the timing data increases. Experimental data were collected during the Seabed Characterization Experiment at the New England Mud Patch and the New England Shelf Break. This thesis attempts to improve the arrival times through the use of sub-array focusing but concludes that this method is not feasible due to the experimental data exhibiting a high level of variance in the sub-bottom timing returns, likely due to the presence of scatterers in the sediment layer. Therefore, the mean and variance of the direct path, bottom, and sub-bottom timing returns were calculated using Gaussian process regression. Furthermore, the results show that layer thickness and sound speeds are highly coupled, making it challenging to uniquely determine seabed properties.",
        "authors": [
            "Paige Pfenninger"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158932",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Shining a Light on the Nucleus: Photonuclear Measurements from Correlations to Charmonium",
        "abstract": "The atomic nucleus is comprised of a collection of nucleons (protons and neutrons), which are bound together by the nucleon-nucleon (NN) interaction that originates from Quantum Chromodynamics (QCD). While most nucleons experience the force from the rest of the nucleus as a single net “mean-field” interaction that binds them relatively weakly, a small but impactful fraction are in configurations called “Short-Range Correlations” (SRCs), in which they pair with another nucleon at very short distance to experience strong interactions, significant binding, and high momentum. Hard, high-energy scattering reactions in which an SRC pair is broken apart, knocking both nucleons out of the nucleus, provide the ability to probe the details of these SRC configurations in the nucleus. Previous measurements have had limited statistics and kinematic reach, and the theoretical tools available were insufficient to draw quantitative conclusions regarding the ground-state properties of SRCs. The studies described in this thesis represent the first global analysis of SRC breakup measurements in order to present a unified picture of SRCs within light- to medium-size nuclei. This includes the use of a novel theoretical framework, the Generalized Contact Formalism, which connects scattering cross-section measurements and the ground-state properties of the SRC pair, to quantitatively interpret a variety of electron-scattering measurements. This is brought to culmination by a report on the first measurement of SRC pairs via the use of hard meson photoproduction reactions, which, despite differing significantly from the mechanics of electron-scattering, is well-described under a common framework, pointing to a consistent and universal picture of SRCs across reaction channels. I also report on the first measurement of J/ψ photoproduction in the near- and below-threshold kinematic region, giving the first insights to the gluonic structure of bound nucleons in the large-x “valence” region and providing constraints on a gluonic “EMC effect”. In addition to these studies, I provide details on the search for Primakoff production of axion-like particles using the photoproduction data taken for this experiment, and I conclude by describing studies of nucleon spin structure measurements that will be performed at the forthcoming U.S. Electron-Ion Collider.",
        "authors": [
            "Jackson R. Pybus"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158837",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Systems-Theoretic Framework For Safety-Driven Development of System Architectures",
        "abstract": "Modern complex systems are increasingly expected to exhibit emergent properties such as safety and security even as they become more complex, interconnected, and reliant on software than ever before. Because of this evolution in the characteristics of these systems, the methods available today for developing system architectures no longer provide systems engineers with adequate design support. As a result, it is becoming increasingly challenging for systems engineers to develop system architectures that exhibit emergent properties like safety. This thesis addresses this problem by developing a safety-driven architecture development framework that enables the design of emergent properties such as safety into a system architecture from the beginning. The key idea is that the results from a hazard analysis process known as Systems Theoretic Process Analysis (STPA) should drive design decisions. The framework therefore starts with an initial STPA analysis of the system to determine how unsafe or undesirable behavior could occur. Structured and systematic processes are then provided to help systems engineers use the STPA results to develop the required control behavior of the system and explore possible system architecture options to implement that control behavior. This framework therefore enables systems engineers to make more informed early architectural design decisions driven by safety considerations. This framework is applied to an Urban Air Mobility (UAM) case study to demonstrate that it provides the necessary design support to enable the development and refinement of an air traffic management (ATM) architecture for UAM. When creating a system architecture, assumptions may also need to be made to mitigate the inherent uncertainties and lack of detailed information about the system at that early stage of design. However, these assumptions are used as the basis for design decisions, and it is important that they remain valid to avoid flaws in the architecture arising when underlying assumptions become invalid. Thus, this thesis also develops and demonstrates a supporting framework to help identify these underlying assumptions and ensure they remain valid both during system development and after the system is placed into operation. Modern complex systems are increasingly expected to exhibit emergent properties such as safety and security even as they become more complex, interconnected, and reliant on software than ever before. Because of this evolution in the characteristics of these systems, the methods available today for developing system architectures no longer provide systems engineers with adequate design support. As a result, it is becoming increasingly challenging for systems engineers to develop system architectures that exhibit emergent properties like safety. This thesis addresses this problem by developing a safety-driven architecture development framework that enables the design of emergent properties such as safety into a system architecture from the beginning. The key idea is that the results from a hazard analysis process known as Systems Theoretic Process Analysis (STPA) should drive design decisions. The framework therefore starts with an initial STPA analysis of the system to determine how unsafe or undesirable behavior could occur. Structured and systematic processes are then provided to help systems engineers use the STPA results to develop the required control behavior of the system and explore possible system architecture options to implement that control behavior. This framework therefore enables systems engineers to make more informed early architectural design decisions driven by safety considerations. This framework is applied to an Urban Air Mobility (UAM) case study to demonstrate that it provides the necessary design support to enable the development and refinement of an air traffic management (ATM) architecture for UAM. When creating a system architecture, assumptions may also need to be made to mitigate the inherent uncertainties and lack of detailed information about the system at that early stage of design. However, these assumptions are used as the basis for design decisions, and it is \r\nimportant that they remain valid to avoid flaws in the architecture arising when underlying assumptions become invalid. Thus, this thesis also develops and demonstrates a supporting framework to help identify these underlying assumptions and ensure they remain valid both during system development and after the system is placed into operation.",
        "authors": [
            "Justin Wei Siang Poh"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158793",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Enhancing Robotic Manipulation of Liquid Using a Digitally Fabricated Intelligent Wearable Device",
        "abstract": "Despite recent exponential advances in computer vision and reinforcement learning, it remains challenging for robots to interact with liquids due to visual obstructions, transparent liquids, and fine-grained splashes. Yet, a substantial opportunity exists for robotics to excel in liquid identification and manipulation, given its potential role in chemical handling in laboratories and various manufacturing sectors such as pharmaceuticals or beverages. Recent advancements in electronic wearables, designed to replicate or surpass the functions and attributes of human skin, and their convergence with machine learning have provided opportunities to enhance the capabilities of robotic systems. Here, we present a novel approach for liquid class identification and position estimation with the robotic wearable device that can ‘see through’ the container, leveraging electrical impedance sensing. We design and mount a digitally embroidered electrode array to a commercial robotic gripper. Coupled with a customized impedance sensing board, we collect data on liquid manipulation with a swept frequency sensing mode and a frequency-specific impedance measuring mode. Our developed learning-based models achieve an accuracy of 93.33% in classifying 9 different types of liquids (8 liquids + air) and 97.65% in estimating the liquid position in the cup without any vision system present. We investigate the effectiveness of our system with a series of ablation studies. These findings highlight our work as a promising solution for enhancing robotic manipulation in liquid-related tasks.",
        "authors": [
            "Young Joong Lee"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158941",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Input Adaptive Allocation of Language Model Computation",
        "abstract": "Computationally intensive decoding procedures—including search, reranking, and self-critique— can improve the quality of language model (LM) outputs in problems spanning code generation, numerical reasoning, and dialog. Existing work typically applies the same decoding procedure for every input to an LM. But not all inputs require the same amount of computation to process. Can we allocate decoding computation adaptively, using more resources to answer questions whose answers will be harder to compute? We present an approach that predicts the distribution of rewards given an input and computation budget, then allocates additional computation to inputs for which it is predicted to be most useful. We apply this approach in two decoding procedures: first, an adaptive best-of-k procedure that dynamically selects the number of samples to generate as input to a reranker; second, a routing procedure that dynamically responds to a query using a decoding procedure that is expensive but accurate, or one that is cheaper but less capable. Across a suite of programming, mathematics, and dialog tasks, we show that accurate computation-allocation procedures can be learned, and reduce computation by up to 50% at no cost to response quality, or improve quality by up to 10% at a fixed computational budget.",
        "authors": [
            "Mehul Damani"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158949",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On Solving Larger Games: Designing New Algorithms Adaptable to Deep Reinforcement Learning",
        "abstract": "In this thesis, we explore the design of algorithms capable of handling large games where the state space is too large to store strategies in a tabular format from a theoretical perspective. Specifically, we focus on developing algorithms suitable for deep reinforcement learning in two-player zero-sum extensive-form games. There are three critical properties for effective deep multi-agent reinforcement learning: (last/best) iterate convergence, efficient utilization of stochastic trajectory feedback, and theoretically sound avoidance of importance sampling corrections. Chapter 3 introduces Regularized Optimistic Mirror Descent (Reg-OMD), which provably converges to the Nash equilibrium (NE) linearly in last-iterate. Chapter 4 shows that algorithms based on regret decomposition enjoy best-iterate convergence to the NE. Chapter 5 proposes Q-value based Regret Minimization (QFR), which achieves all three properties simultaneously.",
        "authors": [
            "Mingyang Liu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158922",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Systems for Usable Machine Learning",
        "abstract": "Many real-world decision problems are complex, with outcomes difficult to measure and evaluate. The impact of decisions made in these domains is nuanced and takes a long time to be fully realized. Individual mistakes can lead to significant costs, and computational tools such as ML models must be integrated alongside existing, well-established human workflows. These properties of such decision problems means that ML solutions must be usable in order to be effective — in other words, developed and deployed in such a way as to be used by humans in decision-making and improve outcomes. In order improve ML usability, developers create ML tools, or diverse kinds of interfaces that allow users to understand ML models and their predictions. In this thesis, we use real-world case studies to synthesize generalizable lessons for applying usable ML tools to complex, real-world decision problems. Based on experience developing ML tools for child welfare screening, we propose a formal taxonomy of feature properties related to usability and interpretability. We then discuss the design and development of a system to make generating ML explanations that use such interpretable features more effective. Pyreal is a framework and Python library implementation that uses updated data transformers to generate explanations of ML models and predictions using interpretable features. Motivated by the development and customization effort required to develop ML tools for new applications, we then discuss the development of Sibyl, a configurable and comprehensive system for generating usable ML interfaces for a wide range of applications. We then discuss our case study in applying Sibyl to the decision problem of wind turbine monitoring. We then discuss Explingo, our system for transforming traditional ML explanations into natural language narratives to further improve the usability of ML outputs. We finish by discussing the practical lessons this work demonstrates related to the need for usable ML, the challenges specific to these complex applications, ethical questions, and future directions.",
        "authors": [
            "Alexandra Zytek"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158953",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Building World Models with Neural Physics",
        "abstract": "World models learn the dynamics of environments in a data-driven manner, enhancing performance and efficiency in downstream tasks such as control, design, recognition, and generation, thanks to cost-effective simulation and differentiability. A pre-trained world model should ideally (1) accurately simulate ground-truth dynamics, (2) adapt easily to novel configurations, and (3) generalize across diverse physical effects. Previous attempts in this area have either utilized differentiable model-based physics with few parameters exposed or trained for specific scenarios with minimal physical priors integrated. These world models fall short of their objectives, limiting their applicability in real-world accuratecritic deployments and scalability to larger pre-trained world models. In this thesis, we aim to build world models with neural physics, a hybrid neural-physics framework that models the basic dynamics with differentiable physics while learning all additional modules through neural networks. By integrating neural physics, the world models adhere closely to physical principles while efficiently learning diverse effects. The modular structure of neural physics allows world models to generalize to novel configurations simply by installing different pretrained neural modules. We will demonstrate the effectiveness of this novel framework in applications such as reconstruction, robotic control, and scientific discovery.",
        "authors": [
            "Pingchuan Ma"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158927",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Development of Additively-Manufactured Quadrupole Mass Filters for Low-Cost and High-Performance Applications",
        "abstract": "With a growing need for more compact and affordable mass spectrometers, many efforts have been made to miniaturize quadrupole mass filters (QMFs). Unfortunately, these efforts have yielded devices with inadequate performance for practical applications in analytical chemistry. This study reports the successful creation of a low-cost, high-performance QMF by means of additive manufacturing. Vat photopolymerization of glass-ceramic feedstock was used to create a novel, monolithic structure, and selective electroless nickel-boron plating metallizes the structure, forming a completed QMF that is lightweight and inexpensive to produce (20 USD per device). Furthermore, additive manufacturing allows QMF dimensions to be rapidly scaled to the optimal sizes for a given application, which is larger than most prior affordable quadrupole designs. Despite the limited precision of additive manufacturing, optimization techniques can be leveraged to produce high-quality devices with smooth surfaces. As a result, our QMFs achieved mass resolutions up to 164 at 69 Da, with abundance sensitivities sufficient to detect carbon-13 isotopes at lower masses—a level of performance comparable to commercial devices. These results indicate that additive manufacturing, properly employed, can significantly advance the state of the art of QMFs and other mass spectrometry technologies.",
        "authors": [
            "Colin C. Eckhoff"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158944",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Structuring Representation Geometry in Self-Supervised Learning",
        "abstract": "The central promise of deep learning is to learn a map 𝑓 : 𝒳 → ℝ_𝑑 that transforms objects 𝒳—represented in their raw perceptual forms, such as images or molecular strings—into a representation space ℝ_𝑑 where everything that is hard to do with raw perceptual data becomes easy. For instance, measuring the similarity between two objects [scientific notation] expressed as tensors of pixel intensities is non-trivial in their raw form, but becomes straightforward if 𝑓 maps these objects to a space where simple Euclidean distances, ‖𝑓(𝑥₁) − 𝑓(𝑥₂)‖₂ are meaningful measures of similarity. While this simple recipe has shown standout success in a range of tasks, certain applications require representations that encode richer structural relationships beyond pairwise similarity. For instance, tasks that encode relational information— such as “𝑋 is a parent of 𝑌 ” or “𝐴 is a treatment for 𝐵”—require embedding spaces that capture richer structural relationships. In this thesis, we explore what 𝑓 should encode in order to be useful for a range of unknown downstream tasks, from the point of view of the geometric structure of representation space. We investigate this question in the context of self-supervised learning, a paradigm that extracts meaningful representations by leveraging the structure of the data itself without relying on explicit labels. Specifically, we propose adding additional geometric structure to the embedding space by enforcing transformations of input space to correspond to simple (i.e., linear) transformations in the embedding space. To this end, we introduce an equivariance objective and theoretically prove that its minima forces transformations on input space to correspond to rotations on the spherical embedding space. Our proposed method significantly improves performance on downstream tasks, and ensures sensitivity in embedding space to important variations in data (e.g., color, rotation) that existing contrastive methods do not achieve.",
        "authors": [
            "Sharut Gupta"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158966",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Cost Optimized Logistics for Commercial Operations in Low Earth Orbit and Cislunar Space",
        "abstract": "Designing profitable mission and logistics architectures is necessary to establish a profitable commercial market and support a robust space economy. It is the goal of the National Aeronautics and Space Administration (NASA) to establish such an economy in low Earth orbit (LEO) through the implementation of commercial LEO destinations and to commission self-sustaining lunar infrastructure through the Artemis missions. The ISS and the Apollo lunar landers demonstrated the ability to provide safe and reliable habitation, but the cost to support these missions has been on the order of billions of United States Dollars (USD). Minimizing the operational costs of commercial space systems will be required if commercial companies expect to generate a profit from their services. To address this, this thesis derives and demonstrates a manual cost optimization method for space system mission architectures, with respect to logistical and system design. In tandem, a computational tool called the Cost model for Space system Operations (COST-O) was developed. The demonstration included the iteration of a logistics and system design vector for two cases: a commercial LEO space station, and a commercial lunar in-situ resource utilization (ISRU) liquid oxygen generation system. These mission architectures were modelled and simulated in SpaceNet which first analyzed for feasibility and then were processed by COST-O. This data was used to make financial forecasts and were analyzed for cost sensitivity. The results suggest that for a commercial LEO space station, a closed loop ECLSS, large stockpile of resources, reduced resupply cadence, and a combination of tourists and visiting crew would be a profitable architecture at the crew capacity of at least three paying customers present on the station per day with an annual operational cost of 1,129,731,710 USD. Profits would be achieved by the end of ten years of steady state operations at the current market price of 3.12 million USD per crew member per day. Attempts to minimize this cost should first be made in the cadence of funded astronaut technician flights, as crew launches contribute most to the overall operational cost. Future work should address ways to minimize this, such as reducing the required amount of astronaut technicians that must be present at any given time. For a commercial lunar ISRU liquid oxygen generation system, an architecture supporting a closed loop system, using Starship as the launch and landing vehicle, a prepositioned stockpile of resources at the lunar surface, and a hydrogen reduction agent is most cost optimal, with an annual operating cost of 19,275,486,559 USD, and profitability achieved at the design rate of twenty metric tons of liquid oxygen produced and sold per year. At the current market price of 1.2 million USD per kilogram, the system would be profitable by the end of the first year of steady state operations. Attempts to minimize this operational cost further should improve the recyclability of the system. Future work should evaluate added robustness to the architecture by delivering multiple systems and should model deliberate cargo packing decisions.",
        "authors": [
            "Ireland Brown"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158866",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On the dynamics and interparticle forces of electrostatically stabilized colloidal suspensions",
        "abstract": "In a broad spectrum of industrial and biomedical applications, the equilibrium and dynamic properties of colloidal suspensions play a pivotal role, with systems ranging from simple gold nanoparticles in electrolyte solutions to complex assemblies like micelles, vesicles, nanocapsules, and dendritic polymers. Typically, these systems are approached through the Derjaguin–Landau–Verwey–Overbeek (DLVO) theory and Poisson-Boltzmann models, frameworks that approximate charged particles as point charges to predict interparticle interactions. While these frameworks have been instrumental for low-concentration, idealized systems, it falls short in capturing critical behaviors in more concentrated regimes. In such environments, overlooked phenomena—such as excluded volume effects and ion-ion correlations—become essential in shaping the colloidal system’s equilibrium and dynamics. By leveraging advanced computational techniques, we systematically interrogate these mesoscale interactions, offering insights that extend beyond the traditional paradigms of mean-field theory and enhance our understanding of colloidal behavior in complex environments. The first part of this work presents the development of efficient algorithms that significantly advance the computational speed of induced polarization calculations within Brownian Dynamics simulations of polarizable colloidal particles. By establishing a new benchmark in simulation methodologies, these algorithms lay the groundwork for exploring complex soft matter systems, enabling deeper insights into the dynamic and equilibrium properties of colloidal suspensions beyond the limitations of conventional theories. Together, these advancements provide a robust computational framework for examining mesoscale interactions in concentrated colloidal systems, where ion correlations, finite ion volumes, and thermal fluctuations critically influence behavior. The next part of this work focuses on the study of equilibrium properties of charged soft matter systems in crowded environments through the implementation of robust computational techniques. We meticulously examine charge-density correlations and clustering behaviors that arise due to the complex electrostatic interactions between colloidal particles. At high ion concentrations, the system undergoes distinct structural transitions that are modulated by the ionic strength and specific particle characteristics. These transitions are characterized by emergent patterns in the spatial distribution of charges, forming structured clusters that reflect the balance between electrostatic and entropic forces. We further our studies by computing the potential of mean force (PMF) between metallic nanoparticles, a measure of the effective interaction potential that inherently captures how particles interact across various separation distances in an electrolyte. The PMF analysis reveals oscillatory behavior in particle interactions at different concentrations. Our study delivers robust free energy profiles, enabling a more nuanced understanding of the electrostatic forces at play in dense colloidal suspensions. These insights shed light on the mechanisms of charge screening and packing within high-density systems. The final part of this thesis focuses on the study of the non-linear transport properties of concentrated macroions to external electric fields, revealing intricate dependencies on both ionic structure and external electric fields. Our studies reveal how conductivity is modulated by charge density correlations and field strength. A notable disruption of local ionic atmospheres was observed with increasing field strengths, which in turn accelerates ion mobility and significantly alters the transport properties. We further advance the investigation into the dynamic response of concentrated macroions and electrolytes by examining their behavior under time-varying electric fields. Through simulations involving frequency sweeps and chirp signals, we discerned that the dynamic response of these concentrated charged soft matter systems is best understood through the lens of two distinct transport regimes—characterized by short- and long-time responses. This bifurcation enables the introduction of a relaxation time scale that captures the intricate coupling between ionic correlations and the macroscopic system response, highlighting the pivotal role of excluded volume effects in densely populated environments. The study provides a detailed framework for manipulating ion transport in concentrated electrolytes and macroions, paving the way for innovations in fields reliant on precise control of electrostatic conditions and ionic mobility.",
        "authors": [
            "Emily Krucker-Velasquez"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158882",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Reproduction, settlement, and phenology of intertidal barnacles: Implications for larval dispersal",
        "abstract": "Knowledge of the consequences of ocean warming on marine populations and communities is urgent. Warming oceans are predicted to result in changes to the seasonal timing of reproduction and settlement (phenology); faster development rates and, for crustaceans, smaller larvae; reduced larval dispersal distances; and reduced connectivity between coastal populations. However, these predictions are largely based on laboratory and modelling studies, with little observational research to explore how these interactions unfold in natural ecosystems where temperature variability is pervasive. In this thesis, I investigate the links between reproduction and settlement timing of intertidal barnacles, and I explore the extent to which the timing of these events is explained by environmental and astronomical cycles and by water column conditions. In Chapter 2, I assess the cycles driving Chthamalus fissus reproduction and settlement in Southern California, and I offer a first order estimate of alongshore larval transport. I found that barnacles were reproductively active almost year-round, with clear lunar cyclicality and modest seasonality. Conversely, settlement exhibited little cyclicality on any timescale. Chapters 3, 4, and 5 focus on the effects of temperature on Semibalanus balanoides early life history along a steep temperature gradient in the northwest Atlantic over twenty years of warming. In Chapter 3, I investigate the effects of intertidal temperature on reproduction timing, analyzing separately the processes of fertilization, embryonic brooding, and larval release. In Chapter 4, I estimate larval duration in natural populations, and I measure the impact of temperature on larval duration in the laboratory and field. In Chapter 5, I investigate the effects of water temperature on larval size at settlement. I found that warmer nearshore temperatures significantly correlated with shorter brooding times of developing embryos, shorter field-estimated larval duration, and smaller larval settlers. Notably, the interplay between benthic reproduction, pelagic development, and temperature variability across space and time created counter-intuitive patterns in larval duration, size, and likely dispersal. Together, these findings point to the importance of reproductive timing in determining dispersal and population connectivity, and they highlight the need for extensive field measurements to quantify phenology and phenology shifts in benthic systems.",
        "authors": [
            "Jane B. Weinstock"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158871",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Computational Thermo-Chemo-Mechanics Framework for the Large-Scale Simulation of Material and Structural Failure in Hypersonic Environments",
        "abstract": "Materials and structures subjected to the extreme conditions of hypersonic flight undergo complex degradation and fracture processes. This thesis presents a theoretical formulation and a computational framework that enables the large-scale simulation of thermochemically fracturing solids exhibiting complex post-fracture interface response. The continuum theory is based on a general thermodynamically-consistent description of the coupled multiphysics problem, and the numerical formulation extends the scalable discontinuous Galerkin(DG)/Cohesive Zone Modeling paradigm to thermo-chemo-fracture mechanics. The approach is distinguished by its unified DG treatment of the coupled problems, which facilitates the analysis of fracture propagation, fracture-dependent heat and mass transfer as well as thermally-activated solid-phase chemical reactions. The framework is verified against two analytical solutions of boundary value problems drawn from thermo-poro-elasticity and thermally-driven delamination. Three-dimensional simulations of a benchmark thermochemically-driven fracture problem illustrate the parallel scalability of the fully-coupled computational framework. We utilize this framework to render models of passive oxidation-induced fracture in ultrahigh temperature ceramics computationally tractable. First, a rigorous constitutive theory is shown to capture the molecular diffusion of oxidant through the reaction product layer using only fundamental transport properties, i.e. without the need for calibration to reaction experiments. The physical processes observed on the diminutive scale of an oxide layer are explicitly resolved, but the approach is limited to microscale analyses by scale separation. We sidestep this limitation by specializing the general theory under specific phenomenological assumptions, thereby yielding a practical model that can reproduce oxidation experiments. We use this specialized model to analyze oxidation-induced swelling, fracture and delamination in SiC/coating systems, and unveil the coupled thermochemical response as well as fracture morphologies in the vicinity of critical flaws. Then, we conduct a parametric study of three-dimensional coatings that exposes the channeling mechanisms above penny-shaped delaminations of various sizes. The computational analyses identify a transition from decussating to circumferential channel cracking that explains the wide variety of surface channel cracks observed in experiment. The physical mechanisms and fracture morphology regimes are corroborated by a simple structural theory. Finally, cohesive fracture models, splitting methods and thermal solvers are developed specifically for applications to thermally shocked ceramics. Simple and rigorous calibration procedures are proposed which facilitate the direct analysis of fragmentation and comminution in brittle solids subjected to extreme advective heat transfer. The presented examples serve as evidence that the framework can successfully enable three-dimensional, thermochemically-coupled fracture analyses of unprecedented physical fidelity, which furnish new insights into complex hypersonic thermal protection system response.",
        "authors": [
            "Daniel N. Pickard"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158883",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Expanding options for the mechanical characterization of biological materials",
        "abstract": "The mechanical properties of biological tissues change over time and with disease progression, and they provide important information regarding the limits a tissue can sustain before injury. Therefore, quantifying these properties in biological materials and their synthetic simulants could be instrumental for accurate medical diagnoses, treatment of disease, and prediction of traumatic injury survivability. Conventional methods of mechanical testing, such as uniaxial tension, compression, and nanoindentation,  provide highly repeatable and reliable results for the stiff materials for which they were originally developed. However, the same cannot be said when these methods are applied to the characterization of soft and biological materials due to limitations of specimen size, fixturing capabilities, and sample preparation. Volume Controlled Cavity Expansion (VCCE) is a recently developed technique to measure local mechanical properties of soft materials in their natural environment. Through the highly controlled expansion of a fluid bubble at the tip of an injection needle, paired with simultaneous measurement of the resisting pressure, a local signature of a material's mechanical response can be obtained. \r\n\r\nThis thesis presents the first systematic application of VCCE to biological materials. It begins by presenting a cautionary example of the limitations of soft material testing, focusing on the synthetic silicone and tissue simulant polydimethylsiloxane (PDMS). We find that the wide range of mechanical properties  reported in literature are due to biases imparted by different testing methods. We then use VCCE to examine the elastic response of gelatin, whole blood clot and liver tissue, demonstrating with high repeatability that subtle mechanical changes occur within a matter of days as these tissues age. Finally, this work applies VCCE to investigate what happens to these materials after elastic expansion, and throughout a process of controlled damage. Biological materials are found to demonstrate toughening  that does not appear  in gelatin and PDMS. Because of these observed differences, we caution against using gelatin and PDMS for simulating the behavior of biological materials in extreme loading cases. Combining these findings, this thesis provides evidence that  more widespread adoption of VCCE in mechanical testing would provide a path to better understanding of the mechanics of soft and biological materials, with implications in fundamental mechanics research as well has in biological and healthcare applications.",
        "authors": [
            "Hannah Martin Varner"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158845",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "ΔB₀ Field Control in High Field MRI with Local Multcoil Shim Arrays",
        "abstract": "Local multicoil ΔB₀ shim arrays enable low-cost, simple to fabricate, and physically small, static magnetic field control in magnetic resonance imaging. The presented thesis will show frameworks for coil current calculation for homogeneity and novel selective excitation applications. As MRI RF coils trend towards repositionable and flexible systems for their ease of use and tight-to-the-patient fit, ΔB₀ shim arrays are left behind for lack of rapid, patient-on-the-table calibration. We show an inverse problem approach with physics-based regularization and adaptation to accelerate calibration by over 50 fold. The numerical tools developed for calibration also proved useful for design to enable novel upper bounds on ΔB₀ shim performance and new tools for automatic application and anatomy-specific local multicoil array design.",
        "authors": [
            "Nicolas Arango"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158963",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Scalable and Modular Manufacturing of Insect-Scale Aerial Robots Towards Swarm Flight Demonstrations",
        "abstract": "Insects demonstrate remarkable capabilities in navigating complex environments and executing tasks such as pollination and coordinated object transport. Inspired by these biological feats, insect-scale micro aerial vehicles (MAVs) have been developed with advanced flight functionalities, including collision resilience and aerial acrobatics. Despite these advancements, MAVs weighing less than a gram continue to face critical challenges in design, assembly, and repair. Additionally, limitations in sensing and control have prevented the realization of swarm-like behaviors, thereby constraining research on collective actions and potential applications such as distributed sensing. To overcome these obstacles, this work introduces a scalable and modular fabrication method for sub-gram MAVs. A parametric design algorithm automatically generates laser cutting templates from a minimal set of design parameters, while stereolithographic 3D printing is employed to fabricate static components such as airframes and connectors, significantly streamlining the production process. This modular approach improves assembly efficiency and repairability, reducing fabrication time by more than half. Using this methodology, two sub-gram MAVs successfully demonstrated controlled hovering and coordinated payload transport. These results represent a significant step toward enabling insect-inspired robotic swarms, providing a platform for future studies on collective flight behaviors and swarm robotics.",
        "authors": [
            "Yi-Hsuan Hsiao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158965",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Coarse Modality",
        "abstract": "One of the early successes of the application of possible worlds semantics to the analysis of natural language is Kratzer’s account of modality. A large part of the subsequent literature on modals has sought to expand the crosslinguistic coverage of that framework, and, in so doing, many new generalizations and constraints have been proposed and re-examined. The present dissertation situates itself within this tradition and makes both an empirical and theoretical contribution. Using the Italian adverb magari as the main empirical source, it will be argued that there exists a previously unnoticed type of modality which is referred to here as “coarse”. Its most evident manifestation is a special type of epistemic possibility, one that comes with an “antievidential” requirement. Antievidential possibility in assertions and questions is discussed in Chapters 1 and 3 respectively. Chapter 2 frames coarse modality as a more general phenomenon that comes about through modification of modal expressions. The theoretical argument of this dissertation is a novel corroboration of Kratzer’s premise semantics approach. It will be argued that the most natural and general account of coarse modality is possible by utilizing the premise set, a powerful resource of the system, in a novel way.",
        "authors": [
            "Enrico Flor"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158905",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Lagrangian perspective of mesoscale biophysical interactions in the subtropical ocean",
        "abstract": "The most kinetic energy in the ocean is at the mesoscale, which includes highly dynamic physical perturbations that persist for months, a biologically relevant timescale for phytoplankton growth and bloom development. Importantly, mesoscale currents and the associated biological responses (i.e., biophysical interactions) are not spatiotemporally static, so they are difficult to characterize. In this thesis, we interpret phytoplankton observations in an objective Lagrangian manner, or with a frame of reference that follows the motion of water parcels experienced by drifting organisms. We build a Lagrangian coherent eddy tracking algorithm that identifies the boundaries of water masses trapped for a month or longer. Using this tool, we assess the variability of the lateral advective properties of eddies across the North Pacific Subtropical Gyre, finding that only half of the remotely sensed eddies identified from the traditional, Eulerian sea level anomaly method trap waters for these timescales. We then statistically compare satellite-observed chlorophyll-a anomalies associated with eddies that trap versus mix across their boundaries. Lagrangian coherent vortices have more anomalous biological signatures in the gyre, so we argue that the role of leaky eddies in altering biogeochemistry may be underestimated due to lateral dilution. We also highlight substantial regional and seasonal variability in the dominant biophysical interactions within the oligotrophic regime, helping to explain inconsistencies of in situ eddy observations across this region. Lastly, we show how the Lagrangian water mass histories of in situ samples shape the phytoplankton community in the open ocean, quantified with amplicon sequencing and internal genomic standards. In non-eddy waters, we found that cyanobacteria are advantaged over eukaryotic phytoplankton when lateral mixing is minimized for several months. In or near mesoscale eddies, where vertical perturbations are a source of new nutrients, eukaryotic phytoplankton gene abundance has no dependence on the lateral mixing histories. The results suggest dispersal and niche generation drive phytoplankton variability but in different ways in and outside eddies. This thesis emphasizes how Lagrangian tools reveal mesoscale structures (otherwise invisible with Eulerian reference frames) that trap, transport, and transform ecosystems, generating phytoplankton patchiness and variability in the surface ocean.",
        "authors": [
            "Alexandra E. Jones-Kellett"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158812",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Floor Plan Design Collaborator: A Data-Driven Approach to Assist Human Architects in Design Exploration",
        "abstract": "After a long AI winter since the 1980s, artificial intelligence is now experiencing a renaissance due to enhanced computing power and access to vast amounts of data. Today, machines can talk, sing, and draw like human experts. Despite this progress, we are still far from the vision where human designers and AI collaboratively discuss and develop designs. This study argues that a data-driven approach holds great potential in the design process by quickly learning from existing examples and generating new alternatives for exploration. To support this claim, the study presents a generative framework that learns from existing examples and generates new designs. Specifically, the proposed framework employs Bayesian networks to encode site layout data and floor plan examples, generating new design examples through a Markov Chain Monte Carlo (MCMC) sampling procedure. Experiments on real-world examples demonstrate that the framework effectively summarizes the statistical information of given design examples and generates unseen examples based on the learned knowledge. The transparency of the data representation and the inner workings of the proposed framework facilitate an active feedback loop in the iterative learning and generation process between human designers and machines. Observations throughout the study reveal intrinsic limitations and potential improvements of contemporary optimization-based approaches from the perspective of both lateral and vertical design development.",
        "authors": [
            "Woongki Sung"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158842",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Hidden Roots of Neoliberal Success in Agrarian Transformation: State Engagement, Farmer Professionalization, and Technological Interdependence in the Senegal River Valley",
        "abstract": "Recent scholarship celebrates irrigated rice in the Senegal River Valley (SRV) as a success story. This is remarkable considering the SRV’s history of agrarian transformation, which critics characterize as incoherent, erratic, and self-destructive. How did this turnaround happen? How did good seeds emerge from bad soil? Conventional explanations point to enlightened market-based reforms and technological upgrading following state withdrawal from most agricultural activities. In other words, the SRV is portrayed as a triumph of neoliberalization. This dissertation offers an alternative, additive view. In Paper 1, I situate the SRV’s transformation in broad historical context, showing how notions of development, technological change, and poverty alleviation have evolved and the implications for what strategies are pursued. I illustrate how a popular contemporary development model— appropriate technology (AT) 2.0, an evolution of Schumacher’s 1970s AT 1.0—that valorizes smallscale technologies and market-led interventions is attractive in explaining successes like the SRV, even as it proves ultimately reductive. In Paper 2, I demonstrate how the state, despite policies curtailing its activities and a dominant narrative asserting its disengagement, continues to play an active role in the SRV. By imparting practical skills, such as pump operation, contract negotiation, and bookkeeping, state action helped farmers professionalize. A durable effect is a “we’re in this together” state-farmer mentality. When this relationship is tested, well-respected intermediaries, often religious leaders, intercede. In Paper 3, I show how farmers construct assemblages of resources, skills, and knowledge to achieve their goals. They rely on negotiation skills and social ties with local leaders, appealing to “public interest” couched in religious terms. In forsaking key aspects of the dominant assemblage to pursue alternatives, farmers exercise their agency and enhance market functioning by permitting flexibility, acknowledging technological interdependencies, and mitigating recurrent risks. This dissertation offers hope that successful agrarian development is possible in challenging, resource-constrained environments. Based on 11 months of fieldwork, I show how state and farmer actions bolstered market reforms, underpinning their success. In centering on-the-ground realities, I move beyond dominant explanations and neat theoretical classifications to reveal underreported but nonetheless fundamental processes and mechanisms through which development occurs.",
        "authors": [
            "Brian Jonars Besana Spielberg"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158867",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "An Instrument for the Measurement of Soft Material Nonlinear Mechanical Response",
        "abstract": "Soft material research has seen significant growth in recent years, with emerging applications in robotics, electronics, and healthcare diagnostics where understanding material mechanical response is crucial for precision design. Traditional methods for measuring nonlinear mechanical properties of soft materials require specially sized samples that are extracted from their natural environment to be mounted on the testing instrument. This has been shown to compromise data accuracy and precision in various soft and biological materials. To overcome this, the Volume Controlled Cavity Expansion (VCCE) method was developed. This technique tests soft materials by controlling the formation rate of a liquid cavity inside the materials at the tip of an injection needle, and simultaneously measuring the resisting pressure which describes the material response. Despite VCCE’s early successes, expansion of its application beyond academia has been hindered by cost, size, and expertise. In response to this, the first portable, bench-top instrument utilizing VCCE is presented here. This device, built with affordable, readily available components and open-source software, streamlines VCCE experimentation without sacrificing performance or precision. It is especially suitable for space-limited settings and designed for use by non-experts, promoting widespread adoption. The instrument’s efficacy was demonstrated through testing Polydimethylsiloxane (PDMS) samples of varying stiffness. This study not only validates instrument performance, but also sets the stage for further advancements and broader applications in soft material testing. All data, along with acquisition, control, and post-processing scripts, are made available on GitHub.",
        "authors": [
            "Brendan M. Unikewicz"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158836",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Algorithmic Advances in Range-Aided Navigation",
        "abstract": "This thesis contributes to the advancement of range-aided simultaneous localization and mapping (RA-SLAM) through algorithmic developments and real-world demonstrations. Broadly speaking, SLAM is the process by which an agent combines sensor measurements to simultaneously create a map of the world and localize itself within this map. SLAM has been called the ‘holy grail’ of field robotics, and in many instances it is a critical enabling capability for autonomous agents to operate in the real world. RA-SLAM is the specific case of SLAM which incorporates point-to-point distance measurements (e.g., distance measurements between an autonomous underwater vehicle and an acoustic buoy) into the inference process. The ability to leverage such measurements is desirable, as they can help in resolving ambiguities (e.g., am I in hallway A or B) and the relevant sensors are often low-cost and simple to integrate (and thus pose the potential to be widely deployed). However, there are theoretical challenges that have historically limited the reliability of RASLAM approaches. At the root of these challenges is the issue that a single range measurement does not uniquely determine the relative position between two points. In state-of-the-art RASLAM formulations, this ambiguity manifests as non-convexity in the maximum a posteriori inference problem. As a result of this non-convexity, standard local-search optimizers are highly dependent on quality initializations to obtain the correct state estimate. To address this issue of reliability, this thesis presents the first certifiably correct algorithm for RA-SLAM. This algorithm, Certifiably Correct RA-SLAM (CORA), is capable of (i) obtaining globally optimal solutions for many real-world RA-SLAM problem instances and (ii) providing certificates of correctness for these solutions. CORA leverages a novel semidefinite programming (SDP) relaxation of the RA-SLAM problem, which it solves efficiently using the Riemannian Staircase methodology. This methodology allows CORA to typically obtain globally optimal solutions faster than the existing state-of-the-art local solvers. These results expand our understanding of problems suited for efficient global solvers and highlight the key problem structures that appear necessary to develop and deploy such solvers, pointing towards exciting future directions in trustworthy model-based autonomy. We demonstrated the performance of CORA on a range of real-world RA-SLAM datasets, including a set of large-scale multi-agent experiments conducted as part of this work. In these experiments CORA reliably estimates agents’ trajectories in both single- and multi-robot settings. CORA gracefully scales to large problems consisting of multiple agents and tens of thousands of robot poses. These experiments not only validate CORA’s performance, but also fill an existing gap in open-source datasets available to the research community and provide practical insights to guide future deployments of autonomous navigation systems in large, complex environments.",
        "authors": [
            "Alan A. Papalia"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158863",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "thesis in the field of Mechanical Engineering: Relevance for Human-Robot Collaboration: Definitions, Systems, Algorithms, and Applications",
        "abstract": "Human-Robot Collaboration (HRC) combines the strengths of human and robotic capabilities to accomplish complex tasks, yielding significant impacts in various domains. To enable seamless interaction in dynamic and unpredictable environments, robots are required to efficiently and accurately perceive their surroundings, align reasoning with human cognition, anticipate key attributes, and generate safe, effective actions to support humans proactively. This thesis introduces relevance, a novel concept inspired by human cognition, to improve the efficiency, safety, and intelligence of HRC. Relevance enables robots to prioritize objects based on their importance to human goals, allowing them to concentrate computational resources on key elements. This focused approach reduces input space for essential algorithms, minimizes processing delays, and enhances safety and adaptability in dynamic environments, facilitating more natural and intuitive collaboration with humans. This thesis systematically explores the concept of relevance, introducing a hierarchical model for relevance quantification that combines scene understanding in cluttered environments with an event-based, multi-modality framework, enabling real-time relevance determination based on human objectives, preferences, spatial-temporal relationships, and constraints. A relevance-based perception strategy further directs models to prioritize key areas, reducing computational and inference times, while two new safety metrics—Critical Collision Probability (CCP) and Average Collision Probability (ACP)—quantify reduced collision risks in Human-Robot Collaboration (HRC). Additionally, a relevance-driven framework integrates relevance quantification with dynamic scene understanding and decision-making, achieving high human objective and relevance prediction accuracy. An advanced human intention prediction framework using head orientation, object affordance, and hand movement also enhances precision, accuracy, and F1 scores over baseline models. Results demonstrate that relevance quantification significantly reduces task planning time by 79.56% and inquiries by 80.84%, with a real-world coffee-serving demonstration highlighting its potential for proactive, autonomous assistance. Furthermore, the safe motion generation algorithm reduces collision incidents by 63.76% and collision frames by 44.74%, supporting accurate, safe robotic assistance in dynamic environments. The concept of relevance enhances the efficiency, safety, and intelligence of human-robot collaboration (HRC) within dynamic and unpredictable environments, supporting a deeper integration of robotics into diverse real-world applications. Its potential extends beyond HRC, with promising applicability in autonomous driving and other complex domains where adaptive, context-aware decision-making is essential.",
        "authors": [
            "Xiaotong Zhang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158873",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Co-Designing Efficient Systems and Algorithms for Sparse and\r\nQuantized Deep Learning Computing",
        "abstract": "Deep learning models are becoming increasingly complex, expanding from 1D text and 2D images to 3D point clouds, while their size continues to grow exponentially. This trend highlights the need for greater efficiency. This thesis systematically explores efficiency in two resource-intensive domains—autonomous driving and generative AI—by focusing on fundamental model compression techniques: sparsity and quantization, alongside the co-optimization of systems and algorithms. Sparsity is crucial for autonomous vehicle (AV) applications. LiDAR processing, which requires 3D sparse computation, is inefficiently handled by current GPU libraries, creating a performance bottleneck in AV perception. To address this, we propose TorchSparse++, a high-performance GPU system for 3D sparse convolution, achieving 1.7-3.3× speedups over state-of-the-art libraries. Additionally, we introduce BEVFusion, an efficient multi-sensor fusion framework that fuses information in bird’s-eye-view (BEV) space, reducing computation by 1.9× while enhancing accuracy compared to prior methods. Generative AI is constrained by the massive size of models, necessitating quantization for efficient deployment. This thesis presents two GPU systems for accelerating large language models (LLMs): TinyChat for edge LLM deployment and QServe for cloud-based LLM serving. TinyChat boosts edge LLM inference by 3× using activation-aware weight quantization (AWQ). QServe further improves performance with activation and KV cache quantization, enhancing the throughput of NVIDIA TensorRT-LLM by 1.2-2.4× on A100 GPUs. Finally, we introduce HART, an efficient autoregressive image generation method that achieves 4.5-7.7× higher throughput compared to diffusion models while maintaining visual quality. HART achieves this improvement by leveraging quantized, or discrete, visual tokens to capture the high-level structure of images, while a lightweight diffusion model is used for fast inference of finer details.",
        "authors": [
            "Haotian Tang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158928",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Learning from Past Market Outcomes: Evidence from the Music Industry",
        "abstract": "We leverage unique features of music albums to investigate how musicians learn from current products when developing new products. We find that songs on a musician’s next album tend to be more similar to the songs that are more successful on that musician’s current album. This effect is stronger when the musician has less experience, and when the song on the current album is more novel (for that musician). Our findings suggest that musicians learn from the success of previous songs when developing new songs, and that learning is stronger if the musician has more need to learn, and when the song contains more new information.",
        "authors": [
            "Jason Du"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158804",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Dynamic Markers",
        "abstract": "When I was a child, I was certain that all clouds came from New \tJersey. After passing through the Lincoln Tunnel, I-95 would gradually ascend, lifting our car to eye level with the billowing clouds emerging from beneath us. These clouds rose from the Meadowlands, a great marsh just two miles west of Manhattan, a landscape that has become defined by the infrastructure that occupies it. Nearly equal in land mass and opportunity to Manhattan, this landscape managed to resist holistic transformation due to our inability to control its water. Rather than becoming a prosperous site for agriculture in the 19th century, or the next metropolis in the early 20th, the Meadowlands fell out of focus and became a site to absorb the infrastructural networks needed to uphold rapid development at its edges.\r\n\r\nThe Meadowlands was sutured shut by the networks interlaced through it in an attempt to erase the failures of the past. Utilizing this landscape as an urban sponge neglected that the marsh hosted a series of ecological infrastructures of its own. The Meadowlands' soft, uncertain ground once managed variations in the water level, but the draining of the ground that came with development reduced its capacity, making pump stations essential for managing water in inhabited areas. Unlike the other forms of infrastructure in the Meadowlands, the presence of the pump station is subdued, its invisibility upholds the illusion that the developments within this landscape are not threatened by their surroundings. However, steady sea level rise and an increase in storm surges have caused these pumps to fail, pulling the veil on their existence and more importantly, the essential role they play in our continued occupation of this landscape. The urgent need to increase the capacity of the pump station provides an opportunity to reconsider their agenda.\r\n\r\nThis thesis proposes the Dynamic Marker, a new type of infrastructure that redefines the relationship between human systems and ecological flows. Grafted onto existing pump stations in the Meadowlands, it releases water as mist from 800 feet in the air, transforming the hidden mechanics of water management into a moment of wonder. The Dynamic Marker fosters microclimates and ecological connections, transforming infrastructure into a dynamic process that evolves with its surroundings. Over time, it becomes both a memorial to the marsh and a provocation for the future, inviting a rethinking of infrastructure as a participatory and adaptive force that responds to its surrounding ecology.",
        "authors": [
            "Evan Ortiz"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158828",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Genetic algorithm gradient ascent (GAGA) optimization\r\nof compact symmetry-breaking photonic crystals",
        "abstract": "Fundamental limits of thermal radiation are imposed by Kirchhoff’s law, which assumes the electromagnetic reciprocity of a material or material system. Thus, breaking reciprocity can enable breaking barriers in thermal efficiency engineering¹. This thesis presents 1D photonic crystals composed of Weyl/Dirac semimetal and dielectric layers, whose structures are optimized to maximize the nonreciprocity of infrared radiation absorptance/emittance in planar and compact designs. Two different mechanisms to enable nonreciprocal infrared absorbers/emitters are simulated and compared – anomalous Hall effect in Weyl semimetals 2 and electric-current-induced Fizeau drag in either Dirac or Weyl semimetals3 . To engineer an ultra-compact absorber structure that does not require gratings or prisms to couple light, a genetic algorithm (GA) was used to maximize nonreciprocity in the design globally, followed by the application of the numerical gradient ascent (GAGA) algorithm as a local optimization to further enhance the design. The first absorber design takes advantage of the intrinsic nonreciprocity of time-reversal symmetry (TRS) breaking Weyl semimetals due to their pseudomagnetic field in momentum space. GAGA methodology is then applied to design and optimize a flat absorber using inversion (IS) breaking Weyl/Dirac semimetals as active layers, in which tunable nonreciprocity is induced through an applied DC current bias. This momentum bias imparts plasmon Fizeau drag, the drag of an electrical current on propagating surface plasmon polaritons (SPPs). A semi-classical theory recently developed is used to model SPP transport along interfaces of 3D semimetals under Fizeau drag3 . Lastly, in both cases the optimization algorithm accounts for both s- and p-polarized absorptance spectra to create a final design suitable for thermal applications, which maximizes the nonreciprocal absorptance of p-polarized light and simultaneously minimizes the parasitic, reciprocal absorptance of s-polarized light.",
        "authors": [
            "Hannah T. Gold"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158958",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Navigating RAD Conversions: Suggestions for Public Housing Rehabilitation",
        "abstract": "Public housing in the United States, a critical resource for nearly 1.7 million residents, faces significant challenges due to aging infrastructure and chronic operating funding shortfalls. The Rental Assistance Demonstration (RAD) program, authorized by Congress in 2012, aims to address these issues by leveraging private financing to rehabilitate and modernize public housing properties. Although the RAD program has been around for more than a decade and leveraged over $18.5 billion of construction investments, close to 75% of the more than 2500 eligible local PHAs are yet to benefit from it. This thesis examines the evolution of RAD programs, including the two newer tools, RAD/Section 18 Blend and Faircloth-to-RAD, and their adoption by public housing authorities (PHAs).\r\nThe research incorporates a review of HUD program and policies, RAD implementation data, and interviews with industry practitioners, including PHAs, developers, and consultants, to understand the hurdles preventing the adoption of the program and the characteristics of successfully structured projects. This thesis offers insights into how specific strategies are used to overcome the hurdles and provides practical recommendations for PHAs seeking to leverage RAD for public housing preservation and development. Key findings highlight the importance of utilizing available funding sources to achieve financial feasibility and enhancing organizational skills and capacity.",
        "authors": [
            "Yu Yan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158788",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Accelerating Distributed Deep Neural Network Training\r\nand Fine-Tuning Through Resource Interleaving",
        "abstract": "The ever-growing increase in dataset and model sizes of deep learning has created a massive demand for efficient GPU clusters. As the number of GPUs increases, the communication overhead of distributed Machine Learning (ML) training and fine-tuning workloads quickly takes up a significant portion of iteration time. Yet state-of-the-art ML schedulers tend to ignore the communication pattern of ML jobs when placing workers on GPUs. This thesis advocates for communication-aware resource scheduling as a critical approach to optimizing network utilization in ML clusters. We introduce a key idea for accelerating Deep Neural Network (DNN) jobs that interleaves the communication demands of different jobs sharing a network link. To illustrate this concept of interleaving, we first demonstrate how intentionally creating unfairness in bandwidth share between different DNN jobs improves their iteration times. Building on this insight, we present two novel systems designed to minimize network congestion and accelerate DNN training and fine-tuning jobs. The first system, Cassini, achieves interleaving using a centralized approach. In contrast, the second system, MLTCP, achieves the same goal using a distributed approach. Both systems are practical and readily deployable, depending on the service provider’s preference on deploying centralized or distributed solutions. In particular, Cassini, is a centralized network-aware job scheduler for ML clusters. Cassini introduces a novel geometric abstraction to consider the communication pattern of different jobs while placing them on network links. To do so, Cassini uses an Affinity graph that finds a series of time-shift values to adjust the communication phases of a subset of jobs such that the communication patterns of jobs sharing the same network link are interleaved with each other. Second is MLTCP, a distributed technique to approximate an interleaved centralized flow schedule. At the heart of MLTCP lies a straight-forward principle based on a key conceptual insight: by scaling the congestion window size (or sending rate) based on the number of bytes sent at each iteration, MLTCP flows eventually converge into a schedule that reduces network contention. To evaluate these systems, we conduct experiments using real-world DNN models on a testbed with Nvidia A100 GPUS. Cassini and MLTCP improve the average iteration times by up to 1.6× and 1.9×, respectively, demonstrating their effectiveness in reducing network congestion and accelerating ML workloads.",
        "authors": [
            "Sudarsanan Rajasekaran"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158918",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Tailored Mechanical Response of 3D Microgranular Crystals with Hierarchical Architecture",
        "abstract": "Granular media exhibit extraordinary impact-mitigating properties due to their nonlinear grain-to-grain interactions, enabling efficient energy dissipation and wave perturbation under dynamic loading—behaviors unattainable in conventional monolithic materials. Recent efforts have sought to engineer granular systems with tunable mechanical responses, though few have begun to realize them as functional architected materials. Here, we introduce a two-level architected granular framework that programs spherical microgranular media across both grain-level (ellipsoidal microvoids) and bulk granular packing-level architectures, offering surprising control over static and dynamic properties. Using nanoindentation experiments, we reveal tunable quasi-static stiffness behavior, where hollow architected granular packings can exhibit superior mass-normalized energy dissipation compared to their fully dense counterparts. Finite element simulations uncover a structurally engineered Poisson effect, enabling nonlocal contact mechanisms that enhance load-bearing capacity across different packing structures. Future custom direct impact experiments demonstrate a potential route the effectiveness of our multi-scale design in dynamically programming energy dissipation. Our findings demonstrate that a hierarchical granular crystal exhibits enhanced specific energy absorption at a fraction of the weight of their fully dense counterparts and unique nonlocal stress redistribution, surpassing classical granular mechanics through architectural design. This work establishes a path toward lightweight, tunable, and impact-resistant metamaterials, with broad applications in nonlinear waveguiding, energy dissipation, and protective systems.",
        "authors": [
            "Samuel D. Figueroa"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158956",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "American (Ise): On the Lifecycle of Stadiums in the United States",
        "abstract": "When the Kingdome in Seattle was completed in 1976, it was celebrated as a marvel of modern engineering, expected to last for centuries. Yet, in an ironic twist, it was demolished by implosion in 2000, surviving only twenty-four years. The Kingdome epitomizes the issue of short lifespans that has plagued American stadiums since the post-war era. A broad survey of these structures reveals an average lifespan of just three decades—a startlingly brief tenure for buildings of their scale and significance. These stadiums also follow a distinctive model of renewal. Similar to the Shikinen Sengu ritual at the Ise Shrine, a new stadium is often constructed adjacent to its predecessor. However, unlike Ise, where materials from the old shrine are reused and disseminated throughout Japan’s network of shrines, old stadiums are almost always demolished and discarded. This thesis seeks to superimpose Ise as a model onto American stadiums, envisioning an architecture that embraces both impermanence and longevity through circularity. Investigations into the barriers to circularity specific to stadiums serve as the foundation for design proposals, spanning scales from the detail to the site. The project ultimately imagines a stadium in a constant process of disassembly and renewal, where its spatial and programmatic potential challenge paradigms of completeness. In the context of a climate crisis demanding waste reduction, and for a typology notorious for its excess, stadiums can learn to do more with less.",
        "authors": [
            "Mackinley Wang-Xu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158895",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Causal Foundations for Pragmatic Data Science",
        "abstract": "A key goal of scientific discovery is the acquisition of knowledge that is practically useful for societal endeavors, such as the development of medicine or the design of fruitful economic policies. In this thesis, I place front and center the role that scientific models play in the process of decision-making, emphasizing the importance of causal models in science, i.e., models which describe the possible effects of actions upon a system. The work contained explores central topics in this domain, including causal discovery (learning causal models from data), causal representation learning (learning how to coarse-grain observations into causally sensible “macro-variables”), and end-to-end causal inference (the interplay between causal discovery and downstream decision-making).",
        "authors": [
            "Chandler Squires"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158952",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Development of a computational tool and dynamometer for optimizing variable-speed centrifugal pump selection for a containerized, direct-drive photovoltaic electrodialysis desalination system",
        "abstract": "This thesis presents an optimized centrifugal pump selection methodology to improve the hydraulic efficiency of MIT’s Global Engineering and Research (GEAR) Center’s containerized, direct-drive photovoltaic electrodialysis desalination system capable of producing up to 300m3 of potable water per day. The novel flow-commanded current control scheme of this containerized desalination plant (CDP), which enables its minimal energy storage, also means that the centrifugal pumps used are operated at variable speeds to respond to the solar irradiance. Unfortunately, centrifugal pumps are typically designed for fixed operating conditions, and manufacturers often only report pump performance at their rated frequency. By estimating the hydraulic resistances of the CDP and testing potential pumps on a redesigned dynamometer, a MATLAB-based tool was developed to quickly and iteratively characterize pump performance at their expected operating points in the CDP. A\r\n\"Compatibility Factor\" metric, defined by the normalized area under a pump’s efficiency-flow curve at its operating points, was devised to quantify each pump’s efficiency across the entire operating range of flow rates achievable under the CDP’s system constraints. Using this methodology, two 7.5 kW pumps were selected per diluate and concentrate channels to the electrodialysis stacks for alternate operation use. Following testing pumps on a dynamometer, this work outlines a methodology for characterizing a pump’s variable-speed efficiency at its operating points in any modeled system. This approach facilitates informed pump selection for the CDP to increase its water production and reduce its specific energy consumption, with an estimated improvement in hydraulic efficiency from 10% in GEAR Center’s previous system to over 30%. Overall, this work is applicable to various photovoltaic pumping systems aiming to reduce carbon emissions through variable-speed operation.",
        "authors": [
            "Muriel A. McWhinnie"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158831",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Deep learning initialized compressed sensing (Deli-CS) in volumetric spatio-temporal subspace reconstruction",
        "abstract": "Object Spatio-temporal MRI methods offer rapid whole-brain multi-parametric mapping, yet they are often hindered by prolonged reconstruction times or prohibitively burdensome hardware requirements. The aim of this project is to reduce reconstruction time using deep learning. Materials and methods This study focuses on accelerating the reconstruction of volumetric multi-axis spiral projection MRF, aiming for whole-brain T1 and T2 mapping, while ensuring a streamlined approach compatible with clinical requirements. To optimize reconstruction time, the traditional method is first revamped with a memory-efficient GPU implementation. Deep Learning Initialized Compressed Sensing (Deli-CS) is then introduced, which initiates iterative reconstruction with a DL-generated seed point, reducing the number of iterations needed for convergence. Results The full reconstruction process for volumetric multi-axis spiral projection MRF is completed in just 20 min compared to over 2 h for the previously published implementation. Comparative analysis demonstrates Deli-CS’s efficiency in expediting iterative reconstruction while maintaining high-quality results. Discussion By offering a rapid warm start to the iterative reconstruction algorithm, this method substantially reduces processing time while preserving reconstruction quality. Its successful implementation paves the way for advanced spatio-temporal MRI techniques, addressing the challenge of extensive reconstruction times and ensuring efficient, high-quality imaging in a streamlined manner.",
        "authors": [
            "S. S. Schauman",
            "Siddharth S. Iyer",
            "Christopher M. Sandino",
            "Mahmut Yurt",
            "Xiaozhi Cao",
            "Congyu Liao",
            "Natthanan Ruengchaijatuporn",
            "Itthi Chatnuntawech",
            "Elizabeth Tong",
            "Kawin Setsompop"
        ],
        "journal_conference_name": "Magnetic Resonance Materials in Physics, Biology and Medicine",
        "publisher": "Springer International Publishing",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158263",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Sorption-based atmospheric water harvesting: from atoms to applications",
        "abstract": "Thirteen thousand trillion liters of water in the atmosphere is a natural resource found anywhere on the earth, and available to anyone. Sorption-based atmospheric water harvesting (SAWH) is the extraction of water vapor using sorbent materials across a broad spectrum of relative humidity, which opens new avenues to address water scarcity faced by two-thirds of the world’s population. SAWH technologies gained significant attention in 2017 with the development of a solar-powered system utilizing metal-organic framework (MOF) sorbents to extract water from the air. While groundbreaking, this proof-of-concept device produced only a few milliliters of water, far from sufficient to meet even a single person’s daily water needs. A large gap thus remains between laboratory discoveries and real-world applications. This thesis aims to advance the understanding of SAWH technologies from atoms to applications. It begins with a multiscale perspective on SAWH technologies towards real-world applications, addressing knowledge gaps across various length scales. Through this multiscale approach, we developed a framework that can bridge material innovations with device realization. At the molecular scale, the thesis seeks to address a fundamental challenge: the inability to directly observe water sorption processes. To overcome this long-standing challenge, we introduced the use of cryogenic transmission electron microscopy (cryo-TEM) to probe water sorption in nanoporous materials at the single-pore level. This approach allows us to image water sorption and material structures with atomic resolution. Owning to the high resolution and in situ capabilities of cryo-TEM, we resolved a partially water-filled state of MOF crystals and observed that water molecules tend to occupy the centers of pores and fill neighboring pores once adjacent ones are filled. This technique offers new insights into sorption mechanisms and holds significant potential for the development of new sorbent materials. Building on the material-device-bridging framework, we proposed a dual-stage device architecture inspired by multistage distillation in desalination, where condensation heat from one stage drives desorption in the next, increasing productivity and thermal efficiency. To guide materials selection based on operating conditions, a universal thermodynamic model is developed to predict the efficiency of sorbent materials given their sorption isotherms. Additionally, this analysis reveals practical strategies to improve devicelevel sorption kinetics and heat transfer performance, pushing the technology toward thermodynamic limits. At the global scale, the framework enables the optimization of material deployment tailored to diverse climatic conditions. The real-world impact is further demonstrated through a technoeconomic assessment, which illustrates SAWH technology’s competitiveness with bottled and tap water and pathways to further improve its cost-effectiveness. The thesis concludes with an outlook on future opportunities for SAWH technologies and a discussion of their societal and environmental impacts at scale, including their potential role in mitigating climate change.",
        "authors": [
            "Yang Zhong"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158799",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Falling isn't the End: Reimagining Demolition as a Creative Practice",
        "abstract": "This thesis investigates resilience not as an endpoint but as a condition of continuous transformation. It critiques the shortcomings of current architectural discourse in addressing climate disasters, waste, and carbon footprints. While these crises are widely acknowledged, architecture often operates within restrictive economic, legal, and cultural systems, relegating resilient design to the periphery or diminishing its potential impact.\r\nCollapse, traditionally perceived as failure, is reimagined here as a generative moment—an opportunity to rethink materials, systems, and the narratives that shape them. Central to this exploration is the concept of assembly, where materials are designed with deliberate life spans—some transient, others enduring. By anticipating the gaps and shifts that arise when permanence is no longer assumed, this thesis proposes new possibilities for adaptive design and architectural resilience within the evolving rhythms of life.\r\nTo articulate these ideas, the thesis employs speculative scenarios and temporal media. These tools position architecture as a system in flux, evolving in tandem with societal and environmental changes. Through narrative-driven methodologies, this work seeks to expand architectural discourse, prompting reflection on the discipline’s foundational assumptions while connecting it to broader cultural and systemic challenges.\r\nUltimately, this thesis redefines resilience—not as resistance or mere survival but as a dynamic and imaginative practice. It advocates for architecture’s leadership within the broader zeitgeist of sustainability, transforming pressing global challenges into opportunities for creative agency and systemic reinvention.",
        "authors": [
            "So Jung Lee"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158896",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Variational Lower Bound to Mitigate Batch Effect in\r\nMolecular Representations",
        "abstract": "High-throughput drug screening – using cell imaging or gene expression measurements as readouts of drug effect – is a critical tool in biotechnology to assess and understand the relationship between the chemical structure and biological activity of a drug. Since large-scale screens have to be divided into multiple experiments, a key difficulty is dealing with batch effects, which can introduce systematic errors and non-biological associations in the data. We propose InfoCORE, an Information maximization approach for COnfounder REmoval, to effectively deal with batch effects and obtain refined molecular representations. InfoCORE establishes a variational lower bound on the conditional mutual information of the latent representations given a batch identifier. Experiments on drug screening data reveal InfoCORE’s superior performance in a multitude of tasks including molecular property prediction and molecule-phenotype retrieval. Additionally, we show results for how InfoCORE offers a versatile framework and resolves general distribution shifts and issues of data fairness by minimizing correlation with spurious features or removing sensitive attributes.",
        "authors": [
            "Chenyu Wang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158954",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "From Data, to Models, and Back: Making Machine Learning Predictably Reliable",
        "abstract": "Machine learning systems exhibit impressive performance, but we currently lack scalable ways to anticipate their successes, failure modes, and biases. This position limits our ability to deploy these systems in the appropriate contexts, and to build systems which we can confidently deploy in high-risk settings. Motivated by this state of affairs, this thesis aims to develop design principles for predictably reliable machine learning. Our ultimate goal is to enable developers to know when their models will work, anticipate when they will fail, and understand “why” in both cases. In pursuit of this goal, this thesis combines large-scale experiments with theoretical analysis to form a precise understanding of the ML “pipeline,” from training data (and the way we collect it), to learning algorithms, to deployment. Fully realized, such an understanding would allow us to build ML systems the same way we build buildings or airplanes—safely, scalably, and with a robust grasp of the underlying principles. In this thesis, we focus on four design choices within this pipeline: model deployment (Part I), dataset creation (Part II), data collection (Part III), and algorithm selection (Part IV). For each of these design choices, we use targeted experiments to uncover the corresponding principles that actually underlie the behavior of ML systems. We distill these principles into concise conceptual models which allow us to both reason about existing systems and design improved ones. Along the way, we will revisit, challenge, and refine various aspects of conventional wisdom surrounding ML model development.",
        "authors": [
            "Andrew Ilyas"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158964",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Multi-Subject Image Generation",
        "abstract": "Diffusion models excel at text-to-image generation, especially in subject-driven generation for personalized images. However, existing methods are inefficient due to the subject-specific fine-tuning, which is computationally intensive and hampers efficient deployment. Moreover, existing methods struggle with multi-subject generation as they often blend identity among subjects. In this thesis, we present FastComposer which enables efficient, personalized, multi-subject text-to-image generation without fine-tuning. FastComposer uses subject embeddings extracted by an image encoder to augment the generic text conditioning in diffusion models, enabling personalized image generation based on subject images and textual instructions with only forward passes. To address the identity blending problem in the multi-subject generation, FastComposer proposes cross-attention localization supervision during training, enforcing the attention of reference subjects localized to the correct regions in the target images. Naively conditioning on subject embeddings results in subject overfitting. FastComposer proposes delayed subject conditioning in the denoising step to maintain both identity and editability in subject-driven image generation. FastComposer generates images of multiple unseen individuals with different styles, actions, and contexts. It achieves 300–2500 speedup compared to fine-tuning-based methods and requires zero extra storage for new subjects. FastComposer paves the way for efficient, personalized, and high-quality multi-subject image creation.",
        "authors": [
            "Tianwei Yin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158959",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "If These Hills Could Speak",
        "abstract": "If these hills could speak, what would they reveal, and how would they express it? This central question guides this thesis, which examines three hills in the heart of Ibadan, Southwest Nigeria— each occupied by the ruins of colonial monuments. Before the construction of these structures, the hills served as sanctuaries, providing water, food, and safety. However, under British colonial rule, architecture was utilized to disrupt this harmonious relationship. Over the course of 50 years, three monuments were erected that mark Britain’s colonial imprint on the city: a neoclassical courthouse (1925), built to assert control over the central market; a 60-foot tower (1936), which displaced the surrounding forests; and a theater (1977), built during a time of national struggle for unity and identity. Today, at the foot of these hills, a community has forged a way of life within a broken system. By repurposing and subverting structures in ways their creators never intended, this community embodies a praxis and poiesis of adaptive creativity within the built environment. This process represents a transformative act of pidginization—a collective tactic for repair, resistance, and reappropriation in response to an ongoing, imposed socio-political order. For these hills to speak again, the ruins must be transformed. This thesis begins that process by applying acts of pidginization learned from below to the three ruins. It proposes their conversion through deconstruction and de-monumentalization, with the aim of fostering economic development, ecological restoration, and cultural production in the city.",
        "authors": [
            "Tejumola Bayowa"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158841",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Do What You Say—Computing Personal Values Associated with Professions Based on the Words They Use",
        "abstract": "Members of a profession frequently show similar personality characteristics. In this research, we leverage recent advances in NLP to compute personal values using a moral values framework, distinguishing between four different personas that assist in categorizing different professions by personal values: “fatherlanders”—valuing tradition and authority, “nerds”—valuing scientific achievements, “spiritualists”—valuing compassion and non-monetary achievements, and “treehuggers”—valuing sustainability and the environment. We collected 200 YouTube videos and podcasts for each professional category of lawyers, academics, athletes, engineers, creatives, managers, and accountants, converting their audio to text. We also categorize these professions by team player personas into “bees”—collaborative creative team players, “ants”—competitive hard workers, and “leeches”—selfish egoists using pre-trained models. We find distinctive personal value profiles for each of our seven professions computed from the words that members of each profession use.",
        "authors": [
            "Aditya Jha",
            "Peter A. Gloor"
        ],
        "journal_conference_name": "Algorithms",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158295",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "It Is Time to Standardize Principles and Practices for Software Memory Safety",
        "abstract": "In this Inside Risks column, we explore memory-safety standardization, which we argue is an essential step to promoting universal strong memory safety in government and industry, and, in turn, to ensure access to more secure software for all. During the last two decades, a set of research technologies for strong memory safety—memory-safe languages, hardware and software protection, formal approaches, and software compartmentalization—have reached sufficient maturity to see early deployment in security-critical use cases. However, there remains no shared, technology-neutral terminology or framework with which to specify memory-safety requirements. This is needed to enable reliable specification, design, implementation, auditing, and procurement of strongly memory-safe systems. Failure to speak in a common language makes it difficult to understand the possibilities or communicate accurately with each other, limiting perceived benefits and hence actual demand. The lack of such a framework also acts as an impediment to potential future policy interventions, and as an impediment to stating requirements to address observed market failures preventing adoption of these technologies. Standardization would also play a critical role in improving industrial best practice, another key aspect of adoption.",
        "authors": [
            "Robert Watson",
            "John Baldwin",
            "Tony Chen",
            "David Chisnall",
            "Jessica Clarke",
            "Brooks Davis",
            "Nathaniel Filardo",
            "Brett Gutstein",
            "Graeme Jenkinson",
            "Ben Laurie",
            "Alfredo Mazzinghi",
            "Simon Moore",
            "Peter Neumann",
            "Hamed Okhravi",
            "Alex Rebert",
            "Alex Richardson",
            "Peter Sewell",
            "Laurence Tratt",
            "Muralidaran Vijayaraghavan",
            "Hugo Vincent",
            "Konrad Witaszczyk"
        ],
        "journal_conference_name": "Communications of the ACM",
        "publisher": "Association for Computing Machinery",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158237",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Natural Language Foundation Models in Medical Artificial Intelligence",
        "abstract": "Over the past decade, the transformative rise of deep learning, particularly large language models (LLMs), has inspired experts across diverse fields, including healthcare, to think deeply about how artificial intelligence (AI) can revolutionize their fields. In this time, general foundation models, rather than narrow and highly specialized task-specific systems, have begun to emerge as the dominant paradigm. In healthcare, AI systems are already seeing widespread implementation in a variety of real-world use cases, perhaps without adequate evaluation and validation. Indeed, their often impressive ability to process natural language, a crucial medium of knowledge and communication in medicine, suggests that many of these modern foundation models may hold immense promise in the healthcare space. However, there exists a need to better study and understand their strengths, limitations, and robustness, particularly in more realistic and clinically relevant settings.\r\n\r\nThis thesis focuses on two key classes of natural language-driven foundation models --- Contrastive Language Image Pretraining (CLIP) models, and Large Language Models (LLMs) --- and investigates how such models can encode and deliver useful clinical knowledge, for tasks like chest x-ray interpretation, differential diagnosis, history taking, and clinical management. As a whole, this thesis aims to further our collective understanding of the potential of natural language foundation models in medicine, while emphasizing the need for significant further research to address real-world challenges and understand the scopes in which such systems can be implemented safely and efficaciously.\r\n\r\nIn the first chapter, I provide an overview of some relevant background, including contrastive language-image pretrained models, large language models, and their evaluation in the medical domain. \r\n\r\nIn chapter 2, we improve the CLIP architecture for chest x-ray interpretation through a novel regularization technique applied during pre-training, and use this model for the zero-shot identification of chest x-ray findings.\r\n\r\nIn chapter 3, we examine the reliability of CLIP-style models.  First, we evaluate their robustness to shortcut learning to understand the potential protective effects of text self-supervision. Next, we explore how conformal prediction can be used to control zero-shot classification performance and preempt compatible inputs for these CLIP-style models.\r\n\r\nIn chapter 4, I describe the development of Articulate Medical Intelligence Explorer (AMIE), a conversational diagnostic AI fine-tuned with simulated medical dialogue. We evaluate the diagnostic capabilities of AMIE in two randomized studies with primary care physicians; first, in challenging clinicopathological conference (CPC) cases, and then in virtual text-based objective structured clinical examinations (OSCE).\r\n\r\nIn chapter 5, we explore AMIE's management reasoning capabilities in two subspecialty domains: genetic cardiovascular disease and breast oncology. In these studies, we design domain-specific assessments for case management and compare AMIE's performance to generalists under subspecialist evaluation, as well as studying its potential assistive effect.",
        "authors": [
            "Anil Palepu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158802",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Co-Living in Seoul: Addressing Housing Needs and Redefining Rental Market Trends",
        "abstract": "Co-living emerged as a novel asset class in the mid-2010s, addressing the housing needs of urban residents affected by rising housing costs, increasing urban migration, and the growing prevalence of single-person households. In South Korea, co-living has gained attention as a viable alternative to traditional housing, driven by unique local dynamics, including the decline of the dominant Jeonse system and a significant shortage of housing tailored to single-person households. With a growing preference for monthly rental systems over the Jeonse systems, both local conglomerates and start-ups have capitalized on the opportunity to offer company-operated co-living spaces. As the market grows, major international investors and global co-living providers have also entered, reflecting a unique market environment where institutionalized housing options are expanding alongside a notable shift in rental transaction systems. In this new era of urban housing, co-living is rapidly expanding and gaining popularity. This thesis seeks to answer the following question: What factors have driven the emergence and growth of the co-living market in Seoul, and what is its growth potential? To address this, it starts with an analysis of market drivers, provider strategies, and regulatory developments, followed by projections of market potential and an assessment of potential threats and mitigation strategies for long-term viability of co-living in Seoul. The goal is to offer insights for co-living providers to optimize their spaces and services. The findings suggest that while co-living addresses unmet housing demand, its long-term success depends on balancing operational efficiency with tenant satisfaction. While these strategies are applicable in other cities, they are particularly critical in Seoul, where the Jeonse system remains a strong and historically preferred alternative. In Seoul, co-living serves a dual mission: introducing an innovative housing model and reshaping the paradigm of the Wolse rental housing system. To succeed, co-living operators must clearly articulate their unique value proposition, addressing both the housing needs of urban residents and the broader evolution of the rental market.",
        "authors": [
            "Suhyeon Park"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158888",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Defining the Influence of Host Cell Proteostasis Networks and\r\nTemperature on Influenza Evolution",
        "abstract": "Viruses accumulate mutations and evolve more rapidly than any domain of life. Not only does the random acquisition of mutations drive this high evolutionary rate, but constant pressure from the host also contributes. As minimalistic pathogens, viruses rely on host machineries to synthesize, fold, and degrade their proteins. These proteostasis machineries can influence the accessible sequence landscape of viral proteins, and thus shape their evolution. Furthermore, the entire viral replication cycle takes place within the host cell. Therefore, the environment of the host, including factors such as temperature, can influence the evolutionary trajectory of viral proteins. The overarching goal of my thesis work is to better understand the influence of the host cell environment, with a particular focus on the proteostasis networks and the temperature of the cell.\r\nMy first project uses deep mutational scanning to elucidate the roles of the host proteostasis networks in defining influenza hemagglutinin’s evolutionary ability. My second project takes a similar approach to investigate how high or low temperature impact the accessible sequence space of HA. My third project combines both proteostasis network and temperature perturbations to investigate how the host cell environment can impact HA’s ability to escape neutralizing antibodies. My final project leverages the high mutation rate of influenza to study the phenomenon of error catastrophe, and the impact of altered proteostasis network environments on buffering the effect of mutations. Together, these studies clearly define a role for both the host proteostasis networks as well as temperature environment in determining influenza’s accessible sequence space, currently underappreciated factors in predicting how viruses evolve to evade selection pressures and a critical component to consider for successful vaccine and drug development as well as pandemic preparedness.",
        "authors": [
            "Jessica Patrick"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158935",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Fast Multi-query Planning in Graphs of Convex Sets",
        "abstract": "Planning in Graphs of Convex Sets (GCS) is a recently developed optimization framework that seamlessly integrates discrete and continuous decision making. It naturally models and effectively solves a wide range of challenging planning problems in robotics, including collision-free motion planning, skill chaining, and control of hybrid systems. In this thesis, we study the multi-query extension of planning through GCS, motivated by scenarios where robots must operate swiftly within static environments. Our objective is to precompute optimal plans between predefined sets of source and target conditions, in an effort to enable fast online planning and reduce GCS solve times. Our solution consists of two stages. Offline, we use semidefinite programming to compute a coarse lower bound on the problem’s cost-to-go function. Then, online, this lower bound is used to incrementally generate feasible plans by solving short-horizon convex programs. We demonstrate the effectiveness of our approach through a variety of experimental domains: collision-free motion planning for a warehouse robot arm, item sorting for a top-down suction gripper, and footstep planning for a bipedal walker. In particular, in a warehouse-like scenario involving a seven-joint robot arm, our method generates higher-quality paths up to 100 times faster than existing motion planners.",
        "authors": [
            "Savva Morozov"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158967",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Max-Stable Processes,  Measure Transport & Conditional Sampling",
        "abstract": "The modeling of extremes, known as extreme value theory (EVT), aims to understand events characterized by extreme deviations from the mean of a probability distribution. These events are significant in fields such as finance, environmental science, engineering, and insurance. EVT aims to predict the occurrence and impact of these events, which often have severe consequences. Applications of EVT include modeling extreme market movements in finance, natural disasters in environmental sciences, structural reliability in engineering, and catastrophic event risk management in insurance. Conditional sampling and simulation methods, such as normalizing flows and measure transport, are crucial for estimating extremes at un-monitored sites or under specific conditions, thereby improving our understanding and risk management strategies. The goal of this thesis is to make significant contributions to both extreme value theory and measure transport, as well as to establish a link between the two. First, we develop new Markov chain Monte Carlo algorithms for conditional sampling of max-stable processes. Next, we create models that incorporate physical laws, encoded by partial differential equations, to extend max-stable processes into regions without observations. Third, we design specialized transport map frameworks for distributions with bounded support, enabling accurate and efficient sampling and inference. Finally, we use transport maps parameterized by neural networks to learn and condition the distributions of shortest path statistics in polymer systems, accelerating the prediction of microstructural evolution under various conditions.",
        "authors": [
            "Dimitris C. Konomis"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158893",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design Exploration of a Miniaturized Stirling Engine",
        "abstract": "Increased interest in long-term space exploration has increased demand for small yet powerful energy sources, especially for remote and harsh environments where traditional power sources may be impractical. In such scenarios, space probes and high-reliability systems necessitate innovative solutions to meet their growing power and thermal management requirements while maintaining small form factors. Presently, micro power systems fall short of achieving the desired efficiencies for these applications, typically hovering around 2% [1]. Stirling engines, with their proven capability to attain high thermodynamic efficiency (30-40%), offer a promising solution if this efficiency can be maintained in a miniaturized form [2]. This study delves into the design space of a miniaturized Stirling engine with a target input of 2Wth, which could be tailored for small-scale (mesoscale ~cm3 ) high-efficiency power generation or micro-cooling. Previous research has laid the groundwork for understanding the thermodynamics of miniaturized Stirling engines, exposing substantial challenges, including overwhelming parasitic losses at this scale. The current study endeavors to mitigate these losses and explore the path to optimal efficiencies through Simulink modeling. Simulations have demonstrated design spaces capable of producing mechanical efficiencies as high as 14% with a 2Wth input, marking significant progress in addressing the limitations of current micro power systems. The research's innovative approach has significant implications for enabling the power generation required for small space probes, particularly for long durations and need self-sustaining power over extended periods [3], [4]. As the study advances, it holds the promise of developing a physical prototype using the findings from the design space study, which helps push the field forward for future power generation and micro-cooling in small-scale space technology. This thesis aims to map the design space of a miniaturized Stirling engine focusing on mitigating parasitic losses to achieve markedly greater efficiency compared to existing technologies.",
        "authors": [
            "Ryann Hee"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158848",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Systems-Theoretic Approach to Design of Early Concepts for Novel, Complex Systems in Aerospace",
        "abstract": "The complexity of engineered systems has grown exponentially over the last forty years. One of the main challenges in modern engineering is managing this complexity, particularly as the pace of technological change continues to accelerate across industries. Traditional approaches to generating early concepts for novel, aerospace control-oriented systems typically employ a design-first approach, ignoring critical steps required to truly understand the intent and context of a new system. This tendency also leads to a focus on low-level, highly granular design activities that seek to integrate advanced technologies together for technology’s sake. Unfortunately, today’s applied early concept generation methods do not facilitate the effective generation of early system concepts and an initial high-level design for aerospace control systems. To address these shortcomings, this thesis proposes a systematic and rigorous framework to generate early system concepts using Systems-Theoretic Accident Model and Processes (STAMP) principles and a new lens to examine system intent for a novel, complex system. This work also introduces a new level of abstraction for a portfolio-of-systems context and a method to propose an initial design artifact for new systems that is both architecture-agnostic and relevant during the earliest system engineering lifecycle activities. This method, Systems-Theoretic Concept Design, uses a top-down, three-phased approach to conduct mission analysis and determine the intent for a new system within a specific portfolio-level context. Upon building this intent model, the method enables the synchronization of stakeholder mental models through the use of transformation models built using the principles of Systems Theory. Finally, in its last phase, this early design concept generation framework delivers an initial design artifact that is technology- and requirements-agnostic in the language of Systems Theory using the semantics of STAMP. This initial design artifact is in the form of the Portfolio-of-Systems control structure, a control structure that frames a portfolio’s desired high-level capability as a control problem at a new level of abstraction while enabling analysis and examination of complex interactions across systems that may operate asynchronously or in geographically separated operating environments.",
        "authors": [
            "Alexander P. Hillman"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158885",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Green Herrings in a Yellow Room: A Counter Production of The Yellow Wallpaper",
        "abstract": "Charlotte Perkins Gilman’s The Yellow Wallpaper is a designer’s work of critical fabulation. Published in 1892, the short story follows an unnamed woman prescribed a “rest cure” by her husband, John. Confined to a room wrapped in gothic yellow wallpaper, the narrator becomes obsessed with its patterns. As her mind deteriorates, she sees a woman trapped behind the paper. This production reimagines Charlotte’s bedroom as not yellow, but green—a rich, vibrant green laced with the medium responsible for its provocative coloration: arsenic. The toxic pigment, invented in the late 18th century, induces bodily ailments, mental instability, and even death when used in textiles. Interiors threatened tenants with toxins as this green spread through 19th-century Europe before reaching New England and our narrator. Though known as an author and suffragette, Charlotte was first a designer. As a student in the inaugural class of the Rhode Island School of Design, she studied the arts just miles from the ports where the green pigment began its early residence. Her writing draws from arsenic publications, her scenes mimic medical case studies, and archives suggest she was aware of these toxic walls. This theatrical table reading positions the authoring of The Yellow Wallpaper within the simultaneous stories of the arsenic wallpaper. Why does the author mimic material traces of the green while redirecting her readers to the yellow? When does the color transition from literal to abstract? This work recontextualizes the foundational feminist text by unfabulating the story through design—questioning Charlotte’s literary misdirections and the public discourse surrounding the toxic color.",
        "authors": [
            "Leanah Sloan Aulgur"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158824",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Contact-aware and multi-modal robotic manipulation",
        "abstract": "Intelligent robotic manipulation has advanced significantly in recent years, driven by progress in foundational cognitive models, sensor-fusion techniques, and improvements in actuators and sensors. However, most contemporary robotic systems still lack the ability to effectively recognize and understand contact dynamics, which are critical for performing manipulation tasks beyond basic pick-and-place operations. This thesis argues and proves that contact awareness is essential for the successful deployment of robotic systems, not only in structured environments such as factories but also in unstructured settings like domestic households. Achieving contact awareness necessitates advancements in three key areas: the development of improved contact-sensing hardware, the creation of more expressive frameworks for representing and interpreting contact information, and the design of efficient modality-fusion algorithms to integrate these capabilities into robotic action planning. This work addresses these challenges by (1) proposing novel mechanical designs that enable touch sensors to adopt more compact and versatile forms while enhancing their durability and manufacturability, (2) introducing a foundational representation learning framework capable of learning a shared tactile latent representation, which can be transferred across different sensors and downstream tasks, and (3) developing a compositional diffusion-based approach for action prediction that integrates tactile sensing signals with other perception modalities, thereby enabling learning across diverse environments and promoting policy reuse. Along the way, this thesis demonstrates that tactile sensors can be both compact and versatile, challenging common perceptions to the contrary. It also establishes that tactile sensing is indispensable not only for high-precision tasks, such as electronics assembly, but also for everyday activities, including cooking and tool usage.",
        "authors": [
            "Jialiang Zhao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158785",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Organizational Forms and Practices: Essays on Implications for Frontline Workers and Performance",
        "abstract": "In three essays, this dissertation explores how organizational forms and workforce practices shape frontline work experiences and organizational performance. Using both quantitative and qualitative methods, I explore how frontline workers experience work and what factors shape their performance. In the first essay, I examine how workforce practices in nursing homes relate to organizational performance. Specifically, I evaluate performance on resident health outcomes for both pre-pandemic and COVID-19 conditions. Combining Federal and state administrative data sets with non-public data on early COVID-19 spread and mortality, I investigate the degree to which the organization of work for frontline workers predicted resident health as a measure of organizational performance for nursing homes. In a period of global stress on health and care systems, I seek to understand to what extent prepandemic predictors of performance remained important. When nurses spent more time with residents, residents experienced better care both before and during the pandemic. Yet contrary to expectation, the role of clinical outsourcing became more relevant during the pandemic, potentially reflecting greater workforce flexibility or targeted COVID-19 workforce support to facilities that outsourced nursing activities before the pandemic. These results depict how environmental changes and alternative performance measures call into question established relationships in the high-performance work systems literature. In the second essay, I use in-depth interviews and field observations to uncover the process of constructing ownership culture in an employee-owned firm. I demonstrate how workers co-create their own control system, supported by a high financial value of ownership, strategic managerial communication, peer pressure, and performance management. This critical case challenges the dominant view in the employee-ownership literature that success requires formal worker participation in decision-making. Further, it investigates the “black box” of culture-building in an employee-owned firm. The third essay builds on this understanding by evaluating the stated motives of individual worker-owners in a home care cooperative. The cooperative developed as a pilot initiative with non-profit partners to develop a home care organization that would provide quality jobs and quality care, while integrating immigrant workers. I traced the workers’ justifications for joining and participating in these cooperatives. Rather than aligning with expected motives from previous studies or with Worker Center motives, I find that these workers adapted motives to reflect their realities, such as multiple jobs and a lack of labor rights in practice. This analysis emphasizes the decoupling of workers’ experiences from stated organizational goals, emphasizing the importance of collecting workers’ perspectives. Taken together, these three essays contribute insights into how frontline workers shape organizational performance by interpreting organizational context, culture, and structure. Results indicate that organizational performance is not merely a function of workplace practices, but rather, directly influenced by frontline workers based on their individual motives and roles in workplace culture. These findings imply that by directly engaging with frontline workers’ motives, organizational leaders and policymakers can design organizations that improve work and performance.",
        "authors": [
            "Karen MacKenzie Scott"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158850",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Leveraging Information Sharing for Satellite Navigation and Coordination",
        "abstract": "As the number of objects in orbit grows, so does the risk of collisions. The sheer volume of collision warning messages far exceeds the capacity of human analysts, placing a significant burden on satellite operators and underscoring the need for autonomous, decentralized traffic management. Unlike centralized conjunction analysis, decentralized space traffic management distributes coordination across multiple independent nodes, allowing satellites to collaborate directly. This approach could enhance the resilience, speed, and international cooperation of space operations, helping to manage the space environment. For decentralized space traffic management to be viable, satellites must possess an accurate understanding of both the locations and intentions of other satellites. While satellites have precise knowledge of their own state, this accuracy diminishes when predicting the state of others. This gap is due to the limitations of onboard measurement systems and knowledge of each satellite’s structure, configuration, and maneuverability. Such differences motivate the exploration of information sharing between operators to improve coordination. Sharing information could benefit both individual operators and the broader space community by enabling more accurate trajectory predictions, facilitating formal maneuver negotiations, and enhancing overall orbital safety and efficiency. The main contribution of this thesis is to develop methods for autonomous satellite decision-making. By advancing the state of satellite autonomy, we can enhance high-level decision-making processes, enabling more adaptive and intelligent satellite coordination. This thesis begins by developing a multi-agent reinforcement learning environment to simulate satellite interactions in complex, high-dimensional settings. Then, we relax the assumption on synchronous communications and explore an alternate learning framework that relies on asynchronous communication between satellites. Our final contribution lies in a game-theoretic model of operator behavior in non-cooperative settings. Space is a competitive environment, and willingness to collaborate is mixed. As a result, we use game theory to obtain strategies to determine maneuvering and timing.",
        "authors": [
            "Sydney Dolan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158894",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Chemical Sensing of N-Nitrosodimethylamine and Methane",
        "abstract": "In Chapter 1, an introduction to chemical sensing is presented. Several modalities are introduced, including optical, gravimetric as well as chemiresistive together with brief in-troductory backgrounds. Subsequently, metrics to assess sensor performance are sum-marized. Finally, some strategies to combat interferants during chemical sensing are dis-cussed.\r\n\r\nIn Chapter 2, published work on a luminescent method to determine levels of N-nitrosamines is presented. This work involved the synthesis of five phosphines bearing N-heterocycles, followed by coordination with Cu(I) to give luminescent complexes. Emission spectra spanned the visible range, demonstrating the tuneability of these compounds. The complexes’ interactions with N-nitrosamines were also examined through spectroscopy and crystallography.\r\n\r\nIn Chapter 3, development of free-volume promoting monomers and catalysts for in-sertion polymerization is demonstrated. Insertion polymerized material was compared to that synthesized using Ring Opening Metathesis Polymerization (ROMP), showing that the former had superior properties for methane detection through higher surface areas and po-rosity.\r\n\r\nIn Chapter 4, the structure activity relationship of components within a previously pub-lished methane sensing assembly was thoroughly examined to identify how changes in humidity levels influenced sensing response. Poly-4-vinylpyridine modification was per-formed under flow conditions, while the chemical composition of the polyoxometalate (POM) component was also varied. Humidity was determined to most significantly affect the POM and influence the electrical contact between carbon nanotubes and gold.\r\n\r\nFinally, Chapter 5 presents several modifications of the parent porous framework out-lined reported in Chapter 3. A soluble monomer bearing adamantyl substituents was suc-cessfully synthesized by attachment of isopropyl units. Its propensity to participate in inser-tion polymerization was then examined. Sulfonation and nitration of the parent polymer I-AntN were also conducted and the product analyzed. Attempts at copolymerization of AntN with CO were also described.",
        "authors": [
            "Haosheng Feng"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158926",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Explicit formulas for weighted orbital integrals for the inhomogeneous and semi-Lie arithmetic fundamental lemmas conjectured for the full spherical Hecke algebra",
        "abstract": "As an analog to the Jacquet-Rallis fundamental lemma that appears in the relative trace formula approach to the Gan-Gross-Prasad conjectures, the arithmetic fundamental lemma was proposed by Wei Zhang and used in an approach to the arithmetic Gan-Gross-Prasad conjectures. The Jacquet-Rallis fundamental lemma was recently generalized by Spencer Leslie to a statement holding for the full spherical Hecke algebra. In the same spirit, there is a recent conjectural generalization of the arithmetic fundamental lemma to the full spherical Hecke algebra. This paper formulates another analogous conjecture for the semi-Lie version of the arithmetic fundamental lemma proposed by Yifeng Liu. Then this paper produces explicit formulas for particular cases of the weighted orbital integrals in the two conjectures mentioned above.",
        "authors": [
            "Evan Chen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158792",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Data Interpretation and Management for an Atmospheric Probe Mission to Venus",
        "abstract": "After nearly 40 years without a dedicated U.S. mission to Venus, the Rocket Lab Mission to Venus is planning to launch a small probe to analyze the composition of Venus’ cloud layers. As the probe descends through the atmosphere, it will spend around five minutes in the cloud deck, from 66 km to 48 km above the surface, and roughly 20 minutes total in the atmosphere [French et al., 2022]. The probe’s primary scientific instrument, the Autofluorescence Nephelometer (AFN), will gather data by measuring the light scattering off particles, providing insight into their chemical composition based on refractive index and particle size [Baumgardner et al., 2022]. Unfortunately, the natural phenomena described by Mie scattering [Mie, 1908], the physics theory underpinning the AFN, holds that light scattering for a small solid angle is fundamentally degenerate: different combinations of refractive index and particle size can lead to identical light scattering. This degeneracy limits scientists’ ability to uniquely determine physical parameters of interest, leading some previous authors to rely upon helpful, but perhaps limiting, assumptions that mitigate this degeneracy. Complicating matters still further, the probe’s communication with Earth is subject to a strict data budget, limiting the amount of AFN measurements that may be used for analysis to begin with. This thesis addresses two important problems associated with the Rocket Lab Mission to Venus: 1) how to mitigate the light scattering degeneracy with minimal assumptions and 2) how to transmit valuable information within the limited data budget. To address the first problem, I introduce a data retrieval algorithm, based upon Bayesian statistical inference [Lindley, 1965], which combines a physical model of the instrument and a prior probability distribution describing each physical property. In some cases, this method can estimate the correct particle size and refractive index of a particle as the maximum likelihood value, from a single measurement even as it relaxes certain assumptions that were previously standard in the field, such as a small refractive index range. Using my data retrieval algorithm, I reanalyze the data collected by the Pioneer MultiProbe Mission to Venus’ nephelometers without the need for supplementary data from a different instrument [Ragent and Blamont, 1980]. I also provide new insight into the particle size and refractive index distributions seen by the Pioneer Mission’s small probes, which had not been possible with previous techniques. To address the second problem, I propose a data strategy for limited data missions like the Rocket Lab Mission to Venus. The method developed in this work relies upon Gaussian Mixture Models, which can efficiently represent multiple measurements as",
        "authors": [
            "Maria Regina Apodaca Moreno"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158902",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Empowering Place: Unlocking Value for Investors by Integrating\r\nIndigenous Values in Luxury Hospitality",
        "abstract": "The luxury hospitality industry has long been attuned to shifting consumer preferences, particularly as travelers increasingly seek unique, meaningful experiences. In today’s global market, trends centered on personalization, wellness, authenticity, and regeneration—further accelerated in the post-pandemic travel era—present both challenges and opportunities for real estate investors. This shift raises a critical question: How and where can value be unlocked in this evolving landscape?\r\n\r\nThis thesis explores how real estate investors can maximize value creation in the luxury hospitality sector by leveraging traditional performance metrics alongside a complementary\r\nframework designed to uncover underexplored opportunities and enhance collaboration among stakeholder groups. Through the analysis of two case studies—Salterra Resort & Spa in South\r\nCaicos, Turks & Caicos Islands, British West Indies, and Puntacana Resort and Club in the Dominican Republic—the study demonstrates the practical application of this framework in\r\ntropical, coastal, and island regions, where the interaction between tourism, local communities, and fragile ecosystems is particularly pronounced. By showcasing its success, this research provides adaptable stakeholder rubrics and qualitative system dynamics causal loop diagrams as templates, while broadening the scope for innovation and inspiring further exploration of sustainable, value-driven approaches in luxury hospitality.",
        "authors": [
            "Nadra Alia Peragallo"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158878",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Two's More Fun than One: How the Presence of Multiple Nutrients Changes Microbial Competition and Foraging in Unexpected Ways",
        "abstract": "Microbes exist in incredibly diverse environments with many possible resources (i.e. nutrients) to compete and forage for. To make this complex system tractable, ecologists often study microbes in the presence of a single resource in order to predict and explain what happens with multiple resources. But what gets lost when we do this? Are there phenomena that only emerge in the presence of multiple resources? Here, I explore the ecological implications of three phenomena that each require the presence of at least two resources. First, I show that the diauxic lags that occur when a microbe needs to switch between resources after one is depleted can allow ‘fast-switcher’ microbes to coexist with competitors that exclude them in single-resource environments. Then, I derive a rich temporal niche structure that arises from variations in the order in which resources are depleted in ecosystems with a pulsed resource supply and show that these temporal niches reshape community structure, vastly increasing the expected diversity of microbial ecosystems. Finally, I present a novel differential strategy in which a microbe attempting to intercept a moving source of multiple resources can treat one resource as an attractant and the other as a repellent to significantly increase its chances of successfully intercepting the source as compared to just being attracted to the resources released by the source. Each of these phenomena fundamentally requires the presence of at least two resources and reshapes microbial behavior and ecology. Thus, they collectively highlight the need to carefully consider how characterizations from single-resource environments actually combine to determine what happens in multi-resource environments and what new dynamics must be accounting for in such a bottom-up approach. I conclude with an argument that the case of two resources may be particularly relevant to study due to how much complexity can emerge at just the first step up from one resource to two.",
        "authors": [
            "Blox Willow Bloxham"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158877",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Robot Fleet Learning From Heterogeneous Data",
        "abstract": "One of the key roadblocks for training generalist robotic models today is heterogeneity. Previous robot learning methods often collect data to train with one specific embodiment for one task, which is expensive and prone to overfitting. Similar to humans, robots and embodied agents inherently have to deal with heterogeneous inputs and outputs due to the nature of the perception-action loops across diverse environments. The data format and distributions collected from these systems and used for training them are varied in different modalities such as color, depth, tactile, and proprioceptive information, and/or collected in different domains such as simulation, real robots, and human videos. Moreover, fleets of robots and machines ingest massive amounts of streaming data generated by interacting with their environments in a distributed fashion, and teams of robots shall co-acquire diverse skills through their experiences in varied settings. The core idea behind my research, fleet learning, is to embrace the heterogeneous nature of robot learning to develop efficient and general algorithms. In this thesis, I will present a few angles toward tackling such challenging problems and application domains, ranging from tokenizing data, aligning representations, and merging policies, to composing skills. We develop insights and theories, often from linear settings, for how fleet learning can lead to more principled and effective use of robotic data and propose algorithmic progress, often through alignments, toward building generalist robotic foundation models. Empirically, we show advanced robotic manipulation capabilities by leveraging data from multimodal sensory inputs and multiple domains. In addition to outperforming several previous state-of-the-art across simulation and real-world benchmarks, we develop intelligent systems for robotic applications such as package handling in warehouses as well as dexterous tool-use tasks that have applications such as manufacturing, logistics, and household robots.",
        "authors": [
            "Lirui Wang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158917",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Decadal to centennial-scale climate interactions across the Indo-Pacific region",
        "abstract": "An improved understanding of decadal to centennial-scale climate variability is critical for properly attributing recently observed low-frequency changes to internal climate oscillations and/or anthropogenic forcings as well as improving predictability of decadal variability. This thesis investigates ocean and atmospheric circulation changes and associated impacts within the tropical Indo-Pacific, where low-frequency changes in heat and freshwater impact the livelihoods of billions of people. Because the instrumental record is too short to investigate centennial variability, this thesis leverages numerical simulations and records from paleoclimate archives to provide insights into low-frequency tropical dynamics. In Chapter 2, we explore the dynamics that drive Indonesian Throughflow surface transport variability using a series of forced global high-resolution ocean simulations. We show that surface wind changes associated with Pacific decadal variability drive changes in the western boundary currents that modulate the Indonesian throughflow, consistent with mechanisms identified on interannual timescales. This work identifies a relationship between atmospheric circulation and transport through a key low-latitude passageway. Motivated by paleoclimate evidence of multi-year droughts in Southeast Asia, we investigate their potential drivers in Chapter 3 using an ensemble of coupled climate model simulations. These simulations illustrate that Indo-Pacific internal variability dominated Southeast Asian rainfall extremes during the last millennium, although the influence of volcanic eruptions was detectable. We found that multi-year pluvials were contributed by both Pacific and Indian Ocean modes, while droughts were largely only driven by Pacific Ocean impacts. Our analysis not only quantifies the role of internal and external drivers to Southeast Asia rainfall but also presents a probabilistic analysis framework that may be useful for water resources prediction. Lastly, in Chapter 4 we reconstruct the Indian and Pacific Walker circulations and the Indian Ocean Basin Mode by synthesizing tropical records (corals, tree-rings, and speleothems) of past ocean and atmospheric conditions to investigate basin interactions over the past four centuries. Our results demonstrate that IndoPacific climate was generally coupled on decadal-centennial timescales throughout the past four centuries but was notably decoupled in the early 19th century. Using climate models, we attribute this decoupling to a series of strong volcanic eruptions. Dynamically, we link this inter-basin decoupling to volcanically induced changes in hemispheric temperature gradients, which modulate the teleconnections across the Indo-Pacific. These past disruptions in basin interactions provide context for ongoing and simulated future decoupling under a high emission scenario, as global warming also alters interhemispheric temperature gradients. This thesis sheds light on the complex dynamics that drive ocean-atmosphere variability across the Indo-Pacific on decadal to centennial timescales.",
        "authors": [
            "Shouyi Wang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158787",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Examining the Economic Impact of Anti-Warehouse Development Policies in California: A Case Study of the San Diego Market",
        "abstract": "This thesis conducts a detailed examination of the implications of anti-warehouse development policies in San Diego, focusing on their impacts on key economic indicators from 2024 to 2034. The research provides an overview of the U.S. industrial market, addressing crucial topics such as logistics market size, job creation, and the growth of e-commerce, while also exploring the NIMBY phenomenon and its influence on community opposition to developments, including a discussion of Bill 98 and its legislative implications. A specific focus on the industrial market in Southern California reveals important insights into job growth, rental rates, and market dynamics in San Diego. Through a comprehensive analytical approach, the study addresses the effects of development policies by presenting ten distinct scenarios that project delivery volumes, uncovering potential reductions ranging from 10% to 90% compared to a baseline scenario without restrictions. The analysis anticipates vacancy rates and job losses across various years, utilizing the LINEST function for forecasting key market indicators, including asking rents and asset valuations. Additionally, the research highlights the critical importance of logistics categories and decarbonization strategies to meet net-zero goals, as well as contemporary warehouse design trends and transportation innovations. The conclusions drawn from this research emphasize the complexities of balancing community interests with economic growth and sustainability in the region, as well as the broader economic implications of restrictive development policies on San Diego's warehouse industry, which could adversely affect the economic vitality of the warehouse sector.",
        "authors": [
            "Peggy Ghasemlou"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158829",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Computational-Experimental Process Development for Laser Powder Bed Fusion Additive Manufacturing",
        "abstract": "Laser powder bed fusion (LPBF) additive manufacturing (AM) is instrumental for advances in high-value industries such as aerospace and medical devices. However, widespread adoption is still held back, in part due to challenges with powder handling, identification of process parameters, part qualification and quality control, and low build rates that lead to high part costs. This thesis presents workflows, tools, and understanding for practitioners and researchers seeking to address these challenges, in particular (i) powder spreading, (ii) parameter selection, and (iii) build rate improvement. Cohesive powders (D50 ≤ 20 𝜇𝑚) are challenging to spread and therefore not commonly used in LPBF, but promise more stable melting conditions during laser melting and potentially allow for finer geometrical resolution. Various spreading strategies are explored using an integrated discrete element-finite element (DEM-FEM) framework and a schematic process window for counter-rotational roller spreading is proposed. A new strategy of spreading with a transversely oscillating tool is chosen for experimental implementation and validated using a custom-built mechanized powder spreading testbed. Powder layers are analyzed using X-ray transmission imaging and layer quality is statistically correlated to kinematic spreading parameters. A methodology for performing melt track experiments using high-precision metal templates as well as a machine learning-based automated image analysis tool is presented and applied to melt track scaling studies. Based on single track parameter studies with layer thicknesses and laser spot sizes of up to 600 𝜇𝑚, a dimensionless LPBF process window using the normalized enthalpy Δ𝐻 / ℎₛ as well as the Fourier number is developed. A workflow for rapid LPBF build parameter selection is proposed, that is shown to fabricate near-full dense parts (up to 99.99 %) on the first attempt. Build rate scaling analysis reveals the trade-off between laser spot size and laser scan speed given laser power limitations. Further, LPBF with a standard powder (15 − 45 𝜇𝑚) is compared to a fine powder (0 − 25 𝜇𝑚) under similar processing conditions. The fine powder exhibits superior melt track stability and continuity, as well as significantly increased melt track cross-sectional area, allowing build rate to be increased by almost 20 %. Finally, to enable better understanding of the underlying thermo-fluid dynamics of the melt pool, an approach for computational model parameter estimation using Bayesian inference is presented and applied to the important model parameter of laser absorptivity. This is within the context of a Smooth Particle Hydrodynamics (SPH) computational melt pool model, developed collaboratively by researchers at the Technical University of Munich. The diffuse interface approach employed in SPH is validated using a discretization refinement study, showing the sensitivity of physical phenomena characteristic for LPBF, such as the vapor-induced recoil pressure, to computational hyper-parameters. These combined contributions enhance both practical implementation and theoretical understanding of LPBF, ultimately advancing the field of additive manufacturing towards more cost-effective and higher quality LPBF processes.",
        "authors": [
            "Reimar Weißbach"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158884",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Remembering Energic Connectivities: Appropriate Technology and Domestic Infrastructure in the Energy Crisis",
        "abstract": "The electric grid is a large, complex machine. And yet, it represents but one, narrow framework for energic relations. Visions for just and sustainable futures – for social and ecological repair – should wander further afield. One place they could go is home. In this essay, the Appropriate Technology Small Grants Program, an oft-forgotten chapter of U.S. energy history, shows us how small-scale, place-based inventors transformed homes and neighborhoods into converters and conductors of nearby flows and potentials. At the height of the energy crisis of the 1970s, these inventors pursued a distributed solution to shortage. Along the way, they re-wired the material and conceptual strictures of the modern dwelling and broke into a vast reserve of lowcost, renewable power. Home, they showed, was a workshop to understand and design energic connectivities. But tracing the effects of home-based appropriate technology leads us somewhere else – to the frontiers of energy extraction, where social justice activists proved that small-scale, place-based energy systems could replace unjust mines and dams. What emerged, then, through renewed attention to the possibilities for home and energy, was a powerful counter to the logics of sacrifice at both ends of the energy continuum. Today, as we chart our own response to crisis, it helps to remember how others tried to create solidarities and resist tradeoffs with small-scale, place-based infrastructures. We can, I think, do more with energy.",
        "authors": [
            "Turner Day Adornetto"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158811",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Congestion Control for DNN training clusters",
        "abstract": "The modern DNN workloads generate network traffic having striking differences with the conventional data-center traffic. DNN training jobs generate periodic traffic pattern where all subsequent flows depend on the completion of the currently running flow. Although this periodic behavior calls for a new non-conventional congestion control protocol for DNN training clusters, it also creates an unprecedented opportunity to approximate optimal schedule for DNN jobs in a distributed manner without requiring priority queues, centralized information, or switch hardware support. Prior work on MLTCP proposed updates to existing congestion control algorithms to make them capable of minimizing network congestion when DNN jobs compete for the network. In this thesis, we propose several techniques to expand the scope of prior work to support DNN jobs with more complex communication patterns or parallelization strategies, and further improve the performance speedup over TCP. With two straightforward ideas of updating the congestion control parameters, we expand the performance benefits of MLTCP to a wider set of periodic DNN jobs. Augmenting existing congestion control algorithms with MLTCP provides an effective guiding mechanism to a random search to find the optimal interleaved schedule for competing DNN jobs. Our contributions boost this guided search to improve performance further. We provide detailed theoretical analysis and extensive flow-level simulations to take a deep dive into the convergence, performance speedup, and fairness of MLTCP with the proposed changes.",
        "authors": [
            "Sanjoli Narang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158950",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Acoustic scattering of spherical directional waves by smooth and statistically rough solid elastic cylinders",
        "abstract": "Realistic sonars radiate spherically spreading waves and have directivity. Therefore, they insonify a target over a finite number of Fresnel zones and span a continuum of oblique incident angles, even when the center of the beam is at normal incidence. These effects strongly influence both the overall scattered pressure levels and resonances. For example, because of the spreading of the beam and associated oblique insonification within the beam, normal modes associated with axially propagating guided waves are excited that would not have otherwise existed for an idealized incident plane wave. This thesis analyzes acoustic scattering by solid elastic cylinders insonified by realistic sonars both theoretically and experimentally. A theoretical model to predict scattering by arbitrary-length cylinders is derived based on the apparent volume flow accounting for the above-mentioned practical sonar properties, namely, spherical spreading and directionality. The formulation is first bench-marked against the formally exact T-matrix solution and tested against previously published laboratory data for finite cylinders. It is found that the formulation outperforms the T-matrix solution in predicting laboratory observations at near-normal incidence. Laboratory experiments are then conducted on arbitrary length smooth cylinders insonified by a directional sonar, with a small number of Fresnel zone excited, to evaluate the theory for monostatic as well as bistatic geometries. The formulation is found to outperform the classical scattering models in predicting the new measurements. For example, resonances associated with axially propagating guided waves excited at broadside incidence observed in the experiments are predicted by the proposed formulation but not by the classical models. The measurements are found to agree well with predictions in terms of overall scattering levels and resonance locations. In addition to testing the predictions, the bistatic laboratory observations presented herein substantiate the significant effects on scattering due to the properties of the incident field from practical sonars. The comparison between theoretical and experimental results is then extended for the more complex case involving statistically rough elastic cylinders with one-dimensional Gaussian roughness. The roughness is found to have a considerable impact on all aspects of scattering—overall levels as well as locations and shapes of resonances. General agreement is found between the theoretically predicted and measured ensemble averaged scattered pressure. Both the theory and data reveal two main observations in the ensemble-averaged scattered field: overall scattered pressure levels are seen to decrease, and resonance effects are diminished compared to the corresponding case of smooth cylinders. Effect of various statistical properties of the rough cylinder, namely, different root mean square (RMS) roughness for fixed correlation length and different correlation lengths for fixed RMS roughness on the scattered field are investigated. Finally, the fluctuations of the scattered field are analyzed using the derived formulation.",
        "authors": [
            "Miad Al Mursaline"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158835",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Commodifying and Consuming Endocrine Drugs in Republican China (1920s–1940s)",
        "abstract": "Since the introduction of hormone pharmaceuticals into China during the early twentieth century, these substances became objects of fascination for a growing urban elite class. Drawing from newspapers, medical journals, and advertisements, this article examines the unique trajectories of hormone medicine in China. In conversation with previous scholarship on the dynamics of advertising and consuming hormones in China, this article examines specifically the discourses around the production and science of hormones. The circulation of hormones was informed by ideas of traditional Chinese medical cosmologies and enrolled in a nationalist movement encouraging the consumption of hormones produced by emerging Chinese medical entrepreneurs. This article provides a case study in a postcolonial context that problematizes historiographies depicting a linear transition of global hormone science from backwards to scientific, from traditional to modern.",
        "authors": [
            "Thelma Yuanzhi Wang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158808",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Strange Attitudes on Top",
        "abstract": "This dissertation investigates how attitude verbs of belief and desire engage with embedded material of a similar nature. Chapter 1 looks at the (cross-linguistically unusual) Slovenian existential doxastic attitude verb dopuščati (‘allow for the possibility’) and the embedding of epistemic modal verbs under it. Chapter 2 looks at the (overall puzzling) want and its Slovenian counterpart hoteti, and at their behaviour with respect to embedded doxastic attitudes, epistemic adverbs, and epistemic adjectives. Chapter 3 looks at the (cross-linguistically unusual) Koryak variable-force variable-flavour attitude verb ivək (‘think’, ‘allow for the possibility’, ‘say’, ‘suggest’) and at how its apparent bouletic flavour (‘wish’, ‘hope’, ‘fear’) is derived with the help of covert desiderative components inside the embedded clause. Attitude verbs have the standard role as quantifiers over possible worlds (Hintikka 1962), parameters of evaluation are assumed to contain a set of worlds called the information state (Yalcin 2007; a.o.), which the attitude verb modifies and passes to the embedded clause, while the epistemic modal base is taken to be ‘local’, forming a subset of the information state (Mandelkern 2017, 2019a). Some of the overarching theoretical contributions are the introduction of a new parameter of evaluation (‘selected state’), which is crucial in modelling embedding under non-universal attitude verbs, and a refined view of epistemic modality. Subjective epistemic modality is proposed to involve a second constraint on the shape of the modal base, whose effect is to strengthen embedded necessity claims and help derive the infelicities observed in chapters 1 and 2. We also address the connection between beliefs and desires in the context of various desire interpretations (wants in chapter 2, hopes and wishes in chapter 3).",
        "authors": [
            "Maša Močnik"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158857",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Tackling Algorithmic Problems on Massive Graphs",
        "abstract": "As datasets grow increasingly larger, traditional computational models, which require reading the entire input, become impractical due to constraints on time, memory, and randomness. This thesis explores alternative algorithmic approaches for processing massive graphs under these constraints. Specifically, we focus on algorithms for the following graph problems. Motif Counting and Sampling: This involves developing efficient algorithms for counting and sampling small motifs (constant sized subgraphs) like stars and triangles, which are crucial for applications in biology, chemistry, and social networks. The thesis presents improved methods for both approximate and exact counting and sampling of general motifs. Graph Sparsification and Spanners: The problem of sparsifying graphs involves removing (usually most) edges of the input graph in a way that preserves essential properties such as connectivity and approximate distances. This thesis introduces algorithms for constructing sparse spanning graphs, as well spanners – sparse subgraphs that approximate distances up to a multiplicative factor. We obtain faster algorithms in parallel settings, and also initiate the study of average case graph inputs in the sublinear setting, and obtain results beyond the worst case lower bounds We investigate both of these problems in different models, including sublinear query access, local computation algorithms (LCAs), and the MPC model, and also discuss implications of these in distributed and parallel models of computation.",
        "authors": [
            "Amartya Shankha Biswas"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158948",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "An Identity-Oriented Systems Engineering Framework for Complex Sociotechnical Systems: A case study of Zero Robotics",
        "abstract": "Historical and ongoing discrimination of certain identity groups such as by racial, gender, social class, and other differences leads to persistent inequalities in various fields of society including socioeconomic, health system, political powers, education opportunities, etc. Technology however often entrenches or sustains the hierarchies and further strengthens these social inequalities. While there are many frameworks for studying complex systems, a framework with a focus on advancing social justice and an integration of technological and social considerations is missing. This work introduces the Intersectional Antiracist Technology Framework as a new tool and applies it to an existing complex system of Zero Robotics in STEM education. STEM education, with increasing importance in the modern world’s competitions, is one of the most popular methods to cultivate students’ interests and capabilities in solving complex problems. However, the disparities in access to quality STEM learning opportunities and inclusion in STEM activities remain significant challenges in promoting social equality. This work builds upon the systems engineering tools and uses the innovative Intersectional Antiracist Technology Framework to describe, explain, and evaluate an existing complex system of Zero Robotics. Zero Robotics is an education outreach program that is designed as an early intervention to enroll students in aerospace and related fields. The program aims to serve students across the pipeline and provide them with learning opportunities through interactions with a space robot. It is a perfect example of a complex sociotechnical system that has technological and social factors. Through the case study of Zero Robotics, data are collected through interviews, surveys, participant observation, and available documents. Qualitative program outcomes are assessed from student surveys before and after the Zero Robotics competition. This work is the first attempt to apply the Intersectional Antiracist Technology Framework to an existing complex system that is being managed by the author. The findings from this study demonstrate insights that can be gained about complex, sociotechnical systems by viewing them from multiple Stakeholder perspectives and blending the information about the technical and social design aspects.",
        "authors": [
            "Yiyun Zhang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158821",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Observations of Surfzone Vorticity Using Optical Remote Sensing",
        "abstract": "The surfzone is the dynamic interface between the land and ocean, where waves shoal and break as they reach shallow water near the shore. Currents and circulation patterns in the surfzone transport sediment, nutrients, pollutants, and other materials along and across the coast, and can create hazardous conditions for swimmers (rip currents). However, understanding of the strength and structure of eddies and vortices in the flow field primarily remains limited to numerical models and theory. Here, novel observations of surfzone vorticity at small [O(10m)] and large [O(100m)] spatial scales are presented and related to incident wave conditions and the measured underlying bathymetry. Field experiments were conducted at a sandy beach on the Atlantic Ocean, and nearshore flows were observed using optical remote sensing (coastal imaging) and in situ sensors. Remote sensing algorithms are expanded from previous applications to estimate high spatial resolution two-dimensional surface flows by tracking the motion of naturally occurring foam throughout the surfzone. Estimated currents are correlated with in situ flow measurements, and errors increase as the sea-surface viewing angle becomes more oblique and image quality decreases. Large spatial-scale vorticity estimated using remotely sensed flows increases with alongshore bathymetric inhomogeneity, and complex circulation patterns corresponding to holes and channels in the seafloor persist for days at a time. Small spatial-scale vorticity estimated from a 5-m diameter ring of 14 current meters increases with the directional spread of the incident wave field, consistent with increased vorticity injection from the crest-ends of breaking waves. Small spatial-scale vorticity estimated using remotely sensed flows is spatially variable and correlated with the amount of wave breaking observed at a given location. Enhanced vorticity at large and small spatial scales occurs in the inner surfzone, and virtual drifters released into the remotely sensed flow fields demonstrate cross-shore variability in dispersion and mixing. This thesis expands the understanding of vorticity dynamics in the surfzone through unique field observations and provides new tools for coastal research and monitoring through development of remote sensing techniques.",
        "authors": [
            "Ciara Jaya Dooley"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158879",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Characterizing Engineered Skeletal Muscle Rings as Actuators Using Strain Sensing Methods",
        "abstract": "A novel instrument was designed to characterize a force exertion model of engineered skeletal muscle rings. The instrument uses strain gauges to transduce a muscle ring contraction and has a verified resolution of 5 μN and 1.4 μm over the ranges of 5 μN and 1400 μm respectively. Experiments were carried out with four muscle ring specimens at six different structural stiffnesses. Each ring was excited at 1 Hz for 30 seconds while force and displacement was monitored. It was determined that the relationship between muscle contractile distance and force is related by a negative power function.",
        "authors": [
            "Laura M. Rosado"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158858",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Impact of Environmental Regulation on Data Center Valuation",
        "abstract": "Artificial intelligence has become one of the defining trends of modern society, with applications spanning virtually every industry. This societal shift has also influenced the real estate landscape. While data centers have existed for decades, it is only in recent years that they have garnered significant attention, demonstrated by their strong rent growth and compressed cap rates.1 Along with the attention over data centers, there also has been extensive research on how data centers impact the environment, such as \"Quantifying the Sustainability Impact of Data Center Availability\" by Manish Marwah et al. which present how data center power architecture may impact the environment and \"The Environmental Footprint of Data Centers in the United States\" by Md Abu Bakar Siddik, Arman Shehabi, and Landon Marsto. This research delves into quantifying the environmental impacts of data centers, specifically focusing on carbon and water footprints. However, what remains unexplored is how environmental regulations influence the valuation of data centers as a distinct real estate property type. This thesis examines how data center valuations could be impacted if existing environmental regulations were applied to regions where data centers are concentrated. The findings reveal a complex dynamic: while penalties under these regulations would reduce net operating income (NOI), potentially devaluing these assets, the same regulations would discourage new development, exacerbate the already constrained supply, and ultimately drive-up market rents for these properties. As a result, these opposing forces create ambiguity regarding the net impact of such regulations on data center valuations, with the outcome depending on which force prevails. What is clear, however, is that tenants would bear the brunt of these regulations, as landlords are likely to pass on increased costs through higher rents. On the other hand, while the environmental impacts of data centers and AI applications is critical to achieving sustainability goals, the societal benefits of AI solutions—ranging from advancements in healthcare to increased operational efficiencies—must also be considered. Balancing these competing priorities presents a unique challenge for policymakers and investors, with significant implications for the future of real estate and the digital economy.",
        "authors": [
            "Donghyun Lee"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158830",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Data-Driven General Purpose Foundation Models for\r\nComputational Pathology",
        "abstract": "The field of computational pathology has undergone a remarkable transformation in recent years. Researchers have leveraged supervised and weakly-supervised deep learning with varying degrees of success to address a wide range of tasks, including cancer subtyping and grading, metastasis detection, survival and treatment response prediction, tumor site-of-origin identification, mutation prediction, biomarker screening, and more. However, traditional task-specific models often require extensive labeled data and struggle to generalize across diverse pathology tasks. This limitation motivates the exploration of foundation models, which promise a more scalable, versatile solution by learning broad representations that can be adapted to various downstream applications. In this thesis, I will investigate the capabilities and limitations of data-driven foundation models in computational pathology. Specifically, I will explore two frameworks for developing general-purpose encoder models for pathology images: one using paired image-text data, and another leveraging self-supervised learning on large-scale unlabeled images. Additionally, I will examine downstream applications of these foundation models, including zero-shot transfer to gigapixel whole slide images and the development of an interactive multimodal AI assistant for pathologists.",
        "authors": [
            "Ming Yang (Max) Lu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158957",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Numerical investigations of vortex dynamics: bursting, twist waves, and sensitivity analysis",
        "abstract": "Vortical structures are ubiquitous in real-world fluid flows, from the vortices generated by swimming fish to the wakes of aircraft and propellers. They form the backbone of high Reynolds number turbulent flows. Their dynamics are governed by non-linear processes, leading to a range of vortical instabilities that significantly influence engineering applications. Despite decades of research, many questions remain about core mechanisms responsible for the dynamic evolution of vortical structures due to the nonlinearity and complexity of flows at high Reynolds numbers. A particular scenario that lacks systematic investigation is vortices with initial core-size variations, which leads to the phenomena of twist wave propagation and vortex bursting. In this thesis, we first examine straight vortex tubes with initial core-size perturbations at high Reynolds numbers by performing high-fidelity numerical simulations. The differential rotation along the vortex tubes generates twist wave packets that propagate and collide, resulting in a sudden increase in the local core size – the phenomenon of bursting. We analyze the effects of perturbation amplitudes on the detailed evolution at each stage, including the underlying mechanisms for the growth and decay of the bursting structure. The bursting process is associated with significant energy dissipation, which is quantified and compared to that of unperturbed vortex tubes. Meanwhile, vortices in real fluid flows are often nonrectilinear and experience strain from environmental or self-induced effects. We extend our study to curved vortex tubes and investigate the impact of centerline non-rectilinearity on twist wave propagation and the stability of the bursting structure. Additionally, we adopt a relatively recent geometric perspective on vortical flows and analyze the helicity dynamics during the flow evolution. To systematically initialize vortex dynamics simulations based on a late-time or time-averaged flow metric, we explore different methods for sensitivity analysis of two-dimensional vortical flows. The sensitivity values obtained are then used in gradient-based optimizations, which shows promising pathways for control and optimization of vortical flow applications. Additionally, we present a numerical study of the locomotion of a rotating cylinder pair with periodic gaits in a low Reynolds number flow. We characterize the motion pattern and efficiency of the cylinder pair through a combination of theoretical arguments and numerical simulations, which provides a foundation for potential engineering applications at the microscale. Overall, our findings provide understanding of fundamental mechanisms for vortex bursting and associated twist wave dynamics at high Reynolds numbers, explorations of sensitivity analysis for vortical flow applications, along with insights into locomotion at low Reynolds numbers.",
        "authors": [
            "Lingbo Ji"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158892",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Precisely Loose: Unraveling the Potential of Particles",
        "abstract": "Random, irregular, erratic, arbitrary, unspecifiable, and unpredictable—particles. In a post-extractive future, our reliance on standardized materials, continuously sourced through the exploitation of raw resources, will no longer be sustainable. Instead, architecture will increasingly contend with materials that defy standardization. This thesis focuses on these non-normative materials—particles, encompassing construction demolition debris, manufacturing defects, naturally occurring gravels, and locally sourced mineral waste. Ubiquitous yet underutilized, these materials hold potential not only for use, but also for reuse. However, they are often dismissed as rigid and unpredictable ingredients that require precise manipulation and cumbersome processing in order to achieve predictable results. What kind of architecture could emerge if we embraced the inherent nature of these particles, not as rigid materials to be controlled, but as dynamic, fluid entities? By embracing their uncertainty as a generative design agent, how would design approaches and construction processes transform? This thesis presents a catalogue of precisely loose methods for engaging with particles. These methods offer an alternative design approach that moves beyond the obsession with refinement and control over material behavior. By pouring, pushing, reconfiguring, and containing—in lieu of identifying, cutting, placing, and stacking—this series of interactions explores the potential of plurality, investigating how loosely controlled particles can adapt to collaborative construction processes. In doing so, this thesis redefines architectural material culture rooted in rubble, offering a framework to reimagine our relationship with the irregular, the unpredictable, and the overlooked.",
        "authors": [
            "Jeonghyun Yoon"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158833",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Quality-Centric Single-Image Procedural Material Generation",
        "abstract": "Procedural materials, represented as functional node graphs, are ubiquitous in computer graphics for photorealistic material appearance design. They allow users to perform intuitive and precise editing to achieve desired visual appearances. However, even for experienced artists, creating a procedural material given an input image requires professional knowledge and significant effort. Current inverse procedural material modeling approaches enable the automatic generation of procedural materials from input images. However, the visual quality of the generated materials is fundamentally limited by insufficient high-quality training data from industry-standard procedural materials, reliance on token-space supervision without visual feedback, and the absence of approximation-free node parameter post-optimization. My thesis presents advanced dataset augmentation, model training, and parameter post-optimization algorithms to address these challenges, significantly improving the perceptual match between the generated procedural material and the input image. Furthermore, the methodologies can be applied to other inverse procedural graphics problems to expedite similar artistic creation processes.",
        "authors": [
            "Beichen Li"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158945",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Building Insurance",
        "abstract": "Over the past 350 years, the building insurance industry has been shaped by a series of major urban fires, each incrementally standardizing risk assessment and property valuation as financial products of risk management. In recent years, however, climate change has introduced unprecedented weather events that challenge the fine tuned models of insurance; in particular, the rise of wildfires in California and the Pacific Northwest have led to local withdrawal of insurance altogether. Within these contexts, the spatial conditions inherited by a highly insured past continually sustain separation, individual prosperity, and standard assemblies as inheritances of expansionist agendas. At this juncture of system failure, this thesis asks: how can architecture rethink more cooperative forms of building and living together that localize risk sharing, responsibility, and stewardship? While wildfire defense strategies put forth by insurance companies and building code armor stick-frame American single family home and its aesthetic traditions, this thesis proposes a new building typology entirely: a neighborly cooperative of adjoined homes. Under a single roof, property lines are transformed into sites of mutual stewardship, manifesting insurance no longer as an abstract response to risk, but as a series of social and spatial relationships between neighbors.",
        "authors": [
            "Charles Perot Janson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158891",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Application of foundation models for molecular representation in cancer drug discovery and precision oncology",
        "abstract": "Drug discovery is a resource-intensive and time-consuming process, often requiring decades of effort and substantial financial investment, with a high risk of failure. Despite advances in high-throughput screening technologies, the size of chemical space presents a significant challenge: it is not feasible to experimentally screen all potential drug-like molecules. Most commercially available chemical libraries consist of molecules that are synthesized on demand from pre-existing building blocks, further limiting the exploration of novel chemotypes. This thesis aims to explore whether drug discovery could be accelerated by leveraging advances in deep learning (DL) models to identify promising hit candidates and improve the prediction of drug response in cancer. Development of cancer drugs that will be effective on a predictable set of targets remains a major challenge. We are developing a DL model capable of identifying potentially novel cancer drug chemotypes and reliably predicting drug response on cancer cell line targets. Leveraging recent progress in transformer-based architectures and graph neural networks, we use molecular language models, graph models and cell foundation models to embed both molecular and genomic data into low-dimensional subspaces and then use standard machine learning (ML) tools in these low-dimensional spaces to predict the efficacy of the molecules in particular cell lines. We utilize the large-scale drug repurposing and oncology datasets from the PRISM project at the Broad Institute, which provide a wealth of drug repurposing and oncology data, enabling robust training of ML models. We show that these vector embeddings are superior to existing methods, as they enable more accurate drug response predictions. The first part of this thesis is dedicated to development of a deep learning cancer drug discovery model, focused on in silico screening of chemical space to search for cancer drug candidates. The second part is focused on development of a precision oncology model, based on a multichannel neural network architecture. Our pipeline involves training single-target models on drug molecular structures, followed by integrating genomic data to enhance biological context and train a hybrid model capable of predicting drug response for novel drug:target pairs. Our results demonstrate that vector embeddings produced by the proposed framework outperform existing approaches, offering a more accurate and efficient means of exploring chemical space. This work highlights the transformative potential of ML/DL methods in drug discovery, enabling targeted, cost-effective exploration of chemical libraries, and advancing the development of precision oncology treatments.",
        "authors": [
            "Khrystofor Khokhlov"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158915",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "OPTASAT: An Open-Source, Flexible Software Framework for Small Satellite Operations",
        "abstract": "The unprecedented growth in access to space has created a corresponding growth in the number of spacecraft and the number of people operating spacecraft. This has meant that many of these operators are operating spacecraft for the first time. Gone are the days when the only operators of spacecraft were national governments, militaries, and massive corporations. The operators of small spacecraft today include many early-career individuals who need the tools to enable them to make strong decisions in the behavior of their spacecraft. The tools for operating spacecraft are often overlooked by teams focusing on the spacecraft themselves, but these operating tools are critical for mission success. Spacecraft operations tools have not developed in a similarly low-cost, widespread fashion as the spacecraft have. The best tools for modeling and understanding the situation of a satellite in space remain locked behind high barriers to entry including high cost, long training, and complex interfaces. In the same way that satellites have gone from the size of automobiles to the size of toasters, the software for operating them needs to go from expensive, complicated, high-performing suites to simple, flexible, approachable options that are accessible to the democratized space operators. New spacecraft operations staff need straightforward, direct interfaces which give them the knowledge of where their spacecraft is, where it will be, and what it will be able to do, and they need to know when all the options at their disposal are viable. Operators also need to be given the capability to adjust their software in whatever ways are necessary to tailor it to the particular parameters of their missions, to reflect the incredible variety of spacecraft and missions that exist today. A gap exists in spaceflight software. Users need software that can perform their mission planning tasks in the short term and to inform them of the upcoming parameters of their spacecraft which concern them, whether this is the spacecraft’s location, solar illumination, orientation, or any other property which is relevant to their particular mission. This software must also allow the users to be aware of the expected output of their sensors, especially imaging sensors, such that they may have an understanding of what they are imaging and what it ought to look like. Finally, this software must be open-source, enabling the user to audit the software and make changes to the software to customize it to their preferences, which may differ from anything the original software developer could have imagined. Such spaceflight software does not yet exist. This dissertation develops and presents OPTASAT, the Open-source Python Tool for Awareness of Spacecraft and Analysis of Telemetry, which provides an extensible, modular interface for incorporation of multiple tools which contextualize spacecraft data in a manner which maximizes usefulness for the operators. A priority is visualization of data to facilitate rapid understanding and distillation of the complexity of a spaceflight operation. This software has been released as a fully-featured, open-source software toolkit which performs the mission analysis components deemed most crucial to those who stand to benefit from it. This software is intended to fulfill the needs of small spacecraft missions. Several particular application cases are studied, including that of an Earth Sensing mission, and Astronomy mission, and modeling communications for a real laser crosslink mission. These case studies are evaluated for their ability to present the relevant information to the operator. For Earth Sensing, this involves displaying information regarding the spacecraft’s location with respect to the Earth, and enabling the selection of ground targets for imaging. For astronomy, the relevant information concerns the stars visible in the sky, and the spacecraft’s relationship to sources of interference like the Sun and Moon. For the laser crosslink example, we study the operator’s understanding of the spacecraft as they pass over a ground station and determine the operational configurations available for this communication. OPTASAT fills gaps in the field. OPTASAT presents users with a tool which is flexible and intuitive to use for understanding data from spacecraft in a way that is not currently available in the offerings on the market. Additionally, it takes functionality that is currently available in proprietary paid software and makes it available for free, in an open source offering that is accessible to everyone. OPTASAT will allow spacecraft operators (especially those operating spacecraft for the first time) to confidently know the state of their spacecraft, enabling them to make the best decisions for their satellites. This will reduce barriers to entry and smooth the learning curve, reducing the amount of overhead to new spacecraft operators. OPTASAT will be yet another step in the ongoing process of making space more accessible to a larger pool of users.",
        "authors": [
            "Thomas Joseph Murphy III"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158868",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Assessing Impacts of Digital Sketching on Concept Generation in Early Stage Design",
        "abstract": "Digital design tools have become increasingly popular for facilitating designers with different steps of the design process because they can simplify or automate certain components of these steps. Computer Aided Design (CAD) tools have assisted designers with tasks such as modeling and visualizing products prior to production and easily creating engineering drawings for manufacturing. Artificial Intelligence (AI) tools are being explored as collaborators who assist designers with interpreting sketches, assessing user needs, and generating ideas. Digital sketching tools such as tablets are a popular way for designers to easily create drawings that include different colors and styles and create multiple drafts of a concept by copying and pasting elements from previous sketches. However, the introduction of new tools into the design process always has broader implications for the design process. For instance, using CAD tools too early in the process can lead to design fixation and result in designers thinking a concept is more refined than it actually is due to the high quality and polish of the visualization created. Many researchers are now investigating when and how the best way to use AI tools in the design process is, but all struggle with the associated ethical implications of using the right training data and ensuring that the results are validated due to the serious risks related to misuse of AI. This dissertation focuses on one such digital design tool: tablets that are used for sketching. In an effort to expand the discipline’s understanding of how tablet use for sketching may enhance or detract from the design process, this thesis describes a series of studies investigating differences in ideation sketch attributes between tablets and paper/pen. Several of these sketch attributes have been linked with success in design- for instance, creating more sketches during ideation is linked with having better eventual design outcomes. This work investigates how sketch quality and quantity is impacted by the tools used for a short high level brainstorming session as well as a more detailed engineering concept generation task. Subsequently, it explores differences in content or novelty of ideas generated using each medium. Finally, it examines ways in which designers’ ideas evolve throughout the ideation process on both tablets and pen and paper. These aspects of the ideation process are important to understand, especially if the use of tablets leads to different results. The first area of investigation is related to exploring differences in sketch metrics including quantity, quality, and understandability between different sketching tools. These metrics have been found to be related to longer-term design outcomes and perceived creativity of concepts, so understanding the effect of the tablet on these sketch metrics can provide an understanding of how using a tablet for sketching could enhance or detract from overall design performance. The first study in this section investigates differences between pencil, pen, and tablet sketches during a short concept generation exercise and finds that sketch quality was highest for pencil drawings and lower for pen drawings but that tablet drawings do not significantly differ in quality from either pencil or pen drawings. Subsequently, a longer engineering design specific concept generation exercise was conducted to compare tablet sketching to pen and paper sketching. Here, there were no differences found in sketch quantity or understandability between paper and tablet. However, sketch quality, smoothness, and proportion/accuracy were all found to be higher on pen and paper than tablet. The second area of investigation explores whether or not using a tablet influenced designers’ ideation patterns. For instance, does the ability to copy and paste result in designers creating more interrelated ideas during brainstorming instead of exploring a variety of different design directions? There were no major differences found in the overall quantity of concept evolution present between tablet sketching and pen and paper sketching. However, tablet sketches across an ideation session had statistically significantly more concept chaining (related ideas appearing in a row) than paper and pen sketches despite having a similar number of related ideas overall. Additionally, concept chaining patterns were different for design prompts that had more than one functional requirement since not all ideas addressed all parts of the design prompt. However, for these prompts, the results from the primary functional requirement exhibited the same concept chaining patterns with more chaining present for tablet sketching than paper and pen sketching. The final area of investigation explores how designers’ ideas themselves are influenced by the sketching tool used through explorations of concept novelty and concept evolution. One study investigated novelty differences in concepts generated on tablet vs paper and found no correlation between the sketching tool used and the novelty of concepts generated. A second study was conducted to specifically compare designers’ own understanding of the interrelatedness of their ideas with the interrelatedness that could be assessed from the functional similarity of their sketches. Here, designers’ and reviewers’ assessments were found to not be aligned. In other words, sketches as standalone design artifacts did not communicate the extent of interrelatedness of concepts that was clear to the designer. Furthermore, the sketching tool used (tablet vs paper and pen) does not influence the level of agreement between designer and reviewer assessments. As such, using a tablet for sketching does not enhance or detract from the level of interrelatedness represented in sketches. These results suggest that assessing visual or functional similarity from sketches alone, regardless of the sketching tool used, may be insufficient in understanding all the relationship between a series of concepts as understood by the designer. Overall, these results indicate that using tablets as sketching tools does not have a clear significant benefit or burden on designers during ideation. It does not appear to enhance designers’ creative skills when it comes to sketch quantity or novelty though it did result in lower quality sketches, which has implications for the perceived creativity of concepts. Tablets were found to exhibit more instances of concept chaining than paper and pen sketches, though this trend did not persist when designers assessed their own concepts. Finally, this dissertation demonstrates that it is critical to seek designer input in identifying similarities across sketches as functional similarity may not be aligned with designers’ own understanding of which of their ideas are related.",
        "authors": [
            "Madhurima Das"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158800",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exciton Dynamics and Optical Properties of Lead Halide\r\nPerovskite Nanocrystals: From Nanorods to Nanocubes",
        "abstract": "Lead halide perovskites, particularly CsPbBr3, have emerged as leading light emitters for their spectral purity, brightness, and facile synthesis. Their soft, ionic lattice makes them unusually defect tolerant but introduces problems with stability. Additionally, dephasing mechanisms and coupling to phonons are not yet well understood in these semiconductors. \r\nIn the first part of the thesis, I investigate highly confined, anisotropic CsPbBr3 nanorods, elucidating the photophysics governing their broad single-particle linewidths. I utilize ensemble and single particle photoluminescence techniques across a wide temperature range in order to pinpoint exciton-phonon coupling mechanisms, structural and surface effects, and spin mixing in these novel materials.\r\nIn the second part of the thesis, I focus on the opposite size regime, where collective behaviour dominates the optical properties. I develop a novel spectroscopy to pinpoint dephasing mechanisms that could reduce superradiant and coherent emission in order to promote rational design and future integration of these nanocrystals into quantum information devices.",
        "authors": [
            "Tara Šverko"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158955",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention",
        "abstract": "Key-value (KV) caching plays an essential role in accelerating decoding for transformer-based autoregressive large language models (LLMs). However, the amount of memory required to store the KV cache can become prohibitive at long sequence lengths and large batch sizes. Since the invention of the transformer, two of the most effective interventions discovered for reducing the size of the KV cache have been Multi-Query Attention (MQA) and its generalization, Grouped-Query Attention (GQA). MQA and GQA both modify the design of the attention block so that multiple query heads can share a single key/value head, reducing the number of distinct key/value heads by a large factor while only minimally degrading accuracy. In this work, we show that it is possible to take Multi-Query Attention a step further by also sharing key and value heads between adjacent layers, yielding a new attention design we call Cross-Layer Attention (CLA). With CLA, we find that it is possible to reduce the size of the KV cache by another while maintaining nearly the same accuracy as unmodified MQA. In experiments training 1B- and 3B-parameter models from scratch, we demonstrate that CLA provides a Pareto improvement over the memory/accuracy tradeoffs which are possible with traditional MQA, potentially enabling future models to operate at longer sequence lengths and larger batch sizes than would otherwise be possible.",
        "authors": [
            "William Brandon"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158929",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Novel Structures for Scalable Vertical Gallium Nitride Power Devices",
        "abstract": "Solid state electronic devices have been the backbone of modern power systems for decades. However, as we enter an era fuelled by renewable energy and defined by pervasive electrification, novel power devices must be developed to address the increasingly stringent demands for high power density and efficiency. In this thesis, the theory and fabrication of several new gallium nitride (GaN) power devices will be developed to push beyond current device limitations.\r\n\r\nA key advancement surrounds the acknowledgment that vertical GaN power devices are fundamentally three-dimensional. Fabrication of these devices does not readily benefit from the decades of expertise gained in planar processing within the silicon industry. Instead, we will present how a new approach to creating vertical fin-based devices will enable self-aligned fabrication of vertical GaN finFETs and related devices. \r\n\r\nWithin this work, we also explore the scalability of vertical GaN finFETs. Working with 8-inch GaN substrates, we demonstrate that vertical finFETs can be fabricated using a fully CMOS compatible process flow. This enables a scalable pathway to the widespread adoption of GaN by leveraging existing manufacturing capabilities.\r\n\r\nAs a final look towards the future of GaN devices, we explore methods to surpassing the one-dimensional, unipolar limit of GaN through devices known as superjunction. The theory that has been highly successful for Si devices is applied to GaN, and a new framework for designing devices is presented. Using our approach to creating vertical fin-based devices, we are able to fabricate record high aspect-ratio demonstrations of a new class of fin diodes that reveal a promising path towards the next generation of GaN power devices.",
        "authors": [
            "Joshua Andrew Perozek"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158939",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Ponderomotive Forces in Pilot-Wave Hydrodynamics",
        "abstract": "Droplets bouncing on a vibrating bath may self-propel (or ‘walk’) via a resonant interaction with their self-induced pilot wave. In pilot-wave hydrodynamics (PWH), the spontaneous emergence of coherent, wave-like statistics from chaotic trajectories has been reported in several settings. Owing to the similarity of PWH to Louis de Broglie’s realist picture of quantum mechanics, the question of how such statistics emerge has received considerable recent attention.\r\n\r\nA compelling setting where coherent statistics emerge in PWH is the hydrodynamic analog of the quantum corral. When walking droplets are confined to a circular cavity or ‘corral’, a coherent statistical pattern emerges, marked by peaks in the positional histogram coincident with extrema of the cavity eigenmode. Stroboscopic models that idealize the drop’s bouncing dynamics as being perfectly resonant with their Faraday wave field have proven incapable of capturing the emergent statistics.\r\n\r\nIn this thesis, we present new experimental and theoretical findings in a variety of pilotwave hydrodynamical settings where non-resonant bouncing plays a key role in the droplet dynamics and emergent statistics. First, we find that modulations to resonant bouncing influence the stability threshold of a Bravais lattice. Second, we demonstrate that resonant bouncing can be disrupted by the imposition of suboctave driving, which may be used to induce a rearrangement of bound states of bouncing droplets.\r\n\r\nWe then proceed to an integrated experimental and theoretical study of the hydrodynamic corral, highlighting the role of non-resonant bouncing in the emergent statistics. We first introduce a new experimental method for simultaneously measuring the drop position and pilot wave height. We then report new measurements of the pilot wave and vertical bouncing dynamics. We demonstrate that the complex pilot wave arising in corrals may play the same role as suboctave driving in disrupting resonant walking. Our experimental findings motivate a new theoretical framework that predicts that modulations in the histogram emerge as a consequence of ponderomotive effects induced by non-resonant bouncing. We then connect the ponderomotive drift observed in hydrodynamic corrals to extant theories of quantum mechanics.",
        "authors": [
            "Davis J. Evans"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158854",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Shut Up and Dribble? Exploring the Real Estate Strategies and Trends of NBA Teams",
        "abstract": "NBA teams have always had to think about real estate through one certain lenses: the arena they play their 41 home games in (plus any subsequent playoff games). But now, NBA teams have evolved past only just thinking about the arena. Teams have increasingly gotten involved in real estate development. This thesis seeks to explore the impact of real estate as a revenue driver for NBA teams, trends observed, and strategic decisions that teams must consider. This thesis will explore current real estate activities of all 30 NBA teams and will examine the choices that teams must make regarding arenas, real estate development, and practice facilities. The findings will help teams and municipalities understand best practices for team-driven real estate, and how strategies can vary team by team based on their situations.",
        "authors": [
            "Viet Nguyen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158797",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Microfluidic Platform for Vascularized Tissue Models",
        "abstract": "This thesis presents a microfluidic platform designed to support 3D vascularized tis­sue models for microphysiological systems. The platform delivers pneumatic pressure and vacuum signals to drive fluid flow and pressure on tissue culture devices with integrated pumps and back-pressure regulators. The mechanical performance of the pumps and back-pressure regulators is characterized. Tissue compartments in each device contain endothelial and stromal cells suspended in a hydrogel during culture. An oxygenating reservoir stores and replenishes oxygen in circulating cell culture me­dia. During assembly, screws are used to compress an elastomeric membrane, forming a seal and transmitting pneumatic pressure signals from the connection manifold to acutate the fluidic control elements. After a biological experiment the tissue culture devices can be disassembled, cleaned, and re-used, thus enabling cost-effective experi­mentation and prototyping. Each of the 4 layers of the tissue culture devices arc ma.de of thermoplastic polymers, and their design is translatable to injection molding for future production at scale. The design and manufacturing methods for the platform and individual device features are discussed. Two major biological experiments are presented to demonstrate the platform's ability to support emergent vascularization in the tissue culture device over 7 days. Microscope images show development of perfusable microvessel networks.",
        "authors": [
            "Matthew Johnson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158859",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Fabrication and Characterization of Horizontally Aligned\r\nCarbon Nanotube Thermoplastic Bulk Nanocomposite\r\nLaminates",
        "abstract": "Carbon nanotubes (CNTs) have advantaged mass-specific mechanical properties and excellent thermal and electrical conductivity, making them an attractive reinforcement for composite systems. Due to an increasing need for more sustainable materials, incorporation of CNTs into thermoplastic matrices presents a promising solution for recyclable and repairable polymer nanocomposites (PNCs). This thesis presents an approach to fabricating and characterizing thermoplastic PNCs that incorporate ultra-high volume fractions of horizontally-aligned carbon nanotubes (HA-CNTs). An MIT-developed bulk nanocomposite laminating (BNL) process was adapted to fabricate multi-ply, unidirectional composites with poly(methyl methacrylate) (PMMA) and acrylonitrile butadiene styrene (ABS) matrices. For the HA-CNT/PMMA system, the BNL process was tailored to fabricate 4-ply and 8-ply laminates with fiber volume fraction v_f > 45 vol.%, using a 9 wt.% PMMA in anisole solution. Through characterization via X-ray microcomputed tomography (µCT), scanning electron micrography (SEM), thermogravimetric analysis (TGA), Fourier transform infrared (FTIR) spectroscopy, and polarized Raman spectroscopy, HA-CNT/PMMA laminates were shown to be free of micro-scale voids with weak or non-existent process-structure interactions, i.e., the CNTs had negligible effect on the polymer structure. TGA and IR helped demonstrate that the BNL process did not lead to decomposition or chemical changes to neat PMMA, and FTIR also revealed that the fabrication process did not induce covalent bonding between CNTs and PMMA. The crystalline behavior of PMMA was studied via dynamic scanning calorimetry (DSC) as well as X-ray diffraction (XRD), which demonstrated that BNL processing temporarily lowers neat PMMA glass transition temperature T_g by 4 ◦C with no permanent change after removal of thermal history. However, CNT inclusion leads to higher laminate T_g by 11 ◦C as shown through both DSC and dynamic mechanical analysis (DMA), which can be explained by CNT constraints on polymer chain movement as opposed to any crystallinity changes in the PMMA. Storage modulus of 8-ply HA-CNT/PMMA laminates was shown to be more than 600% of neat PMMA via DMA, while a decrease in tan(δ) of the laminate compared to neat PMMA indicates an increase in elastic behavior due to CNT inclusion. 4-ply laminates were subjected to a minimum radius of curvature test showing a ∼ 50% increase in yield strain compared to neat PMMA. Electrical properties of 4-ply HA-CNT/PMMA laminates were measured via 4-point probe testing, which demonstrated good Ohmic contact between CNTs, with conductivity of ∼ 2 × 10⁴ S m⁻¹ and anisotropy ratio of 1.2. A preliminary investigation was completed to evaluate the feasibility of using the BNL process for the HA-CNT/ABS system. Uniform suspensions of ABS in anisole were developed to use the BNL polymer infiltration method of spin-coating and vacuum-assisted infusion. It was shown that the nature of the ABS suspension led to uneven polymer distribution over the HA-CNTs. This work has demonstrated the successful incorporation of high volume fractions of aligned CNTs into PMMA thermoplastic matrices as well as the electrical conductivity of such composites, opening an avenue to the development of other high v_f thermoplastic PNCs and exploration into additional multifunctional capabilities.",
        "authors": [
            "Yuying Lin"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158815",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Leveraging Right and Left Ventricular Coupling for Optimization of Mechanical Circulatory Support",
        "abstract": "Mechanical circulatory support devices have the potential for profound impact on cardiogenic shock patients. They enable volume propulsion and pressure gradient generation first by unloading and later by decoupling native cardio-vascular interactions, which reduces cardiac load and energy consumption while increasing organ perfusion in the face of disease.  However, there is a potential price in that coupling evolved to optimize blood flow dynamics and the complex interplay between individual cardiovascular components and interposing organs like the lung. Disrupting native coupling with mechanical support risks decompensation if the heart and lung cannot tolerate these changes.\r\n\r\nOne particularly concerning consequence of altered coupling is that upwards of 40% of patients with left-sided mechanical support face ensuing right heart failure, which requires urgent action and often is associated with even higher mortality rates. We hypothesized that better understanding of right heart function and the mechanisms of right heart (in)tolerance to left-sided support will improve device utility by aiding device selection as well as titration throughout a patient’s clinical course. In particular, we focused on right and left ventricular coupling, which consists of serial coupling across the closed-loop cardiovascular circuit, and parallel coupling that enables intracardiac interdependence and force transmission between the ventricles. Each interaction plays a critical role in a patient’s tolerance to mechanical support and optimal setpoint.\r\n\r\nWe used a series of controlled porcine experiments to evaluate right and left heart coupling during mechanical support. In each set of experiments, we induced graded models of disease that range from health to progressive impairment, enabling evaluation of  mechanical support across a spectrum of right and left heart states. Through these studies, we improved mechanistic understanding of the differences between right and left heart function, and how those differences dictate the response to left-sided support. Specifically, we found that pulmonary vascular compliance enabled a unique right heart adaptability to varied flow, but limitations in compliance due to disease yield right heart intolerance to support. We leveraged the indwelling pump to dynamically alter load in the system, creating a method to rapidly evaluate pulmonary vascular compliance adaptability and therefore predict the need for right-sided support. Finally, we created a metric using device-organ interactions for tracking right-left coupling over time, which can aid optimization of device speed based on relative right and left ventricular volume setpoints. Translation of these findings to the clinic could better inform use of mechanical circulatory support technologies with the goal of improving outcomes for cardiogenic shock patients.",
        "authors": [
            "Kimberly Kate Lamberti"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158818",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Designing Sustainable Recommender Systems",
        "abstract": "Recommender systems are widely deployed to serve users with content they like. However, content must be created and insufficient demand dampens a creator’s production incentive. We argue that the canonical recommender system may not be sustainable if, by promoting the content each user likes the most, it suppresses the creation incentive of the less popular but still valuable content. We propose a “sustainable recommender system” solution – subsidize creators with demand according to their “sensitivity,” which measures how easily a creator can be incentivized by demand, and their “contribution,” which measures how important a creator is to users overall. Theoretically, we prove that this algorithm maximizes long-term user utility by internalizing the externality of user choice on other users. Computationally, our main innovation is to estimate creator contribution using computer vision, where we train a deep-learning model to compute how creator distribution affects system-wide user utility. Analyzing data from a large content platform, we show that our algorithm incentivizes valuable creators and sustains long-term user experience.",
        "authors": [
            "Lei Huang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158881",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Business and Redevelopment Outline for the Re-Use of a Prime Site in South Boston",
        "abstract": "This development and business plan considers the neighborhood context and current market conditions characterizing the subject site’s redevelopment potential. The subject site, further defined in this thesis, is a prime parcel of land in the South Boston neighborhood of Boston, MA currently improved and used for quick-serve restaurant operations. Proximate to the Seaport, Fort Point, and Dorchester, South Boston is surrounded by demand drivers resulting in explosive growth that make it one of the most desirable and expensive housing submarkets in the entire City of Boston. Development considerations are fully defined in the report including zoning, equity, financial projections, ground lease, and market-level factors. A conclusion is made on the feasibility of the proposed project with recommendations for next steps resulting from the modeled base-case scenario. Market assumptions and any unresolved development issues are clearly identified and discussed.",
        "authors": [
            "Zachary D. Proman"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158849",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Computational Modeling of Biological Function",
        "abstract": "How biological function emerges from complex molecular patterns is a fundamental question in biology. Addressing this question requires a deep exploration of the concepts of genotype and phenotype, which serve as the foundation of this inquiry. This dissertation focuses on providing a quantitative approach through the lens of computation to dissect the dynamic relationship between genotype and phenotype. In particular, recent advancements in high-content genotyping methods, such as genome-wide association studies (GWAS) and single-cell RNA sequencing, have provided powerful tools for mapping the molecular basis of biological function, but also have introduced challenges due to the high dimensionality, vast combinatorial possibilities, and multimodal characteristics of the data. The overarching goal of this dissertation is first to provide a critical discussion on the theories of genotype and phenotype as they relate to biological function and propose new methods to map their relationship. Specifically, we present the integrated genetics framework designed to analyze and interpret the manifold of genotypes and their associated phenotypes simultaneously. We applied this approach to develop a multimodal foundation model for human transcriptomics at the cellular level. To further test the capabilities of this method, we apply it to dissect the aging process. The results of this study provide novel concepts and methods for analyzing the genetic data along with phenotypic information with higher resolution. Moreover, the results shed light on uncovered potential cross-tissue biomarkers that are undetectable through conventional gene expression analysis alone. Overall, this study aims to advance our understanding of the dynamic interplay between gene patterns and phenotypic manifestation and demonstrates the potential of computational modeling in uncovering new dimensions of cellular function and complexity.",
        "authors": [
            "Farhan Khodaee"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158814",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Goal Inference from Open-Ended Dialog",
        "abstract": "Embodied AI Agents are quickly becoming important and common tools in society. These embodied agents should be able to learn about and accomplish a wide range of user goals and preferences efficiently and robustly. Large Language Models (LLMs) are often used as they allow for opportunities for rich and open-ended dialog type interaction between the human and agent to accomplish tasks according to human preferences.\r\n\r\nIn this thesis, we argue that for embodied agents that deal with open-ended dialog during task assistance:\r\n\r\n1. AI Agents should extract goals from conversations in the form of Natural Language (NL) to be better at capturing human preferences as it is intuitive for humans to communicate their preferences on tasks to agents through natural language.\r\n\r\n2. AI Agents should quantify/maintain uncertainty about these goals to ensure that actions are being taken according to goals that the agent is extremely certain about.\r\n\r\nWe present an online method for embodied agents to learn and accomplish diverse user goals. While offline methods like RLHF can represent various goals but require large datasets, our approach achieves similar flexibility with online efficiency. We extract natural language goal representations from conversations with Large Language Models (LLMs). We prompt an LLM to role play as a human with different goals and use the corresponding likelihoods to run Bayesian inference over potential goals. As a result, our method can represent uncertainty over complex goals based on unrestricted dialog. We evaluate in a text-based grocery shopping domain and an AI2Thor robot simulation. We compare our method to ablation baselines that lack either explicit goal representation or probabilistic inference.",
        "authors": [
            "Rachel Ma"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158960",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Data-Rich Personalized Causal Inference",
        "abstract": "There is a growing interest in individual-level causal questions to enable personalized decision-making. For example, what happens to a particular patient’s health if we prescribe a drug to them, or what happens to a particular consumer’s behavior if we recommend a product to them? Conducting large-scale randomized experiments to answer such questions is impractical—if not infeasible—due to cost, the level of personalization, or ethical concerns. Observational data offer a valuable alternative, but their lack of explicit randomization makes statistical analysis particularly challenging. In this thesis, we exploit the richness of modern observational data to develop methods for personalized causal inference. In the first part, we introduce a framework for causal inference using exponential family modeling. In particular, we reduce answering causal questions to learning exponential family from one sample. En route, we introduce a computationally tractable alternative to maximum likelihood estimation for learning exponential family. In the second part, we leverage ideas from doubly robust estimation to enable causal inference with black-box matrix completion under a latent factor model.",
        "authors": [
            "Abhin  . Shah"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158911",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Multi-Agent Hybrid Prediction in Autonomous Driving",
        "abstract": "In autonomous driving, the hybrid task of predicting both high-level actions and lowlevel trajectories of human behaviour is fundamental to safe downstream decision-making. Much of the existing work in behaviour prediction tackle this problem without sufficiently modelling agent-agent interactions, limiting their ability to capture the full range of possible joint outcomes. Another key challenge in multi-agent prediction is the intractable prediction space that grows exponentially in the number of agents and duration of the prediction horizon. As a result, scalability is a major challenge. This thesis presents two approaches to address these challenges in multi-agent hybrid prediction. In our first approach, we model interactions and address scalability by learning to factor the joint prediction distribution. We observe that agents do not interact with all other agents in the scene, but rather, there are groups that strongly interact. Therefore, we group agents and represent the high-level interaction outcomes of groups with discrete variables. We additionally assume that inter-group interactions are sparse and can be sufficiently represented with a directed acyclic graph. These assumptions enable us to factor the distribution into a product of factors, effectively reducing the prediction space, and providing an order in which to easily sample discrete values. We evaluate the performance of this method on a large-scale autonomous driving dataset and show that it exceeds prior methods in coverage of possible interaction outcomes by 24% to 48% on various multi-agent validation data splits, while maintaining state-of-the-art prediction error. Our second approach represents agents in a traffic scene as a set of concurrent hybrid models and assumes a collision avoidance model of interactions, rather than learning the model from data like the first approach. Our method begins enumeration based on a simpler collision-agnostic prior distribution. Based on our factored representation, we determine the next best assignment to the prior. We extract bounding conflicts to correct the prior and increasingly reduce the error between the distribution used by enumeration and our collision-aware posterior distribution. Our experiments show that enumeration using A* with bounding conflicts (A*BC) is faster than A* and is therefore better at addressing scalability. In terms of prediction metrics, we find that our collision-aware posterior performs worse than the collision-agnostic prior and suggest future directions for improvement.",
        "authors": [
            "Tiffany Yee Kay Yau"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158832",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Engineering Scalable Quantum Systems From First-Principles to Large-Scale Control",
        "abstract": "Color centers in solids are promising platforms for quantum communication, sensing, and computing, featuring highly coherent optical transitions, as well as native electron and nuclear spins that can be used as quantum memories. Existing state-of-the-art demonstrations have shown that multi-qubit control, spin-photon entanglement, and heralded entanglement are possible with devices consisting of a few color centers. However, the path to scaling the number of color centers integrated in these devices to the thousands or millions needed for advanced quantum networking and computing applications remains unclear. In particular, the requirement for highly coherent quantum operations both necessitates operation at cryogenic temperatures, and precise classical control signals delivered to each color center. Precise qubit control greatly increases the system complexity, while the cryogenic operation limits the amount of power that the system can dissipate. Both factors severely limit the number of color centers that can realistically be included in a single device using existing methods. This work will tackle the scaling problem from a system-level perspective from two directions. Firstly, I will quantify performance trade-offs between coherence, temperature, and optical properties of the group-IV color centers. A novel color center system, the ¹¹⁷SnV⁻ hyperfine color center, will be presented and its advantages compared to traditional group-IV color centers will be explored. Secondly, a method to integrate color centers with application specific integrated circuit (ASICs) will be demonstrated. The ASICs provides multiplexed control signals and increased control field efficiency, thus decreasing both the wiring complexity and thermal load per qubit. This work will thus pave the way to color center-based devices in which the number of qubits is not limited by the complexity or power dissipation of the control system.",
        "authors": [
            "Isaac B. W. Harris"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158925",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Learning Generalizable Systems by Learning Composable Energy Landscapes",
        "abstract": "How can we construct intelligent embodied agents in the physical world? Such agents should be able to autonomously solve tasks that have not been seen before, subject to external disturbances in the environment, as well as new combinations of factors such as lighting, varying sensor inputs, and unexpected interactions with agents and other objects. An important subgoal towards constructing such intelligent agents is to construct models that can robustly generalize, not only to distributions of tasks similar to ones seen at training time but also to new unseen distributions. This departs from standard machine learning techniques which usually assume identical training and test distributions. Towards this goal, in this dissertation, we’ll illustrate how we can achieve certain forms of generalization by estimating energy landscapes over possible predictions for each task, with accurate predictions assigned lower energy. This modeling choice formulates prediction as a search process on the energy landscape, enabling zero-shot generalization to new constraints by adapting the energy landscape. In addition, this allows us to generalize to entirely new distributions of tasks in a zero-shot manner by composing multiple learned energy landscapes together. In this dissertation, we first introduce a set of techniques to train energy landscapes and an algebra in which we can compose and discover composable energy landscapes. Next, we illustrate how energy landscapes can be composed in a diverse set of ways, ranging from logical operators, probability distributions, graphical models, constraints, and hierarchical compositions, enabling effective generalization across vision, decision-making, multimodal, and scientific settings.",
        "authors": [
            "Yilun Du"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158938",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Cooling Innovation and Circularity: Addressing Water Stress in the Age of AI-Driven Data Centers",
        "abstract": "This thesis examines the growing demand for data centers and the critical challenges posed by their water and energy consumption. As artificial intelligence (AI) technologies expand, the infrastructure supporting these systems has become essential. The study highlights the projected increase in data center capacity driven by AI workloads and focuses on the impact in water-stressed regions across the United States. Given the resource-intensive nature of data centers, the research explores cooling technologies aimed at reducing environmental impact. Traditional air cooling is compared with innovative liquid and evaporative cooling techniques. Additionally, the thesis promotes circular economy principles, emphasizing resource efficiency, reuse, and regeneration as a pathway to sustainable operations.",
        "authors": [
            "Reem Kseibati"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158889",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Addressing Challenges in Object-Based Robot Navigation and Mapping",
        "abstract": "Developing fully autonomous systems that can safely traverse and interact with the environment has been a long-term objective in robotics. Many relevant tasks, such as planning and mobile manipulation, require the robot to possess an object-level understanding of the ambient world. In particular, it would be crucial to maintain a globally consistent objectbased map of the environment for these operations. Without external assistance – such as a prior map or a motion capture system – the robot needs to navigate and map the environment using an object-based SLAM system. This thesis is dedicated to addressing several key challenges in developing object SLAM systems. The first challenge arises from the ambiguity of object poses in single-view observations. When an object is observed from a single vantage point, it can often have multiple probable poses due to symmetry, occlusion, or perceptual failures. It would be difficult for an object SLAM system to incorporate such ambiguous measurements. To address this issue, we introduce an ambiguity-aware object SLAM method. We use Gaussian max-mixture models to represent and efficiently track the multiple object pose hypotheses, and gradually disambiguate the poses to construct a globally consistent object-level map. The second challenge is the performance degradation of neural networks when deployed in novel robot operating environments, commonly known as the domain gap problem. Specifically, when a pre-trained 6DoF object pose estimator is used in a novel environment, its pose predictions are often corrupted by outliers, and quantifying their uncertainties becomes difficult. Using these noisy predictions with unmodeled uncertainties as measurements in an object SLAM system can lead to significant estimation errors. To mitigate the problem, we propose a SLAM-supported self-training pipeline for domain adaptation of 6DoF object pose estimators. We exploit robust pose graph optimization (PGO) results to pseudo-label robot-collected images and fine-tune 6D object pose estimators. In particular, we develop an Automatic Covariance Tuning (ACT) method to model pose prediction uncertainties automatically during the PGO process. The third challenge is environmental changes. As changes occur in the scene, such as object insertion, removal, or rearrangement, the robot needs to efficiently detect these changes and update the map accordingly. While detecting and reflecting scene changes is relatively straightforward with handcrafted map representations like point clouds or voxels, it becomes significantly more difficult with learned radiance-field-based scene representations, such as Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) models. In this thesis, we develop a radiance-field-based 3D change detection method to identify 3D object-level scene changes. Our approach can rapidly detect object changes in cluttered environments represented with radiance field models from as few as a single post-change image observation. We also develop efficient update methods for NeRF and 3DGS models to reflect physical object rearrangements, guided by sparse post-change images. By addressing these challenges, this thesis advances the robustness and adaptability of object SLAM systems in real-world environments, paving the way for more reliable and autonomous robotic systems capable of complex interactions with the environment.",
        "authors": [
            "Ziqi Lu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158807",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Complementary Cost‐Effective Electrochemical Platforms for Point‐Of‐Use Biosensing",
        "abstract": "The COVID‐19 pandemic has illustrated the urgent need for rapid and affordable point‐of‐use diagnostics. Electrochemical biosensors are useful for such applications because they enable quantitative readout and show drastically improved sensitivity compared to prevalent lateral flow technologies. However, to‐date, the poor quality of commercially‐available, mass‐produced electrodes has prohibited the scaled production and commercialization of such biosensors beyond glucose sensing. Low‐cost gold leaf electrodes have previously been developed that can be fabricated with no specialized equipment at the point‐of‐use. These electrodes are more effective for biosensing than prevalent commercially‐available systems. Yet, their manual fabrication can be tedious and is not scalable in its current form. Here, performance of mass‐produced gold electrodes generated using roll‐to‐roll manufacturing is evaluated, offering the potential to scale production. Upon comparison of these electrodes with the gold leaf, it is found that these electrodes are high quality, equivalent to the gold leaf electrodes, and support biosensing applications through the detection of both DNase I and BtsI‐v2 activity with comparable performance. These results demonstrate the role of complementary technologies to achieve point‐of‐use sensing by enabling flexibility between mass‐produced manufacture and on‐site production.",
        "authors": [
            "Mason Monaco",
            "Marjon Zamani",
            "Ava Sarram",
            "Chao‐Chi Kuo",
            "Chathurika Abeyrathne",
            "Miaosi Li",
            "Ariel L Furst"
        ],
        "journal_conference_name": "Advanced Sensor Research",
        "publisher": "Wiley",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158292",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Essays on Sustainability in Agriculture and Food Systems",
        "abstract": "Agriculture and food systems face severe challenges from climate change, population growth, and food insecurity. These unprecedented issues leave millions vulnerable to hunger and malnutrition, underscoring the urgent need for a transition toward sustainable agriculture and food systems. The first research stream in this thesis focuses on promoting sustainability in agriculture, particularly through contract farming. In Chapter 2, we model contract farming as a bi-level optimization problem for a farmer and a company. We analytically demonstrate that different contract structures offer varying incentives for farmers to invest in quality-improving efforts, resulting in different levels of quality for agricultural products. Empirical analysis of production-level data supports these model predictions.\r\n\r\nThe second research stream examines sustainability in food systems, specifically addressing the issue of food waste. In Chapter 3, we explore the impact of online grocery shopping on household food waste. Using large-scale Nielsen Consumer Panel data and instrumental variable analysis, we establish a statistically significant causal relationship, showing that households with higher frequency of online grocery shopping experience lower waste per capita, a proxy of household food waste. These findings emphasize the role of digital platforms in fostering sustainable consumption and call for continued support for online grocery shopping to mitigate consumer-level food waste. In Chapter 4, we turn to retail-level food waste. We design and implement behavioral interventions aimed at reducing food waste in restaurant kitchens in Ghana. As a Sub-Saharan African country, Ghana faces both food waste and food insecurity. Through a six-week field experiment and a difference-in-differences analysis, we demonstrate that interventions focused on public- and private-interest lead to 9% and 19% reductions in food waste in kitchens, respectively. Follow-up surveys and further analyses reveal that this result may be related to the demographic/socioeconomic characteristics of workers (e.g., age and income), their perception of power distance within the management hierarchy, and their satisfaction with restaurant management.",
        "authors": [
            "Xinming Liu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158806",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Tracking carbon fluxes across ocean interfaces using dissolved gas observations",
        "abstract": "The cycling and exchange of carbon between Earth’s systems play a pivotal role in regulating climate, yet two major carbon fluxes remain poorly constrained: the biological carbon pump (BCP) and carbon release from Arctic permafrost. This thesis focuses on dissolved gases as tracers and drivers of these processes through both autonomous and field-based observations. It encompasses (i) improvements to sensor-based measurements of O₂, (ii) the use of these measurements to assess the strength of the BCP in two distinct export regimes, and (iii) isotopic approaches to carbon dioxide (CO₂) and methane (CH₄) dynamics at a coastal permafrost site. The first part of the thesis is centered around the NASA EXPORTS campaign and studies the BCP at two contrasting field sites. Using autonomous platforms, carbon export was evaluated at both sites and demonstrated that at the lower productivity site, a greater proportion of fixed carbon was routed to sinking particulate organic carbon (POC), while the higher productivity site resulted in near equal proportions of dissolved organic carbon production and sinking POC. These findings underscore the value of autonomous sensors in capturing spatial and temporal variability in oceanic carbon cycling. The second part of this thesis shifts focus to the Arctic, where rapid warming threatens to mobilize vast (~1,500 Pg) amounts of carbon currently stored in permafrost. This study presents observations from the spring thaw at a coastal Arctic site and demonstrated that even sites with high CH₄ and CO₂ concentrations drew less than 10% of their carbon source from ancient permafrost sources. The variability in CH₄ and CO₂ emissions reflects the complex interplay between hydrological changes, primary productivity, and microbial processes. The research highlights the need for regular monitoring of Arctic rivers, which integrate changes in the terrestrial system, as a potential early warning system for abrupt permafrost thaw. This thesis leverages the fundamentals of dissolved gas geochemistry to examine key climate-relevant biogeochemical cycles across diverse environments that are sensitive to global change. These insights contribute to refining Earth system models and emphasize the need for expanded monitoring to predict future shifts in global carbon cycling and climate dynamics.",
        "authors": [
            "Shawnee Nicole Traylor"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158813",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design and Optimization of Tunneling Nanoelectromechanical Switches",
        "abstract": "As silicon complementary metal-oxide-semiconductor (CMOS) technology nears its scaling limits, nanoelectromechanical (NEM) switch relays have emerged as promising candidates for complementing CMOS technology due to their superior characteristics, including zero leakage, steep subthreshold swings, high on-of current ratios, and robustness in harsh environments. However, the practical integration of NEM switches still faces challenges such as high actuation voltages, stiction, and slower switching speeds compared to CMOS. One promising strategy to mitigate these issues is the integration of a self-assembled monolayer (SAM) to create tunneling NEM switches. Such switches could achieve nanometer-scale mechanical modulation of gaps between electrodes, showing the potential to overcome the limitations of a conventional NEM switch by exhibiting low actuation voltages, high switching speeds, and minimizing stiction. Nevertheless, the tunneling NEM switches reported to date still show limited performance and require intricate fabrication processes. Additionally, functional tunneling NEM switches demonstrated are limited to two-terminal architectures. This thesis explores innovative designs, fabrication techniques, and material choices to address these limitations and to develop tunneling NEM switches with enhanced performance and reliability for next-generation NEM logic applications. To this end, switches with various structures have been fabricated and investigated, and their respective characteristics are analyzed. In a three-terminal lateral structure fabricated using entirely conventional nanofabrication techniques, switching is demonstrated in both contact and tunneling modes. While operation in direct contact mode shows a high on-of ratio, the integration of the SAM leads to a significantly reduced actuation voltage of 2 V and a lower hysteresis. Further, two-terminal vertical structured devices are studied in tunneling mode, and they consistently demonstrate operation cycles exceeding 100, with a maximum of over 7000, which manifests the reliability prospects of SAM. The trends in IV characteristics indicate that the SAM might have experienced physical deformation due to compression, highlighting a potential area for future research in the molecular engineering of the self-assembly monolayer.",
        "authors": [
            "Tong Dang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158940",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Techniques for Foundational End-to-End Verification of Systems Stacks",
        "abstract": "Today's software is full of bugs and vulnerabilities. Formal verification provides a promising remedy through mathematical specifications and machine-checked proofs that the implementations conform to the specifications. However, there could still be bugs in the specifications or in the verification tools, which could lead to missed bugs in the software being verified. Therefore, this dissertation advocates for foundational end-to-end verification, a proof-based software development method that can mitigate both of these concerns:\r\n\r\nIt is end-to-end in the sense that the correctness proofs of individual components are used to discharge the assumptions of adjacent components throughout the whole stack, resulting in end-to-end theorems that only mention the top-most and bottom-most specifications, so that bugs in intermediate specifications cannot invalidate the soundness of the end-to-end statement anymore.\r\n\r\nThe method is foundational in the sense that the soundness of the proofs relies only on the foundations of mathematics and on the correctness of a small proof-checking kernel, but not on the correctness of other, domain-specific verification tools, because these tools are either proven correct once-and-for-all, or they output proofs that are checked by the kernel.\r\n\r\nEnsuring that all the reasoning can be checked by the same small foundational kernel requires considerable effort, and the first part of this dissertation presents techniques to reduce this effort:\r\n\r\nOmnisemantics, a new style of semantics that can be used instead of traditional small-step or big-step operational semantics, offer a smooth way of combining undefined behavior and nondeterminism, and enable forward-simulation compiler correctness proofs with nondeterministic languages, whereas previous approaches need to fall back to the much less convenient backward simulations if support for nondeterminism is needed.\r\n\r\nLive Verification is proposed, a technique to turn an interactive proof assistant into a programming assistant that displays the symbolic state of the program as the user writes it and allows the user to tweak the symbolic state as long as the tweaks are provably sound. An additional convenience-improving feature is that instead of stating lengthy loop invariants, the user only needs to give the diff between the symbolic state before the loop and the desired loop invariant, resulting in shorter and more maintainable annotations. Finally, in order to make Live Verification practical, a number of additional proof techniques is presented.\r\n\r\nThe second part of the dissertation shows how these techniques were useful in three collaborative case studies: An embedded system running on a verified processor with an end-to-end proof where the software-hardware interface specification cancels out, a cryptographic server with an end-to-end proof going from high-level elliptic-curve math all the way down to machine code, and a trap handler to catch unsupported-instruction exceptions whose correctness proof combines program-logic proofs about C-level functions, a compiler correctness proof, and proofs about hand-written assembly.",
        "authors": [
            "Samuel Gruetter"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158951",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Using AI to Improve Price Transparency in Real Estate Valuation",
        "abstract": "This thesis explores the integration of artificial intelligence (AI) into real estate valuation, focusing on visual property attributes to enhance traditional Hedonic models. By incorporating Vision Language Models (VLMs) and generative AI, the research evaluates the potential of these technologies to assess non-standard variables like aesthetic appeal, condition and cohesiveness of interior and exterior property photos. The study contrasts traditional hedonic regression models, which rely on quantifiable factors such as square footage and location, with a new approach that includes AI-generated scores derived from property photos. The study employs three distinct models: the No_Rubric Model, the Composite Model, and the Verbose Model with the Hedonic model serving as the baseline for evaluating their performance. The results demonstrate that incorporating visual data significantly improves model\r\naccuracy, aligning valuations more closely with buyer preferences and sold prices. This shift addresses the industry's need for price transparency and highlights how developers can design properties that better meet market demands.",
        "authors": [
            "Cunjia Xu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158862",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Investigation of Long-timescale Behavior of Positive DC\r\nStreamer Coronas",
        "abstract": "Positive DC streamers are filamentary low-temperature discharges that are relevant to many applications, including sterilization, ionic wind generation, agriculture and atmospheric electricity. Even when excited by a DC voltage, streamers in atmospheric-pressure air typically self-pulsate with a frequency of several kilohertz. The generally-accepted explanation for DC streamer self-pulsation is that it is driven by recovery of the electric field near the tipped anode, due to electrostatic removal of ionic space charge from the inter-electrode gap over inter-pulse timescales. However, this theory has not been validated, either experimentally or numerically. Most prior works investigating DC streamers have focused on the streamer propagation phase (a few tens of nanoseconds) - few have investigated longer timescales, including the bridging of the electrode gap by the streamer and the subsequent current pulse (hundreds of nanoseconds) and the period in-between streamer pulses, leading up to initiation of the next streamer discharge (hundreds of microseconds). The work presented in this thesis focuses on investigation of the longer timescales of positive DC streamer development in a tip-to-plane geometry, in particular beyond the streamer propagation phase, through the current flow and inter-pulse phases. This begins with an experimental study to measure the long-timescale development of the electric field inside a streamer corona using the E-FISH laser diagnostic technique. This shows some surprising results, which do not seem to be consistent with the theory of DC streamer selfpulsation being driven by electric field recovery at the anode. The near-anode electric field is not observed to recover during the inter-pulse period - instead, the near anode behavior seems to be dominated by a persistent glow discharge and a curious wave-like feature is observed in the electric field, traveling towards the anode on ionic timescales. This is followed by the development of a 1.5D reduced-order numerical model of a DC streamer, which is optimized for solving over long timescales via a ‘triple-stack’ of transient solvers. The model is able to fully resolve the boundary sheath layers of the plasma and is able to capture detailed behavior of the cathode sheath development during bridging via the use of a kinetic flux boundary condition for the charged species. This model is firstly applied to modeling the bridging and current flow phases of streamer development, and its prediction shows a good qualitative match to the behavior of the experimental current pulse. Parameter sweeps show that the streamer current pulse is sensitive to the assumed radial behavior and the rate of electron-ion recombination, but insensitive to the applied boundary conditions or secondary emission. The final section describes an extension of the 1.5D streamer model to simulate the streamer inter-pulse phase and initiation of a second streamer. It is shown that initiation of a second streamer can be predicted by a fluid model and that radial expansion of positive ions plays an important role; however, it has proven difficult to integrate that effect into the 1.5D model. The model results are consistent with streamer self-pulsation being due to electric field recovery; however, comparison with the results of the E-FISH experiment suggest there may be different mechanisms driving positive DC streamer self-pulsation, depending on the presence or not of a glow discharge on the anode.",
        "authors": [
            "Lee R. Strobel"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158816",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Computational Methods to Improve Satellite Attitude Determination and Control with a Focus on Autonomy, Generalizability, and Underactuation",
        "abstract": "The attitude determination and control system (ADCS) onboard a satellite uses sensors to measure orientation and angular velocity, enabling the satellite to manage angular momentum, counteract disturbances, and point in the desired directions. Many historical ADCS approaches are designed for constant pointing goals, high accuracy sensors, powerful actuators, or larger, high-inertia satellites. Many modern satellites are small satellites (tens of kilograms or less), with lower-cost actuators and sensors, and may have more complicated attitude goals. This dissertation presents a variety of computational approaches to improve ADCS performance by leveraging detailed satellite dynamics modeling and estimation, disturbance inclusion, and trajectory planning–all optimized for efficient onboard computation suitable for small satellites. The proposed framework generalizes ADCS operations, allowing it to adapt automatically to different satellite types, mission requirements, and operational goals, reducing reliance on predefined ground-based commands. This framework can be used in place of standard control laws to make ADCS more autonomous and “hands-off,” calculating its own slews and desaturation while meeting pointing goals, even in cases of underactuation or large disturbances. This generalized and autonomous framework is a contribution of this work, alongside each of its components, which can be individually used in their own right. One key component of this work is a generalized state estimator that integrates a dynamic model of the spacecraft. This estimator demonstrates high accuracy across various satellite configurations, achieving angular error as low as 0.01◦ in low Earth orbit (LEO) with highquality sensors (but no star trackers), compared to the typical 1◦ error of conventional methods. The estimator can account for biases, sensor errors, and external disturbances, ensuring robust performance (e.g., 0.1◦ error in LEO) even with lower-quality sensors (MEMS gyroscopes, plus magnetometers and sun sensors). This adaptability highlights the increased autonomy of the system, as it requires minimal human intervention to maintain high accuracy across diverse mission scenarios. Another major contribution is the integration of disturbance modeling into control laws. By accounting for disturbances directly (either individually or as an all-in-one value tracked by the estimator), rather than through reactive measures like integral control, the proposed methods improve stability and performance, particularly for underactuated systems–improving pointing accuracy by up to 20 degrees. The developed control laws are adaptable to various actuator configurations, disturbance environments, and pointing objectives. This flexibility extends to modifying pointing goals, such as aligning specific vectors rather than requiring a fully specified orientation, enhancing mission adaptability. This work also implements a novel trajectory planning method that generates efficient pointing trajectories for both constant and time-varying goals. The method, based on the Augmented Lagrangian iterated-LQR (ALTRO) approach, creates sequential mission trajectories that optimize performance even under underactuation or disturbance conditions. The planned trajectories are followed by two types of robust closed-loop controllers, applicable across satellite architectures ranging from large weather satellites to 3U CubeSats. By enabling onboard trajectory planning and adaptive control adjustments, this method significantly reduces the need for ground-based planning and interventions, further advancing autonomous operation. The combined framework of estimation, disturbance-aware control, and trajectory planning achieves significantly higher accuracy than traditional ADCS approaches. This enables the use of commercial off-the-shelf components in high-performance missions, overcoming the limitations of low-cost sensors and actuators. The proposed methods allow satellites to operate with weaker or fewer actuators, such as magnetic-only control, while still achieving precise pointing, thereby expanding the feasibility of more autonomous, robust, and cost-effective satellite operations.",
        "authors": [
            "Patrick McKeen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158874",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Non-orthogonal multiple access using guessing random additive noise decoding aided macrosymbols",
        "abstract": "We propose guessing random additive noise decoding-aided macrosymbols (GRANDAM) as a nonorthogonal multiple access (NOMA) method that can detect, error correct, and decode multiple users in multiple input multiple output (MIMO) systems that involve imperfect channel estimation, symbol-wise asynchronous transmission, and interference. GRAND-AM is a NOMA method that uses both joint multiuser detection and joint error correction decoding to handle multiple access interference (MAI) from the users of interest. Our method avoids codebook design and iterative decoding techniques, which are associated with other commonly researched NOMA techniques. We introduce the concept of a macrosymbol, which is constructed from the combination of all user symbols, for the joint detection component of GRANDAM. For the error correction decoding component, we introduce multiple access channel (MAC) codes, which are codes that are used to split the channel rate between users and correct errors due to the MAI. Each user has their information bits encoded with independent MAC codes, which can be short, low rate linear codes such as cyclic redundancy check (CRC) codes or space time codes such as the Alamouti code. We use a soft detection variant of GRAND, a near maximum likelihood (ML) universal decoding algorithm that inverts noise effect sequences from a sequence of symbols to arrive at a codeword, to correct the received sequence of macrosymbols, and ensure that all user codebooks are simultaneously satisfied in the joint decoding process. We show that the methodology of using joint detection and joint decoding at the receiver leads to lower error rates compared to an individual detection and decoding technique, and has comparable performance to an orthogonal multiple access (OMA) system with a similar code rate and length.",
        "authors": [
            "Kathleen Yang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158962",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design and Testing of a Hovercraft with Electroaerodynamic Propulsion",
        "abstract": "Electroaerodynamic (EAD) multistaged ducted (MSD) thrusters are a novel solid-state thruster architecture that has been shown to provide order-of-magnitude improvements in thrust density compared to single-stage EAD thrusters. This makes MSD thrusters well-suited for use in EAD hovercraft, where generating sufficient pressure is crucial for hovering. This study explored the feasibility of a wire-to-airfoil corona discharge MSD thruster powered hovercraft through a scaled-down prototype and final design. The hovercraft was tethered to a ground-based power supply and carried a payload mass to simulate having on-board power electronics to limit the scope of the project. The design of an EAD hovercraft involved applying the principles of hovercraft lift to a design optimization that implements the recently developed EAD MSD thruster model. A hovercraft prototype was designed and constructed to validate the models applied during the design phase and to test hovering capabilities without a payload. Using the manufacturing lessons and insights gathered in the prototype testing, a full-scale model was designed and built to hover while having an additional payload capacity that would be representative of a set of power electronics.",
        "authors": [
            "Matthew Quiram"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158851",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Using Systems Architecture and the EVDT Framework\r\nfor Monitoring Methane Emissions in Rio de Janeiro",
        "abstract": "Methane is a powerful greenhouse gas that has important implications for climate change. Over the past decade, satellites have rapidly improved their ability to detect this gas from above the atmosphere. This Thesis uses two Systems Engineering frameworks, Systems Architecture and EVDT, to examine a case study of methane monitoring in Rio de Janeiro, Brazil. Data from one of these novel satellite systems, GHGSat, is taken over the Seropédica landfill near the city, and compared to Rio’s own IPCC- and GPC-derived greenhouse gas inventory. This is followed by a participant observation in the summer of 2024 involving interviews, discussions, and site visits. A near-doubling of methane was observed over Seropédica, raising questions about the cause of this increase. The direct engagement with Stakeholders provided by this study contributes to a literature gap in satellite monitoring of urban landfills in southeastern Brazil.",
        "authors": [
            "Frederick Henry Oladimeji Ajisafe Jr."
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158786",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Evaluating Chongqing Tiandi Project: An Asset Management Perspective",
        "abstract": "This thesis uses the Chongqing Tiandi project as a case study to analyze the entire process of development and asset management for large-scale urban renewal projects in China's second-tier cities. It focuses on the motivations and outcomes of Shui On Land's transition from an asset-heavy to an asset-light model. Based on theoretical analysis (Chapter 2), corporate-level financial analysis (Chapter 3), and project-level in-depth studies and interviews (Chapter 4), the thesis explores the logic and impact of this strategic transformation from multiple perspectives. The theoretical analysis summarizes real estate lifecycle management theory, portfolio theory, and corporate strategic transformation theory, providing a framework to examine Shui On Land's strategic decisions. The financial analysis reveals that, from 2015 to 2017, Shui On Land faced significant financial pressure with high debt ratios and cash flow constraints, necessitating systematic asset disposals. While the company disposed of multiple assets during this period, Chongqing Tiandi's 79.2% equity disposal was particularly strategic due to its position as a high-risk, low-return asset within the company's portfolio. The project-level analysis and interviews demonstrate that replicating successful development models from first-tier cities in second-tier markets faces unique challenges. In Chongqing Tiandi's case, these challenges manifested in multiple ways: limited residential price premiums due to local land supply policies, substantial investment requirements for super high-rise developments exceeding $1 billion, and persistently low office rental rates in the local market. These factors compromised the project's financial self-sustainability and made it particularly vulnerable in Shui On's portfolio, especially when compared to projects in other second-tier cities like Wuhan. The development and subsequent equity sale of Chongqing Tiandi not only provided essential financial support for Shui On Land but also reflected a strategic decision to divest from a project where market conditions created both immediate challenges and future uncertainties. This research provides valuable references for the development of large-scale projects in China's second-tier cities, emphasizing the need for developers to utilize funds efficiently, adapt flexibly to market changes, and focus on achieving long-term value. These insights hold significant implications for sustainable development in complex market environments.",
        "authors": [
            "Junsi Yang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158890",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "For and Beyond the Plaques: Sustainable Certification Adoption\r\n and Its Impact on Real Estate Decision-Making in the Boston-Cambridge Market",
        "abstract": "As demand for green and healthy buildings grows, real estate developers face complex decisions regarding building certification adoptions, which have become influential in real estate market dynamics. This thesis investigates how developers in the competitive Boston-Cambridge area navigate the sophisticated certification landscape—focusing on LEED, ENERGY STAR, WELL, Fitwel, and WiredScore/SmartScore—to gain competitive advantages, attract and retain tenants, maximize financial performance, and align with regulatory requirements and ESG goals.\r\nUsing a mixed-methods approach, including quantitative analysis of certification overlaps and trends, along with qualitative insights from industry interviews, the study provides a comprehensive understanding of how real estate developers strategically use certifications to influence asset value while meeting tenant and investor expectations. Findings offer potentially actionable insights into how certifications shape market positioning and inform the decision-making process in real estate development.",
        "authors": [
            "Shenglin Huang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158865",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On the nature and measurement of variational bias: a developmental perspective",
        "abstract": "Natural selection cannot work with imaginary phenotypes, only those realized by developmental systems. The observed diversity of life on Earth occupies only a subset of conceivable forms in the absence of selection. This is because of the non-linear and discrete nature of genotype-to-phenotype maps as an outcome of the developmental system. Despite that, it is widely accepted in population and quantitative genetic modelings that the phenotypic production from random mutations is isotropic and uniform. Conventional methods linking genetic variants and phenotypic variation often assume that the origin of phenotypic variation is purely due to genetic and environmental factors. Here, in this thesis, I adopt a developmental causation view which proposes that patterns of variation may emerge as an inherent consequence guided by physico-chemical principles and that part of the nature can not be fully reducible to genetic factors. The distribution of phenotypic variants that arise from genetic and environmental variation is influenced by the developmental processes that transform the embryonic phenotype into the adult form. This developmental process is subject to constraints that stem from the structure, character, composition, or dynamics of development. We term such a constraint as developmental bias. Despite the prevalence of developmental bias, detecting and testing its role remains a challenge. To address this gap, in the thesis, I propose frameworks and showcase examples aimed at identifying developmental bias and testing its implications in shaping phenotypic evolution. Specifically, I answer three questions: (1) How does the central conponent of nonlinear genotype-to-phentype map --- transcriptional regulation --- bias the analyses of gene-gene interactions? (2) How to disentangle the contribution of developmental bias in trait-trait interdependencies? (3) How expression variability affects gene retention and gene expression evolution following gene and genome duplication.",
        "authors": [
            "Haoran Cai"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158887",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Near-Optimal Learning and Planning in Separated Latent MDPs",
        "abstract": "We study computational and statistical aspects of learning Latent Markov Decision Processes (LMDPs). In this model, the learner interacts with an MDP drawn at the beginning of each epoch from an unknown mixture of MDPs. To sidestep known impossibility results, we consider several notions of δ-separation of the constituent MDPs. The main thrust of this paper is in establishing a nearly-sharp statistical threshold for the horizon length necessary for efficient learning. On the computational side, we show that under a weaker assumption of separability under the optimal policy, there is a quasi-polynomial algorithm with time complexity scaling in terms of the statistical threshold. We further show a near-matching time complexity lower bound under the exponential time hypothesis.",
        "authors": [
            "Fan Chen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158934",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "thesis in the field of Chemical Oceanography: Marine iodine biogeochemistry: inorganic speciation, redox dynamics and organic complexation",
        "abstract": "Iodine holds significant importance across various disciplines, including medicine, industrial processes, organic synthesis, paleoclimatology, atmospheric chemistry and modern climate science. The ocean, as a major surficial iodine reservoir and the primary source of this element to the atmosphere, plays a central role in global iodine cycling. Despite significant progress, key aspects of iodine cycling in the marine environment remain poorly understood. This thesis leverages recent advances in high-precision techniques, including liquid chromatography and mass spectrometry, to enhance our understanding of marine iodine biogeochemistry. Detailed analyses of the major inorganic iodine species in seawater, iodide and iodate, were conducted in the oligotrophic waters of the North Pacific and the oxygen minimum zones of the Eastern Tropical Pacific. The observed distributions reflect the impact of both in situ and ex situ processes on dissolved iodine concentrations, offering valuable insights into the prevalence and extent of anoxic conditions within oxygen minimum zones. Iodate formation rates were investigated through surface seawater incubations using iodide-129, a long-lived radioisotope, as a tracer. The experimental results underscore the pivotal role of particles in mediating redox transformations between iodide and iodate, while also emphasizing the significance of iodine species with intermediate oxidation states in these processes. Building on this observation, a significant focus of this thesis is the characterization of dissolved organic iodine in the ocean. Two innovative methodologies for identifying dissolved organic iodine compounds are presented. The first approach focuses on labelling cultures of the cyanobacterium Synechococcus with iodide-129 to generate a diagnostic isotopic pattern in resultant dissolved organic iodine complexes. The second approach employs sequential purification and isolation of a target compound from a large-volume seawater sample collected in the North Pacific. Collectively, the findings presented in this thesis significantly enhance our understanding of iodine cycling in the marine environment, offering novel insights into the distribution and composition of both inorganic and organic iodine, as well as the rates and dependencies governing iodine cycling processes. Furthermore, the methodologies introduced here pave the way for future research to elucidate the mechanisms driving iodine redox transformations in seawater, refine the marine distribution of inorganic iodine, and advance the molecular characterization of dissolved organic iodine.",
        "authors": [
            "Iulia-Mădălina Ștreangă"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158819",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Mechanisms of terrestrial organic carbon export and preservation in the marine environment",
        "abstract": "Export of terrestrial carbon from land to sea is a globally important carbon flux that is poorly constrained and has implication for atmospheric carbon levels over modern and geologic timescales. Many factors control the fate of exported carbon and the subsequent impact on carbon budgets, including the timescales of export, the composition of organic matter, and degradation processes. This thesis uses biomarkers, bulk geochemical tools, and incubation studies to interrogate the factors controlling terrestrial carbon export and preservation in the marine environment. The thesis focuses on two globally important river systems that collectively deliver 25% of the total terrestrial carbon flux to the ocean, the Ganges-Brahmaputra (G-B) Rivers and the Amazon River. The first two chapters focus on the G-B Rivers, utilizing compound specific biomarker analysis within a high sedimentation rate (30 cm/yr) terrestrial archive in the Bay of Bengal, we interrogate (i) timescales of organic carbon export from land to sea, and (ii) basin-scale geochemical responses to rice agriculture expansion. These analyses utilize the radiocarbon ages and stable carbon-13 isotopic composition of lipids produced by Archaea and Bacteria. We identify that ca. 75% of these biomarkers experience millennial scale storage in the G-B basin, in agreement with previously assessed plant-derived compounds, highlighting that an overarching soil stabilization mechanism controls the age of exported terrestrial organic matter. Individual biomarkers and bulk geochemical analysis chronicle the change in methane-derived soil carbon within the basin due to rice paddy expansion, highlighting that 49% of Bangladesh’s methane emissions from 1990-2008 have been abated by soil storage. The last two chapters focus on the Amazon River, to examine the fate of terrestrial organic carbon in the marine environment, (iii) utilizing geochemical analysis of historical sediments and sediments from a field campaign in 2023, and (iv) utilizing terrestrial and marine endmembers in incubation experiments simulating the dynamic coastal environment. Sediment geochemical and biomarker analyses highlight the preservation of an isotopically distinct terrestrial endmember in the coastal sediments, which has led to at least 50% underestimation of the burial efficiency. Quantitative stable isotope probing incubations using 13C-lignin indicate the dual role of microbially-mediated and photo-degradation, and highlight that the microbial communities primarily responsible for lignin degradation in the marine environment are of terrestrial origin, and identify a new ecological role for Bathyarchaeota. This thesis integrates diverse biogeochemical techniques across the terrestrial-marine interface to examine important open questions in globally important carbon budgets, merging isotope geochemistry, microbiology and earth science. The findings contribute to our understanding of the modern carbon cycle and the impact of anthropogenic perturbations of the last decades and into the future.",
        "authors": [
            "Brenna L. Boehman"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158872",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design Concepts for High-Acceleration Linear Actuators\r\nfor Precision Motion",
        "abstract": "Advances in semiconductor photolithography scanners have made it possible to produce smaller, more affordable chips with higher throughput. Some of the key lithographic scanner components supporting these advancements are electromagnetic actuators responsible for positioning the long-stroke (LS) and short-stroke (SS) stages of the reticle stage in its scan direction. Such actuators need to provide the highest thrust at the deceleration and reacceleration phases when the stages turn around at the ends of the scanning trajectory. Thus, enhancing their acceleration capability and force output is essential for boosting chip throughput. However, the improved performance may demand large current densities that are unsustainable in terms of the associated power dissipation generated by ohmic losses in the copper coils. In this thesis, we continued a previous study conducted in our lab that explored the use of mechanical contact forces managed by a piezoelectric stack actuator (PEA). In this configuration, intermittent contact by the PEA can be used to apply forces to decelerate and reaccelerate the SS stage with respect to the LS stage during turnaround events. With such force assist, the non-contact precision actuators responsible for positioning the SS stage with respect to the LS stage no longer need to generate large thrusts for the deceleration and reacceleration. As a result, we can in principle decrease the weight and power loss of the SS-stage precision actuators, which thus lowers the thrust requirements for the LS-stageAdvances in semiconductor photolithography scanners have made it possible to produce smaller, more affordable chips with higher throughput. Some of the key lithographic scanner components supporting these advancements are electromagnetic actuators responsible for positioning the long-stroke (LS) and short-stroke (SS) stages of the reticle stage in its scan direction. Such actuators need to provide the highest thrust at the deceleration and reacceleration phases when the stages turn around at the ends of the scanning trajectory. Thus, enhancing their acceleration capability and force output is essential for boosting chip throughput. However, the improved performance may demand large current densities that are unsustainable in terms of the associated power dissipation generated by ohmic losses in the copper coils. In this thesis, we continued a previous study conducted in our lab that explored the use of mechanical contact forces managed by a piezoelectric stack actuator (PEA). In this configuration, intermittent contact by the PEA can be used to apply forces to decelerate and reaccelerate the SS stage with respect to the LS stage during turnaround events. With such force assist, the non-contact precision actuators responsible for positioning the SS stage with respect to the LS stage no longer need to generate large thrusts for the deceleration and reacceleration. As a result, we can in principle decrease the weight and power loss of the SS-stage precision actuators, which thus lowers the thrust requirements for the LS-stage actuators responsible for accelerating both the LS and SS stages, resulting in lowered power consumption. Using the single degree-of-freedom experimental setup previously built in our lab, we conducted several characterization experiments to develop a PEA position feedback controller augmented by a hysteresis-compensated feedforward trajectory to shape the contact compression and forces. We find that introducing a viscoelastic contact interface is essential for stabilizing the PEA controller and slowing the contact dynamics to remain within the controller bandwidth. Our feedforward trajectory successfully brings a 0.84 kg mass moving towards the PEA with an initial speed of 60 mm/s to zero velocity in approximately 1.5 ms using 36 µm of PEA stroke length. These results demonstrate the feasibility of using PEAs as mechanical assist devices for high-acceleration turnaround events in lithography tools.",
        "authors": [
            "Adam K. Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158901",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Developing Telecom Band-Compatible Molecular Color Centers for Quantum Networking",
        "abstract": "Quantum networking is a new modality of information transmission that will revolutionize the future of telecommunications. However, the realization and widespread use of quantum networking demands low signal loss and distortion over long distances. To achieve this, prospective materials for quantum networking must emit in fiber optics’ optical communications band defined as 1260 to 1625 nm, commonly known as the “telecom band.” Vanadium dopants in silicon carbide have demonstrated near-infrared emission combined with a spin-photon interface, but these systems lack tunability over emission wavelength, preventing emission in the telecom band. This thesis combines the promising electronic structure of these dopants and the inherent tunability of molecular systems to create a family of luminescent paramagnetic vanadium complexes that can achieve both telecom band emission and generalized finetuned control over emission wavelength. Chapters 2 and 3 will outline approaches to target telecom band emission in a series of V_III complexes through a gradual and controlled increase of metal-ligand bonding covalency. This strategy culminates in a series of V_III complexes which tune emission wavelength from 1237 nm to 1424 nm, achieving emission into the telecom band. Chapter 4 will discuss the impact of these strategies on the magnetic properties and spin dynamics of these systems through an analysis of their behavior under high-frequency high-field EPR spectroscopy. This work provides a blueprint for the next generation of molecular spins with optical addressability in the near-infrared regime for applications in quantum networking.",
        "authors": [
            "Rianna Bliss Greer"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158936",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Encoder-Agnostic Learned Temporal Matching for Video Classification",
        "abstract": "In recent years, large transformer-based video encoder models have greatly advanced stateof-the-art performance on video classification tasks. However, these large models typically process videos by averaging embedding outputs from multiple clips over time to produce fixed-length representations. This approach fails to account for a variety of time-related features, such as variable video durations, chronological order of events, and temporal variance in feature significance. While methods for temporal modeling do exist, they often require significant architectural changes and expensive retraining, making them impractical for offthe-shelf, fine-tuned large encoders. To overcome these limitations, we propose DejaVid, an encoder-agnostic method that enhances model performance without the need for retraining or altering the architecture. Our framework converts a video into a variable-length temporal sequence of embeddings, which we call a multivariate time series (MTS). An MTS naturally preserves temporal order and accommodates variable video durations. We then learn pertimestep, per-feature weights over the encoded MTS frames, allowing us to account for variations in feature importance over time. We introduce a new neural network architecture inspired by traditional time series alignment algorithms for this learning task. Our evaluation demonstrates that DejaVid substantially improves the performance of a state-of-the-art large encoder, achieving leading Top-1 accuracy of 77.2% on Something-Something V2, 89.1% on Kinetics-400, and 88.6% on HMDB51, while adding fewer than 1.8% additional learnable parameters and requiring less than 3 hours of training time.",
        "authors": [
            "Darryl Ho"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158930",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Annealing Techniques for Color Center Formation",
        "abstract": "Color centers in diamond have emerged as leading atom-like quantum systems for applications spanning from quantum repeaters to sensors. However, the optical and spin properties of engineered diamond color centers are limited by crystal damage produced during ion implantation, crystal irradiation, and annealing. In this thesis, we develop advanced material processing methods and characterization techniques to address critical challenges in the formation of high-performance diamond color centers to advance towards the efficient creation of desired dopant-vacancy centers with minimal formation of deleterious multi-vacancy clusters.",
        "authors": [
            "Ian Christen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158913",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Singlet exciton fission-enhanced silicon photovoltaics: Interfacial engineering, device design and spectroscopic technique development",
        "abstract": "The growing global energy demand combined with resource and space limitations necessitate enhancements in crystalline silicon solar cells, which are the current dominant solar technology. However, their efficiencies have only increased incrementally over the recent 20 years, as they are starting to approach the theoretical efficiency limit. The main source of loss is thermalization, where energy in excess of the bandgap absorbed by silicon is lost as heat. Singlet exciton fission in organic molecules has been proposed to reduce these losses. By having the organic layer absorb the high energy light and transferring the triplet excitons generated from the singlet fission process to silicon, the photocurrent in this spectral region can be doubled, with the potential of raising the efficiency from the traditional limit of 29.4 % to up to 42 %.\r\n\r\nThe greatest challenge with these devices has been to demonstrate an increase in the silicon photocurrent, a necessary condition to show that the technology is viable. Scientifically, there are three main components to this problem. The first is to successfully couple the triplet excitons to silicon. The second is that not much is understood regarding the exciton and charge carrier dynamics at this interface. Finally, the silicon solar cell architecture should also be considered to extract transferred carriers effectively.\r\n\r\nThis thesis tackles these three parts from an interfacial materials, device architecture and spectroscopy approach. Using tetracene as the singlet fission layer and n-doped silicon, we show that defect-induced states in a thin interlayer of hafnium oxynitride that lie near the band edge of silicon are beneficial for triplet exciton transfer. We also identify that triplet-induced electric field-effect passivation is beneficial for the triplet sensitization process of silicon, and design a new bilayer interface consisting of a zinc phthalocyanine donor layer that introduces preferential near- silicon band edge states, and an ultrathin oxide chemical passivation layer. We then study various device architectures, confirming the importance of using a device designed to extract surface charge carriers efficiently, demonstrating the first enhancements in single-junction silicon solar cell external quantum efficiencies and photocurrent from singlet fission. Finally, we build and use advanced spectroscopy techniques and numerical frameworks to study exciton and charge carrier dynamics in singlet fission-sensitized solar cell materials, confirming that the triplet excitons are contributing to all the positive effects observed in the devices.\r\n\r\nThese results have shown that singlet fission-sensitized silicon solar cells are a viable technology for enhancing silicon solar cell efficiencies beyond the conventional single-junction limit. This interface remains a rich area for fundamental scientific studies, involving coupling between molecular dark states to bulk silicon. We hope that the key findings can help direct research efforts towards scalable implementation of this technology, and stress that the fundamental understanding of the interface also has broad implications to other silicon technologies that can benefit from enhanced quantum yields, including photodetectors.",
        "authors": [
            "Narumi Nagaya"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158864",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Use of System Theoretic Process Analysis (STPA) onNovel Tiltrotor Aircraft to Prevent Mode Confusion",
        "abstract": "Initiatives are underway to develop tiltrotor and vertical take-off and lift (VTOL) aircraft that enhance commercial and military aviation’s autonomy, capability, and survivability. These designs integrate rotary and fixed-wing elements, introducing distinct safety considerations. These safety concerns are largely due to the differing mental models of operators trained in either rotary or fixed-wing aviation, alongside the rising reliance on autonomy. The traditional hazard analysis techniques (e.g., Fault Tree Analysis and Failure Models and Effects Criticality Analysis) do not adequately account for system component interactions or human factors in complex new aircraft designs. System Theoretic Process Analysis (STPA) is a powerful new hazard analysis technique for novel tiltrotor aircraft that includes their unique safety requirements. It is a top-down system hazard analysis technique that identifies loss scenarios (N. G. Leveson and J. Thomas Mar2018). It satisfies the tasks described in MIL-STD-882E (Department of Defense 2023). This research demonstrates the use of STPA to identify and mitigate potential instances of mode confusion between the operator’s mental model and the autonomy’s decision logic in the uniquely dynamic tilt-rotorcraft environment. Two previous tiltrotor aircraft accidents are analyzed utilizing Causal Analysis based on System Theory (CAST) to help set a framework for the importance of human and machine collaboration in systems. These accidents show a trend in the dangers of aircraft system mismanagement between various controllers. The CAST results for these accidents help provide information about how to prevent these types of incidents in the future, setting the stage for the use of STPA on novel tiltrotor aircraft, as demonstrated in this thesis. STPA can be used before design, implementation, and fielding, allowing for better early design of systems and reducing the cost of later redesign or modification.",
        "authors": [
            "Natalie Ann Basnight"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158856",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "More than the sum of parts: deconstructing tissues in their spatial, temporal, and environmental contexts",
        "abstract": "The human body is composed of ~37,000,000,000,000 cells, exquisitely organized into tissues delivering emergent functions beyond individual cells’ capabilities (e.g., the brain’s seemingly-effortless computations, the liver’s wide-ranging chemical processing). In my PhD, I studied how healthy tissues arise from properties and interactions of constituent cells, and how disease outcomes stem from dysregulation of underlying cellular parts. 1) To study how cells’ spatial organization shapes tissue function, I created photochemistry tools to discover gradients in how immune cells combat cancer across a tumor’s core vs. periphery. 2) To then explore spatially-structured tissues, I turned to tuberculosis (TB) granulomas: just centimeters apart, the immune system can kill bacteria in one granuloma or permit years-long bacterial survival in another. Reconciling this paradox, I discovered that bacterial killing needs coordinated signaling across immune cells, but TB-permissive granulomas structurally remodel to inhibit TB spread at the expense of “walling out” immune cells. 3) Connecting disease to lifestyle exposures, I determined tobacco smoking increases TB risk via blood-to-lung migration of TB-permissive cells. 4) Intrigued by past stresses seeding future dysfunction, I studied similar themes in adaptations to high-fat diets, discovering tradeoffs where individual liver cells promote their own survival at the expense of reduced tissue function and increased cancer risk. Through these studies, I dissected tissues and diseases with unprecedented resolution via single-cell multi-omics and mechanistic perturbations, defining the parts, interactions, and causal regulators that underlie tissue (dys)function.",
        "authors": [
            "Constantine Tzouanas"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158827",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Designing Visual Intelligence from Photons to Action",
        "abstract": "For embodied agents to perceive and effectively act within their environment, they must sense the world around them and translate this information into meaningful and safe actions; a process fundamental to both biological and human-engineered systems. Nature has evolved highly attuned visual systems, resulting in diverse and efficient eyes capable of facilitating complex behaviors. Conversely, roboticists have engineered sophisticated cameras and sensors, enabling robots to perform tasks beyond the capabilities of natural systems. This thesis explores the design of visual intelligence by integrating insights from both biology and engineering in two complementary parts. In Part I, we computationally recreate the evolution of vision within simulated embodied agents. By evolving the physical and neural aspects of vision in simulation - and training these visually-capable agents with deep reinforcement learning - we demonstrate that task-specific environmental pressures lead to distinct eye morphologies and behaviors, mirroring observations in biological evolution. This in silico approach enables us to investigate the fundamental principles underlying the emergence of animal eyes and provides a framework for exploring novel sensor designs subject to both biological (e.g., survival) and engineering constraints (e.g., manufacturability). In Part II, we leverage visual cues not typically used in nature (i.e., active illumination and multi-bounce light) to demonstrate enhanced robotic navigation via non-line-of-sight imaging. Using single-photon LiDARs, we capture the temporal propagation of individual photons, enabling the detection of objects around corners. This sensing capability allows us to develop robots that effectively anticipate and avoid hidden obstacles, reducing navigation time by 50% and overall trajectory length by 33%. Together, these works demonstrate how the synthesis of biologically-inspired design principles with advanced sensing modalities can enhance embodied agents' capabilities, while providing insights into both natural vision evolution and robotic perception.",
        "authors": [
            "Aaron Young"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158899",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Verification of Go Channels",
        "abstract": "Goose is a tool for translating a subset of the Go programming language into Perennial/Iris, which is an extension of Coq. However, Goose did not support channels, which are an important synchronization tool that Go is well known for.\r\n\r\nThis thesis presents an extension to Goose to support channels, including a model to represent Go channels and operations in GooseLang, the language defined in Perennial/Iris that Goose translates into, an extension to the Goose translator to support channels, and a library of separation logic specifications that define the expected behavior of channel operations on open channels. Finally, this thesis evaluates how effective this model and library is for verifying Go code containing channels, and discuss some limitations and potential future work.",
        "authors": [
            "Jessica Zhang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159079",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "All Pass Readout With Ring Resonators for Qubit Measurement",
        "abstract": "Quantum computers may advance computing by solving some NP complexity problems, such as factoring and simulating quantum systems. Superconducting qubits, configurable artificial atoms comprised of circuit elements, are a leading platform to create quantum computers. Many schemes for superconducting qubit readout include a weakly coupled port as a capacitor in the feedline, which allows for directionality in the readout signal. However, this impedance mismatch creates problems with resonator linewidth variation, standing waves, and voltage nodes in the feedline, leading to challenges in scaling to larger frequency multiplexed systems. This thesis proposes an all-pass readout scheme that utilizes ring resonators that do not require a weakly coupled port, allowing for more modular qubit readout architectures.",
        "authors": [
            "Alicia Zang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159080",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Multimodal Graphical User Interface for 3D Model\r\nFabrication Through Generative AI",
        "abstract": "In recent years, three-dimensional model generation and manipulation through generative AI has seen significant developments. Current projects enable the generation of threedimensional assets from natural language prompts and input images, as well as functionalityaware model manipulation through mesh segmentation and categorization. However, all these workflows lack a coherent, unified platform that caters to users’ needs and each method’s technologies. Programs that rely on terminal-based commands lack the graphics needed for model interactions, and plugin extensions for 3D modeling applications are unintuitive and hard to extend for new functionalities. Additionally, both approaches require users to have prior computer engineering and/or 3D graphics knowledge. For this thesis, I propose the creation of a web-based, multimodal graphical user interface that consolidates all these different technologies in a single platform. By supporting model stylization and model generation (both from text prompts and input images), users can utilize combined workflows and expand the range of output possibilities for 3D asset creation. Other features in our interface include model uploading, saving, and downloading to enable a continuous stream of work on a single 3D asset. Apart from all this, we expand the current capabilities of existing image-to-3D generation programs by enabling users to combine up to six images together and create a merged 3D object. Each of these images corresponds to a view angle from which the outputted mesh will be built.",
        "authors": [
            "Isabel Báez Alicea"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159092",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Beyond Lifetime Value: A Customer Journey Analysis to\r\nFan Engagement and Spending in Professional Sports",
        "abstract": "This research examines engagement and spending behaviors in a professional sports ecosystem, introducing Customer Journey Analysis (CJA) as a dynamic alternative to traditional customer lifetime value (LTV) models. Through an analysis of over 930,000 net new fans acquired from July 1, 2021, to June 30, 2024, this study identifies critical patterns in acquisition channels, spending behaviors, and engagement metrics over multiple seasons. Notably, the findings highlight the significant influence of early touchpoints, such as ticket purchases and email interactions, on fan progression. Metrics like email open rates and multi-channel engagement emerge as strong predictors of future spending, revealing nuanced insights into fan behavior. This research emphasizes the importance of integrating behavioral and financial metrics to sustain fan involvement. By transitioning from static LTV models to a multi-dimensional CJA framework, actionable strategies are proposed for optimizing engagement channels, improving retention, and driving long-term revenue growth. Key findings reveal that predictive modeling and customer segmentation analysis are instrumental in identifying high-potential fans and distinct audience profiles. Tailored retention strategies, including personalized follow-ups and exclusive engagement incentives, address churn risks while fostering ramp-up and loyalty across diverse fan groups. Future work should explore tenured fan behaviors and incorporate diverse data sources, such as in-venue spending and team performance metrics, to deepen understanding of fan evolution across different lifecycle stages.",
        "authors": [
            "Christina Elizabeth Antonakakis"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159093",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Battery Pack Design and Transient Performance Modeling\r\nfor High-Power Legged Robots",
        "abstract": "Legged robotics has recently shifted toward advanced optimization-based control methods, such as Model Predictive Control (MPC), to generate agile and energy-efficient locomotion. By casting the control problem as an optimization task, robotic systems can account for complex robot dynamics and operational constraints, including joint limits and actuator capabilities. However, high-performance maneuvers also demand rigorous consideration of onboard battery constraints. This work presents an empirically derived lithium-ion battery model that captures transient voltage sag and time-dependent internal battery state, enabling more accurate prediction of feasible power delivery. Additionally, a custom high-power battery pack was designed to meet the power demands of the MIT Humanoid, emphasizing power density, safety, and maintainability. Although the work presented in this thesis does not integrate the battery model into a trajectory optimization framework, it establishes the foundation for future research that aims to couple battery and robot dynamics in robot control. Ultimately, this approach will facilitate safer and more capable legged robots by ensuring that planned trajectories respect both physical and electrochemical constraints.",
        "authors": [
            "Christopher K. Evagora"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159094",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On the Inductive Biases of Conditional Diffusion Models",
        "abstract": "Diffusion models have achieved remarkable progress in recent years across various domains and applications, but how diffusion models generalize is still not well understood. While prior work predominantly focuses on unconditional diffusion models, in this thesis we focus on understanding generalization for conditional diffusion models, which is especially relevant for modern text- or observation- conditioned applications. In particular, we are interested in the inductive biases of conditional diffusion models which predispose them to certain forms of interpolation in regions outside the support of the training data. We observe that neural networks are capable of learning qualitatively different forms of interpolation, which may be influenced by the architecture and capacity of the network and other aspects of neural network training. We develop a potential framework to model the interpolation behavior of neural networks via nonparametric estimation, which happens to have the property of being schedule consistent, or truly denoising at every time step. We find that, assuming a neural network with sufficient capacity, conditional diffusion models are biased towards smoothing, which can lead to non-schedule consistent behavior away from the training data and has a number of interesting consequences.",
        "authors": [
            "Christina Yu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159081",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Using Predictive Models to Identify Trends Among Successful Dual-Use Startups",
        "abstract": "This study examines predictive models for assessing the success of dual-use startups in the United States. Utilizing data from the Small Business Innovation Research (SBIR) and Small Business Technology Transfer (STTR) programs, this research focused on startups founded post-2000 to reflect contemporary technological advancements. A key objective of this study was to create a rich and comprehensive dataset, addressing gaps in the dualuse startup literature and providing a foundation for future research. Machine learning approaches, including Logistic Regression, Random Forest, and Gradient Boosting Machines, were applied to evaluate critical success factors, with XGBoost identified as the most effective model. Despite the challenges of class imbalance, the study highlights the potential of data-driven methodologies to uncover trends and inform strategies for supporting dual-use startups. By integrating predictive modeling with the construction of a robust dataset, this research contributes both to the academic understanding of dual-use innovation ecosystems and to practical frameworks for fostering their growth.",
        "authors": [
            "Samantha Ying"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159082",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "ALFA-Chains: An Artificial Intelligence Approach to Exploit Chain Discovery in Networks",
        "abstract": "Exploit chains play a crucial role in advanced persistent threats (APTs) and other malicious cyber campaigns. Sophisticated attackers can navigate across a network, escalate their privileges, and compromise valuable targets by executing the right exploits in the right order. However, finding these exploits chains is a challenging task requiring a broad knowledge of the vulnerabilities present in computer systems and the exploits that take advantage of them. Networks can be complex, with many hosts and intricate software stacks. Moreover, the range of known exploits and vulnerabilities is constantly growing, complicating the process of determining how they can be linked. This thesis introduces a solution, ALFA-Chains, that automates the discovery of exploit chains by leveraging classical AI planning, Large Language Models (LLMs), and existing exploit/vulnerability databases. ALFA-Chains describes networks and exploits using the Planning Domain Description Language (PDDL), a formal language to represent planning problems. This allows us to use optimized off-the-shelf planners that have been developed by the AI planning community over many years. Our system takes natural language descriptions of exploits and classifies them into categories based on their preconditions and effects. From this intermediary representation, we can programmatically generate PDDL that captures the requirements needed to run the exploit and the access gained by the attacker. Due to this automated approach, ALFA-Chains is able to consider a vast set of exploits when determining if a network is susceptible to exploit chaining. We show how ALFA-Chains can process 1,880 Metasploit exploits and their corresponding 2,002 CVEs to detect exploit chains in a variety of realistic network configurations. We proceed to discuss potential applications of ALFA-Chains, including automated penetration testing and vulnerability prioritization.",
        "authors": [
            "Miguel A. Tulla Lizardi"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159083",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Simulating Weather For A Mixed Reality Platform",
        "abstract": "Complex systems are inherently difficult to teach in a traditional classroom setting. The We’re In This Together (WIT) project aims to provide a different teaching strategy by using AR/VR headsets to situate the students directly inside the system. WIT’s first game attempts to tackle common weather concepts including precipitation and fronts; however, the most recent version fails to demonstrate and model the concepts in an accurate and comprehensible way. This project focuses on developing a brand-new simulation layer for the game that better captures the causes behind common weather phenomena. The new simulation uses a particle-based approach to model the movement of air in the atmosphere and creates a more thorough and interactive experience to help students explore the various aspects of weather.",
        "authors": [
            "Hao Ni"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159089",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "GIM: Guidance as Initialization Method",
        "abstract": "This work makes two contributions: the evaluation of early stop guidance for deep Fully Connected Networks (FCNs) and the introduction of guidance as an initialization method (GIM). Network initialization has been a meaningful and challenging topic in the field of machine learning (ML) for a long time. Many initialization methods exist, ranging from data-independent to data-dependent approaches. Initializations allow for a better understanding of model behavior and improvements in model performance. The novel guidance tool enabled us to propose GIM, a new technique that initializes a model by leveraging representational similarity with respect to models of different architectures. A model with an architecture that performs poorly in a specific task can be initialized with guidance from a model with an architecture that performs well in the respective task. We focus on the case of FCNs in the task of image classification and provide experimental results to validate our approach.",
        "authors": [
            "Juan Sebastian Duitama Cortes"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159090",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Convergence of the Arnoldi Iteration for Estimating Extreme Eigenvalues",
        "abstract": "Krylov subspace methods, like the Arnoldi iteration, are a powerful tool for efficiently solving high-dimensional linear algebra problems. In this work, we analyze the convergence of Krylov methods for estimating the numerical range of a matrix. Prior bounds on approximation error often depend on eigenvalue gaps of the matrix, which lead to weaker bounds than observed in practice, specifically in applications where these gaps are small. Instead, we extend a line of work proving gap-independent bounds for the Lanczos method, which depend only on the matrix dimensions and number of iterations, to the more general Arnoldi case.",
        "authors": [
            "Cecilia Chen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159091",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Realistic Tactile Stylization for Digital Fabrication using Enhanced UV Unwrapping Method",
        "abstract": "While recent advances in Generative AI enable visual stylization of 3D models using image prompts, they typically neglect tactile properties. TactStyle addresses this limitation by enabling creators to enhance 3D models with both visual and tactile properties derived from texture images. Using a fine-tuned image-generation model, TactStyle generates highly accurate heightfields that faithfully replicate the tactile properties of input visual textures and applies them to 3D models. However, applying textures to 3D models presents challenges, such as ensuring even texture resolution, avoiding texture warping, and minimizing visible seams. TactStyle’s current implementation often struggles with significant texture stretching and distortion caused by poor UV mapping, compromising the accurate heightfields and diminishing the tactile fidelity of printed models. Our research systematically evaluates various UV unwrapping methods, including alternative UV projections and an optimization-based neural UV mapping, to improve the realism and accuracy of texture application on 3D models in digital fabrication. Building on these findings, we will release a Blender plugin that integrates the optimal UV unwrapping methods with TactStyle, enabling creators to easily customize their 3D models with accurate tactile properties using only reference texture images. This work enhances the practicality and accessibility of tactile 3D model customization, bridging the gap between visual and tactile design elements.",
        "authors": [
            "Zoe Wong"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159088",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Lifting 2D Vision Models into Structured Scene Representations",
        "abstract": "Intelligent agents can leverage structured scene representations capable of capturing object compositionality, affordances, and semantics as a world emulator. However, 3D scene data is limited, rendering supervised and self-supervised methods ineffective. Recent advances in 2D foundation models exhibit remarkable performance and generalization. Concurrently, several works have demonstrated lifting feature maps produced by these models into a 3D feature representation. This thesis further explores how lifting can be effectively employed to construct pixel-level fidelity structured scene representations.\r\n\r\nLearned scene representations such as NeRF and Gaussian Splatting do not support additional functionality besides novel view rendering. The world is compositional: a scene can be described in terms of objects. Correspondingly, we present a lifting solution for efficient open-set 3D instance segmentation of learned scene representations. Compared to previous approaches, our solution is more than an order of magnitude faster and can handle scenes with orders of magnitude more instances.\r\n\r\nToward identifying affordances, we tackle the problem of zero-shot mesh part segmentation. Learning-based mesh segmentation does not generalize due to a lack of diverse mesh segmentation datasets, while traditional shape analysis methods are overfitted to previous benchmarks. We present a lifting solution for mesh part segmentation that overcomes these limitations, showing comparable performance to top-performing shape-analysis methods on traditional benchmarks while exhibiting much better generalization on a novel mesh dataset curated from an image-to-3D model.\r\n\r\nBeyond feature fields, lifting can be used for a variety of applications, including scene understanding and editing. However, current lifting formulations are inefficient and often exhibit additional unintended modifications. To address these deficiencies, we generalize lifting to semantic lifting, which incorporates per-view masks indicating relevant areas. These masks are determined by querying corresponding per-view feature maps derived from feature fields. However, it is impractical to store per-view feature maps, and the scene representations can be expensive to store and query. To enable lightweight, on-demand retrieval of pixel-aligned relevance masks, we introduce a Vector Quantized Feature Field. We demonstrate the effectiveness of semantic lifting with our method on complex indoor and outdoor scenes from the LERF dataset.",
        "authors": [
            "George Tang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159118",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Transformers as Empirical Bayes Estimators The Poisson Model",
        "abstract": "We study the ability of transformers to perform In Context Learning (ICL) in the setting of Empirical Bayes for the Poison Model. On the theoretical side, we demonstrate the expressibility of transformers by formulating a way to approximate the Robbins estimator, the first empirical Bayes estimator for the Poisson model. On the empirical side, we show that transformers pre-trained on synthetic data can generalize to unseen prior and sequence lengths, outperforming existing methods like Robbins, NPMLE, and ERM monotone in efficiency and accuracy. By studying the internal behavior of the representations of the intermediate layers of these transformers, we found that the representation converges quickly and smoothly over the layers. We also demonstrate that it’s unlikely transformers are implementing Robbin’s or NPMLE estimators in context.",
        "authors": [
            "Mark Jabbour"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159119",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Enabling Semantically Grounded, Long Horizon Planning\r\nand Execution for Autonomous Agents",
        "abstract": "Robots have been playing an ever increasing role in complex environments, often in coordination with teams of systems or humans. Autonomous systems of the future will need to be tightly grounded in the real world, drawing information directly from their environment to develop an understanding of the world. They will need to maintain a semantic understanding of their environment, including the kinds of objects they observe and their relationships to each other. At the same time, they must be able to reason over diverse constraints related to their tasks, such as time limits and resource usage. While there are existing approaches which enable robots to execute tasks with semantic goals, such as finding a certain type of object in a room, they often fail to consider the multitude fo task specific constraints which are vital to robust performance. On the other hand, planners which consider task specific constraints require a human to provide all information about the environment manually. These systems are too cumbersome to model complex tasks, requiring hours of manual effort which is prone to errors. This thesis presents an architecture for semantically grounded planning which leverages the strengths of constraint based planners while automating the environmental modeling step with an advanced semantic perception engine. By automating environmental modeling, we are able to create a system which executes complex semantically grounded tasks such as navigating to certain objects within a certain room, without major user input which is typically required of these systems.",
        "authors": [
            "Lucian Covarrubias"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159120",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Strategizing against online learners in normal form repeated\r\ngames",
        "abstract": "With the advent of machine learning and AI, learning algorithms are becoming more and more prevalent in online learning settings, where sequential decision-making is required. In such settings, the decisions of each agent can affect the utilities (or losses) of the other agents, as well as influence the decisions made by other agents later on in the interaction. Therefore, if an agent is good at anticipating the behavior of the other agents, in particular how they will make decisions in each round as a function of their experience thus far, he could try to judiciously make his own decisions over the rounds of the interaction so as to influence the other agents to behave in a way that ultimately benefits his own utility. In this thesis, we study repeated two-player games involving two agents: a learner, which employs an online learning algorithm to choose his strategy in each round; and an optimizer, which knows the learner’s utility function, parameters and the learner’s online learning algorithm. The optimizer wants to plan ahead to maximize his own utility while taking into account the learner’s behavior. We study this setting in zero-sum and general-sum games. In zero-sum games, we provide algorithms for the optimizer that can efficiently exploit a learner that employs a specific online learning algorithm in discrete and continuous-time dynamics. Specifically, the learner employs the Multiplicative Weights Update (MWU) algorithm for the discrete-time games, and the Replicator Dynamics in the continuous-time games. In general-sum games, we provide a negative result. Our negative result shows that, unless P=NP, there is no Fully Polynomial Time Approximation Scheme (FPTAS) for maximizing the utility of an optimizer against a learner that best responds to the history in each round. We additionally provide exponential-time algorithms that efficiently strategize against a learner that uses MWU, as well as a new way of thinking about strategizing against online learners via calculus of variations.",
        "authors": [
            "Angelos Assos"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159121",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design and Implementation of a Nonblocking Randomized Work Stealing Scheduler",
        "abstract": "This thesis presents FLCN (Free of Locks, Cilk is Now), a nonblocking work-stealing runtime scheduler that supports Cilk multithreaded programming. The existing OpenCilk runtime system uses lock-based synchronization and thus suffers from lock contention, does not provide progress guarantees, and can experience performance degradation with high worker counts and in multiprogrammed scenarios. FLCN leverages the existing runtime system’s provably efficient scheduling algorithm and introduces several new data structures and concurrency protocols to form a correct and performant lock-free system. In addition to enabling fork-join task parallelism, FLCN supports other Cilk features such as reducer hyperobjects. Through analyzing the performance of FLCN on various canonical benchmark programs, I find that for programs with low amounts of work, FLCN performs worse than the existing runtime. However, for most programs, I find that FLCN is either competitive with or marginally outperforms the existing runtime. Additionally, FLCN consistently exhibits higher scalability than the existing runtime, performing especially better when using hyperthreads and in multiprogrammed environments. I also outline future work that could make FLCN a more comprehensive and performant system, including ideas for improving FLCN’s work efficiency that would in turn better its performance on programs with low amounts of work.",
        "authors": [
            "Sabiyyah Ali"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159144",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Productivity in the Workplace for Product Development Teams",
        "abstract": "Productivity is a measure of the value generated for every hour worked. In a product development team, productivity can be affected by endogenous and exogenous factors, such as biological rhythms, work style, availability, work interruptions, team size, location, and the management strategies taken in a project. These factors will have an effect on the amount of effective work value generated in a workweek.\r\n\r\nA mathematical model and a Monte Carlo simulation were used to quantitatively assess the impact of these factors on the estimated cost and duration of a product development project. Based on the model results, we determined that workweek capacity and interruptions in the workplace are central to productivity. In addition, we demonstrated that combining different management strategies could be used to bring the project back on schedule and within budget to reduce the effects of these inefficiencies due to diverse endogenous and exogenous factors.\r\n\r\nFor these reasons, this case study on a product development project will provide insight to engineering managers and project leaders about the effects of these inefficiencies in the workplace. The findings will help pave the way toward a more accurate project estimation and better modeling of project dynamics to reduce the amount of uncertainty in product development teams.",
        "authors": [
            "Jorge Farfan Perdomo"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159145",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Digital Thread Maturity in Manufacturing: A Cross-Industry Study Using the Model-Based Enterprise Capability Assessment Framework",
        "abstract": "Modern-day manufacturing organizations find themselves in volatile and competitive markets with increasing pressure to deliver products faster, at lower cost, and with increased quality. In response to this pressure, many organizations are considering how technological advancements may improve the efficiency of their product development operations. Leading organizations have digitally transformed their businesses by shifting away from manual processes, static documents, and siloed operations toward automation, model-based data, and interconnectivity enabled by a digital thread. Accordingly, organizations pursuing the competitive edge offered through the digitalization of their business operations have often used different assessment tools to benchmark their current capabilities and define their vision for the future of their organizational operations.\r\n\r\nThis thesis proposes a set of model-based and digital thread capabilities that are central to the long-term success of product development operations, along with a corresponding maturity model that may be used to identify gaps between current- and future-state capability implementation. Using the proposed capability maturity model, known as the Model-based Enterprise Capability Assessment Framework (MECAF), this study evaluated and compared capability maturity across various organizations in the Aerospace and Defense, Automotive, and Heavy Machinery industries. Through interviews with each participating organization, this thesis also explores the expected benefits, common challenges, and anticipated value of implementing model-based capabilities. Additionally, this thesis proposes an approach to bridging the gap from strategy to implementation based on the lessons learned and best practices of the organizations studied.",
        "authors": [
            "Michael Scott Peters"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159146",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Detecting Expertise Influence on Teamwork in Sustainable Urban Design Workshops through a System Model",
        "abstract": "The design of sustainable urban communities near transportation hubs, such as train stations, may play a vital role in enhancing neighborhoods by fostering new jobs, encouraging mixed-use developments, and promoting a cleaner environment. The engagement of experts and non-experts is often promoted as part of the urban planning process, yet workshops, while motivating, do not necessarily affect the systems design and long-term sustainability of the neighborhood in a substantive way.\r\n \r\nPrior studies present methods for detecting teamwork during the design of complex systems, including model-based co-creation and urban design workshops. While interactive model-based workshops promote increased engagement of non-experts, the traditional role of experts in framing the design options and the workshop dialogue remain. This thesis research seeks to examine how expertise shapes decision-making in urban sustainability contexts using enhanced system models. \r\n \r\nThe research approach focuses on sustainable urban design workshops for compact city development, following three key steps.  First, a neighborhood system model incorporating a commute flow simulator is developed to support collaborative exploration and design decision-making processes. Second, during a pilot experimental workshop, participants are divided into control and treatment groups, challenged to design a vibrant community with economic, social, and environmental benefits. The treatment group receives an expert-proposed, advocated solution to assess its impact on exploration and decision-making. Finally, results are analyzed using Large Language Models (LLMs) and statistical methods to assess how expert-driven solutions impact teamwork collaboration, decision-making speed, and final design alignment with the advocated solution.\r\n\r\nWhile the pilot workshop primarily serves to validate the approach and test the methodology, conclusive results cannot be drawn due to its exploratory nature. Nevertheless, this research successfully developed a robust urban design system model, enabling stakeholders to generate innovative solutions that foster a thriving community. Additionally, it established a methodology to advance the understanding of expertise in teamwork dynamics, laying a strong foundation for future studies in teamwork analysis and urban design challenges.",
        "authors": [
            "Chen Li"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159136",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Scalable and Sustainable Microwave Power Beaming to\r\nMobile Lunar Surface Assets",
        "abstract": "Lunar missions are hindered by the challenges of maintaining continuous operation, especially during the 14-day lunar night, when solar power sources may be unavailable, causing significant mission delays and limiting efficiency. Frequent returns to charging stations supplied by fixed lunar surface power plants further disrupt workflows and restrict the operational range of lunar vehicles. To address these issues and enhance lunar mission performance, a continuous, secure, and shareable power source is essential. While nuclear power and larger battery systems are viable options for continuous lunar energy supply, they pose challenges such as safety risks, complex deployment, and limited scalability. This thesis focuses on exploring microwave-beamed power systems as a flexible and scalable solution for sustained lunar operations. Ideally, the power source would enable 24/7 operations without requiring vehicles to return to base stations, allowing for unrestricted navigation across the lunar surface, including in permanently shadowed regions (PSR). In addition, it would support the construction of critical infrastructure, accelerating the development of the lunar economy. This thesis aims to support sustained lunar exploration and infrastructure development by exploring the design space for microwave-beamed power systems under three different demand use cases of increasing scale, loosely corresponding to the three phases of the Artemis program: Local (Shackleton Crater), Regional (navigation between equatorial regions and South Pole), and Global (entire lunar surface). A case study focused on the YUTU-2 lunar rover investigates alternative architectures for each use case, comparing power beaming from tall towers vs. satellites. Evaluation reveals that the most effective solution for the Local use case is a tower-based approach featuring a single 100m tower, >10,000 solar modules, and using 1 GHz operating frequency, at a cost of $3.4M/W. For the Regional use case, a satellite-based solution is preferred, utilizing 6-7 satellites per plane, 210,000 solar modules, and a frequency range of 1.0 GHz, at a cost of $1.7M/W - $1.8M/W. The Global use case also favors a satellite-based approach, employing 6 satellites per plane across 5 polar planes, with varying numbers of solar modules and utilizing a frequency of 1 GHz, at a cost of $0.8M/W. The trade studies showed that larger receiver antenna areas and lower frequencies improve performance and cost-effectiveness. Furthermore, larger microwave-beamed power systems leverage economies of scale, lowering the cost per watt by an average of $1M/W when scaling from the Regional to the Global power system, with potential for further reductions through future expansions.",
        "authors": [
            "Chu Pang Alex Ng"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159137",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Study of High Harmonic Fast Waves Interactions in the Scrape off Layer of NSTX-U",
        "abstract": "High-harmonic fast wave (HHFW) heating experiments in the National Spherical Torus Experiment (NSTX) at Princeton Plasma Physics Laboratory (PPPL) have shown that up to 60% of the injected power can be lost in the Scrape-Off Layer (SOL) when the fast wave is able to propagate in front of the antenna [Hosea, Phys. Plasmas 15, 056104 (2008))]. This work discusses progress in modeling HHFW propagation and losses in the divertor region using more realistic SOL plasmas in the NSTX-U SOL 2D geometry. Previous RF studies assume density is a function only of magnetic flux, decaying exponentially, which may be insufficient to accurately determine the wavefield, especially in the divertor and high-field side plasma regions. In this work, the temperature profile is first evaluated by solving the non-linear heat conduction equation using a finite element approach in the Petra-M workbench assuming axisymmetry. A 2D density profile is then obtained from a prescribed outer midplane radial profile assuming pressure is uniform on a flux surface. This approach results in density and temperature profiles in which the strong asymmetric nature of diffusion is successfully captured. In particular, it is shown that for a parallel to perpendicular heat conduction anisotropy ratio of up to 10⁸ the expected exponentially decaying temperature profile is obtained using a non-linear iterative solver with proper mesh refinement conditions. Furthermore, this work focuses on investigating the effect of the SOL plasma density profile on the fast-wave propagation at different antenna phasing. The simulation results show that the gradient of the midplane density profile affects the wavefield pattern. As the density profile broadens, the wavefield intensity is reduced in the SOL and increased in the core. Finally, HHFW power in the plasma was studied by adding electron-ion collision power dissipation as a proxy for HHFW power deposition. The simulation results show that increasing the density gap width between the antenna and the core results in more power deposited in the SOL relative to the core.",
        "authors": [
            "Ricardo Antonio De Levante Rodriguez"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159138",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Quantum Economic Advantage Calculator: An Extension of the Quantum Tortoise and Classical Hare Framework",
        "abstract": "For some algorithmic problems, quantum computation has the potential to provide enormous speedups over classical computers. However, the drastic slowdowns associated with running error-free quantum hardware make achieving these theoretical advantages challenging. Researchers and industry leaders planning for the future would benefit from understanding when it will be both feasible and advantageous to switch to quantum computing platforms. This thesis builds on the framework by Choi, Moses, and Thompson (2023) to evaluate the feasibility and timeline for achieving Quantum Economic Advantage (QEA)—the point at which quantum hardware can outperform comparably-priced classical machines for specific computational tasks. This thesis substantially extends and deepens this framework and introduces a calculator to make these analyses accessible. The model incorporates parameters from quantum hardware vendors, such as physical-logical qubit ratios and overall connectivity, alongside the computational complexities of specific problems, to estimate the year of QEA. Most of the parameters in the tool are freely adjustable, allowing users to explore how varying assumptions about quantum improvement and technological advancement influence the projected timeline for QEA.",
        "authors": [
            "Frederick Mejia"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159127",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Causal Representation Learning for Predicting Genetic Perturbation Effects on Single Cells",
        "abstract": "Advances in sequencing technologies have significantly deepened our understanding of gene regulation in cells. Among these, Perturb-seq has emerged as a powerful technique, enabling high-resolution profiling of transcriptomic responses to genetic perturbations at the single-cell level. Such insights have profound implications for functional genomics and the identification of therapeutic targets. This thesis investigates the efficacy of mechanistic computational models for predicting the effects of previously unseen genetic perturbations on cellular expression profiles. While existing deep learning approaches excel at interpolating within observational data, they often struggle to extrapolate to novel perturbations. To address this limitation, this study introduces a hybrid framework that integrates a linear causal model, grounded in the gene regulatory network, with variational deep learning techniques.\r\n\r\nThe proposed mechanistic model utilizes a learned gene regulatory network to represent perturbational effects as shift interventions that propagate through the network. This approach operates within a low-dimensional gene space, effectively capturing the essential information needed to reconstruct full transcriptomic profiles. By incorporating this mechanistic causal model into a variational autoencoder (VAE), the framework generates detailed and comprehensive transcriptomic responses while maintaining the capacity to handle noisy, large-scale single-cell data.\r\n\r\nTwo deep variational architectures are explored within this framework, corresponding to different output distributions. The single cell variational inference (SCVI) architecture, employing a zero-inflated negative binomial output distribution, demonstrates challenges in learning perturbational data distributions. In contrast, a standard VAE architecture with a Gaussian output distribution on normalized gene expressions, when paired with the structural causal model, achieves superior performance compared to current state-of-the-art methods. This hybrid approach, termed the Single-Cell Causal Variational Autoencoder (SCCVAE), demonstrates robust capabilities in both interpolation and extrapolation.\r\n\r\nFor observed perturbations, the SCCVAE framework reveals latent representations that identify functional perturbation modules and simulate single-gene knock-down experiments across varying penetrance levels. These findings highlight SCCVAE as a powerful tool for interpreting and predicting perturbational responses at the single-cell level, advancing the integration of causal and variational approaches in computational biology.",
        "authors": [
            "Emily Liu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159128",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Leveraging Blockchain Technology for Enhancing Genomic Data Management: A Multidisciplinary Framework for Privacy, Trust, Identity Protection, and Equity",
        "abstract": "The effective adoption of blockchain technology in genomic data management is influenced not only by its technical advantages but also by external factors such as regulatory conditions, and the demands of consumers and patients. This thesis explores the critical factors required for blockchain platforms to thrive in managing genomic data, focusing on how these systems can be structured to address the high-priority needs of various stakeholders, including patients, healthcare providers, regulators, and researchers. Through a comprehensive examination of privacy, security, regulatory compliance, and equity concerns, the research develops a multidisciplinary framework that balances technological innovation with real-world stakeholder expectations. By conducting an in-depth stakeholder analysis and analyzing existing blockchain platforms used for genomics, the thesis presents a roadmap for creating blockchain solutions that are both technologically viable and aligned with the complex social, legal, and ethical landscape of genomic data management. This framework aims to maximize value for all stakeholders while mitigating associated risks, positioning blockchain as a viable tool in the future of personalized medicine.",
        "authors": [
            "Yuner A. Niu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159129",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Healthcare Agents: Large Language Models in Health Prediction and Decision-Making",
        "abstract": "Large Language Models (LLMs) are transforming healthcare, yet utilizing them for clinical applications presents significant challenges. In this thesis, we explore two critical aspects in healthcare AI: (1) leveraging LLMs for multimodal health prediction from wearable sensor data and (2) developing collaborative AI framework for medical decision-making. We first introduce a Health-LLM framework that performs multimodal fusion of temporal physiological signals from wearable devices with contextual metadata to predict health outcomes. By implementing novel context enhancement strategies, our framework demonstrates significant improvements in prediction accuracy across multiple health domains compared to existing benchmarks. Furthermore, we present MDAgents, an adaptive framework that optimizes multi-agent LLM collaboration for complex medical reasoning tasks. MDAgents dynamically configures agent roles and interaction patterns based on task complexity, implementing a hierarchical consensus mechanism that emulates clinical team dynamics. Through comprehensive evaluation on medical diagnosis and reasoning tasks, MDAgents exhibits superior performance in\r\nmultimodal medical reasoning compared to single-agent approaches. Our findings demonstrate that LLMs, when architected for multimodal integration and strategic collaboration, can serve as robust agents in healthcare systems, advancing both preventive medicine through continuous health monitoring and clinical decision support through distributed AI reasoning.",
        "authors": [
            "Yubin Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159124",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Three Essays on the Economics of Land Use, Environmental Value, and Public Spending",
        "abstract": "Across the world, public spending on government programs profoundly alters land use, preservation of environmental value, and the wellbeing of rural populations. These essays explore three such programs and derive lessons for improving their targeting. Chapter 1 tests the effect of conservation easement tax incentives on land conservation in Virginia, using a difference-in-difference design around a 2002 tax reform. This finds that the environmental quality distribution of easements is wide and matches the statewide quality distribution of all undeveloped land, suggesting the program has considerable room to improve targeting. Increasing tax incentives attract donations of similar or lower quality, but targeting tax incentives only at high-quality land would substantially increase high-quality acres at a cost of 1.18 low-quality acres per high-quality acre. Chapter 2 investigates the targeting of short-term incentives for long-term behavior change, focusing on the case of the EQIP agricultural incentives program. The model connects the short-term and long-term effects of incentives as products of the immediate adoption costs and long-term repeated costs and benefits of a practice. If populations vary primarily by adoption cost, targeting groups with the greatest short-term effect will also maximize the long-term effect. If populations vary primarily by long-term costs and benefits, the groups with the greatest short-term impact are those for whom the practice is highly unprofitable in the long run, and a program can improve long-term impacts by instead targeting those for whom the practice is slightly profitable in the long run. A discontinuity analysis comparing successful and unsuccessful EQIP applicants shows that EQIP induces significant short-term change. Chapter 3 investigates the behavior of Mongolian livestock markets after severe weather shocks, and the role that a livestock insurance program may play in smoothing shocks. During severe Mongolian winters, livestock sales increase and prices fall as credit-constrained nomadic herders look to make necessary investments to protect their remaining herd. National integration in livestock markets absorbs a significant share of the weather-related shocks, as 40-60% of district price risk is due to national market fluctuations and 20-40% is due to province effects. This paper finds that national mortality strongly drives price variations, and livestock insurance reduces sales during high-mortality periods.",
        "authors": [
            "Kelsey R. Larson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159125",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Structure, Function, and Interaction in Protein Language Models",
        "abstract": "In recent years, transformer architectures have shown remarkable capabilities in learning meaningful representations from text and images. This approach has been extended to the realm of protein sequences through pretrained protein language models, which have excelled in various protein engineering tasks. In this thesis, we investigate a pre-trained protein language model’s ability to predict protein structure and the effects of mutations. For many advanced protein understanding tasks, such as predicting protein function and protein-protein interactions, fine-tuning of the model is essential. We explore methods to fine-tune the Evolutionary Scale Modeling (ESM2) model, a pretrained protein language model, for predicting protein functions structured as Gene Ontology terms and predicting protein-protein interactions. Notably, we develop a novel method of modeling the hierarchy constraint in GO term prediction that improves training convergence and test performance while making the model hierarchically consistent with GO. This research aims to enhance our understanding of protein language models in decoding complex biological information, thereby contributing to advancements in computational biology.",
        "authors": [
            "Jared Zheng"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159126",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Bridging the Gap: Generative Machines and Inventive Minds",
        "abstract": "Recording technologies, from the phonograph to digital media, have profoundly reshaped the human experience by enabling the capture and reproduction of our sensory world. These technologies allow us to relive experiences through artifacts of remarkable fidelity like photographs and videos, extending the reach of our perception and memory. Of course, we didn’t stop at the phonograph; we have built a rich ecosystem of tools for creating, sharing, and exploring recorded media that have had transformative effects on cognition and culture. Recently, a new and powerful class of tools has emerged: generative models. Unlike recorded media, which reproduces external experiences, generative models can translate our ideas directly into artifacts. Here, ideas refer to abstract mental constructs that seed media creation, externally expressed in text prompts, sketches, vocalizations, or other intuitive representations. Just as recorded media augmented our ability to perceive and remember, generative media promises to expand our ability to imagine and invent by offering a more immediate path from cognition to high fidelity creation. Creative work often has us operating at our limits, negotiating boundaries between knowledge and novelty, skill and aspiration, from individual exploration to collective understanding. Generative models, in principle, have the potential to scaffold and accelerate how we transcend these limits by increasing the efficiency with which we discover and pursue new ideas. In this thesis, I suggest that realizing this potential presents a complex set of challenges that span computation and design. I argue that it requires us to develop a rich stack of precision tools for human-AI co-creation, as we have done and continue to do for recorded media. Specifically, I present contributions across two key dimensions of this:\r\n1. Computational machinery that supports creative work. I present research on topics including visually-driven acoustic simulation, interpretable and controllable sound generation from descriptions, and audiovisual content understanding. Focusing on sound as a case study, I describe systems that effectively represent and manipulate creative knowledge across modalities and levels of abstraction. \r\n2. Interactive systems and studies that investigate the integration of human and machine effort in content creation. This includes work on conceptual integration in AI-assisted story writing, author-in-the-loop description authoring for accessibility of complex scientific figures, and generative constraints for human ideation. In all, this work seeks insights for designing systems that support human creators through exploration, collaboration, and feedback, rather than aiming to replace or constrain human agency and expertise. \r\nTo conclude this thesis, I present a discussion on bridging AI and HCI to gain insights into human creative work and develop stable, generalizable design knowledge for augmenting it. I argue for the design of flexible, parametric tools that enable systematic study of creative behavior under different augmentation designs. Based on this, I propose a conceptual framework to seed the development of a more robust science of human-AI co-creation.",
        "authors": [
            "Nikhil Singh"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159134",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Generative Discovery via Reinforcement Learning",
        "abstract": "Discovering new knowledge is crucial for technological advancement and mirrors how humans and animals learn new skills—often through trial and error. Ancient humans, for example, discovered fire by experimenting with different methods, and children learned to walk and use tools through repeated attempts and failures. In chemistry, scientists find new catalysts by testing various compositions. But how exactly do humans use trial-and-error to improve existing solutions (like learning more efficient ways to walk or synthesizing novel compounds)? Can we design computational models that mimic or exceed human discovery? Such computational models could greatly accelerate progress in science and engineering since they can automate or assist human scientists’ and engineers’ works and discover new knowledge more efficiently (e.g., new compounds, streamlining the robot controller design, etc.). Reinforcement learning (RL) is well-suited for discovery tasks because it enables machines to learn through trial and error. My work overcomes the following major limitation of today’s RL algorithms and thereby advances their discovery potential: Mitigate the bias of reward shaping. RL relies on reward signals from trial-anderror experience, but these signals can be sparse, meaning they are only provided once a desired solution is found and otherwise zero. Most trials, therefore, offer little to no feedback. A common strategy to improve performance under sparse rewards is to provide additional hints (i.e., reward shaping) to guide RL algorithms. However, if these hints are inaccurate, they can steer the algorithm toward worse solutions than those without them. I propose a new RL framework that can be combined with any standard RL algorithm, ensuring that training with hints finds better solutions instead of harming performance. Learning with sub-optimal data. RL can learn not only from online interaction with the world but also from datasets of logged experiences. For expensive or time-consuming tasks like material discovery or robot learning, offline RL could be preferred because it leverages existing data rather than requires new interaction with the world. However, such datasets could contain mostly low-reward solutions, which limits the offline RL algorithm’s performance in finding solutions better than what’s in the dataset (as we show later in this thesis). I introduce sample reweighting strategies that reweight the dataset in a way that current offline RL algorithms trained with the weighted samples are able to discover solutions far better than what’s in the dataset, even if low-reward solutions predominated the dataset. Safety via Diversity. Standard RL algorithms aim to find a single “best” solution. Yet, in many discovery problems—such as drug development—it is more valuable to generate multiple high-rewards solutions with distinct properties (i.e., diversity) than to focus on only one. I study this problem in an emerging discovery task-red-teaming large language models (LLMs). In red-teaming, we desire diverse prompts that trigger undesired outputs from target language models. Current approaches leveraging RL to train an LLM to red-team another one, but they fall short of the diversity of generated prompts and often converge to a few prompts that consistently trigger undesired outputs. I propose to reward the agent to maximize the diversity of generated prompts, which also improves the the success of prompts at triggering undesired outputs from the target LLM.",
        "authors": [
            "Zhang-Wei Hong"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159135",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Learning Diffusion Models to Enable Efficient Sampling for Task and Motion Planning on a Panda Robot",
        "abstract": "A search then sample approach to bilevel planning in the context of task and motion planning is one method of effectively solving multi-step robotics problems. In this planning framework, high-level plans of abstract actions are refined into low-level continuous transitions by sampling controller parameters associated with each action. Efficiently sampling these parameters remains a significant challenge, as exhaustive searches often become computational bottlenecks, especially for tasks requiring complex or multimodal parameter distributions. Moreover, relying on samplers hand-designed by humans is both impractical and limiting. To address these challenges, we propose using diffusion models to learn efficient sampling distributions from demonstrations. By avoiding the limitations of hand-specified and naïve sampling methods, our approach enhances planning efficiency and achieves superior performance across diverse tasks that require learning multimodal parameter distributions to solve successfully.",
        "authors": [
            "Quincy Johnson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159141",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Prompt Injection Generation Using Small Language\r\nModels with Reinforcement Learning with Artificial\r\nIntelligence Feedback",
        "abstract": "Large language models (LLMs) have become an integral part of many fields from customer support automation to research assistants. However, despite their growing adoption, they face significant challenges, particularly when it comes to safety in sensitive contexts. Existing methods like Reinforcement Learning with Human Feedback (RLHF) and keyword filtering have contributed to improving the robustness of these models, but these approaches are very resource-intensive and the models can still be vulnerable to malicious attacks like prompt injections and jailbreaking. One notable limitation in testing defenses against such attacks is the scarcity of appropriate datasets. This thesis investigates the use of small language models (SLMs) to generate goal hijacking messages, a subset of prompt injection messages. Techniques such as LoRA fine-tuning and full fine-tuning of even smaller models are employed in this short form text generation model. We also introduce a fine-tuned SLM enhanced with Reinforcement Learning with Artificial Intelligence Feedback (RLAIF), which removes reliance on slow human feedback by using faster AI-generated feedback instead. By optimizing the reference model and reward functions, we improve alignment with ground truth prompt injection messages while addressing issues such as mode collapse and overfitting. These findings show promise, and further research is necessary to determine how well the approach can generalize to other domains and perform in real-world scenarios. Future work is likely to focus on multilingual datasets and distributed computation to further extend the applicability and efficiency of the method.",
        "authors": [
            "Aneesh Gupta"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159142",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploring the Role of Foundation Models for Training Generalist Robot Learning Policies",
        "abstract": "Numerous methodologies to solving goal-conditioned short-horizon tasks require hundreds of expert demonstrations, but these demonstrations are effort-intensive to collect, reducing the scalability of these approaches. Even with approaches that do work, they may have difficulty generalizing to slightly different settings. In this work, we explore two approaches to training generalist robot learning policies using large-scale foundation models. \r\n\r\nThe first approach aims to use a video foundation model to generate task-conditioned synthetic demonstrations at scale from a single expert demonstration. The objective is to leverage these synthetic demonstrations as proxy for expert demonstrations to train models that learn rewards from expert videos for solving complex visual RL problems. \r\n\r\nThe second approach seeks to improve upon the generalization ability of behavior cloning policies. Moving away from the use of videos for training, we explore using privileged representations such as keypoints or object-poses learned using open-set foundation models. By tracking pose or keypoint correspondences, the aim is to minimize the required number of demonstrations to achieve task completion and improve generalization within classes of objects.",
        "authors": [
            "Eugenia Y. Feng"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159143",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Inferencing Techniques for Enhanced Monitoring of Thermal-Fluid Systems",
        "abstract": "Sensor data augmentation for accurate system monitoring is relevant to many engineering applications, as there is often a gap between available instrumentation and measurement needs. Installing sensors can be limited due to factors such as harsh environmental conditions, the need to avoid operational distortions, and limited space. While continued efforts to develop novel sensor technologies to improve measurement density and quality are important, it is equally crucial to maximize the use of data from existing sensors and measurements. In this work, we employed physics-based methods to solve inverse heat transfer (IHT) problems. Because accurate and well-understood physics models provide strong prior knowledge, physics-based IHT can provide clear solution with use of small amount of temperature measurements. However, existing work in IHT relies on 'perfect' physics models and has been used to solve relatively simple problems such as conduction heat transfer problems. This thesis extends the IHT problem scope to thermal fluid systems, including the efficient use of sensor data and uncertainty quantification (UQ).\r\n\r\nWe leveraged high-resolution thermal-fluid experiments to demonstrate the solution of two types of IHT problems. The first problem estimates the operating conditions of the experiment based on the minimal use of sensors from high-resolution temperature data. The estimated solution is used to reconstruct the entire temperature distribution on a heating surface, while the rest of the data is used to validate the inverse problem methodology. The estimation result is supported by UQ considering measurement errors and modeling errors that adds value to the estimation. The second IHT problem consists of identifying sharp-featured 2D heat source distributions with an array of temperature sensors from a subset of experiment data. Solving IHT involved regularization prior with strong sparsity-promoting capability. The designed iterative solution optimization process finds the unknown heat source distribution as well as regularization hyperparameter. In addition, Bayesian inference enhanced the solution quality by providing UQ of the heat source magnitude.\r\n\r\nExpanding the scope of IHT problems, we also addressed online state estimation in dynamic systems. This work focuses on a hypothetical inverse conduction problem of a transient heat source in a composite materials system. The physics modeling of system is assumed to include uncertainty arising from gap thermal resistance at material interfaces, which complicates the estimation of an internal heat source from external sensor data. To address this challenge, the IHT approach leverages future time-step measurements to correct estimates at the current time step, enabling more efficient use of limited sensor information. The approach is sampling-based and its statistics provides UQ on the quantity of interest.\r\n\r\nWhile this work addresses inverse problems within specific thermal-fluid systems, the methodology is designed for broad applicability beyond these cases. It lays the groundwork for advanced sparse sensing and inverse problem-solving in thermal systems, offering a more efficient, tractable, and reliable tool for engineers and researchers addressing system monitoring with modeling uncertainty. Looking forward, these methodologies could be valuable for digital twin applications, where live sensor measurements are integrated to provide robust, real-time estimation of the state of physical systems.",
        "authors": [
            "Haeseong Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159130",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "1863 Virginia: A short story",
        "abstract": "The question that motivates “1863 Virginia: A short story” is rooted in interracial solidarity and whether it exists outside of a common enemy. During this time in U.S. history, free and enslaved black people; slave-owning and poor white people; and assimilated and resistant native people co-existed. The story follows Indi, a Pamunkey woman, and Abram – a self-liberated and formerly enslaved African man from White House plantation. Due to her tribe's Black Laws, Indi is exiled for giving birth to a child of a Black man. Abram loses the love of his life to his murderous master Mr. Lee and runs away from White House plantation where he stumbles across Indi, Baby Joseph, and another person Indi took in during her time in exile named Sophia. Slave catchers come to Indi’s home looking for Abram and she must decide whether she will give him up or defend him. The text seeks to understand the interior character of people surviving impossible realities while also staying true to the connection of human beings and nature. There is a character Mae, a horse, who expresses herself and the river Pamunkey, who speaks.",
        "authors": [
            "Kelvin Green II"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159139",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Modeling, Design, and Assembly of Spring Tires",
        "abstract": "With a renewed interest in the Moon and the need for autonomous lunar rovers that drive longer distances and operate over extended durations, designing efficient and robust mobility systems is paramount. Created by NASA Glenn Research Center, the spring tire is a compliant airless tire engineered for planetary rover missions in lunar and Martian environments. It consists of hundreds of coiled springs woven together to create a toroidal-shaped mesh wheel that can deform to uneven terrain, providing additional durability and traction. This work aims to apply this technology to two robotic testbeds: ERNEST, an autonomous lunar traversal rover built at NASA Jet Propulsion Laboratory, and IPEx, a lunar regolith mining robot built at Kennedy Space Center. This thesis discusses the modeling of these spring tires with numerical methods along with the design of two spring tire prototypes for use on the aforementioned rover platforms. A streamlined assembly process for these compliant wheels is also outlined as well as the results of compression testing, rough terrain driving, and drawbar pull testing to assess their performance.",
        "authors": [
            "Michael Lu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159140",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Artificial Intelligence Tools, Curricula, and Agents for Creative Learning",
        "abstract": "Children's early development of creativity contributes to their learning outcomes and personal growth. However, as children enter formal schooling systems, their creativity declines. Although Artificial Intelligence (AI)-powered tools for K-12 learning hold immense potential for reducing barriers to creative expression, access to these AI tools and AI knowledge among K-12 students and educators remains inequitable to children from groups underrepresented in STEM. In this thesis, I explore how AI, as an emerging creative medium, can be made more accessible to all young creators. I explore two mechanisms of making a mode of creation more accessible: Creative AI literacy materials for diverse classrooms and AI agentic interactions for scaffolding creative expression for diverse learners. \r\n\r\nUtilizing literacy as a mode of making Creative AI tools accessible, I outline the design and evaluation of various Creative AI curricula that I have developed for diverse groups of K-12 students and teachers. To adapt AI learning to art classrooms, I co-developed the AI and Art curriculum with creative educators, designed specifically for use in creative classrooms with creative educators and learners. I implemented the curriculum with 94 middle and high school students across six week-long sessions. I report findings from teacher co-design sessions and students’ learning experiences. Teachers designed learning objectives and AI tools for their classrooms. Students gained knowledge and skills in art concepts, AI concepts, and the application of art in AI. Students also demonstrated significant shifts in their attitudes towards using AI in the creative process, and their sense of belonging in both AI and art communities was heightened. I discuss how AI curricula can be adapted to diverse disciplines and how art can serve as a meaningful avenue for students to engage with AI concepts. \r\n\r\nUtilizing social interaction from AI agents as a mode of fostering creative expression in children with neurodevelopmental disorders, I designed and applied inclusive child-robot interactions for collaborative creativity, where 32 elementary school children and a social robot collaboratively created picture stories. The robot provided creativity scaffolding during different parts of the creative storytelling process through social interactions such as feedback, question-asking, divergent thinking, and positive reinforcement, while personalizing the scaffolding to meet the unique needs of neurodivergent children. I investigated the impact of the social robot on children’s exhibited creativity and their emergent creative collaborative interactions in storytelling over multiple sessions. Inclusive design practices eliminated creative barriers for children with neurodevelopmental disorders, and the robot's creativity scaffolding interactions positively influenced children’s creative product and creative process in storytelling. I propose Inclusive Co-creative Child-robot Interaction (ICCRI) guidelines for fostering creativity in children with neurodevelopmental disorders, and accommodating diverse creator styles in complex, open-ended creative tasks.\r\n\r\nIn this thesis, I contribute curricula, learning tools, child-robot interactions, and findings from examining long-term child-AI co-creative interactions. I discuss design implications for integrating AI tools, curricula and agents in creative learning environments. This thesis is a step towards empowering all children with powerful modes of creation, while helping them be responsible creators, thinkers and citizens in an AI-driven future.",
        "authors": [
            "Safinah Arshad Ali"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159100",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Systems-Theoretic Approach to Organizational Design and Analysis",
        "abstract": "A significant challenge for large organizations lies in organizational design, particularly for public sector bureaucracies and the largest of industry’s private firms. Organizations tend to turn to organizational design improvements when facing effectiveness and efficiency issues. Unfortunately, these large organizations struggle with organizational design because of the sheer size and complexity of their organization which results in a fragmented and often times faulty approach to improving their organization. Organizations, at their core, are a special type of system or a set of components that operate or work together to achieve some common purpose. Organizations are purely social systems in that their elements are not technical or engineered. \r\n\r\nSystems Theory provides a lens through which these types of social systems can be studied. Just like in engineered systems, an organization's emergent behavior is determined by its internal elements' complex interactions. Traditional organizational design and analysis methods focus on optimizing these internal elements in the hopes of re-integrating optimized elements in pursuit of organizational-level optimal behavior. Just like in traditional systems engineering, component-level optimization does not yield system-level optimal behavior. \r\n\r\nThis thesis codifies a systems-theoretic approach to organizational design and analysis using the language of Systems Theory and the semantics of Systems-Theoretic Accident Model and Processes. By extending traditional Systems-Theoretic Process Analysis (STPA), a tool for hazard analysis used primarily for engineered systems, this work refines STPA’s concepts and terminology to be more accessible for analyzing social systems. Building off this extension, this thesis leverages a contemporary Department of Defense reorganization effort as a case study, illustrating Systems-Theoretic Organizational Design and Analysis (STAODA) as a tool to assess organizational design options.",
        "authors": [
            "Lauren E. Gutierrez"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159101",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Enhancing Coast Guard Infrastructure Management: A Multi-Criteria Framework for Prioritizing Maintenance Projects",
        "abstract": "The United States Coast Guard is currently transforming its decision-making process for prioritizing shore infrastructure maintenance and repair projects. Current decision-making subjectivity appears to be generating inadequate project prioritizations. Stakes are high for an aging infrastructure portfolio in harsh coastal conditions, with increased national reliance on the Coast Guard in a fiscally constrained budgetary environment. Data availability, quality, and fidelity continue to increase, supporting the rationale for more robust and data-informed decision-making frameworks. \r\n\r\nThe research begins with examining Coastal and Shore Operations (CSO) funding history, along with a thorough description of the current Centralized Planned Obligation Prioritization (C-POP) process. The complex, sociotechnical nature of the problem is highlighted by identifying all involved stakeholders and categorizing them through the leading view of stakeholder theory and salience. A detailed review of the governing asset management literature is conducted, gradually narrowing from a broad, international, and asset-type neutral perspective to more tailored infrastructure cross-asset prioritization material. Requisite framework data substance, collection, and analyses are described, and recommendations for data processing improvements are made. \r\n\r\nTwo leading prioritization models are examined: the Importance and Urgency Quadrant Model and the Value Focused Multi-Criteria Decision Model. Their respective data visualizations are generated and analyzed. Using the multi-criteria analysis rooted in multi-attribute utility theory, four portfolios of measurably increasing value are constructed, compared with a baseline portfolio reflecting actual project selections in December 2023. These portfolio iterations include a linear programming solution to the Knapsack Problem of selecting projects that maximize overall portfolio utility within a budget limit while incorporating some of the more social and qualitative system properties. \r\n\r\nA traceable, adaptable, defendable, and objective data-informed multi-criteria framework is proposed, which aims to facilitate the effectiveness of the overall Coast Guard shore infrastructure portfolio in the long term.",
        "authors": [
            "Zachary N. Ballard"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159102",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "CIRCUIT: A Benchmark for Circuit Interpretation and Reasoning Capabilities of LLMs",
        "abstract": "The role of Large Language Models (LLMs) has not been extensively explored in analog circuit design, which could benefit from a reasoning-based approach that transcends traditional optimization techniques. In particular, despite their growing relevance, there are no benchmarks to assess LLMs’ reasoning capability about circuits. Therefore, we created the CIRCUIT dataset consisting of 510 question-answer pairs spanning various levels of analog-circuit-related subjects. The best-performing model on our dataset, GPT-4o, achieves 48.04% accuracy when evaluated on the final numerical answer. To evaluate the robustness of LLMs on our dataset, we introduced a unique dataset design and evaluation metric that enable unit-test-like evaluation by grouping questions into unit tests. In this case, GPT-4o can only pass 27.45% of the unit tests, highlighting that the most advanced LLMs still struggle with understanding circuits, which requires multi-level reasoning, particularly when involving circuit topologies. This circuit-specific benchmark introduces a scalable and reliable automatic evaluation method, transferable to other reasoning domains, and highlights LLMs' limitations, offering valuable insights for advancing their application in analog integrated circuit design.",
        "authors": [
            "Lejla Skelić"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159084",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Toward An Explainable Electric Power Grid Operation Assistant Using Large Language Models",
        "abstract": "This thesis explores potential applications of LLMs for assisting the analyses and decisionmaking of complex electric power grid operators. The power grid is a critical piece of infrastructure currently challenged by increased electrification, integration of renewable energy sources, and distributed energy resources (DERs). Human operators struggle to process the massive amounts of data produced by modern smart grids and need innovative solutions to handle the increased complexity of operational decisions. This thesis investigates the potential role of Large Language Models (LLMs) in grid operation tasks, focusing on interpretability and generalizability while exploring how LLMs can assist operators by providing actionable insights and recommendations. Multiple versions of LLM agents were developed, including naive and tool-assisted designs, and were evaluated on the Learn to Run a Power Network (L2RPN) benchmark for steady-state and cascading failure scenarios. While the LLM agents performed better in scenarios requiring exploratory decision-making, they struggled in steady-state operation and were constrained by their integration with tools and the testing environment. This work was limited by compute constraints, which affected the choice of model and the length of evaluation scenarios, and future work is needed toward seamless interaction of LLMs and power systems simulators, however LLMs have the potential to transform future grid operation, paving the way for more resilient and sustainable energy sector of the 21st century.",
        "authors": [
            "Anish Ravichandran"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159085",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Investigating Model Editing for Unlearning in Large Language Models",
        "abstract": "Data regulations on the Right to be Forgotten such as that in the General Data Protection Regulation (GDPR) of the European Union protect the right of users to remove private information from organizations. With the increasing usage and influence of large language models (LLMs) that are trained on personal data, a question of how to implement the removal of information within these models arises. In addition, large language models (LLMs) are trained on a large corpus of data that is usually scraped from the Web. A current challenge with ensuring reliable and safe outputs from LLMs is false, toxic, harmful or biased information from Web data that is captured in the knowledge of the model. Machine unlearning aims to remove unwanted information from a model, but many methods are inefficient for models with large numbers of parameters or fail to remove the entire scope of information without harming performance in the knowledge that is to be retained. Model editing algorithms solve a similar problem of changing information in LLMs, but they focus on redirecting inputs to a new target rather than removing that information altogether. Despite the parallels between model editing and unlearning, there has yet to be a thorough investigation of the potential of model editing approaches within this setting. In this work, we explore ROME, IKE, and WISE editing algorithms and design new editing targets for an unlearning setting. For evaluating the potential of the model editing algorithms, we focus on unlearning fictitious information using the Task of Fictitious Unlearning (TOFU) benchmark. Through this investigation, we show that model editing approaches can exceed the performance of current unlearning methods at removing information depending on the setting. They share the limitation of traditional unlearning of being unable to encapsulate the scope of what is to be unlearned without damage to overall model performance. We hope to leverage this information to improve methods for unlearning model knowledge and therefore improve the reliability of LLMs.",
        "authors": [
            "Shariqah Hossain"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159086",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Identifying the Role of Transcription Factor RFX3 in 9PDeletion Syndrome",
        "abstract": "9p deletion (9p-) syndrome is primarily characterized by intellectual disability, developmental delays, and autism. This project investigated how much of the neuronal phenotypes of 9p- syndrome could be attributed to RFX3, a transcription factor and autism risk gene. Bulk RNA-seq data of iPSC-derived neurons from patients with 9p- syndrome and CRISPRengineered cell lines was analyzed using Principal Component Analysis, Differential Gene Expression analysis, and Functional Enrichment analysis. The findings indicate that RFX3 plays a significant role but is not the sole driver of the neuronal phenotypes. SMARCA2, a gene linked to intellectual disability and part of the SWI/SNF complex, was identified as a direct target of RFX3 in the commonly deleted region of chromosome 9p. Notably, the combined deletion of RFX3 and SMARCA2 led to greater dysregulation of SMARCA2 expression and SWI/SNF complex components than the deletion of either gene alone. These findings highlight the potential synergistic effects of RFX3 and SMARCA2 in 9p- syndrome and suggest their combined disruption may underlie the neuronal phenotypes observed.",
        "authors": [
            "Lilly Edwards"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159087",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Higher-Order Interactions in Social Systems",
        "abstract": "The de facto representation of a social network is a graph— individuals are represented as nodes, and relationships between pairs of individuals are represented as edges. This results in a powerful abstraction by which social relationships can be systematically studied to understand emergent population-scale behavior. However, many social interactions occur in groups: three individuals may co-author a paper, a team of employees may collaborate on a task, a single tweet may mention four users. Breaking such interactions into a collection of pairwise relationships can oversimplify the rich social contexts in which these individuals know one another. This thesis explores a different paradigm of social network analysis, namely, using \"higher-order\" network models such as hypergraphs and simplicial complexes which can explicitly encode co-present contexts between three or more individuals. The first two projects describe how higher-order interactions can differ from pairwise interactions in terms of micro-level content and macro-level structure, respectively. The latter two projects then develop an applied mathematical toolkit for the algebraic topological analysis of higher-order interactions in social networks.",
        "authors": [
            "Arnab Kumar Sarker"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159095",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Uncanny Valley: An Empirical Study on Human Perceptions of AI-Generated Text and Images",
        "abstract": "This thesis explores how the uncanny valley phenomenon—historically tied to near-human robots—applies to text-based AI interactions and AI-generated images. While the concept has been predominantly studied in the context of robotics, the advent of generative AI reveals that text and visuals that are 'almost, but not quite' human can also provoke unease. \r\n\r\nTwo experiments structure the study. The first examines GPT4-Turbo (GPT4o) text conversations. Sixty participants engaged with one of three “chatbots”: an “Uncanny-Valley Bot” (prompt engineered to fall in the uncanny valley), a “Human-Like Bot” (prompt engineered to converse like humans), or a human control. Godspeed Questionnaire results indicate that the uncanny valley effect surfaces in text-only form: participants consistently rated the “Uncanny-Valley Bot” lowest in anthropomorphism, animacy, likeability, and perceived intelligence. Furthermore, the experiment revealed that the distinction between GPT and humans is becoming increasingly blurred, with 60% of participants mistaking a human for GPT and 40% mistaking GPT for a human. Lastly, results highlighted a strong user preference for naturalness, human imperfections, and vulnerability. While human flaws enhance relatability, deviations that disrupt perceived humanity trigger the uncanny valley.\r\n\r\nThe second experiment investigates AI-generated images produced by Stable Diffusion XL at varying degrees of realism. Fifty-six participants ranked each image’s “strangeness,” revealing that highly realistic or clearly stylized outputs raise fewer concerns. By contrast, images that inhabit the uncanny valley elicited discomfort. To quantify these findings, recognized metrics like Frechet Inception Distance (FID) and Kernel Inception Distance (KID) were used to compare real and AI-generated images. Both metrics strongly correlated with human perceptions, suggesting that distance metrics can be used to determine realism. The study also shows that image generation models can detect visual features associated with the uncanny valley. However, performance drops when the prompt calls for subtle, “mid-range” realism, indicating the model’s difficulty in maintaining comfort and believability at intermediate levels.\r\n\r\nCollectively, the two experiments confirm that uncanny valley responses are not confined to physical robots but persist in text-based dialogue and AI-synthesized images. Yet challenges remain. Short interaction windows, small participant samples, and reliance on selected AI models call for studies on the generalizability of these findings. Future work should adopt longitudinal designs, larger samples, and multiple AI systems. Addressing the uncanny valley in both textual and visual content is essential for advancing user trust, and comfort in AI.",
        "authors": [
            "Deepali Kishnani"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159096",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A framework for determining remote sensing capabilities for ecosystem services valuation",
        "abstract": "Nature provides vital services—clean water, air purification, and climate regulation—to human societies thanks to the \"natural capital\" like forests and lakes on our planet. Accurately measuring and valuing these ecosystem services is crucial for informed economic and development decisions. Remote sensing (RS) technology offers a powerful way to monitor natural capital (e.g., mapping forest cover, assessing water quality). However, current data lack the accuracy and precision needed for robustly monitoring the value of these services. This deficiency has impeded the use of natural capital assessment data in economic decision-making. This research partly addresses this challenge by developing a new framework to investigate the necessary sensor characteristics (spectral, radiometric, temporal, spatial) for effectively monitoring natural capital and quantifying ecosystem services. The framework first identifies the different types of services provided by an ecosystem, then uses a physics-based approach to identify crucial physical parameters and determines the necessary measurements that need to be made from a sensor for their quantification. The sources of uncertainty impacting quantification and value estimation are also analyzed in detail. The approach is integrated to formulate a system utility function that is used to compare performance of existing and proposed RS systems, and the overall results are subsequently used in proposing required capabilities for future remote sensing systems for natural capital monitoring. The framework is demonstrated on a case study focused on the flood attenuation function (service) provided by wetlands. Water budget models are utilized to identify essential parameters for monitoring water storage by wetlands. Using a study area encompassing the Fall Lake Creek reservoir (Oregon, USA), water storage capacity is measured and monitored by integrating USGS digital elevation models with Sentinel-1 synthetic aperture radar, Sentinel-2 optical data, and Planet Scope optical data. Results are validated against USGS published ground truth measurements. A strong correlation (r² of 0.95) was observed with all three datasets. An uncertainty analysis was conducted, using the random fields method, in which synthetic spatially autocorrelated errors were added to the RS datasets. Radiometric uncertainties were studied through addition of gaussian noise as a percentage of reflectance values, and results showed effects of < 2.5% on estimated water volume. Elevation data uncertainties (which were approximated to simulate uncertainties in globally available DEMs) showed higher effects, and errors in estimated storage volumes increased proportionally. A study of inundation (for a case study over Miami, FL) revealed that as the root mean square error of the DEMs increased from 2m to 7 m, the risk of flooding (defined as water depth accumulation of greater than 90 cm) increased more than 3 times. A utility function was developed to evaluate sensors based on their ability to estimate wetland water volumes. This function considers sensor characteristics like spatial, radiometric, and temporal resolution. Notably, the function estimates that a future optical system with 2x improved spatial and 4x improved temporal resolution (compared to Sentinel-2) can increase utility 7-fold.",
        "authors": [
            "Aparajithan Sampath"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159097",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Assessing Blood-Based Laboratory Diagnostics for Alzheimers’s Disease: A Systems Approach",
        "abstract": "This thesis adopts a systems approach to analyze the complex network of stakeholders involved in adopting blood-based laboratory screening tests for Alzheimer’s disease (AD). Traditional diagnostic methods, including cerebrospinal fluid (CSF) testing and positron electron tomography (PET) brain imaging, are invasive, costly, and inaccessible to many. Blood-based tests offer a less invasive and more cost-effective alternative, yet they remain underutilized in clinical practice. By conducting a literature review, stakeholder interviews, and a Kano analysis, the thesis identifies and evaluates the key stakeholder needs to support the widespread adoption of these tests, such as the need for demonstrated clinical performance of these tests, reimbursement, broader education of patients and health care professionals, and safe, effective medicines to treat AD. The research highlights two emerging tests that have published studies demonstrating clinical validation, a key parameter of clinical performance. A stakeholder tension analysis is included with proposed tension resolutions using stakeholder saliency to guide prioritization. Addressing these stakeholder needs could facilitate broader implementation, improve early diagnosis, and support emerging therapeutic interventions for AD, thus reshaping the diagnostic landscape for this increasingly prevalent disease.",
        "authors": [
            "Stephanie Christine Peralta Walker"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159103",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Creating Links: Building an Educational Platform to Ask Relevant Questions in Education",
        "abstract": "In this thesis, I document the findings and process through which we built an educational platform (JANN) to do research while having a positive impact on a community. Through JANN we have coordinated more than 100k hours of tutoring sessions and built (to our knowledge) one of the largest databases of educational recordings in the world. Broadly the contributions here are twofold: first, we demonstrate the research potential building a platform can offer. Second, using our educational platform, we pursue novel questions in the field of education with granular information that is traditionally inaccessible for research.\r\n\r\nAfter introducing the work and describing the construction of the platform, the first chapter details an RCT where we show the effect of receiving tutoring on Math performance. Second, we document how we built an estimator of emotions using audio. The estimator was further validated on our dataset and then used to show that activating emotions are related to better class quality. Third, we document an RCT where Math tutors were asked to dedicate some time per week to teach Socioemotional learning skills. We show that this had a positive effect on learning. Moreover, it also caused tutors to teach longer Math classes. Students showed more trust in their tutors, and ultimately the classes had a higher prevalence of positive emotions. Finally, we also study doing causal inference on observational data on another platform. Using Facebook data we study digital groups and through a regression discontinuity design we find that joining a group has a positive effect on making new friends and can diversify a person's connections in terms of income. \r\n\r\nOverall, we find that building a platform, can broaden the granularity of the data one has access to, make research more scalable, and ultimately also have a positive effect on a community.",
        "authors": [
            "Bernardo García Bulle Bueno"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159104",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Economic Engineering of Personalized Experiences",
        "abstract": "Consumer applications employ algorithms to deliver personalized experiences to users, among others, in search, e-commerce, online streaming, and social media, impacting how users spend their time and money. This dissertation studies the design of such personalization algorithms and the economic consequences of their deployment.\r\n\r\nThe first chapter focuses on the impacts of reward signal precision on online learning algorithms frequently used for personalization. Reward signals are precise when individual measurement is accurate and heterogeneity is low. While some algorithms, which we call \"risk-averse\", favor experiences that yield more precise reward signals and hence favor measurability and homogeneity, others, in the limit, choose experiences independently of the precision of their associated reward signals.\r\n\r\nThe third chapter analyzes how preference measurement error differentially affects user groups in optimal personalization. If such measurement error is symmetric, welfare maximization requires delivering majority-preferred experiences at a rate beyond their proportion in the user population and hence increasing concentration. However, asymmetric preference measurement errors may arise due to users' actions to reduce measurement error. Participants in a survey of TikTok state that they engage in such costly actions.\r\n\r\nThe fifth chapter studies, through the introduction of a new desideratum for market design, how to achieve personalization without infringing on user privacy. Contextual privacy demands that all (preference) information elicited by an algorithm is necessary for computing an outcome of interest in all possible configurations of users’ information. This property is demanding, as it requires that no two pieces of information can jointly but not unilaterally influence the outcome. Algorithms can protect the privacy of users who are queried late and whose information is not used to compute public statistics of the user population, hence achieving the relaxed notion of maximal contextual privacy.\r\n\r\nTwo brief chapters introduce new models of human-machine interaction. The first examines the design of generative models, while the second proposes stated regret of past consumption as a new data modality and presents a corresponding data collection tool.",
        "authors": [
            "Andreas A. Haupt"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159105",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Diagnosing Supply Chain Threats to Defense Innovation",
        "abstract": "As the U.S. Department of Defense (DoD) shifts focus to an era of global power competition, the demand for rapid innovation and disruptive technologies has grown significantly. Prototyping remains a vital tool for advancing technological innovation, enabling early learning and risk reduction in developing complex systems. However, persistent supply chain challenges threaten the success of defense prototyping projects, causing schedule delays, and diminished effectiveness. \r\nThis research identifies the underlying causes of supply chain disruptions specific to Federal Acquisition Regulations (FAR) governed prototyping efforts, offering a socio-technical systems analysis that accounts for stakeholder relationships, market dynamics, and regulatory frameworks. Through extensive data collection, including stakeholder interviews across agencies, organizations, and supply chain roles, 181 issues were identified and analyzed, revealing over 500 contributing factors. The disciplined analysis of these factors identified three systemic root causes: (1) the misapplication of production management strategies that focus on efficiencies at scale and low tolerance for risk; (2) pooled supply chain management functions, which marginalizes prototyping’s unique demands and creates inefficiencies; and (3) regulatory and organizational barriers to entry that deter non-traditional suppliers, hindering innovation.\r\nTo address these systemic challenges, the thesis recommends restructuring organizations to better align with the unique demands and risks of prototyping while simultaneously creating pathways to reduce barriers for new suppliers. Resolving these issues will require a coordinated effort across the prototyping ecosystem. By addressing these root causes, the DoD can improve the efficiency and effectiveness of prototyping programs, ultimately sustaining U.S. technological superiority in an increasingly competitive global environment.",
        "authors": [
            "Donald E. Schneider"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159098",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Efficient Deep Learning Systems for Visual Perception on\r\nthe Edge",
        "abstract": "Deep learning for visual perception on edge devices has become increasingly critical, driven by emerging applications in autonomous driving and AR/VR. Typically, sparse convolution on 3D point clouds and Visual Language Models (VLMs) for image processing are two important methods for visual understanding and reasoning. However, the limited compute resources and memory on edge devices pose significant challenges, necessitating specialized system support for deep learning models. Specifically, the efficiency challenges for edge visual perception are twofold: First, the sparsity and inherent irregularity of point cloud data introduce substantial complexity for parallel processing. Second, the colossal model sizes and amount of computation of LLMs and VLMs render edge deployment particularly challenging. In this thesis, we aim to address the efficiency issues of on-device deep learning via system-algorithm co-design. We first introduce TorchSparse++, a high-performance inference engine for sparse convolution on GPUs. Unlike existing sparse convolution systems, TorchSparse++ well balances the efficiency and implementation simplicity, achieving the best performance across different application scenarios. Specifically, we first create a highly efficient Sparse Kernel Generator that generates performant sparse convolution kernels at less than one-tenth of the engineering cost of the current state-of-the-art system. On top of this, we design the Sparse Autotuner, which extends the design space of existing sparse convolution libraries and searches for the best dataflow configurations for training and inference workloads. Consequently, TorchSparse++ achieves 2.9×, 3.3×, 2.2× and 1.7× measured end-to-end speedup on an NVIDIA A100 GPU over state-of-the-art MinkowskiEngine, SpConv 1.2, TorchSparse and SpConv v2 in inference; and is 1.2-1.3× faster than SpConv v2 in mixed precision training across seven representative autonomous driving benchmarks. It also seamlessly supports graph convolutions, achieving 2.6-7.6× faster inference speed compared with state-of-the-art graph deep learning libraries. Furthermore, to democratize the power of large foundation models in edge AI, we propose AWQ and TinyChat, a hardware-friendly full-stack solution for efficient on-device LLM and VLM deployment. AWQ is a novel quantization method based on the insight that not all weights in an LLM are equally important. Protecting only 1% salient weights can greatly reduce quantization error. Specifically, AWQ employs an equivalent transformation and scales up the salient weight channels to reduce the weight quantization error, during which the scale is determined by collecting the activation statistics offline. Alongside AWQ, we further introduce TinyChat, an efficient and flexible inference framework tailored for 4-bit on-device LLM/VLMs. With on-the-fly dequantization, extensive kernel fusion and platform-aware weight packing, TinyChat offers 2.7-3.7× speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also enables the deployment of the 70B Llama-2 model on mobile GPUs. Together, these techniques significantly reduce the computational and memory costs for deploying deep learning models on edge devices, increasing the accessibility of deep learning for practical application. We hope that this thesis can inspire future research on efficient edge AI across diverse modalities.",
        "authors": [
            "Shang Yang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159099",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Unsupervised Time Series Anomaly Detection Using Time Series Foundational Models",
        "abstract": "The rapid generation of time series data across a wide array of domains—such as finance, healthcare, and industrial systems—has made anomaly detection a critical task for identifying irregular patterns that could signal significant events like fraud, system failures, or health crises. Traditional approaches to time series anomaly detection, including statistical models like ARIMA and deep learning methods, have proven effective but often require an extensive training phase, which can be both data and time-consuming. In recent years, the emergence of foundational models, including large language models (LLMs) and specialized time series models, has opened up new possibilities for anomaly detection. These models, pre-trained on vast and diverse datasets, offer the potential to perform tasks with minimal task-specific training. This thesis investigates the feasibility of leveraging these foundational models for time series anomaly detection, with the aim of determining their effectiveness in detecting anomalies without the traditional training requirements. We also aim to investigate whether foundational models pretrained specifically on time series data yield better results compared to large language models (LLMs) that were not pretrained for time series tasks.",
        "authors": [
            "Linh K. Nguyen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159109",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Leveraging Single-Cell ATAC-Seq for Genomic Language\r\nModels and Multimodal Foundation Models",
        "abstract": "Single-cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq) has emerged as a powerful tool for profiling chromatin accessibility at single-cell resolution. By capturing epigenomic landscapes, scATAC-seq provides critical insights into the regulatory elements that govern gene expression. However, the sparsity of scATAC-seq data, resulting from its low sequencing depth relative to the genome’s potential complexity, poses significant challenges for effective and accurate modeling. To advance the utility of scATAC-seq in modern biology, we explore its integration into deep learning frameworks through two innovative applications. First, we demonstrate how incorporating scATAC data enhances the performance of existing genomic language models by providing complementary context about chromatin accessibility. Specifically, we introduce scATAC to improve SegmentNT, a DNA segmentation model that leverages the Nucleotide Transformer (NT) to predict 14 types of genomic and regulatory elements from DNA sequences up to 30kb at single-nucleotide resolution. Second, we introduce a novel multimodal foundation model that extends existing scRNA-seq foundation models by integrating scATAC-seq data. This model captures crossmodal relationships between gene expression and chromatin accessibility, establishing a unified framework that can be fine-tuned for diverse downstream tasks, including cell type classification and cross-modal imputation. Our work highlights the potential of incorporating scATAC-seq data into existing genomics deep learning strategies, providing a framework for integrating regulatory DNA analysis more seamlessly into genomic modeling.",
        "authors": [
            "Dong Young Kim"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159110",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Impact of Occupational Flexibility on Labor Market Outcomes of Women Following Childbirth",
        "abstract": "The purpose of this study is twofold: (1) determine how occupational flexibility of a couple influence the effects of childbirth on women’s labor market outcomes and (2) measure the parental gender gap and decompose the gap for high and low flexibility occupations. Using data from the Panel Study of Income Dynamics and occupational flexibility characteristics from the O*NET database, we utilize an event study specification to determine the impact of a first child on various labor market outcomes for women within high vs low flexibility occupations and women whose spouses are within high vs low flexibility occupations. Our findings indicate that occupational flexibility impacts labor market outcomes following childbirth. We find an increase in income for women within high flexibility occupations following their first childbirth. Their low flexibility counterparts face an initial income decline that represents 6.1% of the average income. This income decline persists beyond the observed time frame. Occupational flexibility also has positive impacts on employment at the extensive and intensive margin. In the long term, the difference in weeks worked and weekly hours adjustments between women in high and low flexibility occupations is 3.4 weeks and 3.6 hours, respectively. Husbands’ occupational flexibility also exerts a positive, although smaller, positive influence on income and employment at the extensive and intensive margin. Furthermore, we estimate a parental gender gap that is twice as large in low flexibility occupations compared to high flexibility occupations. For instance, the parental gender gap among 30 to 34 year olds is 20.1 percent in the low flexibility cohort but only 12.3 percent in the high flexibility cohort. We also observe the absence of a fatherhood premium among the high flexibility cohort. Given the correlation between occupational flexibility and the gender gap, workplace flexibility shows promise as a policy tool to address gender disparities in labor market outcomes.",
        "authors": [
            "Jia-en Jane Hu"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159111",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Instructify: Demystifying Metadata to Visual Instruction Tuning Data Conversion Supplementary Materials",
        "abstract": "Visual Instruction Tuning (VisIT) data, commonly available as human-assistant conversations with images interleaved in the human turns, are currently the most widespread vehicle for aligning strong LLMs to understand visual inputs, converting them to strong LMMs. While many such VisIT datasets are available, most of them are constructed via ad hoc techniques, separately proposed by different groups, commonly poorly documented, without available (reproducible) code, and employing paid closed-source model APIs like GPT-4, Gemini, or Claud to convert image metadata (labels) to VisIT instructions. This incurs significant cost and difficulty to scale, improve quality, or produce VisIT data for new datasets. In this work, we address these challenges and propose an open and unified recipe and approach, Instructify, for converting available metadata to VisIT instructions using open LLMs. Our multi-stage Instructify features an efficient framework for metadata grouping, quality control, data and prompt organization, and conversation sampling. We show that our approach can reproduce or improve the data quality of the available VisIT datasets when applied to the same image data and metadata sources, improving GPT-4 generated VisIT instructions by ∼3% on average and up to 21% on individual benchmarks using open models, such as Gemma 2 27B and LLaMa 3.1 70B. We further show that our approach enables effective performance scaling (in terms of resulting LMM performance on a large variety of benchmarks) of the produced VisIT data both in terms of quantity and quality. In addition, we explore the impact of multiple factors, including conversation format, base model selection, and resampling strategies.",
        "authors": [
            "Jacob A. Hansen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159112",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Computational Tsirelson's Theorem for All Compiled Nonlocal Games",
        "abstract": "Nonlocal games, defined as cooperative tasks between spatially separated players, have been a foundational tool in the study of quantum advantage and have been useful in classically verifying quantum computations. To address the challenge posed by the spatial separation assumption, Kalai et al. (STOC' 23) introduced a compilation procedure that compiles any nonlocal game into an interactive game between a classical verifier and a computationally bounded quantum prover. This compilation preserves classical soundness and quantum completeness, though quantum soundness has been established only in the asymptotic limit of the security parameter or for specific classes of games. In this work, we advance towards a concrete framework to bound the quantum value of compiled nonlocal games. Building on the notion of nice sum-of-squares certificates, introduced by Natarajan and Zhang (FOCS' 23) to bound the value of the compiled CHSH game, we extend the niceness framework and construct a hierarchy of semidefinite programs that searches exclusively over nice certificates. We show that this hierarchy converges to the optimal quantum value of the game. Additionally, we present a transformation to make any degree-1 sum-of-squares certificate nice. This approach provides a systematic method to reproduce known bounds for special classes of games and showcases the general applicability of the framework to low-degree certificates. Source code: https://github.com/chiragfalor/\r\nNice-SoS-SDP",
        "authors": [
            "Chirag Falor"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159113",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Applied Plankton Image Classification for Imaging FlowCytobot Data",
        "abstract": "As the ability to gather vast quantities of data from oceanographic bioimaging sensors increases, so too does the need to process, analyze, and store that data in a consistent, standard way that enables replicability and accessibility for future studies. The Imaging FlowCytobot (IFCB), an automated submersible flow cytometer, produces high resolution images of plankton at rates up to 10 Hz for months or years, resulting in billions of images. This project compares various methods to categorize incoming images of plankton gathered by the IFCB - Convolutional Neural Nets (CNNs), Vision Transformers (ViT), and self-supervised learning (MAE). The benefits and downsides of each model are analyzed and discussed for future IFCB operators to process their data using the methods that best align with their research questions, along with step-by-step explanations about the pros and cons of each method depending on the use case.",
        "authors": [
            "Barbara R. Duckworth"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159114",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The SpaseCroissant Oven: Automatic Metadata Generation For Open-Source Space Weather Datasets",
        "abstract": "The rise of machine learning (ML) algorithms has led to a parallel rise in ML-ready datasets. A novel metadata schema released by OpenAI and MLCommons called Croissant, which is specifically designed for ML-ready datasets, aims to increase data accessibility, user understanding of data, and accuracy of claims based on data. However, current methods to automatically generate Croissant metadata present difficulties, such as involving manual entries. This can be especially difficult when attempting to preserve information about large ML-ready datasets, which are often derived from large scientific repositories belonging to organizations such as National Aeronautics and Space Administration (NASA). These major scientific repositories provide their own metadata standards, such as NASA’s Space Physics Archive Search and Extract (SPASE) schema but context from this metadata can often be lost during data processing. This thesis presents a novel, improved approach to Croissant metadata generation which involves a hybrid parsing logic and Large Language Model (LLM) inference approach, as well as recommendations for future Croissant standards and SPASE to Croissant schema metadata conversion, that aims to retain this lost context.",
        "authors": [
            "Edenna H. Chen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159115",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploring Fine-Tuning Techniques for Removing\r\nTamper-Resistant Safeguards for Open-Weight LLMs",
        "abstract": "Open-source models present significant opportunities and risks, especially in dual-use scenarios where they can be repurposed for malicious tasks via adversarial fine-tuning. In this paper, we evaluate the effectiveness of Tampering Attack Resistance (TAR), a safeguard designed to protect against such adversarial attacks, by exploring its resilience to full-parameter and parameter-efficient fine-tuning. Our experiments reveal that while TAR enhances tamper resistance compared to models without safeguards, it remains susceptible to variability. Specifically, we observe inconsistencies where the same adversarial attack can succeed under some initializations and fail under others. This is a critical security risk as even a single instance of failure can lead to models being exploited for harmful purposes. These findings highlight the limitations of current tamper-resistant safeguards and emphasize the need for more robust safeguards to ensure the safe and ethical deployment of open-source models.",
        "authors": [
            "Sarah Zhang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159116",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Toward Affordance-Based Generation for 3D Generative AI",
        "abstract": "Recent advances in 3D content creation with generative AI have made it easier to generate 3D models using text and images as input. However, translating these digital designs into usable objects in the physical world is still an open challenge. Since these 3D models are generated to be aesthetically similar to their inputs, the resulting models tend to have the visual features the user desires but often lack the functionality required for their use cases. This thesis proposes a novel approach to generative AI in 3D modeling, shifting the focus from replicating specific objects to generating affordances. We trained models that allow users to create point clouds that satisfy physical properties called affordances, which are properties that describe how an object should behave in the real world. By ensuring that the generated objects have the expected affordances, we explore how existing tools can be augmented to generate 3D objects whose functionality is consistent with their appearances.",
        "authors": [
            "Sean Wang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159117",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Investigation of the energy transfer network in upconverting nanoparticles",
        "abstract": "Upconverting nanoparticles (UCNPs) have emerged as promising luminescent materials for a wide range of applications, including bioimaging, drug delivery, and photovoltaics. The intricate network of energy transfer processes within UCNPs enables their unique ability to convert low-energy infrared (IR) radiation into higher-energy visible light through photon upconversion, presenting significant challenges for accurate modeling. Despite their broad applications, theoretical models of UCNPs remain incomplete, and current models fail to accurately reproduce all experimental results. This thesis presents a comprehensive comparison of prevalent modeling approaches with the aim of developing improved models that more faithfully reproduce experimental observations. Using the Judd-Ofelt theory, we calculated essential transition rate parameters, including electric dipole (ED), magnetic dipole (MD), multiphonon relaxation (MPR), and energy transfer (ET), using constants sourced from the literature. We implemented both Monte Carlo models and Ordinary Differential Equation (ODE) models. Using the calculated rate parameters, we simulate the energy transfer pathways in Yb³⁺-Er³⁺ and Yb³⁺-Tm³⁺ UCNPs. Simulation results from all models were compared with experimental data to evaluate their effectiveness in capturing key luminescent properties such as population evolution, lifetime, saturation curves, and spectral purity.",
        "authors": [
            "Yuxuan Zheng"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159106",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Scalable Embedded Tiny Machine Learning (SETML): A General\r\nFramework for Embedded Distributed Inference",
        "abstract": "The growth of machine learning applications has increased the necessity of lightweight, energyefficient solutions for resource-constrained devices such as the STM32C011F6 microcontroller. However, such devices struggle with supporting larger models even after miniaturization techniques such as quantization and pruning. To facilitate machine learning inference on such devices, this work introduces Scalable Embedded Tiny Machine Learning (SETML), a general framework for distributed machine learning inference on microcontrollers. Furthermore, the framework is designed to be compatible with sensor-based applications that can take advantage of small hardware, such as gesture recognition, by testing binary size constraints with an accelerometer and its supporting library. This work evaluates the latency, power consumption, and cost trade-offs of using multiple small and efficient devices versus a larger device. The STM32C011F6 microcontroller is used as the primary hardware in the tested device network, while evaluation of the system is done in comparison with a device using a similar core processing element, the Seeeeduino XIAO SAMD21.",
        "authors": [
            "Justice Vidal"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159107",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "First-Person Teleoperation of a Bimanual Robotic System",
        "abstract": "First-person teleoperation of robots is a large field of research that could serve many benefits for automation. Teleoperation is a popular method to collect demonstrations for imitation learning that are easily learned by the robot, and thus it’s important to create teleoperation systems that are intuitive and enable human-like perception of a scene. Adding a first-person component to basic teleoperation systems is key to improving operators’ visual perception and making teleoperation possible for extended periods of time. Existing teleoperation systems do not integrate elements that provide the operator with a good perception of the task space, such as a first-person VR view and the ability to leverage the neck to search around the space. They rely on techniques such as third-person view of the space, or provide a first-person view but without the ability to move the neck to look around. This thesis proposes a VR-based teleoperation system with an actuated 5-DoF neck for enabling human-like perception and improving the ability to perform high quality demonstrations for use in imitation learning.",
        "authors": [
            "Nandini Thakur"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159108",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Mapping the Spatial Transcriptome Across Whole Organisms",
        "abstract": "This study utilizes Expansion Sequencing (ExSeq) to thoroughly investigate the spatial transcriptome of the Caenorhabditis elegans (C. elegans) body. Beyond mapping gene distribution within individual specimens, this research sequences multiple C. elegans to identify both shared and distinct transcriptomic features. The findings lay crucial groundwork for future integration of transcriptomic data with in situ connectomics and in vivo neural activity recordings. Understanding the spatial transcriptome in C. elegans is vital for insights into neural circuit coordination, disease mechanisms, and developmental biology.",
        "authors": [
            "Ruihan Zhang"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159122",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Engineering Disease Resistance in a Reservoir Species for the Mice Against Ticks Project",
        "abstract": "This thesis explores the application of genome editing technologies to combat zoonotic infectious diseases through the development of a novel heritable immunization strategy targeting reservoir species. Focusing on Lyme disease, where white-footed mice (Peromyscus leucopus) serve as the primary reservoir, we propose embedding immunity into the germline of these animals to disrupt the disease transmission cycle and reduce the prevalence of the disease in the environment. By establishing genome engineering protocols for Peromyscus and demonstrating heritable protection against Lyme disease in genetically engineered Mus musculus, we show the feasibility of heritable immunization for long-term disease prevention. This work highlights the potential of genetic engineering for ecological interventions, offering a novel approach to public health challenges while fostering responsible community engagement in ecosystem engineering.",
        "authors": [
            "Joanna Buchthal"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159123",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Data futures: Transforming digital traces into public goods in the age of commercial surveillance",
        "abstract": "For decades, government agencies have collected surveys to produce datasets and statistics that serve as public goods, enabling research and empowering communities from whom data are collected. These data sources are costly to collect and are in decline as survey response rates drop. In contrast, increasing quantities of data are collected from the public by companies -- data we unavoidably generate by making purchases, using the Internet, or simply operating a mobile phone.  This data collection might be considered a form of surveying the public, but where privatized datasets empower corporations rather than communities, and the ensuing potential harms cannot be empirically assessed without access to these data. \r\n\r\nThis thesis considers a future where corporations can more accurately track populations and estimate statistics than the government agencies traditionally tasked with such efforts. This thesis illustrates how this future may be nearby and explores resulting questions through case studies. Namely, are there more privacy-preserving or equitable or cooperative ways to manage these data, to benefit the public from whom they are sourced?\r\n\r\nThe first set of case studies use location data from mobile phones, first developing a more privacy-preserving approach by leveraging recurrent neural networks to generate realistic synthetic data, and second developing aggregated mobility metrics to improve country level population estimates and COVID-19 epidemic models. The next set of case studies use web browser data to evaluate risks of cross-site user tracking that are present despite privacy-enhancing browser developments. The first web study repurposes data collected by a data broker; the second uses a dataset we crowdsourced and openly published to benefit this research and future research. For the next set of case studies, we crowdsourced and published a first-of-its-kind open dataset of purchase histories from thousands of Amazon.com users, along with their sociodemographics. We use this dataset to demonstrate how corporate data can provide insights into societal changes and also evaluate privacy risks due to inferring sensitive consumer information from purchases.\r\n\r\nThe data used in this thesis (mobile device locations, web browsing data, purchase histories) are examples of digital traces collected continuously from people throughout everyday activities, without explicit consent. This work points towards cooperative data sharing as a paradigm to empower research that benefits the public while prioritizing consent. Could such a paradigm exist with public support and participation? In order to study this and inform future crowdsourcing efforts, we embedded behavior experiments and surveys into our crowdsourcing tools, shedding light on what impacts users' likelihood to share their data, how users believe their data should be used, and how results differ across demographics.\r\n\r\nThroughout these studies, this thesis asks a broader question: Can we envision, and build towards, a future with alternative data economies that shift the power dynamics of data collection, along with the control and benefits of these data? To begin to address this question, this thesis proposes speculative, privacy-enhancing, and cooperative commerce networks. Such system changes may incur new costs for consumers. The final case study measures consumers' willingness to pay for privacy in new package delivery networks.",
        "authors": [
            "Alex Berke"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159147",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Evaluating the Effects of Pharmaceutical Interventions, Social Policies, and Exogeneous Shocks on People's Health and Behavior",
        "abstract": "Aging individuals tend to suffer from chronic conditions, some of which manifest in midlife (e.g., type 2 diabetes and hypertension) and some later (e.g., neurodegenerative disorders). As the global population increases and as people are living longer, finding strategies to prevent or delay these diseases has become a key priority. Concurrent advances in public health and biomedicine offer an array of pharmaceutical (e.g., oral drugs, vaccines) and non-pharmaceutical solutions (e.g., preventative and behavioral health measures). Meanwhile, exogenous shocks such as pandemics also affect the health and well-being of aging and other vulnerable individuals or populations (e.g., immunocompromised individuals, multigenerational households). In such circumstances, pharmaceutical interventions may not be readily available, forcing governments to implement socio-behavioral policies such as lockdowns and mask-wearing mandates and companies to adopt remote and hybrid work practices. Natural experiments, such as the social isolation induced by the COVID-19 pandemic or incentive-based vaccine distribution programs aimed to bolster vaccine uptake during this time, provide an opportunity to assess retrospectively the effect of federal, state, or local government policies. Another example consists of leveraging new drug approvals and changes in clinical guidelines to learn from electronic health records (EHR) which existing treatments could be repurposed to delay neurodegeneration and/or increase longevity, and if so, for whom they would work best. However, unlike randomized controlled trials, natural experiments suffer from multiple sources of confounding. The use of appropriate causal inference methods can help mitigate confounding bias, including via weighting and regression discontinuity designs. This thesis illustrates the use of existing causal inference approaches in population health and proposes new methods to evaluate the effects of pharmaceutical interventions (Chapters 1 and 2), exogenous shocks (Chapters 3, 4, and 5), and socio-behavioral policies (Chapters 3 and 5) on the health and well-being of aging and other vulnerable individuals or populations. Specifically, Chapters 1 and 2 leverage the target trial emulation framework to study the comparative effectiveness of antidiabetic and antihypertensive drugs towards preventing dementia or delaying its onset, using EHR data from Mass General Brigham healthcare system. Our target trial emulations suggest the diabetes drug metformin and the antihypertensive drug class of angiotensin receptor blockers as potential repurposing candidates for dementia, especially if initiated before age 70. Chapter 3 uses regression discontinuity designs to quantify the benefits of a local vaccine companion program in Massachusetts during the COVID-19 pandemic. We estimate that this initiative may have bolstered vaccine uptake among older adults aged 75+ by up to 22 percentage points. Chapter 4 implements counterfactual time series modeling to estimate pandemic-period excess mortality associated with overdoses in the US, by substance and geography. We find ∼25,650 excess deaths nationally (March 2020-August 2021), disproportionately affecting Southern and Western regions of the country and attributable mainly to synthetic opioids, methamphetamines, and alcohol as well as polysubstance use. Chapter 5 characterizes changes in team coordination among knowledge workers at a large global tech company to better understand the rise of hybrid work practices and their potential implications for well-being. Using two-way fixed effect regression models, we find evidence of voluntary alignment of work schedules with managers and greater co-attendance among employees who were recently hired or work in shared office spaces. Collectively, these five studies demonstrate how we can effectively learn from data about past events, medical records, and office attendance logs, to provide insights that inform the design of future public health strategies.",
        "authors": [
            "Marie-Laure Charpignon"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159148",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Intuitive Audio Interaction and Control in Multi-Source Environments",
        "abstract": "In an increasingly noisy world, managing auditory focus is a persistent challenge. This thesis explores how embodied interactions—primarily head tracking, alongside experiments with gaze tracking, speech commands, and audio-visual segmentation—can enhance user control over complex auditory environments. By linking head orientation to volume adjustments, we investigated whether natural, instinctive movements could serve as intuitive, hands-free mechanisms for isolating and amplifying relevant sounds. User studies revealed that head tracking is effective in structured audio contexts, such as music, where distinct sources are easily separable. However, its utility diminishes in dense, overlapping conversations, highlighting the need for finer control mechanisms. While gaze and segmentation offer promising refinements, cognitive load and system responsiveness remain key challenges. These findings underscore that embodied audio interaction must be adaptive, content-aware, and seamlessly integrated with user intent.This research contributes to human-computer interaction by demonstrating both the potential and limitations of movement-based audio control. Future work should refine multimodal fusion, improve segmentation accuracy, and enhance accessibility to create systems that dynamically respond to users’ natural behaviors—reducing cognitive strain and enabling more fluid, user-centric auditory experiences.",
        "authors": [
            "Erick O. Oduniyi"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159149",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Urban Mining & Regenerative E-Waste Ecosystems: Visions towards Sustainable Entrepreneurial Futures for Informal Settlements and Recycling Communities",
        "abstract": "In the face of the growing challenge of urban waste, especially within rapidly expanding informal settlements projected to house over 45% of the global population by 2050 (United Nations Department of Economic and Social Affairs, 2022), innovative solutions are imperative. The thesis proposes a paradigm shift towards urban mining, emphasizing the significant value embedded in discarded electronics—where a tonne of circuit boards can hold ten times more precious metals than traditional ore (Minnesota Center for Environmental Advocacy, 2022). The global distribution of off-shored e-waste has led to the emergence of informal settlements that depend on e-waste recovery to support livelihoods and income generation. These communities have become prime examples for urban mining, embracing circular economic strategies to find adaptive ways to repurpose e-waste. Accra, Ghana’s Old Fadama, home to one of the largest e-waste sites in the world, has become a vital economic hub for informal e-waste processing.  With a population of over 100,000 dwellers, local and migrant workers have built resilient communities through innovative recycling practices, tech repairs, and DIY digital fabrication methods. However, they face imminent environmental risks, health hazards, and displacement threats.\r\n\r\nFocusing on Old Fadama, the thesis will address the narratives of urban mining communities and look toward a systematic sympoiesis between economic, environmental, and social realities. By doing so, the thesis seeks to answer how we can foster nurturing and circular relationships for informal settlements and develop regenerative ecosystems for urban mining in the city environment. As an integrated field research, case study, and implementation, the thesis will: conduct key urban analysis for understanding e-waste sites and urban mining communities; identify technology interventions and policy recommendations that can improve local conditions; and utilize data-driven communication to advocate for new opportunities for urban systems tied to e-waste extraction through immersive multimedia as part of a public exhibition.\r\n\r\nUsing a novel methodology, the thesis adopts the learnings from the economic, physical, and community-based interventions observed in informal e-waste recovery processes. The thesis combines quantitative data from satellite imagery and remote sensing with qualitative insights gathered through crowdsourced GIS mapping, films, interviews, and creative capacity-building workshops. These combined insights aim to enhance urban models, nurturing the innovation potential already present within urban mining communities. The thesis research will contribute to the previous work of MIT City Science Group’s “Power of Without” initiative, a comprehensive roadmap for understanding and collaborating with informal settlements and proposing non-Western decentralized infrastructure solutions. The thesis aims to provide practical insights for implementing innovations in urban mining communities by developing sustainable e-waste recovery strategies and supporting micro-industries in cities, which could serve as a model for similar contexts globally.",
        "authors": [
            "Georine Pierre"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159131",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Causal Inference Under Privacy Constraints",
        "abstract": "Causal inference is an important tool for learning the effects of interventions in observational or experimental settings. It is widely used in many fields such as epidemiology, economics, and political science to find answers like the average treatment effect of a medical procedure or the individual treatment effect of a personalized ad campaign. In commercial applications, the era of big data allows companies to increase their experiment volume, incentivizing them, in turn, to collect more user data. On one hand, large volumes of data are necessary to train generative models like ChatGPT. At the same time, companies’ increasing use of user data has drawn heavy criticism and consumer backlash, incurring legitimate concerns about privacy and consent. As concerns over user data safety and privacy grow, rules and regulations like GDPR change what kinds of data companies and researchers can acquire and how they can analyze the data. The necessity of now performing causal inference under a range of privacy constrants has carved new spaces for research at the intersection of causal inference and privacy. In my thesis, I will be exploring three paradigms for protecting user data — data minimization, differential privacy and synthetic data — and how to perform causal inference techniques under these new privacy regimes.",
        "authors": [
            "Leon Yao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159132",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Investigation of Multi-Z Impurity Transport in Tokamaks using Neural Networks",
        "abstract": "Achieving clean, sustainable energy at scale is a pressing global challenge. Fusion of light elements holds significant potential to address this critical need. While only experimental fusion reactors are currently operational, significant progress is being made in the research and design of near-future tokamak fusion power plants. Reactor success will depend on a comprehensive understanding of heat and particle transport, including the role of impurities. This thesis focuses on the development of machine-agnostic neural network surrogates for TGLF, designed to predict impurity transport coefficients alongside heat and electron particle fluxes in DD plasmas. Training data are derived from synthetic fluxes generated for L, H, and I confinement modes in Alcator C-Mod, DIII-D, and ASDEX-Upgrade. To reduce training complexity, shot data are discretized by radius, and networks are developed at six ρ coordinates: 0.2, 0.4, 0.6, 0.7, 0.8, and 0.9. Fifteen plasma parameters are selected as inputs to the neural networks after examining TGLF flux sensitivities across all five output channels. Predicted impurity fluxes for arbitrary charge states and masses, ranging from 4He to 184W, are used to derive diffusive and convective transport coefficients. Three types of synthetic TGLF data are created and applied to network training to produce accurate models. The primary synthetic data type approximates experimental data by sampling within a perturbation range of ±10% around a given shot. Supporting data types enhance network performance by improving trends in single-parameter (1D) scans and addressing areas of highest network uncertainty. Hyperparameter optimization and testing resulted in highly accurate networks. Testing set relative errors averaged over ρ = 0.4–0.7 and 0.9 show approximate deviations of 0.12 ± 0.029 for heat flux and 0.42 ± 0.095 for particle flux channels. However, error metrics at ρ = 0.2 and 0.8 require location-specific tuning and potentially more data to match the accuracy achieved at other radii. The networks are used to analyze boron and carbon impurity peaking within machinespecific H-modes. Their predictions are then compared to published results. Qualitative results for boron peaking correlations in ASDEX-Upgrade are clearly reproduced, while carbon peaking trends in DIII-D are weaker. Sparse DIII-D data, which also includes atypical advanced modes, is believed to have contributed to reduced accuracy in these cases. Using H-mode shots spanning low to high local collisionality, impurity diffusion trends with charge state (Z) in ITG and TEM dominated plasmas were examined, showing good agreement with published studies. Additionally, analysis of network-derived convective transport shows that Z-sensitivity increases with collisionality. Network scans of the ion and electron heat flux responses to temperature gradients also reveal the clear presence of a critical gradient at all radii. These results demonstrate that the neural networks developed in this work can reliably reproduce TGLF results and deliver fast predictions of heat, electron particle, and impurity transport in tokamaks.",
        "authors": [
            "Jamal Johnson"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159133",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Deep Learning-Based Classification of Phonotraumatic Vocal Hyperfunction Severity from Stroboscopic Images",
        "abstract": "Phonotraumatic vocal hyperfunction (PVH) is a vocal disorder characterized by damaged vocal folds from excessive or abusive voice use. Clinical assessment of PVH relies on timeconsuming videostroboscopy examination, which poses challenges for large-scale clinical studies. We address the need for more efficient clinical assessment tools by proposing deep learning approaches for automatically detecting PVH severity from stroboscopic images. One of the main challenges in building deep learning models for this task is a lack of labeled stroboscopy data. Motivated by this challenge, we explore two approaches: direct classification and segmentation-then-classification. In the segmentation-then-classification approach, we first train a model to segment the glottis, a clinically relevant part of the vocal fold anatomy. Then, we use the predicted segmentation along with the stroboscopic image as inputs into a classification model. This approach helps to guide the model towards key anatomical features. We achieve up to 0.53 accuracy in four-class PVH severity prediction with the direct classification approach. Incorporating glottal segmentations improves the accuracy to 0.64, underscoring the value of providing anatomically-informed segmentations when assessing PVH severity. By creating an automated PVH severity tool, our work has the potential to help clinicians more efficiently monitor disease progression and to facilitate large-scale screening, thereby contributing to improved patient care.",
        "authors": [
            "Purvaja Balaji"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159150",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Systems Analysis of Plant Responses to Drought",
        "abstract": "Understanding how plants respond to environmental stress is critical for ensuring stable crop performance and predicting how natural populations may adapt to a changing climate. While plant biology has traditionally focused on plant physiology and molecular biology of model plants to elucidate plant responses, there is immense diversity in how plants respond to environmental conditions, arising from complex genotype-by-environment interactions (GxE).  \r\nThis dissertation investigates these themes, aiming to advance our understanding of the mechanisms driving plant responses to environmental stress and providing insights for improving agricultural resilience and sustainability, as well as contributing to evolutionary biology. This thesis focuses on three projects: \r\n(1) While GxE is widely observed in traits and gene expression patterns, the mechanisms driving these interactions remain unclear. This thesis will present a framework using casual inference to study GxE interactions in gene regulatory networks to uncover the molecular mechanisms driving diverse environmental responses. We study two genotypes of the model grass species Brachypodium distachyon, leveraging natural variation and RNA-sequencing to study their responses to drought stress. \r\n(2) Natural perturbations can be used to understand complex traits. In wild species, limited resources drive allocation strategies that balance trade-offs between survival risks and fitness benefits, which is central to their ecology. This thesis particularly focuses on understanding a whole plant trait – carbon allocation – using divergent responses of annual and perennial species of Brachypodium to drought stress. \r\n(3) Does domestication trade-off stress tolerance for rapid growth? Plant domestication is thought to create trade-offs between high yield and stress tolerance, raising concerns about yield stability in future climates. This thesis will present a high-throughput phenotyping approach to study this question, focusing on leaf growth environmental response and its cellular regulatory mechanisms.",
        "authors": [
            "Jie Yun"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159151",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Brewing Resilience: A Case Study in Adapting Small Business Strategy with Systems Thinking",
        "abstract": "This thesis explores how systems thinking—a methodology often reserved for large organizations—can be effectively applied to small businesses facing complex challenges. Using Lamplighter Brewing Co., an independent microbrewery in Cambridge, Massachusetts, as a case study, the research examines how the brewery adapted to the disruptions of the COVID-19 pandemic and the evolving economic landscape that followed. It documents the iterative application of systems thinking principles to identify root causes, leverage points, and actionable solutions to address issues such as declining revenue, rising costs, and misaligned organizational structures.\r\nLamplighter's interventions ranged from restructuring its management and marketing teams to pivoting its sales and production strategies. By leveraging tools such as causal loop diagrams and stock-and-flow models, the brewery uncovered systemic dynamics driving its performance. The research highlights the importance of iterative learning, targeted interventions, and holistic analysis in fostering resilience and sustainability in resource-constrained environments.\r\nWhile focused on the craft brewing industry, the findings offer transferable insights for small businesses in similarly dynamic sectors, demonstrating that systems thinking can empower smaller organizations to navigate complexity, adapt strategically, and thrive amidst uncertainty.",
        "authors": [
            "Andrew C. Jones"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Massachusetts Institute of Technology",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159152",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "First and last as superlatives of before and after",
        "abstract": "First and last have been variously described as ordinals, superlatives, or both. These descriptions are generally not accompanied by extensive argumentation, and those who label first and last as superlatives do not present and argue for a particular decomposition. Thus, first and last’s status as ordinals vs. superlatives and their internal composition remain open issues. In this paper, I argue that first and last are superlatives, in particular the superlative forms of before and after. To argue that first and last are superlatives, I show that they pattern like superlatives and unlike ordinals (second, third, etc.) with respect to plurality, modifier choice, “modal superlatives” with possible, and the ordinal superlative construction. I next argue that the relations between before and first and between after and last show themselves overtly in many languages and in English paraphrases; furthermore, first and last semantically differ in ways that before and after have also been noted to differ. While I acknowledge one observation that prima facie counterexemplifies these claims, I argue that it constitutes a genuine counterexample only if one formalizes my decomposition of first/last using a standard Heimian (Heim in Notes on superlatives. Manuscript, MIT (1999)) entry for -est. The counterexample, which concerns the “upstairs de dicto” reading of superlatives, ceases to be an issue if one treats before and after as simplex and formalizes my decomposition using a Containment Hypothesis-inspired semantics (Bobaljik in Universals in comparative morphology: Suppletion, superlatives, and the structure of words, MIT Press, Cambridge, 2012) for -est.",
        "authors": [
            "Johanna Alstott"
        ],
        "journal_conference_name": "Natural Language Semantics",
        "publisher": "Springer Netherlands",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158273",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Iron-sulfur clusters: the road to room temperature",
        "abstract": "Iron-sulfur proteins perform a wide variety of reactions central to the metabolisms of all living organisms. Foundational to their reaction chemistry are the rich electronic structures of their constituent Fe-S clusters, which differ in important ways from the active sites of mononuclear Fe enzymes. In this perspective, we summarize the essential electronic structure features that make Fe-S clusters unique, and point to the need for studies aimed at understanding the electronic basis for their reactivity under physiological conditions. Specifically, at ambient temperature, both the ground state and a large number of excited states are thermally populated, and thus a complete understanding of Fe-S cluster reactivity must take into account the properties, energies, and reactivity patterns of these excited states. We highlight prior research toward characterizing the low-energy excited states of Fe-S clusters that has established what is now a consensus model of these excited state manifolds and the bonding interactions that give rise to them. In particular, we discuss the low-energy alternate spin states and valence electron configurations that occur in Fe-S clusters of varying nuclearities, and finally suggest that there may be unrecognized functional roles for these states. Graphical abstract",
        "authors": [
            "Brighton A. Skeel",
            "Daniel L. M. Suess"
        ],
        "journal_conference_name": "Journal of Biological Inorganic Chemistry",
        "publisher": "Springer International Publishing",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158250",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "First demonstration of a TES based cryogenic Li2MoO4 detector for neutrinoless double beta decay search",
        "abstract": "Cryogenic calorimetric experiments to search for neutrinoless double-beta decay ( 0 ν β β ) are highly competitive, scalable and versatile in isotope. The largest planned detector array, CUPID, is comprised of about 1500 individual Li 2 100 MoO 4 detector modules with a further scale up envisioned for a follow up experiment (CUPID-1T). In this article, we present a novel detector concept targeting this second stage with a low impedance TES based readout for the Li 2 MoO 4 absorber that is easily mass-produced and lends itself to a multiplexed readout. We present the detector design and results from a first prototype detector operated at the NEXUS shallow underground facility at Fermilab. The detector is a 2-cm-side cube with 21 g mass that is strongly thermally coupled to its readout chip to allow rise-times of ∼ 0.5 ms. This design is more than one order of magnitude faster than present NTD based detectors and is hence expected to effectively mitigate backgrounds generated through the pile-up of two independent two neutrino decay events coinciding close in time. Together with a baseline resolution of 1.95 keV (FWHM) these performance parameters extrapolate to a background index from pile-up as low as 5 · 10 - 6  counts/keV/kg/yr in CUPID size crystals. The detector was calibrated up to the MeV region showing sufficient dynamic range for 0 ν β β searches. In combination with a SuperCDMS HVeV detector this setup also allowed us to perform a precision measurement of the scintillation time constants of Li 2 MoO 4 , which showed a primary component with a fast O(20  μ s) time scale.",
        "authors": [
            "G. Bratrud",
            "C. L. Chang",
            "R. Chen",
            "E. Cudmore",
            "E. Figueroa-Feliciano",
            "Z. Hong",
            "K. T. Kennard",
            "S. Lewis",
            "M. Lisovenko",
            "L. O. Mateo",
            "V. Novati",
            "V. Novosad",
            "E. Oliveri",
            "R. Ren",
            "J. A. Scarpaci",
            "B. Schmidt",
            "G. Wang",
            "L. Winslow",
            "V. G. Yefremenko",
            "J. Zhang"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158261",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Identifying Novel Emotions and Wellbeing of Horses from Videos Through Unsupervised Learning",
        "abstract": "first_pageDownload PDFsettingsOrder Article Reprints\r\nOpen AccessArticle\r\nIdentifying Novel Emotions and Wellbeing of Horses from Videos Through Unsupervised Learning\r\nby Aarya Bhave 1ORCID,Emily Kieson 2ORCID,Alina Hafner 3 andPeter A. Gloor 1,*ORCID\r\n1\r\nMassachusetts Institute of Technology, System Design & Management, Cambridge, MA 02142, USA\r\n2\r\nEquine International, Cambridge CB22 5LD, UK\r\n3\r\nTUM School of Computation, Information and Technology, Technical University of Munich, Arcisstraße 21, 80333 Munich, Germany\r\n*\r\nAuthor to whom correspondence should be addressed.\r\nSensors 2025, 25(3), 859; https://doi.org/10.3390/s25030859\r\nSubmission received: 5 January 2025 / Revised: 22 January 2025 / Accepted: 30 January 2025 / Published: 31 January 2025\r\n(This article belongs to the Special Issue Emotion Recognition and Cognitive Behavior Analysis Based on Sensors)\r\nDownloadkeyboard_arrow_down Browse Figures Review Reports Versions Notes\r\n\r\nAbstract\r\nThis research applies unsupervised learning on a large original dataset of horses in the wild to identify previously unidentified horse emotions. We construct a novel, high-quality, diverse dataset of 3929 images consisting of five wild horse breeds worldwide at different geographical locations. We base our analysis on the seven Panksepp emotions of mammals “Exploring”, “Sadness”, “Playing”, “Rage”, “Fear”, “Affectionate” and “Lust”, along with one additional emotion “Pain” which has been shown to be highly relevant for horses. We apply the contrastive learning framework MoCo (Momentum Contrast for Unsupervised Visual Representation Learning) on our dataset to predict the seven Panksepp emotions and “Pain” using unsupervised learning. We significantly modify the MoCo framework, building a custom downstream classifier network that connects with a frozen CNN encoder that is pretrained using MoCo. Our method allows the encoder network to learn similarities and differences within image groups on its own without labels. The clusters thus formed are indicative of deeper nuances and complexities within a horse’s mood, which can possibly hint towards the existence of novel and complex equine emotions.",
        "authors": [
            "Aarya Bhave",
            "Emily Kieson",
            "Alina Hafner",
            "Peter A. Gloor"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158247",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Biomanufacturing in the U.S.: A MIT Policy Brief",
        "abstract": "",
        "authors": [
            "J. Christopher Love",
            "Elisabeth B. Reynolds",
            "David Goldston",
            "Hannah E. Frye"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158134",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Energy Burden in the United States: An Analysis Using Decision Trees",
        "abstract": "The concept of energy burden (EB) continues to gain prominence in energy and associated policy research as energy prices rise and electricity and heating options diversify. This research offers a deeper understanding of EB dynamics and how EB can be addressed more effectively by discerning the interplay between regional environmental, social, and economic factors. Using decision trees (DTs), a powerful machine learning technique, we explore the multifaceted dynamics that shape EB across the United States (U.S.) by examining how factors like housing quality, demographic variations, access to energy sources, and regional economic conditions interact, creating distinct EB profiles across communities. Following a comprehensive review of existing literature and DT analysis, we map the results to identify the most significant factors influencing EB. We find that no single variable has a determinant effect on EB levels. While there is no uniform regional pattern, regions with higher population density exhibit a stronger correlation between EB and socioeconomic and other demographic factors such as educational attainment levels and racial segregation. Our findings underscore the significance of regional ecologies in shaping EB, revealing how localized environmental and economic contexts amplify or mitigate systemic inequities. Specifically, our analysis reveals significant regional disparities, highlighting the need for localized policies and interventions. We find that a one-size-fits-all approach is insufficient and that targeted, place-based strategies are necessary to address the specific needs of different communities. Policy interventions should prioritize energy democracy, address systemic inequities, and ensure universal energy access through participatory planning, financial assistance, and targeted initiatives such as housing rehabilitation, energy efficiency improvements, and incentives for underrepresented communities.",
        "authors": [
            "Jungwoo Chun",
            "Dania Ortiz",
            "Brooke Jin",
            "Nikita Kulkarni",
            "Stephen Hart",
            "Janelle Knox-Hayes"
        ],
        "journal_conference_name": "Energies",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158246",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Hypergeometric L-functions in average polynomial time, II",
        "abstract": "We describe an algorithm for computing, for all primes p ≤ X , the trace of Frobenius at p of a hypergeometric motive over Q in time quasilinear in X. This involves computing the trace modulo p e for suitable e; as in our previous work treating the case e = 1 , we combine the Beukers–Cohen–Mellit trace formula with average polynomial time techniques of Harvey and Harvey–Sutherland. The key new ingredient for e > 1 is an expanded version of Harvey’s “generic prime” construction, making it possible to incorporate certain p-adic transcendental functions into the computation; one of these is the p-adic Gamma function, whose average polynomial time computation is an intermediate step which may be of independent interest. We also provide an implementation in Sage and discuss the remaining computational issues around tabulating hypergeometric L-series.",
        "authors": [
            "Edgar Costa",
            "Kiran S. Kedlaya",
            "David Roe"
        ],
        "journal_conference_name": "Research in Number Theory",
        "publisher": "Springer International Publishing",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159062",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Review of Pnictogenides for Next-Generation Anode Materials for Sodium-Ion Batteries",
        "abstract": "With the growing market of secondary batteries for electric vehicles (EVs) and grid-scale energy storage systems (ESS), driven by environmental challenges, the commercialization of sodium-ion batteries (SIBs) has emerged to address the high price of lithium resources used in lithium-ion batteries (LIBs). However, achieving competitive energy densities of SIBs to LIBs remains challenging due to the absence of high-capacity anodes in SIBs such as the group-14 elements, Si or Ge, which are highly abundant in LIBs. This review presents potential candidates in metal pnictogenides as promising anode materials for SIBs to overcome the energy density bottleneck. The sodium-ion storage mechanisms and electrochemical performance across various compositions and intrinsic physical and chemical properties of pnictogenide have been summarized. By correlating these properties, strategic frameworks for designing advanced anode materials for next-generation SIBs were suggested. The trade-off relation in pnictogenides between the high specific capacities and the failure mechanism due to large volume expansion has been considered in this paper to address the current issues. This review covers several emerging strategies focused on improving both high reversible capacity and cycle stability.",
        "authors": [
            "Sion Ha",
            "Junhee Kim",
            "Dong Won Kim",
            "Jun Min Suh",
            "Kyeong-Ho Kim"
        ],
        "journal_conference_name": "Batteries",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158294",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Molecular and cellular characteristics of cerebrovascular cell types and their contribution to neurodegenerative diseases",
        "abstract": "Many diseases and disorders of the nervous system suffer from a lack of adequate therapeutics to halt or slow disease progression, and to this day, no cure exists for any of the fatal neurodegenerative diseases. In part this is due to the incredible diversity of cell types that comprise the brain, knowledge gaps in understanding basic mechanisms of disease, as well as a lack of reliable strategies for delivering new therapeutic modalities to affected areas. With the advent of single cell genomics, it is now possible to interrogate the molecular characteristics of diverse cell populations and their alterations in diseased states. More recently, much attention has been devoted to cell populations that have historically been difficult to profile with bulk single cell technologies. In particular, cell types that comprise the cerebrovasculature have become increasingly better characterized in normal and neurodegenerative disease contexts. In this review, we describe the current understanding of cerebrovasculature structure, function, and cell type diversity and its role in the mechanisms underlying various neurodegenerative diseases. We focus on human and mouse cerebrovasculature studies and discuss both origins and consequences of cerebrovascular dysfunction, emphasizing known cell type-specific vulnerabilities in neuronal and cerebrovascular cell populations. Lastly, we highlight how novel insights into cerebrovascular biology have impacted the development of modern therapeutic approaches and discuss outstanding questions in the field.",
        "authors": [
            "Francisco J. Garcia",
            "Myriam Heiman"
        ],
        "journal_conference_name": "Molecular Neurodegeneration",
        "publisher": "BioMed Central",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158284",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Noisy-channel language comprehension in aphasia: A Bayesian mixture modeling approach",
        "abstract": "Individuals with “agrammatic” receptive aphasia have long been known to rely on semantic plausibility rather than syntactic cues when interpreting sentences. In contrast to early interpretations of this pattern as indicative of a deficit in syntactic knowledge, a recent proposal views agrammatic comprehension as a case of “noisy-channel” language processing with an increased expectation of noise in the input relative to healthy adults. Here, we investigate the nature of the noise model in aphasia and whether it is adapted to the statistics of the environment. We first replicate findings that a) healthy adults (N = 40) make inferences about the intended meaning of a sentence by weighing the prior probability of an intended sentence against the likelihood of a noise corruption and b) their estimate of the probability of noise increases when there are more errors in the input (manipulated via exposure sentences). We then extend prior findings that adults with chronic post-stroke aphasia (N = 28) and healthy age-matched adults (N = 19) similarly engage in noisy-channel inference during comprehension. We use a hierarchical latent mixture modeling approach to account for the fact that rates of guessing are likely to differ between healthy controls and individuals with aphasia and capture individual differences in the tendency to make inferences. We show that individuals with aphasia are more likely than healthy controls to draw noisy-channel inferences when interpreting semantically implausible sentences, even when group differences in the tendency to guess are accounted for. While healthy adults rapidly adapt their inference rates to an increase in noise in their input, whether individuals with aphasia do the same remains equivocal. Further investigation of comprehension through a noisy-channel lens holds promise for a parsimonious understanding of language processing in aphasia and may suggest potential avenues for treatment.",
        "authors": [
            "Rachel Ryskin",
            "Edward Gibson",
            "Swathi Kiran"
        ],
        "journal_conference_name": "Psychonomic Bulletin & Review",
        "publisher": "Springer US",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158279",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Half-Covered ‘Glitter-Cake’ AM@SE Composite: A Novel Electrode Design for High Energy Density All-Solid-State Batteries",
        "abstract": "All-solid-state batteries (ASSBs) are pursued due to their potential for better safety and high energy density. However, the energy density of the cathode for ASSBs does not seem to be satisfactory due to the low utilization of active materials (AMs) at high loading. With small amount of solid electrolyte (SE) powder in the cathode, poor electrochemical performance is often observed due to contact loss and non-homogeneous distribution of AMs and SEs, leading to high tortuosity and limitation of lithium and electron transport pathways. Here, we propose a novel cathode design that can achieve high volumetric energy density of 1258 Wh L−1 at high AM content of 85 wt% by synergizing the merits of AM@SE core–shell composite particles with conformally coated thin SE shell prepared from mechanofusion process and small SE particles. The core–shell structure with an intimate and thin SE shell guarantees high ionic conduction pathway while unharming the electronic conduction. In addition, small SE particles play the role of a filler that reduces the packing porosity in the cathode composite electrode as well as between the cathode and the SE separator layer. The systematic demonstration of the optimization process may provide understanding and guidance on the design of electrodes for ASSBs with high electrode density, capacity, and ultimately energy density.",
        "authors": [
            "Min J. Kim",
            "Jin-Sung Park",
            "Jin W. Lee",
            "Sung E. Wang",
            "Dowoong Yoon",
            "Jong D. Lee",
            "Jung H. Kim",
            "Taeseup Song",
            "Ju Li",
            "Yun C. Kang",
            "Dae S. Jung"
        ],
        "journal_conference_name": "Nano-Micro Letters",
        "publisher": "Springer Nature Singapore",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158290",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Nature-inspired orientation-dependent toughening mechanism for TPMS ceramic architectures",
        "abstract": "Triply periodic minimal surfaces (TPMSs) have been extensively studied in many fields of engineering, including bone tissue scaffolds. Recent advancements in manufacturing have enabled the three-dimensional printing of ceramic porous architectures; however, their intrinsic brittleness limits its practical applications. It has been observed that the ossicles of the knobby starfish exhibit a mineralized TPMS structure with lattice distortions (i.e., dislocations), which effectively deviate the crack propagation and enhance the fracture energy. In this article, the aforementioned toughening mechanism has been introduced in a TPMS architecture. We employed finite element models to analyze the effective mechanical properties of the structures under compression, both in the elastic and post-elastic regimes. Our analysis reveals that the introduction of the dislocation induces variations in both elastic and fracture properties of the structures. With particular reference to the fracture behavior, a suitable oriented edge dislocation is able to alter the crack nucleation and propagation, resulting in a tougher structure. Both the elastic and fracture phenomena can be enhanced or reduced by changing the dislocation density.",
        "authors": [
            "Luca D’Andrea",
            "Ting Yang",
            "Ming Dao",
            "Pasquale Vena"
        ],
        "journal_conference_name": "MRS Bulletin",
        "publisher": "Springer International Publishing",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159073",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Design and Validation of a High-Fidelity Left Atrial Cardiac Simulator for the Study and Advancement of Left Atrial Appendage Occlusion",
        "abstract": "Purpose Atrial fibrillation (AF) is the most common chronic cardiac arrhythmia that increases the risk of stroke, primarily due to thrombus formation in the left atrial appendage (LAA). Left atrial appendage occlusion (LAAO) devices offer an alternative to oral anticoagulation for stroke prevention. However, the complex and variable anatomy of the LAA presents significant challenges to device design and deployment. Current benchtop models fail to replicate both anatomical variability and physiological hemodynamics, limiting their utility. This study introduces a novel left atrial cardiac simulator that incorporates patient-derived LAA models within a benchtop circulatory flow loop, enabling high-fidelity LAAO device testing and development. Methods A rigid, patient-derived left atrium (LA) model was 3D printed from segmented MRI data and modified to accommodate attachment of patient-specific LAA models. A library of LAA geometries was fabricated using silicone casting techniques to replicate the mechanical properties of native tissue. The LA-LAA model was integrated into a circulatory flow loop equipped with a pulsatile pump, pressure sensors, and flow probes, allowing real-time hemodynamic analysis. System tunability was demonstrated by varying heart rate, stroke volume, resistance, and compliance to simulate physiological and pathological conditions. Results The simulator accurately replicated LA pressure and flow waveforms, closely approximating physiological conditions. Changes in heart rate, stroke volume, and compliance effectively modulated LAP and LA inflow before and after LAAO. Distinct pressure and flow waveforms were observed with different LAA geometries. Hemodynamic analysis revealed increased left atrial pulse pressure after occlusion, with the greatest increase occurring after complete exclusion of the LAA. The simulator facilitated the evaluation of LAAO device performance, including metrics such as seal and PDL, and served as an effective training tool for iterative device deployment and recapture with visual and imaging-guided feedback. Conclusions The left atrial cardiac simulator offers a highly tunable and realistic platform for testing and developing LAAO devices. It also serves as an effective procedural training tool, allowing for the simulation of patient-specific anatomical and hemodynamic conditions. By enabling these advanced simulations, the simulator enhances pre-procedural planning, device sizing, and placement. This innovation represents a significant step toward advancing personalized medicine in atrial fibrillation management and improving LAAO outcomes.",
        "authors": [
            "Keegan Mendez",
            "Manisha Singh",
            "Patrick Willoughby",
            "Beatrice Ncho",
            "Aileen Liao",
            "Susan Su",
            "Megan Lim",
            "Elijah Lee",
            "Mohamad Alkhouli",
            "Hasan Alarouri",
            "Ellen T. Roche"
        ],
        "journal_conference_name": "Cardiovascular Engineering and Technology",
        "publisher": "Springer International Publishing",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158278",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Recycling of Tantalum Capacitors Via Sulfide Chemistry",
        "abstract": "The fabrication of tantalum capacitors represents more than 35 pct of the total consumption of metallic tantalum with an increasing demand for the high-technology sector. Tantalum capacitors contain a large concentration of tantalum, and the absence of niobium leads to interesting economic outcomes for potential recycling processes. The article discusses such recycling using sulfur, where an AB2O6 crystal structure analogous to the orthorhombic columbite-tantalite series is sulfidized. Sulfide affinities differences between A (Mn, Fe) and B (Nb, Ta) effectively separate the ternary oxide, capitalizing on the distinct chemical properties between A and B elements, in the absence of fluoridic acids. To bypass the fluoride-based chemistry process entirely, a proof of concept of tantalum disulfide (TaS2) production via sulfidation of Ta2O5 and its subsequent metallic reduction via molten sulfide electrolysis are also presented.",
        "authors": [
            "Charles Boury",
            "Antoine Allanore"
        ],
        "journal_conference_name": "Metallurgical and Materials Transactions B",
        "publisher": "Springer US",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158275",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Measurement of multidifferential cross sections for dijet production in proton–proton collisions at √s = 13 TeV",
        "abstract": "A measurement of the dijet production cross section is reported based on proton–proton collision data collected in 2016 at s = 13 Te V by the CMS experiment at the CERN LHC, corresponding to an integrated luminosity of up to 36.3 fb - 1 . Jets are reconstructed with the anti- k T algorithm for distance parameters of R = 0.4 and 0.8. Cross sections are measured double-differentially (2D) as a function of the largest absolute rapidity | y | max of the two jets with the highest transverse momenta p T and their invariant mass m 1 , 2 , and triple-differentially (3D) as a function of the rapidity separation y ∗ , the total boost y b , and either m 1 , 2 or the average p T of the two jets. The cross sections are unfolded to correct for detector effects and are compared with fixed-order calculations derived at next-to-next-to-leading order in perturbative quantum chromodynamics. The impact of the measurements on the parton distribution functions and the strong coupling constant at the mass of the Z boson is investigated, yielding a value of α S ( m Z ) = 0.1179 ± 0.0019.",
        "authors": [
            "Unknown author"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158258",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Uniform volumetric single-cell processing for organ-scale molecular phenotyping",
        "abstract": "Extending single-cell analysis to intact tissues while maintaining organ-scale spatial information poses a major challenge due to unequal chemical processing of densely packed cells. Here we introduce Continuous Redispersion of Volumetric Equilibrium (CuRVE) in nanoporous matrices, a framework to address this challenge. CuRVE ensures uniform processing of all cells in organ-scale tissues by perpetually maintaining dynamic equilibrium of the tissue's gradually shifting chemical environment. The tissue chemical reaction environment changes at a continuous, slow rate, allowing redispersion of unevenly distributed chemicals and preserving chemical equilibrium tissue wide at any given moment. We implemented CuRVE to immunologically label whole mouse and rat brains and marmoset and human tissue blocks within 1 day. We discovered highly variable regionalized reduction of parvalbumin immunoreactive cells in wild-type adult mice, a phenotype missed by the commonly used genetic labeling. We envision that our platform will advance volumetric single-cell processing and analysis, facilitating comprehensive single-cell level investigations within their spatial context in organ-scale tissues.",
        "authors": [
            "Dae Hee Yun",
            "Young-Gyun Park",
            "Jae Hun Cho",
            "Lee Kamentsky",
            "Nicholas B Evans",
            "Nicholas DiNapoli",
            "Katherine Xie",
            "Seo Woo Choi",
            "Alexandre Albanese",
            "Yuxuan Tian",
            "Chang Ho Sohn",
            "Qiangge Zhang",
            "Minyoung E Kim",
            "Justin Swaney",
            "Webster Guan",
            "Juhyuk Park",
            "Gabi Drummond",
            "Heejin Choi",
            "Luzdary Ruelas",
            "Guoping Feng",
            "Kwanghun Chung"
        ],
        "journal_conference_name": "Nature Biotechnology",
        "publisher": "Springer Science and Business Media LLC",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158176",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Cryptographic Censorship",
        "abstract": "We formulate and take two large strides towards proving a quantum version of the weak cosmic censorship conjecture. We first prove “Cryptographic Censorship”: a theorem showing that when the time evolution operator of a holographic CFT is approximately pseudorandom (or Haar random) on some code subspace, then there must be an event horizon in the corresponding bulk dual. This result provides a general condition that guarantees (in finite time) event horizon formation, with minimal assumptions about the global spacetime structure. Our theorem relies on an extension of a recent quantum learning no-go theorem and is proved using new techniques of pseudorandom measure concentration. To apply this result to cosmic censorship, we separate singularities into classical, semi-Planckian, and Planckian types. We illustrate that classical and semi-Planckian singularities are compatible with approximately pseudorandom CFT time evolution; thus, if such singularities are indeed approximately pseudorandom, by Cryptographic Censorship, they cannot exist in the absence of event horizons. This result provides a sufficient condition guaranteeing that seminal holographic results on quantum chaos and thermalization, whose general applicability relies on typicality of horizons, will not be invalidated by the formation of naked singularities in AdS/CFT.",
        "authors": [
            "Netta Engelhardt",
            "Åsmund Folkestad",
            "Adam Levine",
            "Evita Verheijden",
            "Lisa Yang"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158276",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Microhardness, Young’s and Shear Modulus in Tetrahedrally Bonded Novel II-Oxides and III-Nitrides",
        "abstract": "Direct wide-bandgap III-Ns and II-Os have recently gained considerable attention due to their unique electrical and chemical properties. These novel semiconductors are being explored to design short-wavelength light-emitting diodes, sensors/biosensors, photodetectors for integration into flexible transparent nanoelectronics/photonics to achieve high-power radio-frequency modules, and heat-resistant optical switches for communication networks. Knowledge of the elastic constants structural and mechanical properties has played crucial roles both in the basic understanding and assessing materials’ use in thermal management applications. In the absence of experimental structural, elastic constants, and mechanical traits, many theoretical simulations have yielded inconsistent results. This work aims to investigate the basic characteristics of tetrahedrally coordinated, partially ionic BeO, MgO, ZnO, and CdO, and partially covalent BN, AlN, GaN, and InN materials. By incorporating a bond-orbital and a valance force field model, we have reported comparative results of our systematic calculations for the bond length d\r\n, bond polarity αP\r\n, covalency αC\r\n, bulk modulus B\r\n, elastic stiffness C(=[c11−c12]2)\r\n, bond-stretching α\r\n and bond-bending β\r\n force constants, Kleinmann’s internal displacement ζ, and Born’s transverse effective charge e∗T\r\n. Correlations between C/B\r\n, β\r\n/α\r\n, c12c11,\r\n ζ, and\r\n αC \r\nrevealed valuable trends of structural, elastic, and bonding characteristics. The study noticed AlN and GaN (MgO and ZnO) showing nearly comparable features, while BN (BeO) is much harder compared to InN (CdO) material, with drastically softer bonding. Calculations of microhardness H\r\n, shear modulus G,\r\n and Young’s modulus Y\r\n have predicted BN (BeO) satisfying a criterion of super hardness. III-Ns (II-Os) could be vital in electronics, aerospace, defense, nuclear reactors, and automotive industries, providing integrity and performance at high temperature in high-power applications, ranging from heat sinks to electronic substrates to insulators in high-power devices.",
        "authors": [
            "Devki N. Talwar",
            "Piotr Becla"
        ],
        "journal_conference_name": "Department of Materials Science and Engineering",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158245",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "X-Mapper: fast and accurate sequence alignment via gapped x-mers",
        "abstract": "Sequence alignment is foundational to many bioinformatic analyses. Many aligners start by splitting sequences into contiguous, fixed-length seeds, called k-mers. Alignment is faster with longer, unique seeds, but more accurate with shorter seeds avoiding mutations. Here, we introduce X-Mapper, aiming to offer high speed and accuracy via dynamic-length seeds containing gaps, called gapped x-mers. We observe 11–24-fold fewer suboptimal alignments analyzing a human reference and 3–579-fold lower inconsistency across bacterial references than other aligners, improving on 53% and 30% of reads aligned to non-target strains and species, respectively. Other seed-based analysis algorithms might benefit from gapped x-mers too.",
        "authors": [
            "Jeffry M. Gaston",
            "Eric J. Alm",
            "An-Ni Zhang"
        ],
        "journal_conference_name": "Genome Biology",
        "publisher": "BioMed Central",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158285",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Thermogelation of nanoemulsions stabilized by a commercial pea protein isolate: high-pressure homogenization defines gel strength",
        "abstract": "The impact of animal-based food production on climate change drives the development of plant-based alternatives. We demonstrate the use of colloidal thermogelation on a real nanoemulsion system to create structured gels that could be of interest for thermo-mechanical processing of next-generation plant-based food applications. We use a commercial pea protein isolate (PPI) without further purification to stabilize a 20 vol% peanut oil-in-water nanoemulsion at pH = 7 by high-pressure homogenization (HPH) and demonstrate the temperature induced gelation behavior of the nanoemulsion as a function of the HPH processing parameters. Bright-field and laser scanning confocal fluorescence microscopy reveals a diverse microstructure of the aqueous PPI dispersions, with a large amount of insoluble protein particles, cell-wall debris particles, and lipid inclusions. Sedimentation of particulates is prevented by HPH treatment and leads to a loss of the dispersion's thermogelation properties. The non-gelling PPI dispersion stabilizes nanoemulsions and the insoluble components of the PPI dispersions persist throughout the HPH processing. We perform a systematic rheological investigation of the effect of HPH processing on thermogelation and demonstrate that the number of HPH passes n and HPH pressure P control the average nanoemulsion droplet size measured by DLS at a 90° scattering angle. We show that the droplet size defines the final gel strength with a strong inverse dependence of the elastic modulus on droplet size. Furthermore, processing can lead to heterogeneously structured gels that yield over a large strain amplitude range.",
        "authors": [
            "Damian Renggli",
            "Patrick S Doyle"
        ],
        "journal_conference_name": "Soft Matter",
        "publisher": "Royal Society of Chemistry",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158249",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Impact of lesion preparation-induced calcified plaque defects in vascular intervention for atherosclerotic disease: in silico assessment",
        "abstract": "Percutaneous coronary interventions in highly calcified atherosclerotic lesions are challenging due to the high mechanical stiffness that significantly restricts stent expansion. Intravascular lithotripsy (IVL) is a novel vessel preparation technique with the potential to improve interventional outcomes by inducing microscopic and macroscopic cracks to enhance stent expansion. However, the exact mechanism of action for IVL is poorly understood, and it remains unclear whether the improvement in-stent expansion is caused by either the macro-cracks allowing the vessel to open or the micro-cracks altering the bulk material properties. In silico models offer a robust means to examine (a) diverse lesion morphologies, (b) a range of lesion modifications to address these deficiencies, and (c) the correlation between calcium morphology alteration and improved stenting outcomes. These models also help identify which lesions would benefit the most from IVL. In this study, we develop an in silico model of stent expansion to study the effect of macro-crack morphology on interventional outcomes in clinically inspired geometries. Larger IVL-induced defects promote more post-stent lumen gain. IVL seems to induce better stenting outcomes for large calcified lesions. IVL defects that split calcified plaque in two parts are the most beneficial for stenting angioplasty, regardless of the calcified plaque size. Location of the IVL defect does not seem to matter with respect to lumen gain. These findings underscore the potential of IVL to enhance lesion compliance and improve clinical outcomes in PCI. The macroscopic defects induced by IVL seem to have a substantial impact on post-stent outcomes.",
        "authors": [
            "Jonas Sogbadji",
            "Karim Kadry",
            "Gianluca Poletti",
            "Francesca Berti",
            "Elazer R. Edelman",
            "Farhad R. Nezami"
        ],
        "journal_conference_name": "Biomechanics and Modeling in Mechanobiology",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158262",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "From burst to controlled release: using hydrogel crosslinking chemistry to tune release of micro-crystalline active pharmaceutical ingredients",
        "abstract": "Hydrogels have been widely studied as substrates for drug delivery and tissue engineering owing to their biocompatibility and ability to swell in aqueous media. Encapsulation of lipophilic active pharmaceutical ingredients (API) as crystalline micro-/nanoparticles within hydrogel formulations has shown promise for improving their bioavailability and achieving high drug load. Despite the size reduction of the API within the hydrogel mesh, the bioavailability of these formulations is largely governed by the inherent ability of the hydrogel polymer backbone to release the API. In this work, Michael addition-based Polyethylene glycol (PEG) hydrogels are developed for micro-crystalline fenofibrate (Fen) encapsulation. Using a parallelized step emulsification device, API nanoemulsion (NE) loaded micro-hydrogels are fabricated and subsequently subjected to anti-solvent extraction for API crystallization. The bi-molecular nature of the Michael addition reaction provides modular incorporation of crosslinking functional groups leading to precise temporal control over hydrogel degradation, thereby offering a sensitive handle on the release of micro-crystalline fenofibrate. By merely changing the chemical identity of the hydrogel cross-link, complete Fen release could be tuned from 4 hours to 10 days. Furthermore, the interaction of crystallizing Fen and PEG within the micro-hydrogel environment led to eutectic formation. This unique feature offered a second handle on the Fen release from the composite micro-hydrogels.",
        "authors": [
            "Purnima N Manghnani",
            "Arif Z Nelson",
            "Kelvin Wong",
            "Yi Wei Lee",
            "Saif A Khan",
            "Patrick S Doyle"
        ],
        "journal_conference_name": "RSC Pharmaceutics",
        "publisher": "Royal Society of Chemistry",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158253",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Injectable sustained-release hydrogel for high-concentration antibody delivery",
        "abstract": "There is an increasing interest in subcutaneous (SC) delivery as an alternative to the traditional intravenous (IV) for immunotherapies and other advanced therapies. High-concentration formulations of antibodies are needed to meet the limited-volume requirements of subcutaneous SC delivery. Despite this need, there remain challenges in delivering stable and injectable antibodies in these high concentrations. Hydrogel encapsulation of amorphous solid antibodies has been proven to improve the stability and injectability of high-concentration antibody formulations. However, the antibody is quickly released from the hydrogel due to the material's porosity, leading to rapid, uncontrolled drug release kinetics undesirable for the drug's efficacy and safety. In this paper, we propose a dual-network composite hydrogel which leverages interactions between the two polymer networks to achieve controlled release of the antibody. We load the solid form of the antibody at high concentrations within alginate hydrogel microparticles which are then suspended in thermogelling methylcellulose solution to formulate the in situ gelling composite hydrogel. By facile chemical modification of the alginate to tune the microparticles’ gel properties and alginate–methylcellulose interactions, we demonstrate how the composite system can delay release of the drug in a tunable manner and achieve a near-zero order release profile for improved therapeutic efficacy. We show acceptable injectability properties of the composite hydrogel at high antibody concentrations, highlighting the functionalities of dualnetwork encapsulation. We imagine this composite system to be applicable for the sustained delivery of various therapeutic protein forms, especially for high-loading SC formulations.",
        "authors": [
            "Talia Zheng",
            "Patrick S Doyle"
        ],
        "journal_conference_name": "RSC Pharmaceutics",
        "publisher": "Royal Society of Chemistry",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158252",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Measurement of inclusive and diferential cross sections of single top quark production in association with a W boson in proton-proton collisions at √s = 13.6 TeV",
        "abstract": "The first measurement of the inclusive and normalised differential cross sections of single top quark production in association with a W boson in proton-proton collisions at a centre-of-mass energy of 13.6 TeV is presented. The data were recorded with the CMS detector at the LHC in 2022, and correspond to an integrated luminosity of 34.7 fb−1. The analysed events contain one muon and one electron in the final state. For the inclusive measurement, multivariate discriminants exploiting the kinematic properties of the events are used to separate the signal from the dominant top quark-antiquark production background. A cross section of 82.3 ± 2.1 stat − 9.7 + 9.9 syst ± 3.3 lumi pb is obtained, consistent with the predictions of the standard model. A fiducial region is defined according to the detector acceptance to perform the differential measurements. The resulting differential distributions are unfolded to particle level and show good agreement with the predictions at next-to-leading order in perturbative quantum chromodynamics.",
        "authors": [
            "A. Hayrapetyan",
            "A. Tumasyan",
            "W. Adam",
            "J. W. Andrejkovic",
            "T. Bergauer",
            "S. Chatterjee",
            "K. Damanakis",
            "M. Dragicevic",
            "P. S. Hussain",
            "M. Jeitler",
            "N. Krammer",
            "A. Li",
            "D. Liko",
            "I. Mikulec",
            "J. Schieck",
            "R. Schöfbeck",
            "D. Schwarz",
            "M. Sonawane",
            "The CMS collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158260",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Waves dangerous, domesticated, and diagnostic",
        "abstract": "This paper, based on a keynote presented at the MARE People and the Sea Conference 2023 as well as on material from A Book of Waves, examines how oceanographers and coastal engineers in the United States, the Netherlands, Australia, Japan, and Bangladesh study and represent waves. Waves, seen as both chaotic and ordered, ephemeral and enduring, offer insights into how science engages with environmental, national, and planetary futures. The discussion begins in the Netherlands, where centuries-old efforts to resist waves in a nation below sea level have evolved into “building-with-nature” strategies, reframing waves as collaborators in environmental resilience. Historical contexts, from wave folklore to physical scale models, underpin this shift in Dutch wave science. Next, I explore the wave simulation laboratory at Oregon State University, where researchers model tsunami risks from the Cascadia fault line. These experiments connect the Pacific Northwest with Japan’s tsunami research, highlighting challenges in adapting wave knowledge across regions. Finally, I turn to Bangladesh’s Ganges Delta, where Dutch hydrological expertise was applied in mid-20th-century development projects, often with uneven results. This case illustrates the complexities of transposing wave science into diverse settings. I conclude by reflecting on how these scientific practices contribute to understanding the Anthropocene, particularly from the perspective of the Global South’s oceans.",
        "authors": [
            "Stefan Helmreich"
        ],
        "journal_conference_name": "Maritime Studies",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158288",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Instrument stiffness artifacts: avoiding bad data with operational limit lines of G max and E max",
        "abstract": "We derive an operating limit line for the non-ideal artifacts caused by machine stiffness (instrument compliance) which causes measured apparent viscoelastic moduli to be systematically lower than the true values. The limit is represented as a maximum measurable apparent shear modulus G max , or tensile modulus E max , which can be shown explicitly on plots of viscoelastic moduli independent of the applied displacement, load, or frequency. Uncorrected data should be much lower than these limits. Corrected data can be above these limits and credible. These interpretations are supported by studying how correction equations can be re-written in terms of G max or E max and how error propagates in the corrections. We also show how the dynamic compliance representation leads to simpler corrections and how machine stiffness can be calibrated from apparent dynamic compliance measurements of a single sample at two different geometry conditions. Equations are provided for rotational rheometers as well as linear displacement dynamic mechanical analyzers. Used as an operational limit line, G max or E max , the method can assess the credibility of data from others—even without access to their primary data of displacement, force, torque, or amount of correction, which are rarely reported. The method can also anticipate future issues before data are taken, e.g., to understand operational limits when selecting instruments and test geometries.",
        "authors": [
            "Mohammad T. Hossain",
            "Christopher W. Macosko",
            "Gareth H. McKinley",
            "Randy H. Ewoldt"
        ],
        "journal_conference_name": "Rheologica Acta",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159054",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Maximizing Free Energy Gain",
        "abstract": "Maximizing the amount of work harvested from an environment is important for a wide variety of biological and technological processes, from energy-harvesting processes such as photosynthesis to energy storage systems such as fuels and batteries. Here, we consider the maximization of free energy&mdash;and by extension, the maximum extractable work&mdash;that can be gained by a classical or quantum system that undergoes driving by its environment. We consider how the free energy gain depends on the initial state of the system while also accounting for the cost of preparing the system. We provide simple necessary and sufficient conditions for increasing the gain of free energy by varying the initial state. We also derive simple formulae that relate the free energy gained using the optimal initial state rather than another suboptimal initial state. Finally, we demonstrate that the problem of finding the optimal initial state may have two distinct regimes, one easy and one difficult, depending on the temperatures used for preparation and work extraction. We illustrate our results on a simple model of an information engine.",
        "authors": [
            "Artemy Kolchinsky",
            "Iman Marvian",
            "Can Gokler",
            "Zi-Wen Liu",
            "Peter Shor",
            "Oles Shtanko",
            "Kevin Thompson",
            "David Wolpert",
            "Seth Lloyd"
        ],
        "journal_conference_name": "Entropy",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158156",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "On Modular Invariance of Quantum Affine W-Algebras",
        "abstract": "Abstract We find modular transformations of normalized characters for the following W-algebras: (a) W k min ( g ) , where g = D n ( n ≥ 4 ) , or E 6 , E 7 , E 8 , and k is a negative integer ≥ - 2 , or ≥ - h ∨ 6 - 1 , respectively; (b) quantum Hamiltonian reduction of the g ^ -module L ( k Λ 0 ) , where g is a simple Lie algebra, f is its non-zero nilpotent element, and k is a principal admissible level with the denominator u > θ ( x ) , where 2x is the Dynkin characteristic of f, and θ is the highest root of g . We prove that these vertex algebras are modular invariant. A conformal vertex algebra V is called modular invariant if its character t r V q L 0 - c / 24 converges to a holomorphic modular function in the complex upper half-plane on a congruence subgroup. We find explicit formulas for their characters. Modular invariance of V is important since, in particular, conjecturally it implies that V is simple, and that V is rational, provided that it is lisse.",
        "authors": [
            "Victor G. Kac",
            "Minoru Wakimoto"
        ],
        "journal_conference_name": "Communications in Mathematical Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159039",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Shift invariance of half space integrable models",
        "abstract": "We formulate and establish symmetries of certain integrable half space models, analogous to recent results on symmetries for models in a full space. Our starting point is the colored stochastic six vertex model in a half space, from which we obtain results on the asymmetric simple exclusion process, as well as for the beta polymer through a fusion procedure which may be of independent interest. As an application, we establish a distributional identity between the absorption time in a type B analogue of the oriented swap process and last passage times in a half space, establishing the Baik–Ben Arous–Péché phase transition for the absorption time. The proof uses Hecke algebras and integrability of the six vertex model through the Yang–Baxter and reflection equations.",
        "authors": [
            "Jimmy He"
        ],
        "journal_conference_name": "Probability Theory and Related Fields",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158248",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Preliminary results on the long-term operation of RPCs with eco-friendly gas mixtures under irradiation at the CERN Gamma Irradiation Facility",
        "abstract": "Since 2019, a collaboration between researchers from various institutes and experiments (i.e., ATLAS, CMS, ALICE, LHCb/SHiP and the CERN EP-DT group) has been operating several RPCs with diverse electronics, gas gap thicknesses and detector layouts at the CERN Gamma Irradiation Facility (GIF++). The studies aim at assessing the performance of RPCs when filled with new eco-friendly gas mixtures in avalanche mode and in view of evaluating possible aging effects after long high background irradiation periods, for example, high-luminosity LHC phase. This challenging research is also part of a task of the European AidaInnova project. A promising eco-friendly gas identified for RPC operation is the tetrafluoruropropene (C 3 H 2 F 4 , commercially known as HFO-1234ze) that has been studied at the CERN GIF++ in combination with different percentages of CO 2 . Between the end of 2021 and 2022, several beam tests have been carried out to establish the performance of RPCs operated with such mixtures before starting the irradiation campaign for the aging study. Results of these tests for different RPCs layouts and different gas mixtures, under increasing background rates are presented here, together with the preliminary outcome of the detector aging tests.",
        "authors": [
            "L. Quaglia",
            "D. Ramos",
            "M. Abbrescia",
            "G. Aielli",
            "R. Aly",
            "M. C. Arena",
            "M. Barroso",
            "L. Benussi",
            "S. Bianco",
            "D. Boscherini",
            "F. Bordon",
            "A. Bruni",
            "S. Buontempo",
            "M. Busato",
            "P. Camarri",
            "R. Cardarelli",
            "L. Congedo",
            "D. De Jesus Damiao",
            "M. De Serio",
            "A. Di Ciacco"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158287",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Decoding Codon Bias: The Role of tRNA Modifications in Tissue-Specific Translation",
        "abstract": "The tRNA epitranscriptome has been recognized as an important player in mRNA translation regulation. Our knowledge of the role of the tRNA epitranscriptome in fine-tuning translation via codon decoding at tissue or cell levels remains incomplete. We analyzed tRNA expression and modifications as well as codon optimality across seven mouse tissues. Our analysis revealed distinct enrichment patterns of tRNA modifications in different tissues. Queuosine (Q) tRNA modification was most enriched in the brain compared to other tissues, while mitochondrial tRNA modifications and tRNA expression were highest in the heart. Using this observation, we synthesized, and delivered in vivo, codon-mutated EGFP for Q-codons, where the C-ending Q-codons were replaced with U-ending codons. The protein levels of mutant EGFP were downregulated in liver, which is poor in Q, while in brain EGFP, levels did not change. These data show that understanding tRNA modification enrichments across tissues is not only essential for understanding codon decoding and bias but can also be utilized for optimizing gene and mRNA therapeutics to be more tissue-, cell-, or condition-specific.",
        "authors": [
            "Daisuke Ando",
            "Sherif Rashad",
            "Thomas J. Begley",
            "Hidenori Endo",
            "Masashi Aoki",
            "Peter C. Dedon",
            "Kuniyasu Niizuma"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158155",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Rapid prediction of conformationally-dependent DFT-level descriptors using graph neural networks for carboxylic acids and alkyl amines",
        "abstract": "Data-driven reaction discovery and development is a growing field that relies on the use of molecular descriptors to capture key information about substrates, ligands, and targets. Broad adaptation of this strategy is hindered by the associated computational cost of descriptor calculation, especially when considering conformational flexibility. Descriptor libraries can be precomputed agnostic of application to reduce the computational burden of data-driven reaction development. However, as one often applies these models to evaluate novel hypothetical structures, it would be ideal to predict the descriptors of compounds on-the-fly. Herein, we report DFT-level descriptor libraries for conformational ensembles of 8528 carboxylic acids and 8172 alkyl amines towards this goal. Employing 2D and 3D graph neural network architectures trained on these libraries culminated in the development of predictive models for molecule-level descriptors, as well as the bond- and atom-level descriptors for the conserved reactive site (carboxylic acid or amine). The predictions were confirmed to be robust for an external validation set of medicinally-relevant carboxylic acids and alkyl amines. Additionally, a retrospective study correlating the rate of amide coupling reactions demonstrated the suitability of the predicted DFT-level descriptors for downstream applications. Ultimately, these models enable high-fidelity predictions for a vast number of potential substrates, greatly increasing accessibility to the field of data-driven reaction development.",
        "authors": [
            "Brittany C Haas",
            "Melissa A Hardy",
            "Shree Sowndarya S. V.",
            "Keir Adams",
            "Connor W Coley",
            "Robert S Paton",
            "Matthew S Sigman"
        ],
        "journal_conference_name": "Digital Discovery",
        "publisher": "Royal Society of Chemistry",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158096",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "GCBF+: A Neural Graph Control Barrier Function Framework for Distributed Safe Multi-Agent Control",
        "abstract": "Distributed, scalable, and safe control of large-scale multi-agent systems is a challenging problem. In this paper, we design a distributed framework for safe multi-agent control in large-scale environments with obstacles, where a large number of agents are required to maintain safety using only local information and reach their goal locations. We introduce a new class of certificates, termed graph control barrier function (GCBF), which are based on the well-established control barrier function theory for safety guarantees and utilize a graph structure for scalable and generalizable distributed control of MAS. We develop a novel theoretical framework to prove the safety of an arbitrary-sized MAS with a single GCBF. We propose a new training framework GCBF+ that uses graph neural networks to parameterize a candidate GCBF and a distributed control policy. The proposed framework is distributed and is capable of taking point clouds from LiDAR, instead of actual state information, for real-world robotic applications. We illustrate the efficacy of the proposed method through various hardware experiments on a swarm of drones with objectives ranging from exchanging positions to docking on a moving target without collision. Additionally, we perform extensive numerical experiments, where the number and density of agents, as well as the number of obstacles, increase. Empirical results show that in complex environments with agents with nonlinear dynamics (e.g., Crazyflie drones), GCBF+ outperforms the hand-crafted CBF-based method with the best performance by up to 20% for relatively small-scale MAS with up to 256 agents, and leading reinforcement learning (RL) methods by up to 40% for MAS with 1024 agents. Furthermore, the proposed method does not compromise on the performance, in terms of goal reaching, for achieving high safety rates, which is a common trade-off in RL-based methods.",
        "authors": [
            "Songyuan Zhang",
            "Oswin So",
            "Kunal Garg",
            "Chuchu Fan"
        ],
        "journal_conference_name": "IEEE Transactions on Robotics",
        "publisher": "Institute of Electrical and Electronics Engineers",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158072",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Contactless Multi-Modal Sensing Approach for Material Assessment and Recovery in Building Deconstruction",
        "abstract": "As material scarcity and environmental concerns grow, material reuse and waste reduction are gaining attention based on their potential to reduce carbon emissions and promote net-zero buildings. This study develops an innovative approach that combines multi-modal sensing technologies with machine learning to enable contactless assessment of in situ building materials for reuse potential. By integrating thermal imaging, red, green, and blue (RGB) cameras, as well as depth sensors, the system analyzes material conditions and reveals hidden geometries within existing buildings. This approach enhances material understanding by analyzing existing materials, including their compositions, histories, and assemblies. A case study on drywall deconstruction demonstrates that these technologies can effectively guide the deconstruction process, potentially reducing material costs and carbon emissions significantly. The findings highlight feasible scenarios for drywall reuse and offer insights into improving existing deconstruction techniques through automated feedback and visualization of cut lines and fastener positions. This research indicates that contactless assessment and automated deconstruction methods are technically viable, economically advantageous, and environmentally beneficial. Serving as an initial step toward novel methods to view and classify existing building materials, this study lays a foundation for future research, promoting sustainable construction practices that optimize material reuse and reduce negative environmental impact.",
        "authors": [
            "Sophia Cabral",
            "Mikita Klimenka",
            "Fopefoluwa Bademosi",
            "Damon Lau",
            "Stefanie Pender",
            "Lorenzo Villaggi",
            "James Stoddart",
            "James Donnelly",
            "Peter Storey",
            "David Benjamin"
        ],
        "journal_conference_name": "Sustainability",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158153",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Response Regulator OmpR Negatively Controls the Expression of Genes Implicated in Tilimycin and Tilivalline Cytotoxin Production in Klebsiella oxytoca",
        "abstract": "Klebsiella oxytoca toxigenic strains represent a critical health threat, mainly due to their link to antibiotic-associated hemorrhagic colitis. This serious condition results from the bacteria’s ability to produce tilimycin and tilivalline cytotoxins. Our research highlights the pivotal role of OmpR, a key regulator within the EnvZ/OmpR two-component system, in controlling the virulence factors associated with K. oxytoca. Our findings strongly indicate that OmpR is a repressor of the aroX and npsA genes, the first genes of aroX and NRPS operons, respectively, which are indispensable for producing these enterotoxins. Notably, in the absence of OmpR, we observe a significant increase in cytotoxic effects on Caco-2 cells. These observations identify OmpR as a crucial negative transcription regulator for both operons, effectively managing the release of these cytotoxins. This research deepens our understanding of the mechanisms of toxigenic K. oxytoca and opens promising avenues for targeting OmpR for new therapeutic interventions. By focusing on this innovative approach, we can develop more effective solutions to combat this pressing health challenge, ultimately improving patient outcomes against this pathogen.",
        "authors": [
            "Ramón G. Varela-Nájera",
            "Miguel A. De la Cruz",
            "Jorge Soria-Bustos",
            "Carmen González-Horta",
            "Ma Carmen E. Delgado-Gardea",
            "Jorge A. Yáñez-Santos",
            "María L. Cedillo",
            "Hidetada Hirakawa",
            "James G. Fox",
            "Blanca Sánchez-Ramírez",
            "Miguel A. Ares"
        ],
        "journal_conference_name": "Microorganisms",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158154",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Visual Acuity Outcomes and Influencing Factors in a Cohort of UK Real-World Diabetic Macular Oedema Patients During the First Two Years of Anti-VEGF Treatment",
        "abstract": "Background/Objectives: The visual acuity (VA) outcomes after the first and second years of anti-vascular endothelial growth factor (anti-VEGF) treatment in patients with diabetic macular oedema (DMO) were evaluated, and the factors associated with treatment success were investigated. Methods: Using Medisoft electronic medical records (UK), this retrospective cohort study analysed VA outcomes, changes, and determinants in DMO patients at year 1 and year 2 after initial anti-VEGF injection. Descriptive analysis examined baseline demographics and clinical characteristics, while regression models were used to assess associations between these factors and changes in VA. Results: 728 DMO patients (1035 eyes) treated with anti-VEGFs (ranibizumab, aflibercept, or bevacizumab) at the Northern Ireland Mater Macular Clinic from 2008 to 2021 were evaluated. The mean age was 64.5 (SD 12.8) years, and 59.6% were male. In the first year, the median annual injection number and interval were 6.0 (IQR 5.0&ndash;8.0) and 6.1 weeks (IQR 5.4&ndash;7.8), respectively, and in the second year, they were 3.0 (IQR 2.0&ndash;5.0) and 10.0 weeks (IQR 6.5&ndash;20.1). In the first two treatment years, 83.4% and 79.8% of eyes had improved/stable VA (ISVA) respectively. The injection number, interval, baseline VA, age, and proliferative diabetic retinopathy (PDR) significantly impacted VA outcomes. Conclusions: Our study confirms the effectiveness of anti-VEGF treatments in improving or maintaining vision for DMO patients, consistent with previous real-world clinical data. An elder age, a better baseline VA, low annual injection numbers (&lt;5), and less frequent injection intervals (&ge;12 weeks) were negatively associated with ISVA success in the first two years. These findings have implications for managing patient expectations, allocating resources, and understanding DMO clinical management.",
        "authors": [
            "Qing Wen",
            "Helene Karcher",
            "David M. Wright",
            "Samriddhi Buxy Sinha",
            "Usha Chakravarthy",
            "Catarina Santos",
            "Franklin Igwe",
            "Recivall Salongcay",
            "Katie Curran",
            "Tunde Peto"
        ],
        "journal_conference_name": "Pharmaceutics",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158152",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Search for B ( s ) ∗ 0 → μ + μ - in B c + → π + μ + μ - decays",
        "abstract": "A search for the very rare B ∗ 0 → μ + μ - and B s ∗ 0 → μ + μ - decays is conducted by analysing the B c + → π + μ + μ - process. The analysis uses proton-proton collision data collected with the LHCb detector between 2011 and 2018, corresponding to an integrated luminosity of 9 \\,fb - 1 . The signal signatures correspond to simultaneous peaks in the μ + μ - and π + μ + μ - invariant masses. No evidence for an excess of events over background is observed for either signal decay mode. Upper limits at the 90 % confidence level are set on the branching fractions relative to that for B c + → J / ψ π + decays, R B ∗ 0 ( μ + μ - ) π + / J / ψ π + < 3.8 × 10 - 5 and R B s ∗ 0 ( μ + μ - ) π + / J / ψ π + < 5.0 × 10 - 5.",
        "authors": [
            "LHCb Collaboration"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158251",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Global surgery and climate change: how global surgery can prioritise both the health of the planet and its people",
        "abstract": "Climate change is an emerging global health crisis, disproportionately affecting low- and middle-income countries (LMICs) where health outcomes are increasingly compromised by environmental stressors such as pollution, natural disasters, and human migration. With a focus on promoting health equity, Global Surgery advocates for expanding access to surgical care and enhancing health outcomes, particularly in resource-limited and disaster-affected areas like LMICs. The healthcare industry—and more specifically, surgical care—significantly contributes to the global carbon footprint, primarily through resource-intensive settings, i.e. operating rooms that generate greenhouse gases and substantial medical waste. Therefore, Global Surgery efforts aimed at improving surgical access through an increase in surgical volumes may inadvertently exacerbate health challenges for vulnerable populations by further contributing to environmental degradation. This predicament is particularly pronounced in LMICs, who already suffer from a disproportionate share of the global burden of disease, and where the demand for surgery is rising without corresponding resilient infrastructure. LMICs face a double jeopardy of health inequity coupled with climate vulnerability. As a movement positioned to improve health around the world, Global Surgery has an increasingly significant role in envisioning and ensuring a sustainable future. Global Surgery initiatives must prioritise sustainable infrastructure in both high-income countries (HICs) and LMICs, all while accounting for the unequal polluting contributions between HICs and LMICs and, consequently, moral responsibilities moving forward. Moreover, through targeting upstream causes of poor health at urban and perioperative levels, Global Surgery’s interventions may help to reduce the global burden of disease—avoiding preventable surgeries and their carbon footprints from the outset. Altogether, Global Surgery and climate change are two matters of social justice whose solutions must synergistically centralise the health of both the planet and its most vulnerable people.",
        "authors": [
            "Sophia Chen",
            "Yvan Zolo",
            "Lumbani Ngulube",
            "Moses Isiagi",
            "Salome Maswime"
        ],
        "journal_conference_name": "BMC Surgery",
        "publisher": "BioMed Central",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158061",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Taurine prevents mitochondrial dysfunction and protects mitochondria from reactive oxygen species and deuterium toxicity",
        "abstract": "Taurine, although not a coding amino acid, is the most common free amino acid in the body. Taurine has multiple and complex functions in protecting mitochondria against oxidative-nitrosative stress. In this comprehensive review paper, we introduce a novel potential role for taurine in protecting from deuterium (heavy hydrogen) toxicity. This can be of crucial impact to either normal or cancer cells that have highly different mitochondrial redox status. Deuterium is an isotope of hydrogen with a neutron as well as a proton, making it about twice as heavy as hydrogen. We first explain the important role that the gut microbiome and the gut sulfomucin barrier play in deuterium management. We describe the synergistic effects of taurine in the gut to protect against the deleterious accumulation of deuterium in the mitochondria, which disrupts ATP synthesis by ATPase pumps. Moreover, taurine’s derivatives, N-chlorotaurine (NCT) and N-bromotaurine (NBrT), produced through spontaneous reaction of taurine with hypochlorite and hypobromite, have fascinating regulatory roles to protect from oxidative stress and beyond. We describe how taurine could potentially alleviate deuterium stress, primarily through metabolic collaboration among various gut microflora to produce deuterium depleted nutrients and deuterium depleted water, and in this way protect against leaky gut barrier, inflammatory bowel disease, and colon cancer.",
        "authors": [
            "Stephanie Seneff",
            "Anthony M. Kyriakopoulos"
        ],
        "journal_conference_name": "Amino Acids",
        "publisher": "Springer Vienna",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158031",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "\"My Very Subjective Human Interpretation\": Domain Expert Perspectives on Navigating the Text Analysis Loop for Topic Models",
        "abstract": "Practitioners dealing with large text collections frequently use topic models such as Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF) in their projects to explore trends. Despite twenty years of accrued advancement in natural language processing tools, these models are found to be slow and challenging to apply to text exploration projects. In our work, we engaged with practitioners (n=15) who use topic modeling to explore trends in large text collections to understand their project workflows and investigate which factors often slow down the processes and how they deal with such errors and interruptions in automated topic modeling. Our findings show that practitioners are required to diagnose and resolve context-specific problems with preparing data and models and need control for these steps, especially for data cleaning and parameter selection. Our major findings resonate with existing work across CSCW, computational social science, machine learning, data science, and digital humanities. They also leave us questioning whether automation is actually a useful goal for tools designed for topic models and text exploration.",
        "authors": [
            "Alexandra Schofield",
            "Siqi Wu",
            "Theo Bayard de Volo",
            "Tatsuki Kuze",
            "Alfredo Gomez",
            "Sharifa Sultana"
        ],
        "journal_conference_name": "Proceedings of the ACM on Human-Computer Interaction",
        "publisher": "Association for Computing Machinery",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158190",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Effects of maze appearance on maze solving",
        "abstract": "As mazes are typically complex, cluttered stimuli, solving them is likely limited by visual crowding. Thus, several aspects of the appearance of the maze – the thickness, spacing, and curvature of the paths, as well as the texture of both paths and walls – likely influence the performance. In the current study, we investigate the effects of perceptual aspects of maze design on maze-solving performance to understand the role of crowding and visual complexity. We conducted two experiments using a set of controlled stimuli to examine the effects of path and wall thickness, as well as the style of rendering used for both paths and walls. Experiment 1 finds that maze-solving time increases with thicker paths (thus thinner walls). Experiment 2 replicates this finding while also showing that maze-solving time increases when mazes have wavy walls, which are likely more crowded, rather than straight walls. Our findings imply a role of both crowding and figure/ground segmentation in mental maze solving and suggest reformulating the growth cone models.",
        "authors": [
            "Yelda Semizer",
            "Dian Yu",
            "Qianqian Wan",
            "Benjamin Balas",
            "Ruth Rosenholtz"
        ],
        "journal_conference_name": "Attention, Perception, & Psychophysics",
        "publisher": "Springer US",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158073",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Sequence‐Sensitivity in Functional Synthetic Polymer Properties",
        "abstract": "Recently, a new class of synthetic methyl methacrylate‐based random heteropolymers (MMA‐based RHPs) has displayed protein‐like properties. Their function appears to be insensitive to the precise sequence. Here, through atomistic molecular dynamics simulation, we show that there are universal protein‐like features of MMA‐based RHPs that are insensitive to the sequence, and mostly depend on the overall composition. In particular, we find that MMA‐based RHPs “fold” into globules with heterogeneous hydration patterns. However, the insensitivity to sequence identity observed in MMA‐based RHPs dramatically changes when we substitute the backbone architecture with acrylate or replace the oxygen atom in the side chain with a nitrogen atom (methacrylamide or acrylamide). In such scenarios, the sequence contributes significantly to the compactness and the hydration of monomers. Using principal component analysis and an intersection‐over‐union based index, we demonstrate that different sequences may not overlap in the property space, meaning that their properties are controlled by the sequence rather than fixed composition. We further investigate the sequence‐insensitive capability of the MMA‐based RHPs as previously reported on bacterial phospholipase OmpLA stabilization through heterodimerization. As experimentally observed, such polymers enhance the stability of OmpLA as reliably as its native bilayer environment. The design of such MMA‐based RHPs provides a sequence‐insensitive alternative to protein‐mimetic biomaterials that is orthogonal to the sequence‐structure‐function paradigm of proteins.",
        "authors": [
            "Tianyi Jin",
            "Connor W Coley",
            "Alfredo Alexander‐Katz"
        ],
        "journal_conference_name": "Angewandte Chemie International Edition",
        "publisher": "Wiley",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158097",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Magnetoelectric Extracellular Vesicle Latency-Targeting (MELT) Nanotherapeutic for the Block-Lock-and-Kill HIV Eradication Strategy",
        "abstract": "Background: Human immunodeficiency virus (HIV) establishes latent infections in cellular reservoirs, including microglia. HC69 cells, a microglial model of HIV latency, contain an HIV promoter long terminal repeat (LTR)-GFP reporter and were used for testing the efficacy of a two-step magnetoelectric nanoparticle (MENP) and extracellular vesicle (xEV) latency-targeting (MELT) nanotherapeutic. GFP expression in HC69 at rest is low (GFPLo), and upon exposure to LTR, transcription-activating agents (i.e., TNF-α) are induced to be high expressing (GFPHi). Methods: The first step of MELT utilized ZL0580, an HIV Tat inhibitor loaded into EVs (80%) via incubation. ZL0580-EVs were taken up by GFPLo and blocked LTR transcriptional reactivation by 50% and were 90% less toxic than ZL0580 alone. The second step in MELT involved conjugation of monomethyl auristatin E (MMAE) to MENPs. HPLC measurements showed 80% MMAE attachment to MENPs. Flow cytometry-based measurements of the membrane potential indicated that the membranes of GFPHi HC69 were 60% more polarized than GFPLo HC69 cells. More MMAE–MENPs were internalized by GFPLo HC69. Results: Using a mixed-cell blood–brain barrier (BBB) Transwell model, we demonstrated that 20% of MELT crossed the BBB, was taken up by HC69 cells, and reduced LTR reactivation by 10%. Conclusions: Overall, this study demonstrated that MELT can potentially be utilized as a nanotherapeutic to target HIV latency in microglia.",
        "authors": [
            "Mickensone Andre",
            "Nagesh Kolishetti",
            "Adriana Yndart",
            "Arti Vashist",
            "Madhavan Nair",
            "Andrea D. Raymond"
        ],
        "journal_conference_name": "Biomedicines",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158143",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Finite- and infinite-volume study of DDπ scattering",
        "abstract": "We develop a comprehensive framework for extracting the pole position and properties of the doubly-charmed tetraquark T cc + 3875 from lattice QCD data using the relativistic three-particle formalism. This approach incorporates the effect of the one-pion exchange diagram in DDπ and DD∗ scattering, making it applicable at energies coinciding with the left-hand cut in the partial-wave projected DD∗ amplitude. We present an example application of this framework to existing lattice QCD data at mπ = 280 MeV. We solve the integral equations describing the DDπ reaction, use LSZ reduction to determine the corresponding DD∗ amplitude, and find the values of the infinite-volume two- and three-body K matrices that lead to agreement with lattice DD∗ phase shifts within their uncertainties. Using these K matrices in the three-particle quantization condition, we describe the finite- volume DD∗ spectrum and find good agreement with the lattice QCD energies. Our results suggest that, at this pion mass, the tetraquark appears as a pair of subthreshold complex poles whose precise location strongly depends on the value of the DDπ three-particle K matrix.",
        "authors": [
            "Sebastian M. Dawid",
            "Fernando Romero-López",
            "Stephen R. Sharpe"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158062",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Passive Monitoring of Parkinson Tremor in Daily Life: A Prototypical Network Approach",
        "abstract": "Objective and continuous monitoring of Parkinson’s disease (PD) tremor in free-living conditions could benefit both individual patient care and clinical trials, by overcoming the snapshot nature of clinical assessments. To enable robust detection of tremor in the context of limited amounts of labeled training data, we propose to use prototypical networks, which can embed domain expertise about the heterogeneous tremor and non-tremor sub-classes. We evaluated our approach using data from the Parkinson@Home Validation study, including 8 PD patients with tremor, 16 PD patients without tremor, and 24 age-matched controls. We used wrist accelerometer data and synchronous expert video annotations for the presence of tremor, captured during unscripted daily life activities in and around the participants’ own homes. Based on leave-one-subject-out cross-validation, we demonstrate the ability of prototypical networks to capture free-living tremor episodes. Specifically, we demonstrate that prototypical networks can be used to enforce robust performance across domain-informed sub-classes, including different tremor phenotypes and daily life activities.",
        "authors": [
            "Luc J. W. Evers",
            "Yordan P. Raykov",
            "Tom M. Heskes",
            "Jesse H. Krijthe",
            "Bastiaan R. Bloem",
            "Max A. Little"
        ],
        "journal_conference_name": "Sensors",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158145",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Amplitude analysis of B+ → ψ(2S)K+π+π− decays",
        "abstract": "he first full amplitude analysis of B+ → ψ(2S)K+π+π− decays is performed using proton-proton collision data corresponding to an integrated luminosity of 9 fb−1 recorded with the LHCb detector. The rich K+π+π− spectrum is studied and the branching fractions of the resonant substructure associated with the prominent K1(1270)+ contribution are measured. The data cannot be described by conventional strange and charmonium resonances only. An amplitude model with 53 components is developed comprising 11 hidden-charm exotic hadrons. New production mechanisms for charged charmonium-like states are observed. Significant resonant activity with spin-parity JP = 1+ in the ψ(2S)π+ system is confirmed and a multi-pole structure is demonstrated. The spectral decomposition of the ψ(2S)π+π− invariant-mass structure, dominated by X0 → ψ(2S)ρ(770)0 decays, broadly resembles the J/ψϕ spectrum observed in B+ → J/ψϕK+ decays. Exotic ψ(2S)K+π− resonances are observed for the first time.",
        "authors": [
            "R. Aaij",
            "A. S. W. Abdelmotteleb",
            "C. Abellan Beteta",
            "F. Abudinén",
            "T. Ackernley",
            "A. A. Adefisoye",
            "B. Adeva",
            "M. Adinolfi",
            "P. Adlarson",
            "C. Agapopoulou",
            "C. A. Aidala",
            "Z. Ajaltouni",
            "S. Akar",
            "K. Akiba",
            "P. Albicocco",
            "J. Albrecht",
            "The LHCb collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158259",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Evaluating the reliability of a microperimetry-based method for assessing visual function in the junctional zone of geographic atrophy lesions",
        "abstract": "Purpose To assess the repeatability of a microperimetry methodology for quantifying visual function changes in the junctional zone of eyes with geographic atrophy (GA) in the clinical trial context. Methods A post hoc analysis of the OAKS phase III trial was conducted, which enrolled patients with GA secondary to age-related macular degeneration. Microperimetry using a standard 10 − 2 fovea centered grid was performed at baseline and follow-up visits. GA regions were traced on fundus autofluorescence (FAF) images. Two graders independently registered baseline microperimetry images with baseline FAF images in a sampling of 30 eyes from the OAKS study. Agreement between the two graders’ assessments of mean sensitivity and the number of scotomatous points within a ± 250 𝜇m GA junctional zone was assessed. Results The intraclass correlation (ICC) and coefficient of repeatability (CoR) for the mean junctional zone sensitivity were 0.987 and 0.214 dB, respectively. The ICC and CoR for the total number of scotomatous points within the junctional zone were 0.991 and 1.42, respectively. Conclusions The repeatability of the methodology and its compatibility with standard MP acquisitions appear to make it well-suited for identifying and analyzing retinal sensitivity within high-risk areas of the retina. Summary We present a microperimetry-based methodology for assessing visual function changes in the junctional zone of geographic atrophy lesions using a standard 10 − 2 fovea centered grid in a clinical trial context. The approach’s repeatability and compatibility with standard microperimetry grids may make it useful for assessing the effects of GA therapeutics.",
        "authors": [
            "A. Y. Alibhai",
            "Eric  M. Moult",
            "Muhammad U. Jamil",
            "Khadija Raza",
            "Marco U. Morales",
            "Ramiro Ribeiro",
            "Caroline R. Baumal",
            "James G. Fujimoto",
            "Nadia K. Waheed"
        ],
        "journal_conference_name": "International Journal of Retina and Vitreous",
        "publisher": "BioMed Central",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158526",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Inference Plans for Hybrid Particle Filtering",
        "abstract": "Advanced probabilistic programming languages (PPLs) using hybrid particle filtering combine symbolic exact inference and Monte Carlo methods to improve inference performance. These systems use heuristics to partition random variables within the program into variables that are encoded symbolically and variables that are encoded with sampled values, and the heuristics are not necessarily aligned with the developer's performance evaluation metrics. In this work, we present inference plans, a programming interface that enables developers to control the partitioning of random variables during hybrid particle filtering. We further present Siren, a new PPL that enables developers to use annotations to specify inference plans the inference system must implement. To assist developers with statically reasoning about whether an inference plan can be implemented, we present an abstract-interpretation-based static analysis for Siren for determining inference plan satisfiability. We prove the analysis is sound with respect to Siren's semantics. Our evaluation applies inference plans to three different hybrid particle filtering algorithms on a suite of benchmarks. It shows that the control provided by inference plans enables speed ups of 1.76x on average and up to 206x to reach a target accuracy, compared to the inference plans implemented by default heuristics; the results also show that inference plans improve accuracy by 1.83x on average and up to 595x with less or equal runtime, compared to the default inference plans. We further show that our static analysis is precise in practice, identifying all satisfiable inference plans in 27 out of the 33 benchmark-algorithm evaluation settings.",
        "authors": [
            "Ellie Cheng",
            "Eric Atkinson",
            "Guillaume Baudart",
            "Louis Mandel",
            "Michael Carbin"
        ],
        "journal_conference_name": "Proceedings of the ACM on Programming Languages",
        "publisher": "Association for Computing Machinery",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158236",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Global Bioenergy Availability",
        "abstract": "In efforts to decarbonize and mitigate climate change impacts, some sectors require unprecedented levels of technological innovation to substantially reduce greenhouse gas emissions. This is especially true for air transportation and maritime shipping, where vehicle electrification is currently infeasible due to limitations in available energy storage technologies. The use of biomass to produce readily substitutable (“drop-in”) and low-carbon liquid fuels presents a lever to reduce emissions along the lifecycle of the fuel. The potential to scale-up production of such fuels depends on establishing a robust supply chain for biogenic feedstocks and on building and operating cost-competitive, high-throughput biorefineries. The ultimate ‘ceiling’ for displacing fossil fuels with bio-based fuels is set by the supply of primary biomass. Existing estimates of biomass potential are heterogenous, ranging from 2 to 1200 EJ/yr for energy crops and 8-215 EJ/yr for biomass sourced as waste or residue. We provide a summary of current understanding and outline the underlying assumptions to evaluate whether these conditions are realistic, sustainable, and comprise a future which is worth pursuing. Motifs within the analysis point towards sizable barriers limiting the development of bioenergy to cover future energy demand, such as the logistics of collection and the implications of large-scale land use. For the case of agricultural residues, where such challenges can be partially mitigated, we conservatively estimate a global potential of 18±15 EJ/yr. for 2050, compared to a median estimate from meta-analysis of 27.5 EJ/yr. We dive into this apparent discrepancy, identifying multiple sources of uncertainty. Overall, our findings show that bio-energy alone is unlikely to cover the needs of decarbonizing tough-to-decarbonize transportation modes such as maritime shipping and aviation in a sustainable way.",
        "authors": [
            "Katie Daehn",
            "Evan Coleman",
            "Florian Allrogen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/157942",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Long-lived particle reconstruction downstream of the LHCb magnet",
        "abstract": "Charged-particle trajectories are usually reconstructed with the LHCb detector using combined information\r\nfrom the tracking devices placed upstream and downstream\r\nof the 4 T m dipole magnet. Trajectories reconstructed using\r\nonly information from the tracker downstream of the dipole\r\nmagnet, which are referred to as T tracks, have not been used\r\nfor physics analysis to date. The challenges of the reconstruction of long-lived particles with T tracks for physics use are\r\ndiscussed and solutions are proposed. The feasibility and the\r\ntracking performance are studied using samples of long-lived\r\n and K0\r\nS hadrons decaying between 6.0 and 7.6 m downstream of the proton–proton collision point, thereby traversing most of the magnetic field region and providing maximal sensitivity to magnetic and electric dipole moments. The\r\nreconstruction can be expanded upstream to about 2.5 m for\r\nuse in direct searches of exotic long-lived particles. The data\r\nused in this analysis have been recorded between 2015 and\r\n2018 and correspond to an integrated luminosity of 6 fb−1.\r\nThe results obtained demonstrate the possibility to further\r\nextend the decay volume and the physics reach of the LHCb\r\nexperiment.",
        "authors": [
            "LHCb collaboration"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158039",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Multiple testing for signal-agnostic searches for new physics with machine learning",
        "abstract": "In this work, we address the question of how to enhance signal-agnostic searches by leveraging multiple testing strategies. Specifically, we consider hypothesis tests relying on machine learning, where model selection can introduce a bias towards specific families of new physics signals. Focusing on the New Physics Learning Machine, a methodology to perform a signal-agnostic likelihood-ratio test, we explore a number of approaches to multiple testing, such as combining p-values and aggregating test statistics. Our findings show that it is beneficial to combine different tests, characterised by distinct choices of hyperparameters, and that performances comparable to the best available test are generally achieved, while also providing a more uniform response to various types of anomalies. This study proposes a methodology that is valid beyond machine learning approaches and could in principle be applied to a larger class model-agnostic analyses based on hypothesis testing.",
        "authors": [
            "Gaia Grosso",
            "Marco Letizia"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/157946",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "An empirical design theory for compact drip irrigation emitters",
        "abstract": "With freshwater reserves rapidly diminishing, sustainable irrigation technologies such as drip irrigation must be widely adopted to meet the food demand of a growing global population. Drip irrigation uses a network of pressurized tubes with flow-regulating devices called emitters to minimize conveyance losses, saving up to 65% water compared to flood and furrow irrigation. However, its widespread adoption remains limited due to its high initial capital costs, up to 55% of which are driven by the emitters and tubes. The plastic material consumed by the emitters and tubes is a major driver of their cost. To directly address this cost barrier, this paper details a hydraulic design theory for compact emitters having a common commercial architecture: uniform depth labyrinths with symmetric, triangular teeth. The theory uses geometric symmetry, manufacturing considerations, and clogging constraints to identify three design parameters in emitters that can be used to tune their hydraulic performance without significantly affecting their material volume: the tooth tip gap, labyrinth depth, and the number of tooth pairs. This knowledge allows designers to minimize emitter volume and set architecture a priori, and then use an empirically derived hydraulic model that uses the selected parameters as input arguments to tune flow rate independently. This ensures faster and simpler design iterations. The theory enabled a reduction in emitter material consumption by 67% compared to at least one commercial emitter, potentially cutting the initial capital cost of drip irrigation by up to 10%, making this already sustainable irrigation technology more globally accessible.",
        "authors": [
            "Aditya Ghodgaonkar",
            "Emily Welsh",
            "Benjamin Judge",
            "Amos G. Winter V"
        ],
        "journal_conference_name": "Irrigation Science",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/157945",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Digital Phenotypic Assessment in Neuro-Oncology (DANO): A Pilot Study on Sociability Changes in Patients Undergoing Treatment for Brain Malignancies",
        "abstract": "first_pageDownload PDFsettingsOrder Article Reprints\r\nOpen AccessArticle\r\nA Digital Phenotypic Assessment in Neuro-Oncology (DANO): A Pilot Study on Sociability Changes in Patients Undergoing Treatment for Brain Malignancies †\r\nby Francesca Siddi 1,2,*,Patrick Emedom-Nnamdi 3,Michael P. Catalino 4,Aakanksha Rana 1,5ORCID,Alessandro Boaro 1,2ORCID,Hassan Y. Dawood 1ORCID,Francesco Sala 2,Jukka-Pekka Onnela 3,‡ andTimothy R. Smith 1,‡\r\n1\r\nComputational Neuroscience Outcomes Center, Department of Neurosurgery, Brigham and Women’s Hospital, and Harvard Medical School, Boston, MA 02115, USA\r\n2\r\nSection of Neurosurgery, Department of Neurosciences, Biomedicine and Movement Sciences, University of Verona, 37129 Verona, Italy\r\n3\r\nDepartment of Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA 02115, USA\r\n4\r\nDepartment of Neurosurgery, University of Virginia, Charlottesville, VA 22908, USA\r\n5\r\nMcGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, MA 02139, USA\r\n*\r\nAuthor to whom correspondence should be addressed.\r\n†\r\nPrevious Presentations: This work was virtually presented as an oral poster presentation at the 2021 Annual Meeting of the European Association of Neurosurgical Societies (eEANS), Virtual Congress, 1–7 October 2021; EP13028.\r\n‡\r\nThese authors contributed equally to this work.\r\nCancers 2025, 17(1), 139; https://doi.org/10.3390/cancers17010139\r\nSubmission received: 15 October 2024 / Revised: 24 December 2024 / Accepted: 3 January 2025 / Published: 4 January 2025\r\n(This article belongs to the Special Issue Novel Diagnostic and Therapeutic Approaches in Diffuse Gliomas)\r\nDownloadkeyboard_arrow_down Browse Figures Versions Notes\r\n\r\nSimple Summary\r\nNowadays, smartphones are the principal tool for interactions between people. Mobile health applications might be used to study the cognitive functions in the neuro-oncological population. Many brain tumor patients have cognitive challenges that have an impact on sociability. Digital phenotyping is able to characterize social and spatial dimensions of human behavior from mobile phone call records. The aim of this study was to start to explore this technology in brain cancer patients, focusing on sociability data. The results of this pilot study indicate that a digital assessment in neuro-oncology can be used to characterize and follow the social activity of patients’ lives. Changes in the patient’s social network relate to disease progression, suggesting a new tool to improve the complex evaluation of underserved brain cancer patients.\r\nAbstract\r\nBackground: The digital phenotyping tool has great potential for the deep characterization of neurological and quality-of-life assessments in brain tumor patients. Phone communication activities (details on call and text use) can provide insight into the patients’ sociability. Methods: We prospectively collected digital-phenotyping data from six brain tumor patients. The data were collected using the Beiwe application installed on their personal smartphones. We constructed several daily sociability features from phone communication logs, including the number of incoming and outgoing text messages and calls, the length of messages and duration of calls, message reciprocity, the number of communication partners, and number of missed calls. We compared variability in these sociability features against those obtained from a control group, matched for age and sex, selected among patients with a herniated disc. Results: In brain tumor patients, phone-based communication appears to deteriorate with time, as evident in the trend for total outgoing minutes, total outgoing calls, and call out-degree. Conclusions: These measures indicate a possible decrease in sociability over time in brain tumor patients that may correlate with survival. This exploratory analysis suggests that a quantifiable digital sociability phenotype exists and is comparable for patients with different survival outcomes. Overall, assessing neurocognitive function using digital phenotyping appears promising.",
        "authors": [
            "Francesca Siddi",
            "Patrick Emedom-Nnamdi",
            "Michael P. Catalino",
            "Aakanksha Rana",
            "Alessandro Boaro",
            "Hassan Y. Dawood",
            "Francesco Sala",
            "Jukka-Pekka Onnela",
            "Timothy R. Smith"
        ],
        "journal_conference_name": "Cancers",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/157962",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Microbial methanogenesis fueled by freshwater infiltration and oil biodegradation in the Siljan impact structure, Sweden",
        "abstract": "Deeply fractured rocks of meteorite impact craters are suggested as prime niches for subsurface microbial colonization. Methane can be a product of such microbial communities and seeps of methane from impact craters on Earth are of strong interest as they act as analogs for Mars. Previous studies report signs of ancient microbial methanogenesis in the Devonian Siljan meteorite impact structure in Sweden, but the proportion of microbial methane, metabolic pathways, and potential modern activity remain elusive. In this study, gas composition, hydrochemistry, oil organic geochemistry, and microbial community analyses are reported in 400 m deep fractures of the Siljan impact structure. The results showed a dominantly microbial origin for methane, which was supported by highly negative δ13CCH4 and positive δ13CCO2 values along with multiply substituted isotopologues (Δ13CH3D) that indicated disequilibrium fractionation due to microbial kinetic isotope effects. The presence of C2 to C5 hydrocarbons suggested a minor thermogenic input in the gas mix. Characterization of the microbial community via 16S rRNA gene amplicon sequencing and real-time PCR indicated a low abundance of several methanogenic archaeal populations, which is common for settings with active methanogenesis. Evidence of oil biodegradation suggested that secondary microbial hydrocarbon utilization was involved in the methanogenesis. Low sulfate and high alkalinity in the groundwaters also suggested a dominantly microbial methane formation driven by infiltration of freshwater that was coupled to sulfate reduction and secondary utilization of early mature thermogenic hydrocarbons.",
        "authors": [
            "Femke van Dam",
            "Riikka Kietäväinen",
            "George Westmeijer",
            "Manuel Reinhardt",
            "Shuhei Ono",
            "Mark Dopson",
            "Marcelo Ketzer",
            "Jennifer C. McIntosh",
            "Henrik Drake"
        ],
        "journal_conference_name": "Discover Applied Sciences",
        "publisher": "Springer International Publishing",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/157943",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Workforce Readiness Level (WRL) Deskbook",
        "abstract": "This document describes how Workforce Readiness Levels (WRLs) can be used as a critical part of the strategic development of manufacturing processes for emerging technology. WRLs are designed to align with TRLs and MRLs, and conducting Workforce Readiness Assessments will enable organizations to intentionally prepare their manufacturing workforce. Our Deskbook is offered as a companion to the MRL and TRL Deskbooks to assist industry leaders in following a holistic and integrated approach to technology and workforce readiness.\r\nWe anticipate that WRL assessments will be conducted by both industry leaders as they prepare for workforce transition and expansion, and by researchers and consultants supporting the emergence of innovative and cutting-edge technologies. The outcomes of these assessments will inform not only recruiting and hiring for manufacturing organizations, but also training and development both within organizations and in community colleges and vocational schools.",
        "authors": [
            "Juliet Aiken",
            "Elizabeth Moore",
            "Autumn Fedewa",
            "Taylor Jordan",
            "Frank Field",
            "Elizabeth Unger",
            "Randolph Kirchain"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/157940",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Photonic Integrated Devices Manufacturing Workforce Preparation Assessment Report: Middle Skilled Technicians",
        "abstract": "The Conducere-MIT Collaboratory conducted a Manufacturing Workforce Preparation assessment (MWP) for photonic integrated devices, particularly bio-photonic devices. Leveraging data from O*NET and expert interviews, the team identified the need for two manufacturing technician positions, the Photonic Integrated Circuit Technician and the Functionalization Technician. The team identified tasks, skills, and competencies that are critical and needed at entry for manufacturing technicians producing photonic integrated devices. While tasks between these positions are expected to vary somewhat, the skills and competencies needed to be successful are not expected to vary much. Additionally, needed skills and competencies will likely differ for technicians working predominantly with automated equipment and technologists who conduct their work more manually. Much of what technicians need to be able to do will be learned on the job.\r\nThis report first shares what photonic integrated devices are and summarizes our findings relevant to the market for this emerging technology. The report then details role, task, skill, and competency findings for the technician positions, and provides insight into equipment, education, recruiting, hiring, and training.",
        "authors": [
            "Juliet Aiken",
            "Elizabeth Moore",
            "Autumn Fedewa",
            "Taylor Jordan",
            "Frank Field",
            "Elizabeth Unger",
            "Anuradha Argarwal",
            "Randolph Kirchain"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/157941",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Vehicle Routing Problem Formulation for Efficient Tracking of Objects in Low Earth Orbit",
        "abstract": "The increasing number of resident space objects (RSOs) in low Earth orbit (LEO) endangers the sustainable use of space and necessitates continuous surveillance to prevent collisions. The U.S. Space Surveillance Network (SSN) tracks tens of thousands of LEO RSOs using a suite of ground-based sensors; however, the algorithms that task and schedule these sensors have not improved significantly in the last twenty years. In that time, the number of catalogued LEO RSOs has more than doubled, calling for more efficient tasking algorithms. Prior research has primarily focused on improving the tasking of ground-based sensors for tracking RSOs in geosynchronous Earth orbit (GEO). In this paper, we extend recent work on a vehicle routing problem (VRP) formulation for optimal tasking and scheduling of ground-based radars for tracking GEO RSOs and apply it to tracking LEO RSOs. We introduce a modified VRP formulation, which features discrete time indexing and leverages sparse, binary feasibility matrices for reduced computation time, and present results for several simulations. We show that our approach can compute global and regional optima for tracking (a) 100 targets using 4 ground-based sensors over a 5-hour time horizon in under 5 minutes on a laptop computer and (b) 10,000 targets using 27 ground-based sensors over a 24-hour time horizon in about 4 hours on a high-performance computing cluster.",
        "authors": [
            "Allan Shtofenmakher",
            "Hamsa Balakrishnan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "American Institute of Aeronautics and Astronautics",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158022",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Resurgence in Liouville theory",
        "abstract": "Liouville conformal field theory is a prototypical example of an exactly solvable quantum field theory, in the sense that the correlation functions in an arbitrary background can be determined exactly using only the constraints of unitarity and crossing symmetry. For example, the three point correlation functions are given by the famous formula of Dorn-Otto-Zamolodchikov-Zamolodchikov (DOZZ). Unlike many other exactly solvable theories, Liouville theory has a continuously tunable parameter — essentially ℏ — which is related to the central charge of the theory. Here we investigate the nature of the perturbative expansion in powers of ℏ, which is the loop expansion around a semi-classical solution. We show that the perturbative coefficients grow factorially, as expected of a Feynman diagram expansion, and take the form of an asymptotic series. We identify the singularities in the Borel plane, and show that they are associated with complex instanton solutions of Liouville theory; they correspond precisely to the complex solutions described by Harlow, Maltz, and Witten. Both single- and multi-valued solutions of Liouville appear. We show that the perturbative loop expansions around these different saddle points mix in the way expected for a trans-series expansion. Thus Liouville theory provides a calculable example of a quantum field theory where perturbative and instanton contributions can be summed up and assembled into a finite answer.",
        "authors": [
            "Nathan Benjamin",
            "Scott Collier",
            "Alexander Maloney",
            "Viraj Meruliya"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/157950",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Measurement of the double-diferential inclusive jet cross section in proton-proton collisions at √s = 5.02 TeV",
        "abstract": "The inclusive jet cross section is measured as a function of jet transverse momentum pT and rapidity y. The measurement is performed using proton-proton collision data at s = 5.02 TeV, recorded by the CMS experiment at the LHC, corresponding to an integrated luminosity of 27.4 pb−1. The jets are reconstructed with the anti-kT algorithm using a distance parameter of R = 0.4, within the rapidity interval |y| < 2, and across the kinematic range 0.06 < pT < 1 TeV. The jet cross section is unfolded from detector to particle level using the determined jet response and resolution. The results are compared to predictions of perturbative quantum chromodynamics, calculated at both next-to-leading order and next-to-next-to-leading order. The predictions are corrected for nonperturbative effects, and presented for a variety of parton distribution functions and choices of the renormalization/factorization scales and the strong coupling αS.",
        "authors": [
            "A. Hayrapetyan",
            "A. Tumasyan",
            "W. Adam",
            "J. W. Andrejkovic",
            "T. Bergauer",
            "S. Chatterjee",
            "K. Damanakis",
            "M. Dragicevic",
            "A. Escalante Del Valle",
            "P. S. Hussain",
            "M. Jeitler",
            "N. Krammer",
            "L. Lechner",
            "D. Liko",
            "I. Mikulec",
            "J. Schieck",
            "R. Schöfbeck",
            "The CMS collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158044",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Uniacute Spherical Codes",
        "abstract": "A spherical L-code, where L ⊆ [−1,∞), consists of unit vectors in Rd whose pairwise inner products are contained in L. Determining the maximum cardinality NL (d)\r\nof an L-code in Rd is a fundamental question in discrete geometry and has been\r\nextensively investigated for various choices of L. Our understanding in high dimensions is generally quite poor. Equiangular lines, corresponding to L = {−α, α}, is\r\na rare and notable solved case. Bukh studied an extension of equiangular lines and\r\nshowed that NL (d) = OL (d) for L = [−1, −β]∪{α} with α, β > 0 (we call such\r\nL-codes “uniacute”), leaving open the question of determining the leading constant\r\nfactor. Balla, Dräxler, Keevash, and Sudakov proved a “uniform bound” showing\r\nlim supd→∞ NL (d)/d ≤ 2p for L = [−1, −β]∪{α} and p =  α/β  + 1. For which\r\n(α, β) is this uniform bound tight? We completely answer this question. We develop a\r\nframework for studying uniacute codes, including a global structure theorem showing\r\nthat the Gram matrix has an approximate p-block structure. We also formulate a notion\r\nof “modular codes,” which we conjecture to be optimal in high dimensions.",
        "authors": [
            "Saba Lepsveridze",
            "Aleksandre Saatashvili",
            "Yufei Zhao"
        ],
        "journal_conference_name": "Combinatorica",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/159064",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Intel’s Fall from Grace",
        "abstract": "Intel continues to dominate the microprocessor market for personal computers and datacenter servers for cloud services but it has fallen sharply behind Nvidia, the new platform leader for AI applications and GPU servers.  Intel's decline has two main causes, apart from the innovations at Nvidia. One involves the inability to adapt to new technologies and customers (i.e., mobile and AI) as they emerged.  A second involves the commitment to manufacture its own microprocessors even though advanced semiconductor manufacturing, led by Taiwan Semiconductor Manufacturing Corp. (TSMC), has evolved into a highly specialized capability, separable from design.",
        "authors": [
            "Michael Cusumano"
        ],
        "journal_conference_name": "Communications of the ACM",
        "publisher": "ACM",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158131",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "TeleAbsence: A Vision of Past and Afterlife Telepresence",
        "abstract": "This paper presents our vision of TeleAbsence, extending the concept of telepresence to the past and the afterlife to address the vast emotional and temporal distance caused by the memory of loved ones who drifted apart and faded away. Instead of explicit and literal representations of loved ones, TeleAbsence describes poetic encounters with digital and physical traces left by the absence of others. TeleAbsence fosters illusory communications to conjure the feeling of being there with those no longer with us without using synthetic or generative representations and utterances. Our vision is deeply inspired by the Portuguese concept “Saudade”—the “desire for the beloved thing, people, place, and moment, made painful by its absence.” We present our vision through five design principles: presence of absence, illusory communication, the materiality of memory, traces of reflection, and remote time, grounded in historical and cultural contexts. We present exploratory narratives to illustrate these principles and the concept of ambient co-presence using poetry, phone, piano, and pen as mediums. We discuss challenges and opportunities for future work, including representational strategies to depict lost loved ones, ethical issues, and the possible extension of TeleAbsence to historical public figures.",
        "authors": [
            "Hiroshi Ishii",
            "Daniel Pillis",
            "Pat Pataranutaporn",
            "Xiao Xiao",
            "Hayoun Noh",
            "Lucy Li",
            "Alaa Algargoosh",
            "Jean-Baptiste Labrune"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "MIT Press",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158451",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Reversibly Switching Hydrogen-Responsive Palladium-Graphene Composite Membranes",
        "abstract": "",
        "authors": [
            "Lohyun Kim",
            "Aaron Persad",
            "Chi Cheng",
            "Randall Field",
            "Rohit Karnik"
        ],
        "journal_conference_name": "Advanced Functional Materials",
        "publisher": "Wiley",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/157990",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Estudios feministas de seguridad desde América Latina y el Caribe",
        "abstract": "",
        "authors": [
            "Alessandra Jungs de Almeida",
            "Catherine D'Ignazio"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Biblioteca Universitária/Universidade Federal de Santa Catarina",
        "year": "2025",
        "doi": "https://hdl.handle.net/1721.1/158241",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "SPECTER: efficient evaluation of the spectral EMD",
        "abstract": "The Energy Mover’s Distance (EMD) has seen use in collider physics as a metric between events and as a geometric method of defining infrared and collinear safe observables. Recently, the Spectral Energy Mover’s Distance (SEMD) has been proposed as a more analytically tractable alternative to the EMD. In this work, we obtain a closed-form expression for the Riemannian-like p = 2 SEMD metric between events, eliminating the need to numerically solve an optimal transport problem. Additionally, we show how the SEMD can be used to define event and jet shape observables by minimizing the distance between events and parameterized energy flows (similar to the EMD), and we obtain closed-form expressions for several of these observables. We also present the Specter framework, an efficient and highly parallelized implementation of the SEMD metric and SEMD-derived shape observables as an analogue of the previously-introduced Shaper for EMD-based computations. We demonstrate that computing the SEMD with Specter can be up to a thousand times faster than computing the EMD with standard optimal transport libraries.",
        "authors": [
            "Rikab Gambhir",
            "Andrew J. Larkoski",
            "Jesse Thaler"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157951",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Ortho-Unit Polygons can be Guarded with at most n - 4 8 Guards",
        "abstract": "Abstract An orthogonal polygon is called an ortho-unit polygon if its vertices have integer coordinates, and all of its edges have length one. In this paper we prove that any ortho-unit polygon with n ≥ 12 vertices can be guarded with at most ⌊ n - 4 8 ⌋ guards, which is a tight bound.",
        "authors": [
            "J. M. Díaz-Báñez",
            "P. Horn",
            "M. A. Lopez",
            "N. Marín",
            "A. Ramírez-Vigueras",
            "O. Solé-Pi",
            "A. Stevens",
            "J. Urrutia"
        ],
        "journal_conference_name": "Graphs and Combinatorics",
        "publisher": "Springer Japan",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/159047",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Bias in machine learning applications to address non-communicable diseases at a population-level: a scoping review",
        "abstract": "Background Machine learning (ML) is increasingly used in population and public health to support epidemiological studies, surveillance, and evaluation. Our objective was to conduct a scoping review to identify studies that use ML in population health, with a focus on its use in non-communicable diseases (NCDs). We also examine potential algorithmic biases in model design, training, and implementation, as well as efforts to mitigate these biases. Methods We searched the peer-reviewed, indexed literature using Medline, Embase, Cochrane Central Register of Controlled Trials and Cochrane Database of Systematic Reviews, CINAHL, Scopus, ACM Digital Library, Inspec, Web of Science’s Science Citation Index, Social Sciences Citation Index, and the Emerging Sources Citation Index, up to March 2022. Results The search identified 27 310 studies and 65 were included. Study aims were separated into algorithm comparison (n = 13, 20%) or disease modelling for population-health-related outputs (n = 52, 80%). We extracted data on NCD type, data sources, technical approach, possible algorithmic bias, and jurisdiction. Type 2 diabetes was the most studied NCD. The most common use of ML was for risk modeling. Mitigating bias was not extensively addressed, with most methods focused on mitigating sex-related bias. Conclusion This review examines current applications of ML in NCDs, highlighting potential biases and strategies for mitigation. Future research should focus on communicable diseases and the transferability of ML models in low and middle-income settings. Our findings can guide the development of guidelines for the equitable use of ML to improve population health outcomes.",
        "authors": [
            "Sharon Birdi",
            "Roxana Rabet",
            "Steve Durant",
            "Atushi Patel",
            "Tina Vosoughi",
            "Mahek Shergill",
            "Christy Costanian",
            "Carolyn P. Ziegler",
            "Shehzad Ali",
            "David Buckeridge",
            "Marzyeh Ghassemi",
            "Jennifer Gibson",
            "Ava John-Baptiste",
            "Jillian Macklin",
            "Melissa McCradden",
            "Kwame McKenzie"
        ],
        "journal_conference_name": "BMC Public Health",
        "publisher": "BioMed Central",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157937",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Self-Assembly of a Biologically Plausible Learning Circuit",
        "abstract": "Over the last four decades, the amazing success of deep learning has been driven by the use of Stochastic Gradient Descent (SGD) as the main optimization technique. The default implementation for the computation of the gradient for SGD is backpropagation, which, with its variations, is used to this day in almost all computer implementations. From the perspective of neuroscientists, however, the consensus is that backpropagation is unlikely to be used by the brain. Though several alternatives have been discussed, none is so far supported by experimental evidence. Here we propose a circuit for updating the weights in a network that is biologically plausible, works as well as backpropagation, and leads to verifiable predictions about the anatomy and the physiology of a characteristic motif of four plastic synapses between ascending and descending cortical streams. A key prediction of our proposal is a surprising property of self-assembly of the basic circuit, emerging from initial random connectivity and heterosynaptic plasticity rules.",
        "authors": [
            "Qianli Liao",
            "Liu Ziyin",
            "Yulu Gan",
            "Brian Cheung",
            "Mark Harnett",
            "Tomaso Poggio"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Center for Brains, Minds and Machines (CBMM)",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157934",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Approaching coupled-cluster accuracy for molecular electronic structures with multi-task learning (preprint)",
        "abstract": "",
        "authors": [
            "Hao Tang",
            "Brian Xiao",
            "Wenhao He",
            "Pero Subasic",
            "Avetik R. Harutyunyan",
            "Yao Wang",
            "Fang Liu",
            "Haowei Xu",
            "Ju Li"
        ],
        "journal_conference_name": "Nature Computational Science",
        "publisher": "Springer Science and Business Media LLC",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157988",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Principles and Guidelines for Evaluating Social Robot Navigation Algorithms",
        "abstract": "A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this paper, we pave the road towards common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributions include (a) a definition of a socially navigating robot as one that respects the principles of safety, comfort, legibility, politeness, social competency, agent understanding, proactivity, and responsiveness to context, (b) guidelines for the use of metrics, development of scenarios, benchmarks, datasets, and simulators to evaluate social navigation, and (c) a design of a social navigation metrics framework to make it easier to compare results from different simulators, robots and datasets.",
        "authors": [
            "Anthony Francis",
            "Claudia P?rez-D'Arpino",
            "Chengshu Li",
            "Fei Xia",
            "Alexandre Alahi",
            "Rachid Alami",
            "Aniket Bera",
            "Abhijat Biswas",
            "Joydeep Biswas",
            "Rohan Chandra",
            "Hao-Tien Chiang",
            "Michael Everett",
            "Sehoon Ha",
            "Justin Hart",
            "Jonathan How",
            "Haresh Karnan",
            "Tsang-Wei Lee",
            "Luis Manso",
            "Reuth Mirsky",
            "S?ren Pirk"
        ],
        "journal_conference_name": "ACM Transactions on Human-Robot Interaction",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158075",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "NiNC Catalysts in CO2-to-CO Electrolysis",
        "abstract": "CO2-to-CO electrolyzer technology converts carbon dioxide into carbon monoxide using electrochemical methods, offering significant environmental and energy benefits by aiding in greenhouse gas mitigation and promoting a carbon circular economy. Recent study by Strasser et al. in Nature Chemical Engineering presents a high-performance CO2-to-CO electrolyzer utilizing a NiNC catalyst with nearly 100% faradaic efficiency, employing innovative diagnostic tools like the carbon crossover coefficient (CCC) to address transport-related failures and optimize overall efficiency. Strasser’s research demonstrates the potential of NiNC catalysts, particularly NiNC-IMI, for efficient CO production in CO2-to-CO electrolyzers, highlighting their high selectivity and performance. However, challenges such as localized CO2 depletion and mass transport limitations underscore the need for further optimization and development of diagnostic tools like CCC. Strategies for optimizing catalyst structure and operational parameters offer avenues for enhancing the performance and reliability of electrochemical CO2 reduction catalysts.",
        "authors": [
            "Hao Zhang",
            "Menghui Qi",
            "Yong Wang"
        ],
        "journal_conference_name": "Nano-Micro Letters",
        "publisher": "Springer Nature Singapore",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157938",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Walk of Guilt: Multimodal Deception Detection from Nonverbal Motion Behaviour",
        "abstract": "Detecting deceptive behaviour for surveillance and border protection is critical for a country’s security. With the advancement of technology in relation to sensors and artificial intelligence, recognising deceptive behaviour could be performed automatically. Following the success of affective computing in emotion recognition from verbal and nonverbal cues, we aim to apply a similar concept for deception detection. Recognising deceptive behaviour has been attempted; however, only a few studies have analysed this behaviour from gait and body movement. This research involves a multimodal approach for deception detection from gait, where we fuse features extracted from body movement behaviours from a video signal, acoustic features from walking steps from an audio signal, and the dynamics of walking movement using an accelerometer sensor. Using the video recording of walking from the Whodunnit deception dataset, which contains 49 subjects performing scenarios that elicit deceptive behaviour, we conduct multimodal two-category (guilty/not guilty) subject-independent classification. The classification results obtained reached an accuracy of up to 88% through feature fusion, with an average of 60% from both single and multimodal signals. Analysing body movement using single modality showed that the visual signal had the highest performance followed by the accelerometer and acoustic signals. Several fusion techniques were explored, including early, late, and hybrid fusion, where hybrid fusion not only achieved the highest classification results, but also increased the confidence of the results. Moreover, using a systematic framework for selecting the most distinguishing features of guilty gait behaviour, we were able to interpret the performance of our models. From these baseline results, we can conclude that pattern recognition techniques could help in characterising deceptive behaviour, where future work will focus on exploring the tuning and enhancement of the results and techniques.",
        "authors": [
            "Sharifa Alghowinem",
            "Sabrina Caldwell",
            "Ibrahim Radwan",
            "Michael Wagner",
            "Tom Gedeon"
        ],
        "journal_conference_name": "Information",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158142",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Investigating the Applicability of the Peak Density Thickness Parameter over the Equatorial Region",
        "abstract": "The Peak Density Thickness (PDT) refers to a vertical region in the ionosphere encompassing the F2 peak, where electron density is at its maximum, and extending upward—maintaining a constant density—for a fixed altitude beyond this peak. This study builds on the previously established PDT concept, initially explored at midlatitudes using data from Millstone Hill, by evaluating its applicability and effectiveness over equatorial latitudes using data from the Jicamarca Incoherent Scatter Radar (ISR) in Lima, Peru. A comprehensive analysis of electron density profiles measured by the Jicamarca ISR, spanning 1997 to 2020, was conducted using the Madrigal database to extract the PDT parameter for the F2 layer. Findings from the Jicamarca ISR indicate that the PDT parameter peaks around solar noon, aligning with observations from Millstone Hill. For selected case studies, the Vary-Chap topside model was employed to reconstruct the ionospheric profile above the F2 peak and PDT, demonstrating the model’s enhanced effectiveness when incorporating the PDT parameter over equatorial regions. This research confirms the presence of PDT in equatorial regions, consistent with its behavior at midlatitudes, and underscores the importance of PDT in refining predictive ionospheric models across different latitudes.",
        "authors": [
            "Mohamed O. Shammat",
            "Bodo W. Reinisch",
            "Ivan Galkin",
            "Philip J. Erickson",
            "Jay A. Weitzen",
            "William C. Rideout"
        ],
        "journal_conference_name": "Atmosphere",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158141",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Smart City Products and Their Materials Assessment Using the Pentagon Framework",
        "abstract": "Smart cities are complex urban environments that rely on advanced technology and data analytics to enhance city services&rsquo; quality of life, sustainability, and efficiency. As these cities continue to evolve, there is a growing need for a structured framework to evaluate and integrate products that align with smart city objectives. This paper introduces the Pentagon Framework, a comprehensive evaluation method designed to ensure that products and their materials meet the specific needs of smart cities. The framework focuses on five key features&mdash;smart, sustainable, sensing, social, and safe&mdash;collectively called the Penta-S concept. These features provide a structured approach to categorizing and assessing products, ensuring alignment with the city&rsquo;s goals for efficiency, sustainability, and user experience. The <i>Smart City Pentagon Framework Analyzer</i> is also presented, a dedicated web application that facilitates interaction with the framework. It allows product data input, provides feedback on alignment with the Penta-S features, and suggests personality traits based on the OCEAN model. Complementing the web application, the <i>Smart City Penta-S Compliance Assistant</i> API, developed through ChatGPT, offers a more profound, personalized evaluation of products, including the life cycle phase recommendations using the IPPMD model. This paper contributes to the development of smart city solutions by providing a flexible framework that can be applied to any product type, optimizing its life cycle, and ensuring compliance with the Pentagon Framework. This approach improves product integration and fosters user satisfaction by tailoring products and their materials to meet specific user preferences and needs within the smart city environment. The proposed framework emphasizes citizen-centric design and highlights its advantages over conventional evaluation methods, ultimately enhancing urban planning and smart city development.",
        "authors": [
            "Pedro Ponce",
            "Mario Rojas",
            "Juana Isabel Mendez",
            "Brian Anthony",
            "Russel Bradley",
            "Aminah Robinson Fayek"
        ],
        "journal_conference_name": "Multimodal Technologies and Interaction",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158140",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Model-independent search for pair production of new bosons decaying into muons in proton-proton collisions a √s = 13 TeV",
        "abstract": "The results of a model-independent search for the pair production of new bosons within a mass range of 0.21 < m < 60 GeV, are presented. This study utilizes events with a four-muon final state. We use two data sets, comprising 41.5 fb−1 and 59.7 fb−1 of proton-proton collisions at s = 13 TeV, recorded in 2017 and 2018 by the CMS experiment at the CERN LHC. The study of the 2018 data set includes a search for displaced signatures of a new boson within the proper decay length range of 0 < cτ < 100 mm. Our results are combined with a previous CMS result, based on 35.9 fb−1 of proton-proton collisions at s = 13 TeV collected in 2016. No significant deviation from the expected background is observed. Results are presented in terms of a model-independent upper limit on the product of cross section, branching fraction, and acceptance. The findings are interpreted across various benchmark models, such as an axion-like particle model, a vector portal model, the next-to-minimal supersymmetric standard model, and a dark supersymmetric scenario, including those predicting a non-negligible proper decay length of the new boson. In all considered scenarios, substantial portions of the parameter space are excluded, expanding upon prior results.",
        "authors": [
            "A. Hayrapetyan",
            "A. Tumasyan",
            "W. Adam",
            "J. W. Andrejkovic",
            "T. Bergauer",
            "S. Chatterjee",
            "K. Damanakis",
            "M. Dragicevic",
            "P. S. Hussain",
            "M. Jeitler",
            "N. Krammer",
            "A. Li",
            "D. Liko",
            "I. Mikulec",
            "J. Schieck",
            "R. Schöfbeck",
            "D. Schwarz",
            "M. Sonawane",
            "The CMS collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157949",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Evaluation and Analysis of Next-Generation FY-4A LPW Products over Various Climatic Regions in China",
        "abstract": "Atmospheric water vapor, a significant constituent of the atmosphere, affects the energy balance between Earth’s atmosphere and space, and its changes play a crucial role in the greenhouse effect. Layer precipitable water (LPW), which represents the column-integral water vapor within a vertical range, is increasingly recognized as a key indicator of atmospheric water vapor distributions and variations. Due to its capability for layer-wise monitoring, LPW products have the potential to offer valuable insights into the characteristics and evolution of climatic regions through refined atmospheric spatiotemporal information. However, the observational quality and spatiotemporal variations of LPW products across different climate zones, e.g., the diverse climatic regions in China, have not been systematically assessed. In this paper, we aim to evaluate and analyze the climatic and seasonal variations of FY-4A LPW products across five climatic regions in China, contributing to a deeper understanding of water vapor variability and providing valuable data for climate change research. A surface pressure calibration algorithm for ERA5 data is developed to calculate accurate ERA5 LPW products. The results show that all four FY-4A LPWs are consistent with ERA5 LPWs, with an overall root mean square error (RMSE) of 2.58, 0.90, 1.30, and 1.01 mm, respectively. Furthermore, FY-4A LPWs are underestimated in the temperate monsoon area and overestimated in the subtropical and tropical monsoon regions, while FY-4A observations agree well with ERA5 reanalysis in temperate continental and plateau mountain zones. These analyses highlight the remarkable climate dependency of FY-4A LPWs and their potential for climate-related studies.",
        "authors": [
            "Wenyuan Zhang",
            "Xinyu Xiao",
            "Jinsong Peng",
            "Shubi Zhang",
            "Endrit Shehaj",
            "Gregor Moeller"
        ],
        "journal_conference_name": "Atmosphere",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157961",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Age- and Sex-Based Developmental Biomarkers in Eye Movements",
        "abstract": "Background: Eye movement research serves as a critical tool for assessing brain function, diagnosing neurological and psychiatric disorders, and understanding cognition and behavior. Sex differences have largely been under reported or ignored in neurological research. However, eye movement features provide biomarkers that are useful for disease classification with superior accuracy and robustness compared to previous classifiers for neurological diseases. Neurological diseases have a sex specificity, yet eye movement analysis has not been specific to our understanding of sex differences. Methods: The study involved subjects recruited from 804 sites equipped with RightEye Vision Systems, primarily located in optometry practices across the United States. Subjects completed six eye movement assessments: circular smooth pursuit (CSP), horizontal smooth pursuit (HSP), vertical smooth pursuit (VSP), horizontal saccades (HS), vertical saccades (VS), and fixation stability (FS). Eye movements were analyzed and classified in accordance with age and sex by multiple t-tests and linear regression models. Results: This study represented a large sample size of 23,557 subjects, with 11,871 males and 11,686 females representing ages from birth through 80 years of age. We observed statistically significant differences for all eye movement functions between males and females. Conclusions: We demonstrate that eye movements are sex-specific and offer normative data to compare sex-specific eye movement function by age. Novel baseline metrics can be compared to individual performance, regardless of sex. This study represents significant progress in linking eye movements with brain function and clinical syndromes, allowing researchers and clinicians to stratify individuals by age and sex.",
        "authors": [
            "Frederick Robert Carrick",
            "Melissa Hunfalvay",
            "Takumi Bolte",
            "Sergio F. Azzolino",
            "Mahera Abdulrahman",
            "Ahmed Hankir",
            "Matthew M. Antonucci",
            "Nouf Al-Rumaihi"
        ],
        "journal_conference_name": "Brain Sciences",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157960",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Analysis of Λ b 0 → pK−μ+μ− decays",
        "abstract": "The differential branching fraction and angular coefficients of Λ b 0 → pK−μ+μ− decays are measured in bins of the dimuon mass squared and dihadron mass. The analysis is performed using a data set corresponding to 9 fb−1 of integrated luminosity collected with the LHCb detector between 2011 and 2018. The data are consistent with receiving contributions from a mixture of Λ resonances with different spin-parity quantum numbers. The angular coefficients show a pattern of vector-axial vector interference that is a characteristic of the type of flavour-changing neutral-current transition relevant for these decays.",
        "authors": [
            "R. Aaij",
            "A. S. W. Abdelmotteleb",
            "C. Abellan Beteta",
            "F. Abudinén",
            "T. Ackernley",
            "A. A. Adefisoye",
            "B. Adeva",
            "M. Adinolfi",
            "P. Adlarson",
            "C. Agapopoulou",
            "C. A. Aidala",
            "Z. Ajaltouni",
            "S. Akar",
            "K. Akiba",
            "P. Albicocco",
            "J. Albrecht",
            "The LHCb collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157948",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Learning Probabilistic Boolean Network Model of a Smart Grid with Applications in System Maintenance",
        "abstract": "Probabilistic Boolean Networks can capture the dynamics of complex biological systems as well as other non-biological systems, such as manufacturing systems and smart grids. In this proof-of-concept manuscript, we propose a Probabilistic Boolean Network architecture with a learning process that significantly improves the prediction of the occurrence of faults and failures in smart-grid systems. This idea was tested in a Probabilistic Boolean Network model of the WSCC nine-bus system that incorporates Intelligent Power Routers on every bus. The model learned the equality and negation functions in the different experiments performed. We take advantage of the complex properties of Probabilistic Boolean Networks to use them as a positive feedback adaptive learning tool and to illustrate that these networks could have a more general use than previously thought. This multi-layered PBN architecture provides a significant improvement in terms of performance for fault detection, within a positive-feedback network structure that is more tolerant of noise than other techniques.",
        "authors": [
            "Pedro Juan Rivera Torres",
            "Chen Chen",
            "Jaime Macías-Aguayo",
            "Sara Rodríguez González",
            "Javier Prieto Tejedor",
            "Orestes Llanes Santiago",
            "Carlos Gershenson García",
            "Samir Kanaan Izquierdo"
        ],
        "journal_conference_name": "Energies",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157959",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "UFO Instruction Graphs Are Machine Knittable",
        "abstract": "Programming low-level controls for knitting machines is a meticulous, time-consuming task that demands specialized expertise. Recently, there has been a shift towards automatically generating low-level knitting machine programs from high-level knit representations that describe knit objects in a more intuitive, user-friendly way. Current high-level systems trade off\r\nexpressivity for ease-of-use, requiring ad-hoc trapdoors to access the full space of machine capabilities, or eschewing completeness in the name of utility. Thus, advanced techniques either require ad-hoc extensions from domain experts, or are entirely unsupported. Furthermore, errors may emerge during the compilation from knit object representations to machine instructions. While the generated program may describe a valid machine control sequence, the fabricated object is topologically different from the specified input, with little recourse for understanding and fixing the issue.\r\n\r\nTo address these limitations, we introduce instruction graphs, an intermediate representation capable of capturing the full range of machine knitting programs. We define a semantic mapping from instruction graphs to fenced tangles, which make them compatible with the established formal semantics for machine knitting instructions. We establish a semantics-preserving bijection between machine knittable instruction graphs and knit programs that proves three properties &#8211; upward, forward, and ordered (UFO) &#8211; are both necessary and sufficient to ensure the existence of a machine knitting program that can fabricate the fenced tangle denoted by the graph. As a proof-of-concept, we implement an instruction graph editor and compiler that allows a user to transform an instruction graph into UFO presentation and then compile it to a machine program, all while maintaining semantic equivalence. In addition, we use the UFO properties to more precisely characterize the limitations of existing compilers. This work lays the groundwork for more expressive and reliable automated knitting machine programming systems by providing a formal characterization of machine knittability.",
        "authors": [
            "Jenny Lin",
            "Yuka Ikarashi",
            "Gilbert Bernstein",
            "James McCann"
        ],
        "journal_conference_name": "ACM Transactions on Graphics",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157853",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Procedural Material Generation with Reinforcement Learning",
        "abstract": "Modern 3D content creation heavily relies on procedural assets. In particular, procedural materials are ubiquitous in the industry, but their manipulation remains challenging. Previous work conditionally generates procedural graphs that match a given input image. However, the parameter generation step limits how accurately the generated graph matches the input image, due to a reliance on supervision with scarcely available procedural data. We propose to improve parameter prediction accuracy for image-conditioned procedural material generation by leveraging reinforcement learning (RL) and present the first RL approach for procedural materials. RL circumvents the limited availability of procedural data, the domain gap between real and synthetic materials, and the need for end-to-end differentiable loss functions. Given a target image, we retrieve a procedural material and use an RL-trained transformer model to predict a set of parameters that reconstruct the target image as closely as possible. We show that using RL significantly improves parameter prediction to match a given target image compared to supervised methods on both synthetic and real target images.",
        "authors": [
            "Beichen Li",
            "Yiwei Hu",
            "Paul Guerrero",
            "Milos Hasan",
            "Liang Shi",
            "Valentin Deschaintre",
            "Wojciech Matusik"
        ],
        "journal_conference_name": "ACM Transactions on Graphics",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157855",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "All you need is rotation: Construction of developable strips",
        "abstract": "We present a novel approach to generate developable strips along a space curve. The key idea of the new method is to use the rotation angle between the Frenet frame of the input space curve, and its Darboux frame of the curve on the resulting developable strip as a free design parameter, thereby revolving the strip around the tangential axis of the input space curve. This angle is not restricted to be constant but it can be any differentiable function defined on the curve, thereby creating a large design space of developable strips that share a common directrix curve. The range of possibilities for choosing the rotation angle is diverse, encompassing constant angles, linearly varying angles, sinusoidal patterns, and even solutions derived from initial value problems involving ordinary differential equations. This enables the potential of the proposed method to be used for a wide range of practical applications, spanning fields such as architectural design, industrial design, and papercraft modeling. In our computational and physical examples, we demonstrate the flexibility of the method by constructing, among others, toroidal and helical windmill blades for papercraft models, curved foldings, triply orthogonal structures, and developable strips featuring a log-aesthetic directrix curve.",
        "authors": [
            "Takashi Maekawa",
            "Felix Scholz"
        ],
        "journal_conference_name": "ACM Transactions on Graphics",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157852",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Medial Skeletal Diagram: A Generalized Medial Axis Approach for Compact 3D Shape Representation",
        "abstract": "We propose the Medial Skeletal Diagram, a novel skeletal representation that tackles the prevailing issues around skeleton sparsity and reconstruction accuracy in existing skeletal representations. Our approach augments the continuous elements in the medial axis representation to effectively shift the complexity away from the discrete elements. To that end, we introduce generalized enveloping primitives, an enhancement over the standard primitives in the medial axis, which ensure efficient coverage of intricate local features of the input shape and substantially reduce the number of discrete elements required. Moreover, we present a computational framework for constructing a medial skeletal diagram from an arbitrary closed manifold mesh. Our optimization pipeline ensures that the resulting medial skeletal diagram comprehensively covers the input shape with the fewest primitives. Additionally, each optimized primitive undergoes a post-refinement process to guarantee an accurate match with the source mesh in both geometry and tessellation. We validate our approach on a comprehensive benchmark of 100 shapes, demonstrating the sparsity of the discrete elements and superior reconstruction accuracy across a variety of cases. Finally, we exemplify the versatility of our representation in downstream applications such as shape generation, mesh decomposition, shape optimization, mesh alignment, mesh compression, and user-interactive design.",
        "authors": [
            "Minghao Guo",
            "Bohan Wang",
            "Wojciech Matusik"
        ],
        "journal_conference_name": "ACM Transactions on Graphics",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157854",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Memory Checking Requires Logarithmic Overhead",
        "abstract": "We study the complexity of memory checkers with computational security and prove the first general tight lower bound.         Memory checkers, first introduced over 30 years ago by Blum, Evans, Gemmel, Kannan, and Naor (FOCS '91, Algorithmica '94), allow a user to store and maintain a large memory on a remote and unreliable server by using small trusted local storage. The user can issue instructions to the server and after every instruction, obtain either the correct value or a failure (but not an incorrect answer) with high probability. The main complexity measure of interest is the size of the local storage and the number of queries the memory checker makes upon every logical instruction. The most efficient known construction has query complexity $O(\\log n/\\log\\log n)$ and local space proportional to a computational security parameter, assuming one-way functions, where $n$ is the logical memory size. Dwork, Naor, Rothblum, and Vaikuntanathan (TCC '09) showed that for a restricted class of ``deterministic and non-adaptive' memory checkers, this construction is optimal, up to constant factors. However, going beyond the small class of deterministic and non-adaptive constructions has remained a major open problem.     In this work, we fully resolve the complexity of memory checkers by showing that \\emph{any} construction with local space $p$ and query complexity $q$ must satisfy       $$ p \\ge \\frac{n}{(\\log n)^{O(q)}} \\;. $$      This implies, as a special case, that $q\\ge \\Omega(\\log n/\\log\\log n)$ in any scheme, assuming that $p\\le n^{1-\\varepsilon}$ for $\\varepsilon>0$. The bound applies to any scheme with computational security, completeness $2/3$, and inverse polynomial in $n$ soundness (all of which make our lower bound only stronger). We further extend the lower bound to schemes where the read complexity $q_r$ and write complexity $q_w$ differ. For instance, we show the tight bound that if $q_r=O(1)$ and $p\\le n^{1-\\varepsilon}$ for $\\varepsilon>0$, then $q_w\\ge n^{\\Omega(1)}$. This is the first lower bound, for any non-trivial class of constructions, showing a read-write query complexity trade-off.        Our proof is via a delicate compression argument showing that a ``too good to be true' memory checker can be used to compress random bits of information. We draw inspiration from tools recently developed for lower bounds for relaxed locally decodable codes. However, our proof itself significantly departs from these works, necessitated by the differences between settings.",
        "authors": [
            "Elette Boyle",
            "Ilan Komargodski",
            "Neekon Vafa"
        ],
        "journal_conference_name": "Journal of the ACM",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158076",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Innovative Approach to Sustainable Fertilizer Production: Leveraging Electrically Assisted Conversion of Sewage Sludge for Nutrient Recovery",
        "abstract": "Efforts addressing sludge management, food security, and resource recovery have led to novel approaches in these areas. Electrically assisted conversion of sludge stands out as a promising technology for sewage sludge valorization, producing nitrogen and phosphorus-based fertilizers. The adoption of this technology, which could lead to a fertilizer circular economy, holds the potential to catalyze a transformative change in wastewater treatment facilities toward process intensification, innovation, and sustainability. This paper provides insights into the economic aspects of the technology, policy considerations, and challenges involved in realizing the potential of electrified processes for sludge valorization. To demonstrate the impact of the technology, a case study for its implementation in the United States assuming the municipal wastewater treatment plants market is discussed. It was found that electrically assisted sludge conversion could enable the recovery of nitrogen and phosphorus from waste, representing up to 9% of the nitrogen and 32% of the phosphorus consumption of the U.S. for fertilizer use. This technology also enables full electrification and modularization of the process, thereby presenting significant economic and environmental opportunities.",
        "authors": [
            "Gerardine G Botte",
            "Dayana Donneys-Victoria",
            "Christian E Alvarez-Pugliese",
            "Jedidian Adjei",
            "Selin Sahin",
            "Nathan W Wilson",
            "Kayleigh Millerick",
            "Amy Hardberger",
            "Ariel L Furst",
            "Nicole Hu",
            "Andrew J Medford"
        ],
        "journal_conference_name": "ACS Omega",
        "publisher": "American Chemical Society",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158291",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploring the Dynamics of Canine-Assisted Interactions: A Wearable Approach to Understanding Interspecies Well-Being",
        "abstract": "first_pageDownload PDFsettingsOrder Article Reprints\r\nOpen AccessFeature PaperArticle\r\nExploring the Dynamics of Canine-Assisted Interactions: A Wearable Approach to Understanding Interspecies Well-Being\r\nby Timothy R. N. Holder 1ORCID,Colt Nichols 2ORCID,Emily Summers 2,David L. Roberts 3ORCID andAlper Bozkurt 2,*ORCID\r\n1\r\nDepartment of Aeronautics and Astronautics, Massachusetts Institute of Technology, Cambridge, MA 02139, USA\r\n2\r\nDepartment of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC 27695, USA\r\n3\r\nDepartment of Computer Science, North Carolina State University, Raleigh, NC 27695, USA\r\n*\r\nAuthor to whom correspondence should be addressed.\r\nAnimals 2024, 14(24), 3628; https://doi.org/10.3390/ani14243628\r\nSubmission received: 11 October 2024 / Revised: 15 November 2024 / Accepted: 27 November 2024 / Published: 16 December 2024\r\n(This article belongs to the Special Issue Animal–Computer Interaction: New Horizons in Animal Welfare)\r\nDownloadkeyboard_arrow_down Browse Figures Versions Notes\r\n\r\nSimple Summary\r\nThis study utilizes electronic sensors to investigate the outcomes of Canine Assisted Interactions (CAI), a growing therapeutic field, for both human and animal participants. It represents the first attempt to deploy synchronized wearable systems on both humans and dogs, allowing for the continuous and simultaneous collection of physiological and behavioral data during interactions. Leveraging this data, the research examines the real-time dynamics of CAIs, moving beyond traditional survey-based pre- and post-session evaluations. Three innovative visualization tools—a subsession heatmap, a synchrony table, and a metric correlation matrix—are introduced to better characterize the interactions and bonding within human-dog dyads. Preliminary exploratory analyses provide insights that inspire further investigation into CAI mechanisms. This research marks a significant step forward in using multimodal data collection to deepen our understanding of human-animal interactions, particularly in therapeutic settings.\r\nAbstract\r\nCanine-assisted interactions (CAIs) have been explored to offer therapeutic benefits to human participants in various contexts, from addressing cancer-related fatigue to treating post-traumatic stress disorder. Despite their widespread adoption, there are still unresolved questions regarding the outcomes for both humans and animals involved in these interactions. Previous attempts to address these questions have suffered from core methodological weaknesses, especially due to absence of tools for an efficient objective evaluation and lack of focus on the canine perspective. In this article, we present a first-of-its-kind system and study to collect simultaneous and continuous physiological data from both of the CAI interactants. Motivated by our extensive field reviews and stakeholder feedback, this comprehensive wearable system is composed of custom-designed and commercially available sensor devices. We performed a repeated-measures pilot study, to combine data collected via this system with a novel dyadic behavioral coding method and short- and long-term surveys. We evaluated these multimodal data streams independently, and we further correlated the psychological, physiological, and behavioral metrics to better elucidate the outcomes and dynamics of CAIs. Confirming previous field results, human electrodermal activity is the measure most strongly distinguished between the dyads’ non-interaction and interaction periods. Valence, arousal, and the positive affect of the human participant significantly increased during interaction with the canine participant. Also, we observed in our pilot study that (a) the canine heart rate was more dynamic than the human’s during interactions, (b) the surveys proved to be the best indicator of the subjects’ affective state, and (c) the behavior coding approaches best tracked the bond quality between the interacting dyads. Notably, we found that most of the interaction sessions were characterized by extended neutral periods with some positive and negative peaks, where the bonded pairs might display decreased behavioral synchrony. We also present three new representations of the internal and overall dynamics of CAIs for adoption by the broader field. Lastly, this paper discusses ongoing options for further dyadic analysis, interspecies emotion prediction, integration of contextually relevant environmental data, and standardization of human–animal interaction equipment and analytical approaches. Altogether, this work takes a significant step forward on a promising path to our better understanding of how CAIs improve well-being and how interspecies psychophysiological states can be appropriately measured.",
        "authors": [
            "Timothy R. N. Holder",
            "Colt Nichols",
            "Emily Summers",
            "David L. Roberts",
            "Alper Bozkurt"
        ],
        "journal_conference_name": "Animals",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157958",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Higher dimensional Fourier quasicrystals from Lee–Yang varieties",
        "abstract": "In this paper, we construct Fourier quasicrystals with unit masses in arbitrary dimensions. This generalizes a one-dimensional construction of Kurasov and Sarnak. To do this, we employ a class of complex algebraic varieties avoiding certain regions in C n , which generalize hypersurfaces defined by Lee–Yang polynomials. We show that these are Delone almost periodic sets that have at most finite intersection with every discrete periodic set.",
        "authors": [
            "Lior Alon",
            "Mario Kummer",
            "Pavel Kurasov",
            "Cynthia Vinzant"
        ],
        "journal_conference_name": "Inventiones mathematicae",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/159044",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Domain Adversarial Convolutional Neural Network Improves the Accuracy and Generalizability of Wearable Sleep Assessment Technology",
        "abstract": "Wearable accelerometers are widely used as an ecologically valid and scalable solution for long-term at-home sleep monitoring in both clinical research and care. In this study, we applied a deep learning domain adversarial convolutional neural network (DACNN) model to this task and demonstrated that this new model outperformed existing sleep algorithms in classifying sleep–wake and estimating sleep outcomes based on wrist-worn accelerometry. This model generalized well to another dataset based on different wearable devices and activity counts, achieving an accuracy of 80.1% (sensitivity 84% and specificity 58%). Compared to commonly used sleep algorithms, this model resulted in the smallest error in wake after sleep onset (MAE of 48.7, Cole–Kripke of 86.2, Sadeh of 108.2, z-angle of 57.5) and sleep efficiency (MAE of 11.8, Cole–Kripke of 18.4, Sadeh of 23.3, z-angle of 9.3) outcomes. Despite being around for many years, accelerometer-alone devices continue to be useful due to their low cost, long battery life, and ease of use. Improving the accuracy and generalizability of sleep algorithms for accelerometer wrist devices is of utmost importance. We here demonstrated that domain adversarial convolutional neural networks can improve the overall accuracy, especially the specificity, of sleep–wake classification using wrist-worn accelerometer data, substantiating its use as a scalable and valid approach for sleep outcome assessment in real life.",
        "authors": [
            "Adonay S. Nunes",
            "Matthew R. Patterson",
            "Dawid Gerstel",
            "Sheraz Khan",
            "Christine C. Guo",
            "Ali Neishabouri"
        ],
        "journal_conference_name": "Sensors",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157956",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Sampling from convex sets with a cold start using multiscale decompositions",
        "abstract": "A standard approach for sampling approximately uniformly from a convex body K ⊆ R n is to run a random walk within K. The requirement is that starting from a suitable initial distribution, the random walk should “mix rapidly”, i.e., after a number of steps that is polynomial in n and the aspect ratio R/r (here, K is assumed to contain a ball of radius r and to be contained within a ball of radius R), the distribution of the random walk should come close to the uniform distribution π K on K. Different random walks differ in aspects such as the ease of implementation of each step, or suitability for a specific class of convex bodies. Therefore, the rapid mixing of a wide variety of random walks on convex bodies has been studied. Many proofs of rapid mixing of such random walks however require that the initial distribution of the random walk is not too different from the target distribution π K . In particular, they require that the probability density function of the initial distribution with respect to the uniform distribution π K on K must be bounded above by poly ( n ) : this is called a warm start. Achieving such a warm start often requires a non-trivial pre-processing step before the random walk can be started. This motivates the problem of proving rapid mixing from “cold starts”, i.e., when the density of the initial distribution with respect to π K can be as high as exp ( poly ( n ) ) . In contrast to warm starts, a cold start is usually trivial to achieve. However, rapid mixing from a cold start may not hold for every random walk, e.g., the well-known “ball walk” does not have rapid mixing from an arbitrary cold start. On the other hand, for the “hit-and-run” random walk, Lovász and Vempala proved rapid mixing from a cold start. For the related coordinate hit-and-run (CHR) random walk, which has been found to be promising in computational experiments, a rapid mixing result starting from a warm start was proven only recently, while the question of whether CHR mixes rapidly from a cold start remained open. In this paper, we construct a family of Markov chains inspired by classical multiscale decompositions of subsets of R n into countably many axis-aligned cubes. We show that even with a cold start, the mixing times of these chains are bounded by a polynomial in n and the aspect ratio of the body. Our main technical ingredient is an isoperimetric inequality for K for a metric that magnifies distances between points that are close to the boundary of K. As a byproduct of the analysis of this new family of chains, we show that the coordinate hit-and-run (CHR) random walk also mixes rapidly from a cold start, and also from any point that is not too close to the boundary of the body.",
        "authors": [
            "Hariharan Narayanan",
            "Amit Rajaraman",
            "Piyush Srivastava"
        ],
        "journal_conference_name": "Probability Theory and Related Fields",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157860",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Adversarial Network Optimization under Bandit Feedback: Maximizing Utility in Non-Stationary Multi-Hop Networks",
        "abstract": "Stochastic Network Optimization (SNO) concerns scheduling in stochastic queueing systems and has been widely studied in network theory. Classical SNO algorithms require network conditions to be stationary w.r.t. time, which fails to capture the non-stationary components in increasingly many real-world scenarios. Moreover, most existing algorithms in network optimization assume perfect knowledge of network conditions before decision, which again rules out applications where unpredictability in network conditions presents.\r\nMotivated by these issues, this paper considers Adversarial Network Optimization (ANO) under bandit feedback. Specifically, we consider the task of i) maximizing some unknown and time-varying utility function associated with scheduler's actions, where ii) the underlying network topology is a non-stationary multi-hop network whose conditions change arbitrarily with time, and iii) only bandit feedback (the effect of actually deployed actions) is revealed after decision-making. We propose the UMO2 algorithm, which does not require any pre-decision knowledge or counterfactual feedback, ensures network stability, and also matches the utility maximization performance of any \"mildly varying\" reference policy up to a polynomially decaying gap. To our knowledge, no previous algorithm can handle multi-hop networks or achieve utility maximization guarantees in ANO problems with bandit feedback, whereas ours is able to do both.\r\nTechnically, our method builds upon a novel integration of online learning techniques into the Lyapunov drift-plus-penalty method. Specifically, we propose meticulous analytical techniques to jointly balance online learning and Lyapunov arguments, which is used to handle the complex inter-dependency among queues in multi-hop networks. To tackle the learning obstacles due to potentially unbounded queue sizes and negative queue differences, we design a new online linear optimization algorithm that automatically adapts to the unknown (potentially negative) loss magnitudes. Finally, we also propose a bandit convex optimization algorithm with novel queue-dependent learning rate scheduling that suites drastically varying queue lengths in utility maximization. Our new insights and techniques in online learning can also be of independent interest.",
        "authors": [
            "Yan Dai",
            "Longbo Huang"
        ],
        "journal_conference_name": "Proceedings of the ACM on Measurement and Analysis of Computing Systems",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158129",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Acceleration by Stepsize Hedging: Multi-Step Descent and the Silver Stepsize Schedule",
        "abstract": "Can we accelerate the convergence of gradient descent without changing the algorithmÐjust by judiciously choosing stepsizes?\r\nSurprisingly, we show that the answer is yes. Our proposed Silver Stepsize Schedule optimizes strongly convex functions in\r\n�\r\nlog�\r\n2 ≈ �\r\n0.7864 iterations, where � = 1 +\r\n√\r\n2 is the silver ratio and � is the condition number. This is intermediate between\r\nthe textbook unaccelerated rate � and the accelerated rate �\r\n1/2 due to Nesterov in 1983. The non-strongly convex setting is\r\nconceptually identical, and standard black-box reductions imply an analogous partially accelerated rate �\r\n− log�\r\n2 ≈ �\r\n−0.7864\r\n.\r\nWe conjecture and provide partial evidence that these rates are optimal among all stepsize schedules.\r\nThe Silver Stepsize Schedule is constructed recursively in a fully explicit way. It is non-monotonic, fractal-like, and\r\napproximately periodic of period �\r\nlog�\r\n2\r\n. This leads to a phase transition in the convergence rate: initially super-exponential\r\n(acceleration regime), then exponential (saturation regime).\r\nThe core algorithmic intuition is hedging between individually suboptimal strategiesÐshort steps and long stepsÐsince bad\r\ncases for the former are good cases for the latter, and vice versa. Properly combining these stepsizes yields faster convergence\r\ndue to the misalignment of worst-case functions. The key challenge in proving this speedup is enforcing long-range consistency\r\nconditions along the algorithm’s trajectory. We do this by developing a technique that recursively glues constraints from\r\ndiferent portions of the trajectory, thus removing a key stumbling block in previous analyses of optimization algorithms.\r\nMore broadly, we believe that the concepts of hedging and multi-step descent have the potential to be powerful algorithmic\r\nparadigms in a variety of contexts in optimization and beyond.\r\nThis paper publishes and extends the irst author’s 2018 Master’s Thesis (advised by the second author)Ðwhich established\r\nfor the irst time that judiciously choosing stepsizes can enable acceleration in convex optimization. Prior to this thesis, the\r\nonly such result was for the special case of quadratic optimization, due to Young in 1953.",
        "authors": [
            "Jason Altschuler",
            "Pablo Parrilo"
        ],
        "journal_conference_name": "Journal of the ACM",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158132",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Anomaly-aware summary statistic from data batches",
        "abstract": "Signal-agnostic data exploration based on machine learning could unveil very subtle statistical deviations of collider data from the expected Standard Model of particle physics. The beneficial impact of a large training sample on machine learning solutions motivates the exploration of increasingly large and inclusive samples of acquired data with resource efficient computational methods. In this work we consider the New Physics Learning Machine (NPLM), a multivariate goodness-of-fit test built on the Neyman-Pearson maximum-likelihood-ratio construction, and we address the problem of testing large size samples under computational and storage resource constraints. We propose to perform parallel NPLM routines over batches of the data, and to combine them by locally aggregating over the data-to-reference density ratios learnt by each batch. The resulting data hypothesis defining the likelihood-ratio test is thus shared over the batches, and complies with the assumption that the expected rate of new physical processes is time invariant. We show that this method outperforms the simple sum of the independent tests run over the batches, and can recover, or even surpass, the sensitivity of the single test run over the full data. Beside the significant advantage for the offline application of NPLM to large size samples, the proposed approach offers new prospects toward the use of NPLM to construct anomaly-aware summary statistics in quasi-online data streaming scenarios.",
        "authors": [
            "G. Grosso"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157890",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Deep Water Subsea Energy Storage, Lessons Learned from the Offshore Oil and Gas Industry",
        "abstract": "In a future where a large portion of power will be supplied by highly intermittent sources such as solar- and wind-power, energy storage will form a crucial part of the power mix ensuring that there is enough flexibility in the system to cope with the intermittency. With further development of pumped storage hydro constrained by the lack of remaining suitable topography, a novel Subsea Pumped Hydro Storage concept has emerged as a promising solution to utilize the ocean space for large-scale energy storage. While previous publications address thermodynamic efficiency limits, there is a notable lack of research on turbine selection, design, and cost estimation based on best practices. This paper presents a comprehensive overview of current state-of-the-art subsea engineering and its significant achievements pioneered by the oil and gas industry. This paper introduces a robust methodological framework for calculating the costs of concrete SPHS tanks, factoring in longevity and best installation practices for structures designed to endure for half a century. The results indicate that with an optimized design, the cost of an SPSH concrete storage tank is approximately $0.15/Wh. This work lays the groundwork for future advancements in SPHS, building on the substantial progress within subsea engineering over recent decades, and marks a significant step towards realizing the potential of this concept in the renewable energy landscape.",
        "authors": [
            "Rasmus Juhlin",
            "Alexander H. Slocum",
            "Mohsen Assadi"
        ],
        "journal_conference_name": "Journal of Marine Science and Engineering",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157955",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Imaging the initial condition of heavy-ion collisions and nuclear structure across the nuclide chart",
        "abstract": "High-energy nuclear collisions encompass three key stages: the structure of the colliding nuclei, informed by low-energy nuclear physics, the initial condition, leading to the formation of quark–gluon plasma (QGP), and the hydrodynamic expansion and hadronization of the QGP, leading to final-state hadron distributions that are observed experimentally. Recent advances in both experimental and theoretical methods have ushered in a precision era of heavy-ion collisions, enabling an increasingly accurate understanding of these stages. However, most approaches involve simultaneously determining both QGP properties and initial conditions from a single collision system, creating complexity due to the coupled contributions of these stages to the final-state observables. To avoid this, we propose leveraging established knowledge of low-energy nuclear structures and hydrodynamic observables to independently constrain the QGP’s initial condition. By conducting comparative studies of collisions involving isobar-like nuclei—species with similar mass numbers but different ground-state geometries—we can disentangle the initial condition’s impacts from the QGP properties. This approach not only refines our understanding of the initial stages of the collisions but also turns high-energy nuclear experiments into a precision tool for imaging nuclear structures, offering insights that complement traditional low-energy approaches. Opportunities for carrying out such comparative experiments at the Large Hadron Collider and other facilities could significantly advance both high-energy and low-energy nuclear physics. Additionally, this approach has implications for the future electron-ion collider. While the possibilities are extensive, we focus on selected proposals that could benefit both the high-energy and low-energy nuclear physics communities. Originally prepared as input for the long-range plan of U.S. nuclear physics, this white paper reflects the status as of September 2022, with a brief update on developments since then.",
        "authors": [
            "Jiangyong Jia",
            "Giuliano Giacalone",
            "Benjamin Bally",
            "James D. Brandenburg",
            "Ulrich Heinz",
            "Shengli Huang",
            "Dean Lee",
            "Yen-Jie Lee",
            "Constantin Loizides",
            "Wei Li",
            "Matthew Luzum",
            "Govert Nijs",
            "Jacquelyn Noronha-Hostler",
            "Mateusz Ploskon",
            "Wilke van der Schee",
            "Bjoern Schenke"
        ],
        "journal_conference_name": "Nuclear Science and Techniques",
        "publisher": "Springer Nature Singapore",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157891",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "How information about historic carbon emissions affects support for climate aid: evidence from a survey experiment",
        "abstract": "In recent years, international climate negotiations have reached increasing consensus that the wealthiest countries should make significant financial contributions to offset the damages caused by the climate crisis in poorer countries. Proponents have justified such action based on wealthy countries’ disproportionate responsibility for global warming in the form of past emissions. However, in democratic countries such as the United States, it remains uncertain whether such messages can affect public opinion, especially across partisan lines. We conducted a pre-registered survey from a national online pool (N = 5,002) with a built-in experiment to evaluate the effectiveness of alternative communications strategies associated with historic carbon emissions in increasing support for climate aid. We find that specific attribution claims that reflect a climate justice perspective do boost support for more generous climate aid, but the effects are largely driven by Democrats. We also find that global solidarity frames emphasizing shared responsibility did not affect support for climate aid. Our results have important implications for climate advocacy and our understanding of climate-related attitudes.",
        "authors": [
            "Volha Charnysh",
            "Jared Kalow",
            "Evan Lieberman",
            "Erin Walk"
        ],
        "journal_conference_name": "Climatic Change",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157886",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "InfoPro: Locally Supervised Deep Learning by Maximizing Information Propagation",
        "abstract": "End-to-end (E2E) training has become the de-facto standard for training modern deep networks, e.g., ConvNets and vision Transformers (ViTs). Typically, a global error signal is generated at the end of a model and back-propagated layer-by-layer to update the parameters. This paper shows that the reliance on back-propagating global errors may not be necessary for deep learning. More precisely, deep networks with a competitive or even better performance can be obtained by purely leveraging locally supervised learning, i.e., splitting a network into gradient-isolated modules and training them with local supervision signals. However, such an extension is non-trivial. Our experimental and theoretical analysis demonstrates that simply training local modules with an E2E objective tends to be short-sighted, collapsing task-relevant information at early layers, and hurting the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discarding task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. We evaluate InfoPro extensively with ConvNets and ViTs, based on twelve computer vision benchmarks organized into five tasks (i.e., image/video recognition, semantic/instance segmentation, and object detection). InfoPro exhibits superior efficiency over E2E training in terms of GPU memory footprints, convergence speed, and training data scale. Moreover, InfoPro enables the effective training of more parameter- and computation-efficient models (e.g., much deeper networks), which suffer from inferior performance when trained in E2E. Code: https://github.com/blackfeather-wang/InfoPro-Pytorch .",
        "authors": [
            "Yulin Wang",
            "Zanlin Ni",
            "Yifan Pu",
            "Cai Zhou",
            "Jixuan Ying",
            "Shiji Song",
            "Gao Huang"
        ],
        "journal_conference_name": "International Journal of Computer Vision",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/159179",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Study of the rare decay J/ψ → μ+μ−μ+μ−",
        "abstract": "The rare electromagnetic J/ψ → μ+μ−μ+μ− decay is observed with a significance greatly exceeding the discovery threshold, using proton-proton collision data collected by the LHCb experiment during 2016–2018 at a center-of-mass energy of 13 TeV, corresponding to an integrated luminosity of 5.4 fb−1. The rate of this decay is measured relative to that of the J/ψ → μ+μ− mode. Using the QED model for the four-muon decay in the efficiency estimation, its branching fraction is determined to be B J / ψ → μ + μ − μ + μ − = 1.13 ± 0.10 ± 0.05 ± 0.01 × 10 − 6 , where the uncertainties are statistical, systematic and due to the uncertainty on the branching fraction of the J/ψ → μ+μ− decay.",
        "authors": [
            "R. Aaij",
            "A. S. W. Abdelmotteleb",
            "C. Abellan Beteta",
            "F. Abudinén",
            "T. Ackernley",
            "A. A. Adefisoye",
            "B. Adeva",
            "M. Adinolfi",
            "P. Adlarson",
            "C. Agapopoulou",
            "C. A. Aidala",
            "Z. Ajaltouni",
            "S. Akar",
            "K. Akiba",
            "P. Albicocco",
            "J. Albrecht",
            "The LHCb collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157889",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Imaging the wakes of jets with energy-energy-energy correlators",
        "abstract": "As the partons in a high energy jet propagate through the droplet of quark-gluon plasma (QGP) produced in a heavy-ion collision they lose energy to, kick, and are kicked by the medium. The resulting modifications to the parton shower encode information about the microscopic nature of QGP. A direct consequence, however, is that the momentum and energy lost by the parton shower are gained by the medium and, since QGP is a strongly coupled liquid, this means that the jet excites a wake in the droplet of QGP. After freezeout, this wake becomes soft hadrons with net momentum in the jet direction meaning that what an experimentalist later reconstructs as a jet includes hadrons originating from both the modified parton shower and its wake. This has made it challenging to find experimental observables that provide an unambiguous view of the dynamical response of a droplet of QGP to a jet shooting through it. Recent years have seen significant substantial advances in the theoretical and experimental understanding of the substructure of jets, in particular, using correlation functions, E n → 1 ⋯ E n → k , of the energy flux operator in proton-proton collisions and, recently, in heavy-ion collisions. So far, such studies have focused primarily on the two-point correlator, which allows for the identification of the angular scale of the underlying dynamics. Higher-point correlators hold the promise of mapping out the dynamics themselves. In this paper we perform the first study of the shape-dependent three-point energy-energy-energy correlator in heavy-ion collisions. Using the Hybrid Model to simulate the interactions of high energy jets with the QGP medium, we show that the three-point correlator presents us with a striking new opportunity. We find that hadrons originating from wakes are the dominant contribution to the three-point correlator in the kinematic regime in which the three points are well-separated in angle, forming a roughly equilateral triangle. This equilateral region of the correlator is far from the region populated by collinear vacuum emissions, making it a canvas on which jet wakes are laid out, where experimentalists can map their shapes. Our work provides a key step towards the systematic use of energy correlators to image and unravel the dynamical response of a droplet of QGP that has been probed by a passing jet, and motivates numerous experimental and theoretical studies.",
        "authors": [
            "Hannah Bossi",
            "Arjun S. Kudinoor",
            "Ian Moult",
            "Daniel Pablos",
            "Ananya Rai",
            "Krishna Rajagopal"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157947",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "JWST sighting of decameter main-belt asteroids and view on meteorite sources",
        "abstract": "Asteroid discoveries are essential for planetary-defense efforts aiming to prevent impacts\r\nwith Earth, including the more frequent megaton explosions from decameter impactors.\r\nWhile large asteroids (≥100 km) have remained in the main belt since their formation,\r\nsmall asteroids are commonly transported to the near-Earth object (NEO) population.\r\nHowever, due to the lack of direct observational constraints, their size-frequency distribution —which informs our understanding of the NEOs and the delivery of meteorite\r\nsamples to Earth—varies significantly among models. Here, we report 138 detections\r\nof the smallest asteroids (⪆10 m) ever observed in the main belt, which were enabled by JWST’s infrared capabilities covering the asteroids’ emission peaks and synthetic tracking techniques. Despite small orbital arcs, we constrain the objects’ distances and\r\nphase angles using known asteroids as proxies, allowing us to derive sizes via radiometric\r\ntechniques. Their size-frequency distribution exhibits a break at ∼100 m (debiased cumulative slopes of q = −2.66 ± 0.60 and −0.97 ± 0.14 for diameters smaller and larger than\r\n∼100 m, respectively), suggestive of a population driven by collisional cascade. These\r\nasteroids were sampled from multiple asteroid families —most likely Nysa, Polana and\r\nMassalia— according to the geometry of pointings considered here. Through additional\r\nlong-stare infrared observations, JWST is poised to serendipitously detect thousands of\r\ndecameter-scale asteroids across the sky, probing individual asteroid families and the\r\nsource regions of meteorites “in-situ”.",
        "authors": [
            "Artem Y. Burdanov",
            "Julien de Wit",
            "Miroslav Broz",
            "Thomas G. Muller",
            "Tobias Hoffmann",
            "Marin Ferrais",
            "Marco Micheli",
            "Emmanuel Jehin",
            "Daniel Parrott",
            "Samantha N. Hasler",
            "Richard P. Binzel",
            "Elsa Ducrot",
            "Laura Kreidberg",
            "Michael Gillon",
            "Thomas P. Greene",
            "Will M. Grundy",
            "Theodore Kareta",
            "Pierre-Olivier Lagage",
            "Nicholas Moskovitz",
            "Audrey Thirouin",
            "Cristina A. Thomas",
            "Sebastian Zieba"
        ],
        "journal_conference_name": "Nature",
        "publisher": "Springer Nature",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157797",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Classical correspondence beyond the Ehrenfest time for open quantum systems with general Lindbladians",
        "abstract": "Quantum and classical systems evolving under the same formal Hamiltonian H may exhibit dramatically different behavior after the Ehrenfest timescale t E ∼ log ( ħ - 1 ) , even as ħ → 0 . Coupling the system to a Markovian environment results in a Lindblad equation for the quantum evolution. Its classical counterpart is given by the Fokker–Planck equation on phase space, which describes Hamiltonian flow with friction and diffusive noise. The quantum and classical evolutions may be compared via the Wigner-Weyl representation. Due to decoherence, they are conjectured to match closely for times far beyond the Ehrenfest timescale as ħ → 0 . We prove a version of this correspondence, bounding the error between the quantum and classical evolutions for any sufficiently regular Hamiltonian H(x, p) and Lindblad functions L k ( x , p ) . The error is small when the strength of the diffusion D associated to the Lindblad functions satisfies D ≫ ħ 4 / 3 , in particular allowing vanishing noise in the classical limit. Our method uses a time-dependent semiclassical mixture of variably squeezed Gaussian states. The states evolve according to a local harmonic approximation to the Lindblad dynamics constructed from a second-order Taylor expansion of the Lindbladian. Both the exact quantum trajectory and its classical counterpart can be expressed as perturbations of this semiclassical mixture, with the errors bounded using Duhamel’s principle. We present heuristic arguments suggesting the 4/3 exponent is optimal and defines a boundary in the sense that asymptotically weaker diffusion permits a breakdown of quantum-classical correspondence at the Ehrenfest timescale. Our presentation aims to be comprehensive and accessible to both mathematicians and physicists. In a shorter companion paper, we treat the special case of Hamiltonians that decompose into kinetic and potential energy with linear Lindblad operators, with explicit bounds that can be applied directly to physical systems.",
        "authors": [
            "Felipe Hernández",
            "Daniel Ranard",
            "C. J. Riedel"
        ],
        "journal_conference_name": "Communications in Mathematical Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157859",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Study of charmonium production via the decay to p p¯ at √s = 13 TeV",
        "abstract": "Charmonium production cross-section in proton–proton collisions is measured at the centre-of-mass energy s = 13 TeV using decays to p p ¯ final state. The study is performed using a data sample corresponding to an integrated luminosity of 2.2 fb - 1 collected in 2018 with the LHCb detector. The production cross-section of the η c meson is measured in a rapidity range of 2.0 < y < 4.0 and in a transverse momentum range of 5.0 < p T < 20.0 GeV / c , which is extended compared with previous LHCb analyses. The differential cross-section is measured in bins of p T and, for the first time, of y. Upper limits, at 90% and 95% confidence levels, on the η c ( 2 S ) and h c ( 1 P ) prompt production cross-sections are determined for the first time.",
        "authors": [
            "LHCb Collaboration"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157885",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Measurement of the electric potential and the magnetic field in the shifted analysing plane of the KATRIN experiment",
        "abstract": "The projected sensitivity of the effective electron neutrino-mass measurement with the KATRIN experiment is below 0.3 eV (90 % CL) after 5 years of data acquisition. The sensitivity is affected by the increased rate of the background electrons from KATRIN’s main spectrometer. A special shifted-analysing-plane (SAP) configuration was developed to reduce this background by a factor of two. The complex layout of electromagnetic fields in the SAP configuration requires a robust method of estimating these fields. We present in this paper a dedicated calibration measurement of the fields using conversion electrons of gaseous 83m Kr, which enables the neutrino-mass measurements in the SAP configuration.",
        "authors": [
            "KATRIN Collaboration"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157799",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Search for CP violation in D0 → K0 SK0 S decays in proton–proton collisions at √s = 13 TeV",
        "abstract": "A search is reported for charge-parity CP violation in D 0 → K S 0 K S 0 decays, using data collected in proton–proton collisions at s = 13 Te V recorded by the CMS experiment in 2018. The analysis uses a dedicated data set that corresponds to an integrated luminosity of 41.6 fb - 1 , which consists of about 10 billion events containing a pair of b hadrons, nearly all of which decay to charm hadrons. The flavor of the neutral D meson is determined by the pion charge in the reconstructed decays D ∗ + → D 0 π + and D ∗ - → D ¯ 0 π - . The CP asymmetry in D 0 → K S 0 K S 0 is measured to be A CP ( K S 0 K S 0 ) = ( 6.2 ± 3.0 ± 0.2 ± 0.8 ) % , where the three uncertainties represent the statistical uncertainty, the systematic uncertainty, and the uncertainty in the measurement of the CP asymmetry in the D 0 → K S 0 π + π - decay. This is the first CP asymmetry measurement by CMS in the charm sector as well as the first to utilize a fully hadronic final state.",
        "authors": [
            "CMS Collaboration"
        ],
        "journal_conference_name": "The European Physical Journal C",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157800",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Using magnetic resonance relaxometry to evaluate the safety and quality of induced pluripotent stem cell-derived spinal cord progenitor cells",
        "abstract": "Background The emergence of induced pluripotent stem cells (iPSCs) offers a promising approach for replacing damaged neurons and glial cells, particularly in spinal cord injuries (SCI). Despite its merits, iPSC differentiation into spinal cord progenitor cells (SCPCs) is variable, necessitating reliable assessment of differentiation and validation of cell quality and safety. Phenotyping is often performed via label-based methods including immunofluorescent staining or flow cytometry analysis. These approaches are often expensive, laborious, time-consuming, destructive, and severely limits their use in large scale cell therapy manufacturing settings. On the other hand, cellular biophysical properties have demonstrated a strong correlation to cell state, quality and functionality and can be measured with ingenious label-free technologies in a rapid and non-destructive manner. Method In this study, we report the use of Magnetic Resonance Relaxometry (MRR), a rapid and label-free method that indicates iron levels based on its readout (T2). Briefly, we differentiated human iPSCs into SCPCs and compared key iPSC and SCPC cellular markers to their intracellular iron content (Fe3+) at different stages of the differentiation process. Results With MRR, we found that intracellular iron of iPSCs and SCPCs were distinctively different allowing us to accurately reflect varying levels of residual undifferentiated iPSCs (i.e., OCT4+ cells) in any given population of SCPCs. MRR was also able to predict Day 10 SCPC OCT4 levels from Day 1 undifferentiated iPSC T2 values and identified poorly differentiated SCPCs with lower T2, indicative of lower neural progenitor (SOX1) and stem cell (Nestin) marker expression levels. Lastly, MRR was able to provide predictive indications for the extent of differentiation to Day 28 spinal cord motor neurons (ISL-1/SMI-32) based on the T2 values of Day 10 SCPCs. Conclusion MRR measurements of iPSCs and SCPCs has clearly indicated its capabilities to identify and quantify key phenotypes of iPSCs and SCPCs for end-point validation of safety and quality parameters. Thus, our technology provides a rapid label-free method to determine critical quality attributes in iPSC-derived progenies and is ideally suited as a quality control tool in cell therapy manufacturing.",
        "authors": [
            "Jerome Tan",
            "Jiahui Chen",
            "Daniel Roxby",
            "Wai H. Chooi",
            "Tan D. Nguyen",
            "Shi Y. Ng",
            "Jongyoon Han",
            "Sing Y. Chew"
        ],
        "journal_conference_name": "Stem Cell Research & Therapy",
        "publisher": "BioMed Central",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157837",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Advances in 2D Molybdenum Disulfide Transistors for Flexible and Wearable Electronics",
        "abstract": "As the trajectory of developing advanced electronics is shifting towards wearable electronics, various methods for implementing flexible and bendable devices capable of conforming to curvilinear surfaces have been widely investigated. In particular, achieving high-performance and stable flexible transistors remains a significant technical challenge, as transistors are fundamental components of electronics, playing a key role in overall performance. Among the wide range of candidates for flexible transistors, two-dimensional (2D) molybdenum disulfide (MoS2)-based transistors have emerged as potential solutions to address these challenges. Unlike other 2D materials, the 2D MoS2 offers numerous advantages, such as high carrier mobility, a tunable bandgap, superior mechanical strength, and exceptional chemical stability. This review emphasizes the novel techniques of the fabrication process, structure, and material to achieve flexible MoS2 transistor-based applications. Furthermore, the distinctive feature of this review is its focus on studies published in high-impact journals over the past decade, emphasizing their methods for developing MoS2 transistors into various applications. Finally, the review addresses technical challenges and provides an outlook for flexible and wearable MoS2 transistors.",
        "authors": [
            "Kyoungwon Kwak",
            "Hyewon Yoon",
            "Seongin Hong",
            "Byung Ha Kang"
        ],
        "journal_conference_name": "Micromachines",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157954",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "\"Data comes from the real world\": A Constructionist Approach to Mainstreaming K12 Data Science Education",
        "abstract": "Data science is emerging as a crucial 21st-century competence, influencing professional practices from citing evidence when advocating for social change to developing artificial intelligence (AI) models. For middle and high school students, data science can put formerly decontextualized subjects into real-world scenarios. Many existing curricula, however, lack authenticity and personal relevance for students. A critique of data science courseware cites the lack of \"author proximity,\" in which students do not contribute to the data's production or see their personal experiences reflected in the data. This paper introduces a novel data science curriculum to scaffold middle and high school students in undertaking real-world data science practices. Through project-based learning modules, the curriculum engages students in investigating solutions to community-based problems through visualization and analysis of live sensor data and public data sets. Materials include formative assessments to help educators (especially those from non-math and computing backgrounds) measure their students' abilities to identify statistical patterns, critically evaluate data biases, and make predictions. As we pilot and co-design with teachers, we will look closely at whether the curriculum's resources can successfully support non-technical practitioners engaging in an integrated curriculum.",
        "authors": [
            "Prerna Ravi",
            "Robert Parks",
            "John Masla",
            "Hal Abelson",
            "Cynthia Breazeal"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 2024 ACM Virtual Global Computing Education Conference V. 1",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158080",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "AI Mastery May Not Be For Everyone, But AI Literacy Should Be",
        "abstract": "Despite the abundance of advice from policy bodies, professional associations, advocacy groups, and scholars on how K-12 schools should assimilate AI and provide AI education, practical plans are lacking from K-12 education leaders themselves. Education leaders must make strategic decisions about how to prepare teachers and students for an AI-infused future. Simultaneously, educators need immediate support and guidance on how to manage the arrival of tools that render some existing educational practices obsolete and prompt the need to teach new skills and awareness. Near term, it may be unrealistic to expect all students to master the ability to develop AI applications; universal AI literacy is a more feasible goal. We introduce a set of short-format, modular AI literacy courses and report how they were implemented and affected teachers' and students' knowledge and perceptions of AI. Using an online questionnaire, we collected data from 265 individuals worldwide who accessed the courses, including 190 teachers who implemented them with over 11,800 students. We conducted 17 teacher interviews to gather feedback and to better understand how courses were adapted for local contexts. Teachers reported an increase in their own and their students' knowledge of AI concepts; and increased optimism about the potential benefits of AI to society and their ability to influence the future of AI. Key takeaways are that AI literacy instruction should be designed for adaptability to local contexts and cultures and that steps should be taken to institutionalize the integration of AI literacy into the regular school curriculum.",
        "authors": [
            "Fiona Hollands",
            "Daniella DiPaola",
            "Cynthia Breazeal",
            "Safinah Ali"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 2024 ACM Virtual Global Computing Education Conference V. 1",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158079",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A low-cost, open-source cylindrical Couette rheometer",
        "abstract": "Rheology describes the flow of fluids from food and plastics, to coatings, adhesives, and 3D printing inks, and is commonly denoted by viscosity alone as a simplification. While viscometers adequately probe Newtonian (constant) viscosity, most fluids have complex viscosity, requiring tests over multiple shear rates, and transient measurements. As a result, rheometers are typically large, expensive, and require additional infrastructure (e.g., gas lines), rendering them inaccessible for regular use by many individuals, small organizations, and educators. Here, we introduce a low-cost (under USD$200 bill of materials) Open Source Rheometer (OSR), constructed entirely from thermoplastic 3D printed components and off-the-shelf electromechanical components. A sample fluid rests in a cup while a micro stepping motor rotates a tool inside the cup, applying strain-controlled shear flow. A loadcell measures reaction torque exerted on the cup, and viscosity is calculated. To establish the measurement range, the viscosity of four Newtonian samples of 0.1–10 Pa.s were measured with the OSR and compared to benchmark values from a laboratory rheometer, showing under 23% error. Building on this, flow curves of three complex fluids – a microgel (hand sanitizer), foam (Gillette), and biopolymer solution (1% Xanthan Gum) – were measured with a similar error range. Stress relaxation, a transient test, was demonstrated on the biopolymer solution to extract the nonlinear damping function. We finally include detailed exposition of measurement windows, sources of error, and future design suggestions. The OSR cost is ∼1/25th that of commercially available devices with comparable minimum torque (200 µN.m), and provides a fully open-source platform for further innovation in customized rheometry.",
        "authors": [
            "Makita Erni",
            "A. John Hart",
            "David Trumper",
            "Crystal E. Owens"
        ],
        "journal_conference_name": "Scientific Reports",
        "publisher": "Springer Nature",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157788",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Measurement of boosted Higgs bosons produced via vector boson fusion or gluon fusion in the H → bb¯ decay mode using LHC proton-proton collision data at √s = 13 TeV",
        "abstract": "A measurement is performed of Higgs bosons produced with high transverse momentum (pT) via vector boson or gluon fusion in proton-proton collisions. The result is based on a data set with a center-of-mass energy of 13 TeV collected in 2016–2018 with the CMS detector at the LHC and corresponds to an integrated luminosity of 138 fb−1. The decay of a high-pT Higgs boson to a boosted bottom quark-antiquark pair is selected using large-radius jets and employing jet substructure and heavy-flavor taggers based on machine learning techniques. Independent regions targeting the vector boson and gluon fusion mechanisms are defined based on the topology of two quark-initiated jets with large pseudorapidity separation. The signal strengths for both processes are extracted simultaneously by performing a maximum likelihood fit to data in the large-radius jet mass distribution. The observed signal strengths relative to the standard model expectation are 4.9 − 1.6 + 1.9 and 1.6 − 1.5 + 1.7 for the vector boson and gluon fusion mechanisms, respectively. A differential cross section measurement is also reported in the simplified template cross section framework.",
        "authors": [
            "A. Hayrapetyan",
            "A. Tumasyan",
            "W. Adam",
            "J. W. Andrejkovic",
            "T. Bergauer",
            "S. Chatterjee",
            "K. Damanakis",
            "M. Dragicevic",
            "P. S. Hussain",
            "M. Jeitler",
            "N. Krammer",
            "A. Li",
            "D. Liko",
            "I. Mikulec",
            "J. Schieck",
            "R. Schöfbeck",
            "D. Schwarz",
            "M. Sonawane",
            "The CMS collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157887",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "How J-chain ensures the assembly of immunoglobulin IgM pentamers",
        "abstract": "Polymeric IgM immunoglobulins have high avidity for antigen and complement, and dominate primary antibody responses. They are produced either as assemblies of six µ2L2 subunits (i.e., hexamers), or as pentamers of two µ2L2 subunits and an additional protein termed J-chain (JC), which allows transcytosis across epithelia. The molecular mechanism of IgM assembly with the desired stoichiometry remained unknown. Here, we show in vitro and in cellula that JC outcompetes the sixth IgM subunit during assembly. Before insertion into IgM, JC exists as an ensemble of largely unstructured, protease-sensitive species with heterogeneous, non-native disulfide bonds. The J-chain interacts with the hydrophobic β-sheets selectively exposed by nascent pentamers. Completion of an amyloid-like core triggers JC folding and drives disulfide rearrangements that covalently stabilize JC-containing pentamers. In cells, the quality control factor ERp44 surveys IgM assembly and prevents the secretion of aberrant conformers. This mechanism allows the efficient production of high-avidity IgM for systemic or mucosal immunity.",
        "authors": [
            "Chiara Giannone",
            "Xenia Mess",
            "Ruiming He",
            "Maria R. Chelazzi",
            "Annika Mayer",
            "Anush Bakunts",
            "Tuan Nguyen",
            "Yevheniia Bushman",
            "Andrea Orsi",
            "Benedikt Gansen",
            "Massimo Degano",
            "Johannes Buchner",
            "Roberto Sitia"
        ],
        "journal_conference_name": "The EMBO Journal",
        "publisher": "Nature Publishing Group UK",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157840",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Around the Corner mmWave Imaging in Practical Environments",
        "abstract": "We present the design, implementation, and evaluation of RFlect, a mmWave imaging system capable of producing around-the-corner high-resolution images in practical environments. RFlect leverages signals reflected off complex surfaces (e.g., poles, concave surfaces, or composition of multiple surfaces) to image objects that are not in the RF line-of-sight. RFlect models the reflections and introduces reconstruction algorithms for different types of surfaces. It also leverages a novel method for precisely mapping the location and geometry of the reflecting surface. We also derive the theoretical resolution and coverage for different reflecting surface geometries. We built a prototype of RFlect and performed extensive evaluations to demonstrate its ability to reconstruct the shape of objects around the corner, with an average Chamfer Distance of 2cm and 3D F-Score of 88.6%.",
        "authors": [
            "Laura Dodds",
            "Hailan Shanbhag",
            "Junfeng Guan",
            "Saurabh Gupta",
            "Haitham Hassanieh"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 30th Annual International Conference on Mobile Computing and Networking",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158077",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Automated and Blind Detection of Low Probability of Intercept RF Anomaly Signals",
        "abstract": "Automated spectrum monitoring necessitates the accurate detection of low probability of intercept (LPI) radio frequency (RF) anomaly signals to identify unwanted interference in wireless networks. However, detecting these unforeseen low-power RF signals is fundamentally challenging due to the scarcity of labeled RF anomaly data. In this paper, we introduce WANDA (Wireless ANomaly Detection Algorithm), an automated framework designed to detect LPI RF anomaly signals in low signal-to-interference ratio (SIR) environments without relying on labeled data. WANDA operates through a two-step process: (i) Information extraction, where a convolutional neural network (CNN) utilizing soft Hirschfeld-Gebelein-Rényi correlation (HGR) as the loss function extracts informative features from RF spectrograms; and (ii) Anomaly detection, where the extracted features are applied to a one-class support vector machine (SVM) classifier to infer RF anomalies. To validate the effectiveness of WANDA, we present a case study focused on detecting unknown Bluetooth signals within the WiFi spectrum using a practical dataset. Experimental results demonstrate that WANDA outperforms other methods in detecting anomaly signals across a range of SIR values (-10 dB to 20 dB).",
        "authors": [
            "Kuanl Gusain",
            "Zoheb Hassan",
            "David Couto",
            "Mai Abdel Malek",
            "Vijay K Shah",
            "Lizhong Zheng",
            "Jeffrey H. Reed"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 30th Annual International Conference on Mobile Computing and Networking",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158078",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "SeaScan: An Energy-Efficient Underwater Camera for Wireless 3D Color Imaging",
        "abstract": "We present the design, implementation, and evaluation of SeaScan, an energy-efficient camera for 3D imaging of underwater environments. At the core of SeaScan's design is a trinocular lensing system, which employs three ultra-low-power monochromatic image sensors to reconstruct color images. Each of the sensors is equipped with a different filter (red, green, and blue) for color capture. The design introduces multiple innovations to enable reconstructing 3D color images from the captured monochromatic ones. This includes an ML-based cross-color alignment architecture to combine the monochromatic images. It also includes a cross-refractive compensation technique that overcomes the distortion of the wide-angle imaging of the low-power CMOS sensors in underwater environments. We built an end-to-end prototype of SeaScan, including color filter integration, 3D reconstruction, compression, and underwater backscatter communication. Our evaluation in real-world underwater environments demonstrates that SeaScan can capture underwater color images with as little as 23.6 mJ, which represents 37X reduction in energy consumption in comparison to the lowest-energy state-of-the-art underwater imaging system. We also report qualitative and quantitative evaluation of SeaScan's color reconstruction and demonstrate its success in comparison to multiple potential alternative techniques (both geometric and ML-based) in the literature. SeaScan's ability to image underwater environments at such low energy opens up important applications in long-term monitoring for ocean climate change, seafood production, and scientific discovery.",
        "authors": [
            "Nazish Naeem",
            "Jack Rademacher",
            "Ritik Patnaik",
            "Tara Boroushaki",
            "Fadel Adib"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 30th Annual International Conference on Mobile Computing and Networking",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158065",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "SURF: Eavesdropping on Underwater Communications from the Air",
        "abstract": "This paper investigates how an airborne node can eavesdrop on the underwater acoustic communication between submerged nodes. Conventionally, such eavesdropping has been assumed impossible as acoustic signals do not cross the water-air boundary. Here, we demonstrate that underwater acoustic communications signals can be picked up and (under certain conditions) decoded using an airborne mmWave radar due to the minute vibrations induced by the communication signals on the water surface. We implemented and evaluated a proof-of-concept prototype of our method and tested it in controlled (pool) and uncontrolled environments (lake). Our results demonstrate that an airborne device can identify the modulation and bitrate of acoustic transmissions from an uncooperative underwater transmitter (victim), and even decode the transmitted symbols. Unlike conventional over-the-air communications, our results indicate that the secrecy of underwater links varies depending on the modulation type and provide insights into the underlying reasons behind these differences. We also highlight the theoretical limitations of such a threat model, and how these results may have a significant impact on the stealthiness of underwater communications, with particular concern to submarine warfare, underwater operations (e.g., oil & gas, search & rescue, mining), and conservation of endangered species. Finally, our investigation uncovers countermeasures that can be used to improve or restore the stealthiness of underwater acoustic communications against such threats.",
        "authors": [
            "Poorya Mollahosseini",
            "Sayed Saad Afzal",
            "Fadel Adib",
            "Yasaman Ghasempour"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 30th Annual International Conference on Mobile Computing and Networking",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158064",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Snooping Underwater Communications via Low-Cost mmWave Radars",
        "abstract": "This study examines how an airborne device can intercept underwater acoustic signals exchanged between submerged nodes. It challenges the conventional belief that acoustic communications under the water are safe against eavesdropping since acoustics do not cross the water-air boundary. We show that an airborne mmWave radar can detect and decode underwater acoustic signals by picking up minute surface vibrations induced by these signals. The proof-of-concept was tested in controlled (pool) and uncontrolled (lake) environments, proving that an airborne adversary can identify modulation type, bitrate, and decode symbols from an uncooperative underwater transmitter using its radar sensing capabilities. We demonstrate that the secrecy of underwater links depends on modulation type, providing insights into countermeasures to enhance the security of underwater acoustic communications.",
        "authors": [
            "Poorya Mollahosseini",
            "Sayed Saad Afzal",
            "Fadel Adib",
            "Yasaman Ghasempour"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 30th Annual International Conference on Mobile Computing and Networking",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158074",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Europa Imaging System (EIS) Investigation",
        "abstract": "The Europa Imaging System (EIS) consists of a Narrow-Angle Camera (NAC) and a Wide-Angle Camera (WAC) that are designed to work together to address high-priority science objectives regarding Europa’s geology, composition, and the nature of its ice shell. EIS accommodates variable geometry and illumination during rapid, low-altitude flybys with both framing and pushbroom imaging capability using rapid-readout, 8-megapixel (4k × 2k) detectors. Color observations are acquired using pushbroom imaging with up to six broadband filters. The data processing units (DPUs) perform digital time delay integration (TDI) to enhance signal-to-noise ratios and use readout strategies to measure and correct spacecraft jitter. The NAC has a 2.3° × 1.2° field of view (FOV) with a 10-μrad instantaneous FOV (IFOV), thus achieving 0.5-m pixel scale over a swath that is 2 km wide and several km long from a range of 50 km. The NAC is mounted on a 2-axis gimbal, ±30° cross- and along-track, that enables independent targeting and near-global (≥90%) mapping of Europa at ≤100-m pixel scale (to date, only ∼15% of Europa has been imaged at ≤900 m/pixel), as well as stereo imaging from as close as 50-km altitude to generate digital terrain models (DTMs) with ≤4-m ground sample distance (GSD) and ≤0.5-m vertical precision. The NAC will also perform observations at long range to search for potential erupting plumes, achieving 10-km pixel scale at a distance of one million kilometers. The WAC has a 48° × 24° FOV with a 218-μrad IFOV, achieving 11-m pixel scale at the center of a 44-km-wide swath from a range of 50 km, and generating DTMs with 32-m GSD and ≤4-m vertical precision. The WAC is designed to acquire three-line pushbroom stereo and color swaths along flyby ground-tracks.",
        "authors": [
            "E. P. Turtle",
            "A. S. McEwen",
            "G. W. Patterson",
            "C. M. Ernst",
            "C. M. Elder",
            "K. A. Slack",
            "S. E. Hawkins",
            "J. McDermott",
            "H. Meyer",
            "R. DeMajistre",
            "R. Espiritu",
            "H. Seifert",
            "J. Niewola"
        ],
        "journal_conference_name": "Space Science Reviews",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157798",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions",
        "abstract": "Computer-Generated Holography (CGH) is a set of algorithmic methods for identifying holograms that reconstruct Three-Dimensio-nal (3D) scenes in holographic displays. CGH algorithms decompose 3D scenes into multiplanes at different depth levels and rely on simulations of light that propagated from a source plane to a targeted plane. Thus, for n planes, CGH typically optimizes holograms using n plane-to-plane light transport simulations, leading to major time and computational demands. Our work replaces multiple planes with a focal surface and introduces a learned light transport model that could propagate a light field from a source plane to the focal surface in a single inference. Our model leverages spatially adaptive convolution to achieve depth-varying propagation demanded by targeted focal surfaces. The proposed model reduces the hologram optimization process up to 1.5x, which contributes to hologram dataset generation and the training of future learned CGH models.",
        "authors": [
            "Chuanjun Zheng",
            "Yicheng Zhan",
            "Liang Shi",
            "Ozan Cakmakci",
            "Kaan Ak?it"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|SIGGRAPH Asia 2024 Technical Communications",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157841",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Measurement of the effective leptonic weak mixing angle",
        "abstract": "Using pp collision data at s = 13 TeV, recorded by the LHCb experiment between 2016 and 2018 and corresponding to an integrated luminosity of 5.4 fb−1, the forward-backward asymmetry in the pp → Z/γ* → μ+μ− process is measured. The measurement is carried out in ten intervals of the difference between the muon pseudorapidities, within a fiducial region covering dimuon masses between 66 and 116 GeV, muon pseudorapidities between 2.0 and 4.5 and muon transverse momenta above 20 GeV. These forward-backward asymmetries are compared with predictions, at next-to-leading order in the strong and electroweak couplings. The measured effective leptonic weak mixing angle is sin 2 θ eff ℓ = 0.23147 ± 0.00044 ± 0.00005 ± 0.00023 , where the first uncertainty is statistical, the second arises from systematic uncertainties associated with the asymmetry measurement, and the third arises from uncertainties in the fit model used to extract sin 2 θ eff ℓ from the asymmetry measurement. This result is based on an arithmetic average of results using the CT18, MSHT20, and NNPDF31 parameterisations of the proton internal structure, and is consistent with previous measurements and with predictions from the global electroweak fit.",
        "authors": [
            "R. Aaij",
            "A. S. W. Abdelmotteleb",
            "C. Abellan Beteta",
            "F. Abudinén",
            "T. Ackernley",
            "A. A. Adefisoye",
            "B. Adeva",
            "M. Adinolfi",
            "P. Adlarson",
            "C. Agapopoulou",
            "C. A. Aidala",
            "Z. Ajaltouni",
            "S. Akar",
            "K. Akiba",
            "P. Albicocco",
            "J. Albrecht",
            "F. Alessio",
            "M. Alexander",
            "The LHCb collaboration"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157836",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Thermochromorph: Dynamic Relief Printing with Thermochromic Inks",
        "abstract": "Thermochromorph is a novel relief printing technique that produces multicolored images that transition into each other through changes in temperature. Our process utilizes two sets of CMYK thermochromic inks that exhibit complementary color-changing behaviors: one shifting from color to transparency, the other from transparency to color at the same activation temperature. We describe our printmaking workflow, provide an open-source software toolkit, showcase prints made with our system, and facilitate an artist workshop. By incorporating new materials and technology with the rich history of printmaking, our work extends the expressive capabilities of relief printing as the medium continues to evolve.",
        "authors": [
            "Ticha Sethapakdi",
            "Paris Myers",
            "Tianyu Yu",
            "Juliana Covarrubias",
            "Mackenzie Leake",
            "Stefanie Mueller"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|SIGGRAPH Asia 2024 Art Papers",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157795",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "KnitworkVR: Dual-reality Experience through Distributed Sensor-Actuator Networks in the Living Knitwork Pavilion",
        "abstract": "KnitworkVR integrates dual-reality and digital twin platforms to simulate the Living Knitwork Pavilion in a desert landscape, using real-time sensor data. The sensor network captures movements, interactions, and spatial positioning of occupants, linking electric field sensor data with VR positioning. This creates a sensor-driven immersive experience with dynamic lighting, live animations, and adaptive soundscapes, enabling telepresence and collaborative interaction in both digital and physical environments. This paper explores the functional textile design, sensing hardware, audiovisual system, and VR framework, highlighting the applications of immersive spaces with knitted electronic textiles and distributed physical-digital systems.",
        "authors": [
            "Irmandy Wicaksono",
            "Lancelot Blanchard",
            "Sam Chin",
            "Cristian Colon",
            "Joseph Paradiso"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|SIGGRAPH Asia 2024 Art Papers",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157796",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Manifold Sampling for Differentiable Uncertainty in Radiance Fields",
        "abstract": "SA Conference Papers ’24, December 03–06, 2024, Tokyo, Japan",
        "authors": [
            "Linjie Lyu",
            "Ayush Tewari",
            "Marc Habermann",
            "Shunsuke Saito",
            "Michael Zollh?fer",
            "Thomas Leimk?hler",
            "Christian Theobalt"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|SIGGRAPH Asia 2024 Conference Papers",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158127",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Large Étendue 3D Holographic Display with Content-adaptive Dynamic Fourier Modulation",
        "abstract": "Emerging holographic display technology offers unique capabilities for next-generation virtual reality systems. Current holographic near-eye displays, however, only support a small étendue, which results in a direct tradeoff between achievable field of view and eyebox size. Étendue expansion has recently been explored, but existing approaches are either fundamentally limited in the image quality that can be achieved or they require extremely high-speed spatial light modulators. We describe a new étendue expansion approach that combines multiple coherent sources with content-adaptive amplitude modulation of the hologram spectrum in the Fourier plane. To generate time-multiplexed phase and amplitude patterns for our spatial light modulators, we devise a pupil-aware gradient-descent-based computer-generated holography algorithm that is supervised by a large-baseline target light field. Compared with relevant baseline approaches, ours demonstrates significant improvements in image quality and étendue in simulation and with an experimental holographic display prototype.",
        "authors": [
            "Brian Chao",
            "Manu Gopakumar",
            "Suyeon Choi",
            "Jonghyun Kim",
            "Liang Shi",
            "Gordon Wetzstein"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|SIGGRAPH Asia 2024 Conference Papers",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158089",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Markov-Chain Monte Carlo Sampling of Visibility Boundaries for Differentiable Rendering",
        "abstract": "Physics-based differentiable rendering requires estimating boundary path integrals emerging from the shift of discontinuities (e.g., visibility boundaries). Previously, although the mathematical formulation of boundary path integrals has been established, efficient and robust estimation of these integrals has remained challenging. Specifically, state-of-the-art boundary sampling methods all rely on primary-sample-space guiding precomputed using sophisticated data structures—whose performance tends to degrade for finely tessellated geometries.\r\nIn this paper, we address this problem by introducing a new Markov-Chain-Monte-Carlo (MCMC) method. At the core of our technique is a local perturbation step capable of efficiently exploring highly fragmented primary sample spaces via specifically designed jumping rules. We compare the performance of our technique with several state-of-the-art baselines using synthetic differentiable-rendering and inverse-rendering experiments.",
        "authors": [
            "Peiyu Xu",
            "Sai Bangaru",
            "Tzu-Mao Li",
            "Shuang Zhao"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|SIGGRAPH Asia 2024 Conference Papers",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158126",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Sketching With Your Voice: \"Non-Phonorealistic\" Rendering of Sounds via Vocal Imitation",
        "abstract": "We present a method for automatically producing human-like vocal imitations of sounds: the equivalent of “sketching,” but for auditory rather than visual representation. Starting with a simulated model of the human vocal tract, we first try generating vocal imitations by tuning the model’s control parameters to make the synthesized vocalization match the target sound in terms of perceptually-salient auditory features. Then, to better match human intuitions, we apply a cognitive theory of communication to take into account how human speakers reason strategically about their listeners. Finally, we show through several experiments and user studies that when we add this type of communicative reasoning to our method, it aligns with human intuitions better than matching auditory features alone does. This observation has broad implications for the study of depiction in computer graphics.",
        "authors": [
            "Matthew Caren",
            "Kartik Chandra",
            "Joshua Tenenbaum",
            "Jonathan Ragan-Kelley",
            "Karima Ma"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|SIGGRAPH Asia 2024 Conference Papers",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158128",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A Listeria monocytogenes aptasensor on laser inscribed graphene for food safety monitoring in hydroponic water",
        "abstract": "Consumption of fresh produce, such as leafy greens, is often encouraged as part of a healthy diet. Hence, indoor facilities for hydroponic production of leafy greens are increasingly being established. However, fresh produce entails a higher risk of microbial foodborne illnesses than processed foods. Listeria monocytogenes is a major source of fresh produce contamination and is among the leading causes of severe foodborne illnesses in the United States, with a 16% mortality rate. Tools for rapid monitoring are needed for pathogens such as L. monocytogenes to prevent outbreaks. In this manuscript, we have demonstrated the feasibility of a multi-aptamer approach for development of label-free aptasensors targeting L. monocytogenes in irrigation water for lettuce hydroponic production. We use screening studies with surface plasmon resonance to rationally develop mixtures of relevant aptamers for targeting L. monocytogenes. Based on this screening, multiple aptamers targeting extracellular structures on intact L. monocytogenes were tethered to platinum-modified laser inscribed graphene electrodes. This is the first report of a L. monocytogenes biosensor based on laser inscribed graphene. We show that mixing multiple aptamers with varying affinity improves the diagnostic performance over one aptamer alone in complex sample matrices (lettuce hydroponic water). Multi-aptamer biosensors showed high accuracy for L. monocytogenes and were at least three times more selective than Escherichia coli (Crooks, K12, O157:H7) with an accuracy of 85%. The limit of detection (10 CFU/10 mL) is based on data which were significantly different after calibration toward L. monocytogenes or E. coli (Crooks) and validated against gold standard molecular analysis (polymerase chain reaction). Rapid screening of pathogens is a global need to meet food safety and water quality regulations. This study shows the importance of sensors targeting more than one bacterial surface structure in complex samples relevant to the food-water nexus.",
        "authors": [
            "Nicholas Cavallaro",
            "Geisianny Moreira",
            "Diana Vanegas",
            "Dong Xiang",
            "Shoumen P. A. Datta",
            "Carmen Gomes",
            "Eric S. McLamore"
        ],
        "journal_conference_name": "Discover Food",
        "publisher": "Springer International Publishing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157839",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Do We Learn From Each Other: Understanding the Human-AI Co-Learning Process Embedded in Human-AI Collaboration",
        "abstract": "Beyond collaborating in the AI-supported decision-making setting to achieve complementary performance, human and AI should learn from each other and internalize knowledge from their collaboration. This can enhance their individual performance when working independently after their collaboration. However, this expected dual-pathway co-learning process, including both “human learns from AI” and “AI learns from human”, does not occur spontaneously. Human-AI collaboration designs could have inconsistent and intertwined influences on the co-learning process. Based on the learning cycle theory, this study conducted three online, two-stage, and between-subject behavioral experiments to reveal how human and AI learn from each other. By developing a context where human and AI have comparable and moderate performance on emotion classification tasks, our study provides the first empirical evidence of an effective human-AI co-learning process within human-AI collaboration. However, the AI feedback and collaborative workflow design can lead to unequal and potentially negative impacts on both pathways of the co-learning process in groups with varying levels of cognitive reflection capability. These findings highlight three design principles to facilitate the co-learning process embedded in human-AI collaboration rather than naively deploying a complex AI system.",
        "authors": [
            "Jinwei Lu",
            "Yikuan Yan",
            "Keman Huang",
            "Ming Yin",
            "Fang Zhang"
        ],
        "journal_conference_name": "Group Decision and Negotiation",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/159157",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Opening the AI Black Box: Distilling Machine-Learned Algorithms into Code",
        "abstract": "Can we turn AI black boxes into code? Although this mission sounds extremely challenging, we show that it is not entirely impossible by presenting a proof-of-concept method, MIPS, that can synthesize programs based on the automated mechanistic interpretability of neural networks trained to perform the desired task, auto-distilling the learned algorithm into Python code. We test MIPS on a benchmark of 62 algorithmic tasks that can be learned by an RNN and find it highly complementary to GPT-4: MIPS solves 32 of them, including 13 that are not solved by GPT-4 (which also solves 30). MIPS uses an integer autoencoder to convert the RNN into a finite state machine, then applies Boolean or integer symbolic regression to capture the learned algorithm. As opposed to large language models, this program synthesis technique makes no use of (and is therefore not limited by) human training data such as algorithms and code from GitHub. We discuss opportunities and challenges for scaling up this approach to make machine-learned models more interpretable and trustworthy.",
        "authors": [
            "Eric J. Michaud",
            "Isaac Liao",
            "Vedang Lad",
            "Ziming Liu",
            "Anish Mudide",
            "Chloe Loughridge",
            "Zifan Carl Guo",
            "Tara Rezaei Kheirkhah",
            "Mateja Vukelić",
            "Max Tegmark"
        ],
        "journal_conference_name": "Entropy",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157939",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Beam heating explains critical current suppression measured during ion irradiation of REBCO tapes",
        "abstract": "Reports of critical current (Ic) suppression during cryogenic ion\r\nirradiation of REBCO tapes have raised concerns for the operational margins\r\nof fusion power plant (FPP) magnets. However, the data remain inconclusive\r\nregarding beam heating due to the difficulty of measuring local temperatures\r\nwith contact probes. This leaves a critical knowledge gap concerning the\r\nmechanism behind Ic suppression, and whether the so-called beam on effect is\r\nto be expected under neutron irradiation during FPP operation. In this paper,\r\nwe show that Ic suppression is independent of atomic displacement rate in the\r\nREBCO layer, the latter of which increases twelve-fold as we reduce the beam\r\nenergy from 2400 to 800 keV. At fixed power, we observe statistically identical\r\nsuppression with 150 keV protons, which do not have enough energy to reach\r\nthe REBCO layer, refuting hypotheses about beam on effects being caused by\r\nnuclear displacements or direct ion-Cooper pair interactions. These results show\r\nthat REBCO temperature rise alone can explain Ic suppression, leaving little to no\r\nmargin for alternative mechanisms. With this insight, we developed a method to\r\nmeasure beam spot temperature that does not depend on the specific installation\r\nof our temperature sensor. With this new method, we measured the temperature\r\ngradient across the tape during irradiation and found that thermal resistance at\r\nthe tape/target interface is the controlling variable in Ic suppression. As such,\r\naccelerator-based facilities aiming to reproduce the operation of REBCO magnets\r\nin a nuclear fusion environment should find strategies to minimize interface\r\nthermal resistance. Most importantly, we find that the dose rates expected\r\nin a FPP will not change Ic due to ballistic radiation damage or ion-Cooper\r\npair interactions, allowing us to safely ignore these effects when designing FPP\r\nmagnets.",
        "authors": [
            "Alexis Devitre",
            "David Fischer",
            "N. Riva",
            "M. Rae",
            "Lauryn Kortman",
            "Kevin Woller",
            "Zoe Fisher",
            "Michael Short",
            "Dennis Whyte",
            "Zachary Hartwig"
        ],
        "journal_conference_name": "Superconductor Science and Technology",
        "publisher": "IOP Publishing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157858",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Materials approaches for next-generation encapsulated cell therapies",
        "abstract": "Transplanted cells can act as living drug factories capable of secreting therapeutic proteins in vivo, with applications in the treatment of Type 1 diabetes (T1D), blood borne disease, vision disorders, and degenerative neural disease, potentially representing functional cures for chronic conditions. However, attack from the host immune system represents a major challenge, requiring chronic immunosuppression to enable long-lived cell transplantation in vivo. Encapsulating cells in engineered biomaterials capable of excluding components of the host immune system while allowing for the transport of therapeutic proteins, oxygen, nutrients, metabolites, and waste products represents a potential solution. However, the foreign-body response can lead to isolation from native vasculature and hypoxia leading to cell death. In this prospective article, we highlight materials-based solutions to three important challenges in the field: (i) improving biocompatibility and reducing fibrosis; (ii) enhancing transport of secreted protein drugs and key nutrients and oxygen via engineered, semipermeable membranes; and (iii) improving oxygenation. These efforts draw on several disciplines in materials’ research, including polymer science, surfaces, membranes, biomaterials’ microfabrication, and flexible electronics. If successful, these efforts could lead to new therapies for chronic disease and are a rich space for both fundamental materials’ discovery and applied translational science.",
        "authors": [
            "Siddharth R. Krishnan",
            "Robert Langer",
            "Daniel G. Anderson"
        ],
        "journal_conference_name": "MRS Communications",
        "publisher": "Springer International Publishing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157838",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Graphical vs. Deep Generative Models: Measuring the Impact of Differentially Private Mechanisms and Budgets on Utility",
        "abstract": "Generative models trained with Differential Privacy (DP) can produce synthetic data while reducing privacy risks. However, navigating their privacy-utility tradeoffs makes finding the best models for specific settings/tasks challenging. This paper bridges this gap by profiling how DP generative models for tabular data distribute privacy budgets across rows and columns, which is one of the primary sources of utility degradation. We compare graphical and deep generative models, focusing on the key factors contributing to how privacy budgets are spent, i.e., underlying modeling techniques, DP mechanisms, and data dimensionality.\r\nThrough our measurement study, we shed light on the characteristics that make different models suitable for various settings and tasks. For instance, we find that graphical models distribute privacy budgets horizontally and thus cannot handle relatively wide datasets for a fixed training time; also, the performance on the task they were optimized for monotonically increases with more data but could also overfit. Deep generative models spend their budgets per iteration, so their behavior is less predictable with varying dataset dimensions, but are more flexible as they could perform better if trained on more features. Moreover, low levels of privacy (ε≥100) could help some models generalize, achieving better results than without applying DP. We believe our work will aid the deployment of DP synthetic data techniques by navigating through the best candidate models vis-à-vis the dataset features, desired privacy levels, and downstream tasks.",
        "authors": [
            "Georgi Ganev",
            "Kai Xu",
            "Emiliano De Cristofaro"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158085",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Manipulative Interference Attacks",
        "abstract": "A μ-kernel is an operating system (OS) paradigm that facilitates a strong cybersecurity posture for embedded systems. Unlike a monolithic OS such as Linux, a μ-kernel reduces overall system privilege by deploying most OS functionality within isolated, userspace protection domains. Moreover, a μ-kernel ensures confidentiality and integrity between protection domains (i.e., spatial isolation), and offers timing predictability for real-time tasks in mixed-criticality systems (i.e., temporal isolation). One popular μ-kernel is seL4 which offers extensive formal guarantees of implementation correctness and flexible temporal budgeting mechanisms.\r\nHowever, we show that an untrusted protection domain on a μ-kernel can abuse service requests to other protection domains in order to corrode system availability. We generalize this denial-of-service (DoS) attack strategy as Manipulative Interference Attacks (MIAs) and introduce techniques to efficiently identify instances of MIAs within a configured system. Specifically, we propose a novel hybrid approach that first leverages static analysis to identify software components with influenceable execution times, and second, uses an automatically generated model-based analysis to determine which compromised protection domains can manipulate the influenceable components and trigger MIAs. We investigate the risk of MIAs in several representative system examples including the seL4 Microkit, as well as a case study of seL4 software artifacts from the DARPA Cyber Assured Systems Engineering (CASE) program. In particular, we demonstrate that our analysis is efficient enough to discover practical instances of MIAs in real-world systems.",
        "authors": [
            "Samuel Mergendahl",
            "Stephen Fickas",
            "Boyana Norris",
            "Richard Skowyra"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158086",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Formal Privacy Proof of Data Encoding: The Possibility and Impossibility of Learnable Encryption",
        "abstract": "We initiate a formal study on the concept of learnable obfuscation and aim to answer the following question: is there a type of data encoding that maintains the \"learnability\" of encoded samples, thereby enabling direct model training on transformed data, while ensuring the privacy of both plaintext and the secret encoding function? This long-standing open problem has prompted many efforts to design such an encryption function, for example, NeuraCrypt and TransNet. Nonetheless, all existing constructions are heuristic without formal privacy guarantees, and many successful reconstruction attacks are known on these constructions assuming an adversary with substantial prior knowledge.\r\nWe present both generic possibility and impossibility results pertaining to learnable obfuscation. On one hand, we demonstrate that any non-trivial, property-preserving transformation which enables effectively learning over encoded samples cannot offer cryptographic computational security in the worst case. On the other hand, from the lens of information-theoretical security, we devise a series of new tools to produce provable and useful privacy guarantees from a set of heuristic obfuscation methods, including matrix masking, data mixing and permutation, through noise perturbation. Under the framework of PAC Privacy, we show how to quantify the leakage from the learnable obfuscation built upon obfuscation and perturbation methods against adversarial inference. Significantly sharpened utility-privacy tradeoffs are achieved compared to state-of-the-art accounting methods when measuring privacy against data reconstruction and membership inference attacks.",
        "authors": [
            "Hanshen Xiao",
            "G. Edward Suh",
            "Srinivas Devadas"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158081",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploiting Temporal Vulnerabilities for Unauthorized Access in Intent-based Networking",
        "abstract": "Intent-based networking (IBN) enables network administrators to express high-level goals and network policies without needing to specify low-level forwarding configurations, topologies, or protocols. Administrators can define intents that capture the overall behavior they want from the network, and an IBN controller compiles such intents into low-level configurations that get installed in the network and implement the desired behavior.\r\nWe discovered that current IBN specifications and implementations do not specify that flow rule installation orderings should be enforced, which leads to temporal vulnerabilities where, for a limited time, attackers can exploit indeterminate connectivity behavior to gain unauthorized network access.\r\nIn this paper, we analyze the causes of such temporal vulnerabilities and their security impacts with a representative case study via the ONOS IBN implementation. We devise the Phantom Link attack and demonstrate a working exploit to highlight the security impacts. To defend against such attacks, we propose Spotlight, a detection method that can alert a system administrator of risky intent updates prone to exploitable temporal vulnerabilities. Spotlight is effective in identifying risky updates using realistic network topologies and policies. We show that Spotlight can detect risky updates in a mean time of 0.65 seconds for topologies of over 1,300 nodes.",
        "authors": [
            "Ben Weintraub",
            "Jiwon Kim",
            "Ran Tao",
            "Cristina Nita-Rotaru",
            "Hamed Okhravi",
            "Dave (Jing) Tian",
            "Benjamin Ujcich"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158083",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Secure Sorting and Selection via Function Secret Sharing",
        "abstract": "We revisit the problem of concretely efficient secure computation of sorting and selection (e.g., maximum, median, or top-k) on secret-shared data, focusing on the case of security against a single semi-honest party. Previous solutions either have a high communication overhead or many rounds of interaction, even when allowing input-independent preprocessing.\r\nWe propose a suite of 2-party and 3-party offline-online protocols that exploit the efficient aggregation feature of function secret sharing to minimize the online communication and rounds. In particular, most of our protocols are optimal in terms of both online communication and online rounds up to small constant factors.\r\nWe compare the performance of our protocols with prior works for different input parameters (number of items, bit length of items, batch size) and system parameters (CPU cores, network) and obtain up to 14x improvement in online run time for sorting and selection under some settings.",
        "authors": [
            "Amit Agarwal",
            "Elette Boyle",
            "Nishanth Chandran",
            "Niv Gilboa",
            "Divya Gupta",
            "Yuval Ishai",
            "Mahimna Kelkar",
            "Yiping Ma"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158087",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "High-Throughput Three-Party DPFs with Applications to ORAM and Digital Currencies",
        "abstract": "specific and general secure computation. While two-party DPF constructions are readily available for those applications with satisfiable performance, the three-party ones are left behind in both security and efficiency. In this paper we close this gap and propose the first three-party DPF construction that matches the state-of-the-art two-party DPF on all metrics. Namely, it is secure against a malicious adversary corrupting both the dealer and one out of the three evaluators, its function's shares are of the same size and evaluation takes the same time as in the best two-party DPF. Compared to the state-of-the-art three-party DPF, our construction enjoys 40-120× smaller function's share size and shorter evaluation time, for function domains of 216 -240, respectively.\r\nApart from DPFs as a stand-alone tool, our construction finds immediate applications to private information retrieval (PIR), writing (PIW) and oblivious RAM (ORAM). To further showcase its applicability, we design and implement an ORAM with access policy, an extension to ORAMs where a policy is being checked before accessing the underlying database. The policy we plug-in is the one suitable for account-based digital currencies, and in particular to central bank digital currencies (CBDCs). Our protocol offers the first design and implementation of a large scale privacy-preserving account-based digital currency. While previous works supported anonymity sets of 64-256 clients and less than 10 transactions per second (tps), our protocol supports anonymity sets in the millions, performing {500,200,58} tps for anonymity sets of {216, 218, 220}, respectively.\r\nToward that application, we introduce a new primitive called updatable DPF, which enables a direct computation of a dot product between a DPF and a vector; we believe that updatable DPF and the new dot-product protocol will find interest in other applications.",
        "authors": [
            "Guy Zyskind",
            "Avishay Yanai",
            "Alex Pentland"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158082",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Specification and Verification of Strong Timing Isolation of Hardware Enclaves",
        "abstract": "The process isolation enforceable by commodity hardware and operating systems is too weak to protect secrets from malicious code running on the same machine: attacks exploit timing side channels derived from contention on shared microarchitectural resources to extract secrets. With appropriate hardware support, however, we can construct isolated enclaves and safeguard independent processes from interference through timing side channels, a step towards confidentiality and integrity guarantees.\r\nIn this paper, we describe our work on formally specifying and verifying that a synthesizable hardware architecture implements strong timing isolation for enclaves. We reason about the cycle-accurate semantics of circuits with respect to a trustworthy formulation of strong isolation based on \"air-gapped machines\" and develop a modular proof strategy that sidesteps the need to prove functional correctness of processors. We apply our method on a synthesizable, multicore, pipelined RISC-V design formalized in Coq.",
        "authors": [
            "Stella Lau",
            "Thomas Bourgeat",
            "Cl?ment Pit-Claudel",
            "Adam Chlipala"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158084",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Enabling Perspective-Aware Ai with Contextual Scene Graph Generation",
        "abstract": "This paper advances contextual image understanding within perspective-aware Ai (PAi), an emerging paradigm in human–computer interaction that enables users to perceive and interact through each other’s perspectives. While PAi relies on multimodal data—such as text, audio, and images—challenges in data collection, alignment, and privacy have led us to focus on enabling the contextual understanding of images. To achieve this, we developed perspective-aware scene graph generation with LLM post-processing (PASGG-LM). This framework extends traditional scene graph generation (SGG) by incorporating large language models (LLMs) to enhance contextual understanding. PASGG-LM integrates classical scene graph outputs with LLM post-processing to infer richer contextual information, such as emotions, activities, and social contexts. To test PASGG-LM, we introduce the context-aware scene graph generation task, where the goal is to generate a context-aware situation graph describing the input image. We evaluated PASGG-LM pipelines using state-of-the-art SGG models, including Motifs, Motifs-TDE, and RelTR, and showed that fine-tuning LLMs, particularly GPT-4o-mini and Llama-3.1-8B, improves performance in terms of R@K, mR@K, and mAP. Our method is capable of generating scene graphs that capture complex contextual aspects, advancing human–machine interaction by enhancing the representation of diverse perspectives. Future directions include refining contextual scene graph models and expanding multi-modal data integration for PAi applications in domains such as healthcare, education, and social robotics.",
        "authors": [
            "Daniel Platnick",
            "Marjan Alirezaie",
            "Hossein Rahnama"
        ],
        "journal_conference_name": "Information",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157953",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Gênero e Feminismos no Ensino de Relações Internacionais no Brasil",
        "abstract": "",
        "authors": [
            "Alessandra Jungs de Almeida"
        ],
        "journal_conference_name": "Revista Brasileira de Políticas Públicas e Internacionais",
        "publisher": "No Publisher",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157847",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Expanding the landscape of antibody discovery",
        "abstract": "Library:library screening technologies hold substantial promise for paired antibody:antigen discovery, but challenges have persisted. In this issue of Cell Reports Methods, Wagner et al. introduce a method that combines antibody-ribosome-mRNA complexes, antigen cell surface display, and single-cell RNA sequencing to successfully screen diverse antibody gene libraries against a library of viral receptor proteins.",
        "authors": [
            "Shelbe Johnson",
            "Brandon J DeKosky"
        ],
        "journal_conference_name": "Cell Reports Methods",
        "publisher": "Elsevier BV",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158238",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Data for \"Variations on five-dimensional sphere packings\"",
        "abstract": "This data set includes all the code and data from the paper \"Variations on five-dimensional sphere packings\" by Cohn and Rajagopal.",
        "authors": [
            "Henry Cohn",
            "Isaac Rajagopal"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157699",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "From My Vantage Point: Exploring The Effect of First-Person and Third-Person Perspectives on Social Acceptance in VR Roleplaying Games",
        "abstract": "Virtual reality (VR) roleplaying games designed to promote perspective taking typically involve players assuming the perspective of others from different backgrounds and experiencing a simulated scenario from their everyday life, with the goal of facilitating and enhancing empathy and social acceptance toward marginalized groups. One key question pertains to the extent to which players’ perspective during VR roleplaying games affects their social acceptance of the other. To address this question, we examined the effect of first-person vs. third-person perspective on presence, co-presence, and social acceptance during a VR roleplaying game. Two groups of participants played the same VR roleplaying game from either a first-person perspective or a third-person perspective. Results showed that compared to third-person perspective, first-person perspective led to greater co-presence during the game and engendered higher levels of social acceptance toward the character whose role participants played. These results highlight the importance of using first-person perspective in VR roleplaying games focusing on facilitating and enhancing social acceptance.",
        "authors": [
            "Caglar Yildirim",
            "Sercan Sengun",
            "Eyup Kucuk",
            "Mehmet Akhoroz",
            "D. Fox Harrell"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|International Conference on Mobile and Ubiquitous Multimedia",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158130",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Retrieval of refractivity fields from GNSS tropospheric delays: theoretical and data-based evaluation of collocation methods and comparisons with GNSS tomography",
        "abstract": "This paper focuses on the retrieval of refractivity fields from GNSS measurements by means of least-squares collocation. Collocation adjustment estimates parameters that relate delays and refractivity without relying on a grid. It contains functional and stochastic models that define the characteristics of the retrieved refractivity fields. This work aims at emphasizing the capabilities and limitations of the collocation method in modeling refractivity and to present it as a valuable alternative to GNSS tomography. Initially, we analyze the stochastic models in collocation and compare the theoretical errors of collocation with those of tomography. We emphasize the low variability of collocation formal variances/covariances compared to tomography and its lower dependence on a-priori fields. Then, based on real and simulated data, we investigate the importance of station resolution and station heights for collocation. Increasing the network resolution, for example, from 10 to 2 km, results in improved a-posteriori statistics, including a 10% reduction in the error statistic for the retrieved refractivity up to 6 km. In addition, using additional stations at higher altitudes has an impact on the retrieved refractivity fields of about 1 ppm in terms of standard deviation up to 6 km, and a bias reduction of more than 3 ppm up to 3 km. Furthermore, we compare refractivity fields retrieved through tomography and collocation, where data of the COSMO weather model are utilized in a closed-loop validation mode to simulate tropospheric delays and validate the retrieved profiles. While tomography estimates are less biased, collocation captures relative changes in refractivity more effectively among the voxels within one height level. Finally, we apply tomography and collocation to test their capabilities to detect an approaching weather front. Both methods can sense the weather front, but their atmospheric structures appear more similar when the GNSS network has a well-distributed height coverage.",
        "authors": [
            "Endrit Shehaj",
            "Alain Geiger",
            "Markus Rothacher",
            "Gregor Moeller"
        ],
        "journal_conference_name": "Journal of Geodesy",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157746",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Robust Reinforcement Learning Strategies with Evolving Curriculum for Efficient Bus Operations in Smart Cities",
        "abstract": "Public transit systems are critical to the quality of urban life, and enhancing their efficiency is essential for building cost-effective and sustainable smart cities. Historically, researchers sought reinforcement learning (RL) applications to mitigate bus bunching issues with holding strategies. Nonetheless, these attempts often led to oversimplifications and misalignment with the goal of reducing the total time passengers spent in the system, resulting in less robust or non-optimal solutions. In this study, we introduce a novel setting where each bus, supervised by an RL agent, can appropriately form aggregated policies from three strategies (holding, skipping station, and turning around to serve the opposite direction). It&rsquo;s difficult to learn them all together, due to learning complexity, we employ domain knowledge and develop a gradually expanding action space curriculum, enabling agents to learn these strategies incrementally. We incorporate Long Short-Term Memory (LSTM) in our model considering the temporal interrelation among these actions. To address the inherent uncertainties of real-world traffic systems, we impose Domain Randomization (DR) on variables such as passenger demand and bus schedules. We conduct extensive numerical experiments with the integration of synthetic and real-world data to evaluate our model. Our methodology proves effective, enhancing bus schedule reliability and reducing total passenger waiting time by over 15%, thereby improving bus operation efficiency and smoothering operations of buses that align with sustainable goals. This work highlights the potential of robust RL combined with curriculum learning for optimizing public transport in smart cities, offering a scalable solution for real-world multi-agent systems.",
        "authors": [
            "Yuhan Tang",
            "Ao Qu",
            "Xuan Jiang",
            "Baichuan Mo",
            "Shangqing Cao",
            "Joseph Rodriguez",
            "Haris N Koutsopoulos",
            "Cathy Wu",
            "Jinhua Zhao"
        ],
        "journal_conference_name": "Smart Cities",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157936",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Structural molecular modeling of bacterial integral membrane protein enzymes and their AlphaFold2 predicted water-soluble QTY variants",
        "abstract": "Context Beta-barrel enzymes are an important area of study in the field of structural biology. These proteins serve crucial roles, acting as porins, transporters, enzymes, virulence factors, and receptors. Recent research has unveiled a novel role for beta-barrel enzymes in the bacterial integral membrane as sentinels. They remain inactive when the integral membrane is intact but activate to carry out enzymatic catalysis in response to host immune responses and antibiotics that breach this barrier. Understanding their structure and function is pivotal in grasping their sentinel role in the bacterial integral membrane. Here we present our structural molecular modeling analyses on four bacterial integral membrane beta-barrel enzymes: (a) OMPLA, (b) OmpT, (c) PagP from E. coli, and (d) PagL from Pseudomonas aeruginosa. We superposed the structures of native beta-barrel integral membrane enzymes with their AlphaFold2-predicted QTY variant structures that showed remarkable similarity despite the replacement of at least 22.95% amino acids in transmembrane regions, the superposed structures displayed notable structural similarity, indicated by RMSD values ranging from 0.181 Å to 0.286 Å. We also analyze the hydrophobicity patches and the enhanced hydrophilic surfaces. Our research provide insights into the structural similarity of hydrophobic and hydrophilic beta-barrel enzymes, validating the utility of the QTY code for investigating beta-barrel membrane enzymes. Our results not only demonstrate that the QTY code serves as a straightforward tool for designing water-soluble membrane proteins across various biological contexts, but it may also stimulate experiments to validate our molecular modeling studies. Methods All the QTY variant beta-barrel enzyme structure prediction was performed using the AlphaFold2 program ( https://github.com/sokrypton/ColabFold ) following the provided instructions. Computations were carried out on 11th Gen Intel Core i5-11300H processor with 16 GB RAM and Iris Xe Graphics, 512 GB NVMe SSD. The structures are publicly available on the AlphaFold2 database ( https://alphafold.ebi.ac.uk ) at the European Bioinformatics Institute (EBI). A custom Python script was used to extract the relevant information from the UniProt database. To predict the structures of the QTY variants, AlphaFold2 was utilized. The native sequences for these enzymes were retrieved from UniProt https://www.uniprot.org , and AlphaFold2 structural predictions were performed using the open-source implementation at https://github.com/sokrypton/ColabFold . The predicted variant structures were then superposed with the native structures using PyMOL https://pymol.org/2/ for structural analysis and comparison. This work leverages public databases PDB, UniProt and open-source software AlphaFold2 and PyMOL to computationally model and analyze QTY variant integral membrane beta-barrel enzyme structures. Graphical abstract",
        "authors": [
            "Akash Sajeev-Sheeja",
            "Shuguang Zhang"
        ],
        "journal_conference_name": "Journal of Proteins and Proteomics",
        "publisher": "Springer Nature Singapore",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157745",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The price elasticity of natural gas demand of small consumers in Germany during the energy crisis 2022",
        "abstract": "Understanding how consumers respond to turbulent market conditions is crucial for planning security of natural gas supply. This paper estimates the price elasticity of demand of small consumers in Germany in the period with both high price fluctuations and a fear of natural gas shortage in the aftermath of the Russian invasion of Ukraine. Using granular data between 2018 and 2023, we estimate an Auto Regressive Distributed Lag (ARDL) time series cointegrating model. We find a price elasticity of demand for natural gas of -0.01 for wholesale prices and -0.04 for retail prices. Additionally, we quantify the effects of weather conditions and public awareness on the energy crisis. The results suggest i) that extreme price changes would be required to trigger short-term demand adjustments, and ii) demonstrate the importance of public attention on the crisis situation.",
        "authors": [
            "David Jamissen",
            "Johanne Vatne",
            "Franziska Holz",
            "Anne Neumann"
        ],
        "journal_conference_name": "Energy Efficiency",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157701",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Multifunctional lightweight autonomous vehicles: an agent-based study",
        "abstract": "In mobility-on-demand services, the number of vehicles needed is often determined by peak demand during rush hours, leading to prolonged vehicle idle times during off-peak periods. This surplus capacity presents an opportunity for vehicles to perform additional tasks, potentially enhancing system efficiency and reducing the overall number of vehicles needed in cities. Leveraging agent-based modeling, we evaluate the effectiveness of vehicles catering to on-demand rides and food deliveries in two real-life scenarios: Cambridge, MA, USA, and San Sebastian, Gipuzkoa, Spain. The results show that multifunctional behavior can lead to reduced fleet sizes, with context-specific exceptions. Additionally, a strategic dispatching algorithm is introduced that demonstrates reductions in wait times and overall distances traveled. This research contributes to the understanding of the performance of multifunctional fleets in diverse urban contexts, informing the development of sustainable and resource-efficient mobility systems.",
        "authors": [
            "Naroa Coretti Sanchez",
            "Kent Larson"
        ],
        "journal_conference_name": "Transportation",
        "publisher": "Springer US",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157703",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exact algorithms for continuous pricing with advanced discrete choice demand models",
        "abstract": "We present a spatial Branch and Bound and spatial Branch and Benders Decomposition approach together with the Breakpoint Exact Algorithm (BEA) to tackle the uncapacitated choice-based pricing problem (CPP) where demand is captured by a discrete choice model (DCM) based on the random utility principle. We leverage problem characteristics to reformulate the state-of-the-art simulation-based formulation of the CPP as a mixed-integer linear program (MILP) into a non-convex quadratically constrained quadratic program (QCQP), and then into a non-convex QCQP with linear objective (QCQP-L). We solve this reformulation with an efficient spatial Branch and Bound procedure utilizing the McCormick envelope for relaxations, which are then solved using Benders decomposition. We further exploit utility breakpoints to develop the BEA, which scales polynomially in the number of customers and draws, providing a fast option for low numbers of prices. Our methods are evaluated against solving the MILP, QCQP, or QCQP-L with GUROBI on a mixed logit (ML) parking space operator case study. We outspeed the MILP by several orders of magnitude when optimizing one or two prices and reduce computational time drastically for larger numbers of prices. When comparing to algorithms tailored for the CPP with ML demand specifically, our approaches significantly outperform the state of the art. Our methodology suits all choice-based optimization problems with linear-in-price utilities, given any DCM.",
        "authors": [
            "Tom Haering",
            "Robin Legault",
            "Fabian Torres",
            "Ivana Ljubić",
            "Michel Bierlaire"
        ],
        "journal_conference_name": "OR Spectrum",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157700",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "When Cities Go Nuclear: Exploring the Applications of Nuclear Batteries Toward Energy Transformation",
        "abstract": "Global society faces the pressing question of how to eliminate reliance on fossil fuels while meeting increasing energy demand. In comparison to solar and wind energy, nuclear power has been largely ignored in urban studies research. However, nuclear energy has recently regained attention through the emergence of Small Modular Reactors (SMRs), and as the stakes of decarbonization become increasingly essential. To evaluate situations in which SMRs bring value to urban energy mixes, this paper focuses on Nuclear Batteries (NBs), a specific class of SMRs, that can fit in standard shipping containers. First, we outline an evaluation framework for the use and application of NBs; second, we present use cases for NBs in real-world situations, from disaster relief to grid reinforcement; and third, we discuss the social challenges around this technology.",
        "authors": [
            "Sanjana Paul",
            "Mikita Klimenka",
            "Fabio Duarte",
            "Carmen Crawford",
            "Claire Gorman",
            "Carlo Ratti",
            "Jacopo Buongiorno"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157935",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Acceleration by stepsize hedging: Silver Stepsize Schedule for smooth convex optimization",
        "abstract": "We provide a concise, self-contained proof that the Silver Stepsize Schedule proposed in our companion paper directly applies to smooth (non-strongly) convex optimization. Specifically, we show that with these stepsizes, gradient descent computes an ε -minimizer in O ( ε - log ρ 2 ) = O ( ε - 0.7864 ) iterations, where ρ = 1 + 2 is the silver ratio. This is intermediate between the textbook unaccelerated rate O ( ε - 1 ) and the accelerated rate O ( ε - 1 / 2 ) due to Nesterov in 1983. The Silver Stepsize Schedule is a simple explicit fractal: the i-th stepsize is 1 + ρ ν ( i ) - 1 where ν ( i ) is the 2-adic valuation of i. The design and analysis are conceptually identical to the strongly convex setting in our companion paper, but simplify remarkably in this specific setting.",
        "authors": [
            "Jason M. Altschuler",
            "Pablo A. Parrilo"
        ],
        "journal_conference_name": "Mathematical Programming",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157702",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "“Better Networks” Project is Working to Improve Cybersecurity",
        "abstract": "In collaboration with the MIT Sociotechnical Systems Research Center and the Air Force, the Lincoln Laboratory Supercomputing Center is working to develop better sensors using Laboratory developed Dynamic Distributed Dimensional Data Model (D4M) technology and artificial intelligence algorithms to support defensive cyber operations.",
        "authors": [
            "Kailen Comeau"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "No Publisher",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157666",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Lp -Hardy identities and inequalities with respect to the distance and mean distance to the boundary",
        "abstract": "Firstly, this paper establishes useful forms of the remainder term of Hardy-type inequalities on general domains where the weights are functions of the distance to the boundary. For weakly mean convex domains we use the resulting identities to establish nonexistence of extremizers for and improve known sharp Hardy inequalities. Secondly, we establish geometrically interesting remainders for the Davies-Hardy-Tidblom inequalities for the mean distance function, as well as generalize and improve several Hardy type inequalities in the spirit of Brezis and Marcus and spectral estimates of Davies. Lastly, we apply our results to obtain Sobolev inequalities for non-regular Riemannian metrics on geometric exterior domains.",
        "authors": [
            "Joshua Flynn",
            "Nguyen Lam",
            "Guozhen Lu"
        ],
        "journal_conference_name": "Calculus of Variations and Partial Differential Equations",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/159155",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "MindScape Study: Integrating LLM and Behavioral Sensing for Personalized AI-Driven Journaling Experiences",
        "abstract": "Mental health concerns are prevalent among college students, highlighting the need for effective interventions that promote self-awareness and holistic well-being. MindScape pioneers a novel approach to AI-powered journaling by integrating passively collected behavioral patterns such as conversational engagement, sleep, and location with Large Language Models (LLMs). This integration creates a highly personalized and context-aware journaling experience, enhancing self-awareness and well-being by embedding behavioral intelligence into AI. We present an 8-week exploratory study with 20 college students, demonstrating the MindScape app's efficacy in enhancing positive affect (7%), reducing negative affect (11%), loneliness (6%), and anxiety and depression, with a significant week-over-week decrease in PHQ-4 scores (-0.25 coefficient), alongside improvements in mindfulness (7%) and self-reflection (6%). The study highlights the advantages of contextual AI journaling, with participants particularly appreciating the tailored prompts and insights provided by the MindScape app. Our analysis also includes a comparison of responses to AI-driven contextual versus generic prompts, participant feedback insights, and proposed strategies for leveraging contextual AI journaling to improve well-being on college campuses. By showcasing the potential of contextual AI journaling to support mental health, we provide a foundation for further investigation into the effects of contextual AI journaling on mental health and well-being.",
        "authors": [
            "Subigya Nepal",
            "Arvind Pillai",
            "William Campbell",
            "Talie Massachi",
            "Michael Heinz",
            "Ashmita Kunwar",
            "Eunsol Soul Choi",
            "Xuhai \"Orson\" Xu",
            "Joanna Kuc",
            "Jeremy Huckins",
            "Jason Holden",
            "Sarah M. Preum",
            "Colin Depp",
            "Nicholas Jacobson",
            "Mary Czerwinski",
            "Eric Granholm",
            "Andrew Campbell"
        ],
        "journal_conference_name": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157901",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Beyond Detection: Towards Actionable Sensing Research in Clinical Mental Healthcare",
        "abstract": "Researchers in ubiquitous computing have long promised that passive sensing will revolutionize mental health measurement by detecting individuals in a population experiencing a mental health disorder or specific symptoms. Recent work suggests that detection tools do not generalize well when trained and tested in more heterogeneous samples. In this work, we contribute a narrative review and findings from two studies with 41 mental health clinicians to understand these generalization challenges. Our findings motivate research on actionable sensing, as an alternative to detection research, studying how passive sensing can be used alongside traditional mental health measures to support actions in clinical care. Specifically, we identify how passive sensing can support clinical actions by revealing patients' presenting problems for treatment and identifying targets for behavior change and symptom reduction, but passive data needs to be contextualized with patients to be appropriately interpreted and used in care. We conclude by suggesting research at the intersection of actionable sensing and mental healthcare, to align technical research in ubiquitous computing with clinical actions and needs.",
        "authors": [
            "Daniel Adler",
            "Yuewen Yang",
            "Thalia Viranda",
            "Xuhai Xu",
            "David Mohr",
            "Anna Van Meter",
            "Julia Tartaglia",
            "Nicholas Jacobson",
            "Fei Wang",
            "Deborah Estrin",
            "Tanzeem Choudhury"
        ],
        "journal_conference_name": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157900",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Sensor2Text: Enabling Natural Language Interactions for Daily Activity Tracking Using Wearable Sensors",
        "abstract": "Visual Question-Answering, a technology that generates textual responses from an image and natural language question, has progressed significantly. Notably, it can aid in tracking and inquiring about daily activities, crucial in healthcare monitoring, especially for elderly patients or those with memory disabilities. However, video poses privacy concerns and has a limited field of view. This paper presents Sensor2Text, a model proficient in tracking daily activities and engaging in conversations using wearable sensors. The approach outlined here tackles several challenges, including low information density in wearable sensor data, insufficiency of single wearable sensors in human activities recognition, and model's limited capacity for Question-Answering and interactive conversations. To resolve these obstacles, transfer learning and student-teacher networks are utilized to leverage knowledge from visual-language models. Additionally, an encoder-decoder neural network model is devised to jointly process language and sensor data for conversational purposes. Furthermore, Large Language Models are also utilized to enable interactive capabilities. The model showcases the ability to identify human activities and engage in Q&A dialogues using various wearable sensor modalities. It performs comparably to or better than existing visual-language models in both captioning and conversational tasks. To our knowledge, this represents the first model capable of conversing about wearable sensor data, offering an innovative approach to daily activity tracking that addresses privacy and field-of-view limitations associated with current vision-based solutions.",
        "authors": [
            "Wenqiang Chen",
            "Jiaxuan Cheng",
            "Leyao Wang",
            "Wei Zhao",
            "Wojciech Matusik"
        ],
        "journal_conference_name": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157899",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "EyeTrAES: Fine-grained, Low-Latency Eye Tracking via Adaptive Event Slicing",
        "abstract": "Eye-tracking technology has gained significant attention in recent years due to its wide range of applications in human-computer interaction, virtual and augmented reality, and wearable health. Traditional RGB camera-based eye-tracking systems often struggle with poor temporal resolution and computational constraints, limiting their effectiveness in capturing rapid eye movements. To address these limitations, we propose EyeTrAES, a novel approach using neuromorphic event cameras for high-fidelity tracking of natural pupillary movement that shows significant kinematic variance. One of EyeTrAES's highlights is the use of a novel adaptive windowing/slicing algorithm that ensures just the right amount of descriptive asynchronous event data accumulation within an event frame, across a wide range of eye movement patterns. EyeTrAES then applies lightweight image processing functions over accumulated event frames from just a single eye to perform pupil segmentation and tracking (as opposed to gaze-based techniques that require simultaneous tracking of both eyes). We show that these two techniques boost pupil tracking fidelity by 6+%, achieving IoU~=92%, while incurring at least 3x lower latency than competing pure event-based eye tracking alternatives. We additionally demonstrate that the microscopic pupillary motion captured by EyeTrAES exhibits distinctive variations across individuals and can thus serve as a biometric fingerprint. For robust user authentication, we train a lightweight per-user Random Forest classifier using a novel feature vector of short-term pupillary kinematics, comprising a sliding window of pupil (location, velocity, acceleration) triples. Experimental studies with two different datasets (capturing eye movement across a range of environmental contexts) demonstrate that the EyeTrAES-based authentication technique can simultaneously achieve high authentication accuracy (~=0.82) and low processing latency (~=12ms), and significantly outperform multiple state-of-the-art competitive baselines.",
        "authors": [
            "Argha Sen",
            "Nuwan Bandara",
            "Ila Gokarn",
            "Thivya Kandappu",
            "Archan Misra"
        ],
        "journal_conference_name": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",
        "publisher": "ACM",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157898",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Probing the nature of the χc1(3872) state using radiative decays",
        "abstract": "The radiative decays χc1(3872) → ψ(2S) γ and χc1(3872) → J/ψγ are used to probe the nature of the χc1(3872) state using proton-proton collision data collected with the LHCb detector, corresponding to an integrated luminosity of 9 fb−1. Using the B+ → χc1(3872)K+ decay, the χc1(3872) → ψ(2S) γ process is observed for the first time and the ratio of its partial width to that of the χc1(3872) → J/ψγ decay is measured to be Γ χ c 1 3872 → ψ 2 S γ Γ χ c 1 3872 → J / ψ γ = 1.67 ± 0.21 ± 0.12 ± 0.04 , where the first uncertainty is statistical, the second systematic and the third is due to the uncertainties on the branching fractions of the ψ(2S) and J/ψ mesons. The measured ratio makes the interpretation of the χc1(3872) state as a pure D0 D ¯ ∗ 0 + D ¯ 0 D*0 molecule questionable and strongly indicates a sizeable compact charmonium or tetraquark component within the χc1(3872) state.",
        "authors": [
            "R. Aaij",
            "A. S. W. Abdelmotteleb",
            "C. Abellan Beteta",
            "F. Abudinén",
            "T. Ackernley",
            "A. A. Adefisoye",
            "B. Adeva",
            "M. Adinolfi",
            "P. Adlarson",
            "C. Agapopoulou",
            "C. A. Aidala",
            "Z. Ajaltouni",
            "S. Akar",
            "K. Akiba",
            "P. Albicocco",
            "J. Albrecht"
        ],
        "journal_conference_name": "Journal of High Energy Physics",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157704",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Vista: Machine Learning based Database Performance Troubleshooting Framework in Amazon RDS",
        "abstract": "Database performance troubleshooting is a complex multi-step process that broadly involves three key stages- (a) Detection: determining what's wrong and when; (b) Root Cause Analysis (RCA): reasoning about why is the performance poor; (c) Resolution: identifying a fix. A plethora of techniques exist to address each of these problems, but they hardly work in real-world at scale. First, real-world customer workloads are noisy, non-stationary and quasi-periodic in nature rendering traditional detectors ineffective. Second, real-world production databases execute a highly diverse set of queries that skew the database statistics into long-tail distributions causing traditional RCA methods to fail. Third, these databases typically execute millions of such diverse queries every minute rendering traditional methods inefficient when deployed at scale.\r\nIn this paper we describe Vista, a machine learning based performance troubleshooting framework for databases, and dive-deep into how it addresses the 3 real-world problems outlined above. Vista deploys a deep auto-regressive model trained on a large and diverse Amazon Relational Database Service (RDS) fleet with custom skip connections and periodicity alignment features to model long range and varying periodicity in customer workloads, and detects performance bottlenecks in the form of outliers. Furthermore, it efficiently filters only a top few dominating SQL queries from millions in a problematic workload, and uses a robust causal inference framework to identify the culprit queries and their statistics leading to a low false-positive and false-negative rate. Currently, Vista runs on hundreds of thousands of RDS databases, analyzes millions of workloads every day bringing down the troubleshooting time for RDS customers from hours to seconds. At the end, we also describe several challenges and learnings from implementing and deploying Vista at Amazon scale.",
        "authors": [
            "Vikramank Singh",
            "Zhao Song",
            "Balakrishnan (Murali) Narayanaswamy",
            "Kapil Eknath Vaidya",
            "Tim Kraska"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|ACM Symposium on Cloud Computing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157897",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Clinical Validation of Non-invasive Simulation-Based Determination of Vascular Impedance, Wave Intensity, and Hydraulic Work in Patients Undergoing Transcatheter Aortic Valve Replacement",
        "abstract": "Purpose The impact of Aortic Stenosis (AS) on the left ventricle (LV) extends beyond the influence of the pressure drop across the stenotic valve, but also includes the additional serial afterload imposed by the vascular system. Aortic input impedance is the gold standard for comprehensively studying the contribution of the vascular system to total myocardial afterload, but in the past measurement has been challenging arising from the need for invasive catheterization or specialized equipment to precisely record time-resolved blood pressure and flow signals. The goal of this work was to develop and validate a novel simulation-based method for determining aortic input impedance using only clinically available echocardiographic data and a simple blood pressure measurement. Methods A simulation-based method to determine vascular impedance was developed using echocardiographic data and a brachial blood pressure measurement. Simulation-based impedance was compared to impedance calculated from echocardiographic flow data and pressure data from a non-invasive central pressure measurement device. Results In validation analysis comparing patient-specific simulation-based vascular impedance to non-invasively measured impedance, correlation between methods across a range of vascular parameters varied between R2 = 0.40 and 0.99. A tendency was seen toward underestimation of pressure waveforms in point-by-point comparison of measured and simulated waveforms with an overall mean difference of 4.01 mmHg. Conclusions Requiring only non-invasive clinical data that are widely available, simulation-based vascular impedance has the potential to allow for easier, more widespread, and larger-scale investigation of the effect of vascular impedance on total LV afterload.",
        "authors": [
            "Jonathan Y. Brown",
            "Gabriela V. Fernandez",
            "Jose M. De La Torre Hernández",
            "Michael Murphy",
            "Benjamin S. Wessler",
            "Elazer R. Edelman"
        ],
        "journal_conference_name": "Annals of Biomedical Engineering",
        "publisher": "Springer International Publishing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157670",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "How Do Transformers Model Physics? Investigating the Simple Harmonic Oscillator",
        "abstract": "ow do transformers model physics? Do transformers model systems with interpretable analytical solutions or do they create an “alien physics” that is difficult for humans to decipher? We have taken a step towards demystifying this larger puzzle by investigating the simple harmonic oscillator (SHO), 𝑥¨+2𝛾𝑥˙+𝜔20𝑥=0\r\n, one of the most fundamental systems in physics. Our goal was to identify the methods transformers use to model the SHO, and to do so we hypothesized and evaluated possible methods by analyzing the encoding of these methods’ intermediates. We developed four criteria for the use of a method within the simple test bed of linear regression, where our method was 𝑦=𝑤𝑥\r\n and our intermediate was w: (1) Can the intermediate be predicted from hidden states? (2) Is the intermediate’s encoding quality correlated with the model performance? (3) Can the majority of variance in hidden states be explained by the intermediate? (4) Can we intervene on hidden states to produce predictable outcomes? Armed with these two correlational (1,2), weak causal (3), and strong causal (4) criteria, we determined that transformers use known numerical methods to model the trajectories of the simple harmonic oscillator, specifically, the matrix exponential method. Our analysis framework can conveniently extend to high-dimensional linear systems and nonlinear systems, which we hope will help reveal the “world model” hidden in transformers.",
        "authors": [
            "Subhash Kantamneni",
            "Ziming Liu",
            "Max Tegmark"
        ],
        "journal_conference_name": "Entropy",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157692",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Towards Safer Heuristics With Xplain",
        "abstract": "Many problems that cloud operators solve are computationally expensive, and operators often use heuristic algorithms (that are faster and scale better than optimal) to solve them more efficiently. Heuristic analyzers enable operators to find when and by how much their heuristics underperform. However, these tools do not provide enough detail for operators to mitigate the heuristic's impact in practice: they only discover a single input instance that causes the heuristic to underperform (and not the full set) and they do not explain why.\r\nWe propose XPlain, a tool that extends these analyzers and helps operators understand when and why their heuristics underperform. We present promising initial results that show such an extension is viable.",
        "authors": [
            "Pantea Karimi",
            "Solal Pirelli",
            "Siva Kesava Reddy Kakarla",
            "Ryan Beckett",
            "Santiago Segarra",
            "Beibin Li",
            "Pooria Namyar",
            "Behnaz Arzani"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 23rd ACM Workshop on Hot Topics in Networks",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157896",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Dynamic Expansion and Merging of the Equatorial Ionization Anomaly During the 10–11 May 2024 Super Geomagnetic Storm",
        "abstract": "first_pagesettingsOrder Article Reprints\r\nOpen AccessTechnical Note\r\nDynamic Expansion and Merging of the Equatorial Ionization Anomaly During the 10–11 May 2024 Super Geomagnetic Storm\r\nby Ercha Aa 1,2,*ORCID,Yanhong Chen 2ORCID andBingxian Luo 2ORCID\r\n1\r\nHaystack Observatory, Massachusetts Institute of Technology, Westford, MA 01886, USA\r\n2\r\nNational Space Science Center, Chinese Academy of Sciences, Beijing 100190, China\r\n*\r\nAuthor to whom correspondence should be addressed.\r\nRemote Sens. 2024, 16(22), 4290; https://doi.org/10.3390/rs16224290\r\nSubmission received: 24 October 2024 / Revised: 14 November 2024 / Accepted: 15 November 2024 / Published: 18 November 2024\r\n(This article belongs to the Special Issue Ionosphere Monitoring with Remote Sensing (3rd Edition))\r\nDownloadkeyboard_arrow_down Browse Figures Versions Notes\r\n\r\nAbstract\r\nThis study investigates the responses of the equatorial and low-latitude ionosphere in the American–Atlantic longitude sector during the super geomagnetic storm that occurred on 10–11 May 2024. The investigation utilizes multi-instrument datasets, including ground-based observations (GNSS TEC, ionosonde, and Fabry–Perot interferometer) as well as space-borne satellite measurements (GOLD, Swarm, DMSP, and TIMED). Our findings reveal significant day-to-day variations in the storm-time equatorial ionization anomaly (EIA), summarized as follows: (1) During the main phase of the storm, the low- and mid-latitude ionosphere experienced a positive storm, with TEC drastically enhanced by 50–100% within a few hours. The EIA crests exhibited a substantial poleward expansion, reaching as high as ±35° MLAT. This expansion was caused by the enhanced fountain effect driven by penetration electric fields, along with increased ambipolar diffusion due to transient meridional wind surges. (2) During the recovery phase of the storm, the global ionosphere was characterized by a substantial negative storm with a 50–80% depletion in TEC. The EIA crests were notably suppressed and merged into a single equatorial band, which can be attributed to the composition change effect and the influence of disturbance dynamo electric fields. These results illustrate the complex processes of magnetosphere–ionosphere–thermosphere coupling during a superstorm, highlighting the significant impacts of space weather on the global ionosphere.",
        "authors": [
            "Ercha Aa",
            "Yanhong Chen",
            "Bingxian Luo"
        ],
        "journal_conference_name": "Remote Sensing",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157691",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "MLTCP: A Distributed Technique to Approximate Centralized Flow Scheduling For Machine Learning",
        "abstract": "This paper argues that congestion control protocols in machine learning datacenters sit at a sweet spot between centralized and distributed flow scheduling solutions. We present MLTCP, a technique to augment today's congestion control algorithms to approximate an interleaved centralized flow schedule. At the heart of MLTCP lies a straight-forward principle based on a key conceptual insight: by scaling the congestion window size (or sending rate) based on the number of bytes sent at each iteration, MLTCP flows eventually converge into a schedule that reduces network contention. We demonstrate that MLTCP uses a gradient descent trend with a step taken at every training (or fine-tuning) iteration towards reducing network congestion among competing jobs.",
        "authors": [
            "Sudarsanan Rajasekaran",
            "Sanjoli Narang",
            "Anton A. Zabreyko",
            "Manya Ghobadi"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 23rd ACM Workshop on Hot Topics in Networks",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157894",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "The Case for Decentralized Fallback Networks",
        "abstract": "This paper argues that network and application delivery infrastructures have become highly centralized and are more vulnerable to attacks and disasters than is desirable. It proposes a research agenda for decentralized fallback networks and focuses on a key component---a city-scale decentralized network using existing Wi-Fi access points, which are deployed across almost all buildings in cities. It proposes a routing system that uses information about buildings from geospatial maps instead of traditional routing mechanisms to scale well to millions of Wi-Fi nodes.",
        "authors": [
            "James Lynch",
            "Ziqian Liu",
            "Chenning Li",
            "Manya Ghobadi",
            "Hari Balakrishnan"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|The 23rd ACM Workshop on Hot Topics in Networks",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157895",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Modeling Inertia-Driven Oil Transport Inside the Three-Piece Oil Control Ring of Internal Combustion Engines",
        "abstract": "The three-piece oil control ring (TPOCR), traditionally used in light-duty gasoline engines, is becoming a viable option for heavy-duty gas and hydrogen engines due to its ability to control lubricating oil consumption (LOC) under throttled conditions. Understanding the distribution of oil inside the TPOCR groove, as well as the effects of rail gap and drain hole positions, is critical for optimizing TPOCR and groove designs. In this work, a one-dimensional oil distribution model was developed to simulate inertia-driven oil transport in the TPOCR groove. A novel approach was proposed by first dividing the TPOCR into units composed of a pair of expander pitches. Then, the relationship between the oil outflow rate of the unit and its oil mass was established with the help of three-dimensional two-phase computational fluid dynamics (CFD) simulations. This relationship was then used to model one-dimensional oil transport along the circumference of the TPOCR groove. Incorporating the boundary conditions at the rail gaps and drain holes, this simple model can complete computations for 10,000 cycles within a few seconds, allowing for quick the evaluation of transient behavior and design iterations. Studies on low-load conditions show that the model, with reasonable adjustment for the boundary conditions, can match the oil distribution patterns observed in visualization experiments. This is the first step toward studying oil transport in the TPOCR groove before involving the effects of gas flows.",
        "authors": [
            "Tsung-Yu Yang",
            "Mo Li",
            "Tian Tian"
        ],
        "journal_conference_name": "Lubricants",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157690",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Unsupervised Canine Emotion Recognition Using Momentum Contrast",
        "abstract": "We describe a system for identifying dog emotions based on dogs’ facial expressions and body posture. Towards that goal, we built a dataset with 2184 images of ten popular dog breeds, grouped into seven similarly sized primal mammalian emotion categories defined by neuroscientist and psychobiologist Jaak Panksepp as ‘Exploring’, ‘Sadness’, ‘Playing’, ‘Rage’, ‘Fear’, ‘Affectionate’ and ‘Lust’. We modified the contrastive learning framework MoCo (Momentum Contrast for Unsupervised Visual Representation Learning) to train it on our original dataset and achieved an accuracy of 43.2% and a baseline of 14%. We also trained this model on a second publicly available dataset that resulted in an accuracy of 48.46% but had a baseline of 25%. We compared our unsupervised approach with a supervised model based on a ResNet50 architecture. This model, when tested on our dataset with the seven Panksepp labels, resulted in an accuracy of 74.32%",
        "authors": [
            "Aarya Bhave",
            "Alina Hafner",
            "Anushka Bhave",
            "Peter A. Gloor"
        ],
        "journal_conference_name": "Sensors",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157689",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Exploring the impact of COVID-19 on the grammar of schools in project-based learning contexts",
        "abstract": "While scholars and public figures have positioned the COVID-19 pandemic as an opportunity for school reform, the response to this potential for change by teachers remains underexplored. In turn, we attend to the following research question: how do teachers at project-based learning high schools conceptualize the changes to education that have occurred in response to the COVID-19 pandemic? In analyzing temporally dispersed interviews with eight teachers from four different schools in the United States between 2020 and 2022, we found that participants recognized changes in the pedagogies, curricula, assessments, and structures in their school systems. In particular, teachers conceptualized these educational shifts through the lenses of technological change, a push for student-centered practices, and an embrace of real world applications of learning. However, they also described a reversal of these changes once in person schooling returned, illustrating an inability of the pandemic to affect the “grammar of schools” (Tyack & Tobin, 1994).",
        "authors": [
            "Peter J. Woods",
            "Emma Anderson",
            "Avneet Hira"
        ],
        "journal_conference_name": "Journal of Educational Change",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157560",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "VI-VS: calibrated identification of feature dependencies in single-cell multiomics",
        "abstract": "Unveiling functional relationships between various molecular cell phenotypes from data using machine learning models is a key promise of multiomics. Existing methods either use flexible but hard-to-interpret models or simpler, misspecified models. VI-VS (Variational Inference for Variable Selection) balances flexibility and interpretability to identify relevant feature relationships in multiomic data. It uses deep generative models to identify conditionally dependent features, with false discovery rate control. VI-VS is available as an open-source Python package, providing a robust solution to identify features more likely representing genuine causal relationships.",
        "authors": [
            "Pierre Boyeau",
            "Stephen Bates",
            "Can Ergen",
            "Michael I. Jordan",
            "Nir Yosef"
        ],
        "journal_conference_name": "Genome Biology",
        "publisher": "BioMed Central",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157562",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Learning reaction-transport coupling from thermal waves",
        "abstract": "Although thermal waves are ubiquitous in nature and engineering, the development of diagnostic tools capable of elucidating the roles of reaction and transport remains an unmet need. This limits our comprehension of the physics and ability to predict wave dynamics. Here we demonstrate that thermal properties and chemical kinetics can be learned directly from observing thermal wave dynamics, using partial differential equation-constrained optimization. This enables the determination of unobserved reaction rates without the need for a comprehensive measurement of all state variables, given the model space constrained by governing equations. Examples include steady planar waves and unsteady pulsating waves of which dynamics are commonly observed in nature. We show successful learning of thermal properties and chemical kinetics and reconstruction of wave dynamics with the inferred properties, which enables the comprehension of the intricate reaction-transport coupling from thermal data.",
        "authors": [
            "Suyong Kim",
            "Sili Deng"
        ],
        "journal_conference_name": "Nature Communications",
        "publisher": "Springer Science and Business Media LLC",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158171",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Identifying Money Laundering Subgraphs on the Blockchain",
        "abstract": "Anti-Money Laundering (AML) involves the identification of money laundering crimes in financial activities, such as cryptocurrency transactions. Recent studies advanced AML through the lens of graph-based machine learning, modeling the web of financial transactions as a graph and developing graph methods to identify suspicious activities. For instance, a recent effort on opensourcing datasets and benchmarks, Elliptic2, treats a set of Bitcoin addresses, considered to be controlled by the same entity, as a graph node and transactions among entities as graph edges. This modeling reveals the “shape” of a money laundering scheme—a subgraph on the blockchain, such as a peeling chain or a nested service. Despite the attractive subgraph classification results benchmarked by the paper, competitive methods remain expensive to apply due to the massive size of the graph; moreover, existing methods require candidate subgraphs as inputs which may not be available in practice.\r\nIn this work, we introduce RevTrack, a graph-based framework that enables large-scale AML analysis with a lower cost and a higher accuracy. The key idea is to track the initial senders and the final receivers of funds; these entities offer a strong indication of the nature (licit vs. suspicious) of their respective subgraph. Based on this framework, we propose RevClassify, which is a neural network model for subgraph classification. Additionally, we address the practical problem where subgraph candidates are not given, by proposing RevFilter. This method identifies new suspicious subgraphs by iteratively filtering licit transactions, using RevClassify. Benchmarking these methods on Elliptic2, a new standard for AML, we show that RevClassify outperforms state-of-the-art subgraph classification techniques in both cost and accuracy. Furthermore, we demonstrate the effectiveness of RevFilter in discovering new suspicious subgraphs, confirming its utility for practical AML.",
        "authors": [
            "Kiwhan Song",
            "Mohamed Ali Dhraief",
            "Muhua Xu",
            "Locke Cai",
            "Xuhao Chen",
            "Arvind Mithal",
            "Jie Chen"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|5th ACM International Conference on AI in Finance",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157760",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Renewing Our Focus on Vulnerable Populations Among People Living with HIV",
        "abstract": "The global HIV landscape has changed over the past few decades, with great milestones achieved in both HIV treatment and prevention. Access to lifesaving antiretroviral therapy (ART) has markedly expanded, with a total of 30.7 million (27 million–31.9 million) out of 39.9 million (36.1 million–44.6 million) people living with HIV accessing the medication in 2023 [1]. Continued expansion of access to, initiation of, and adherence to treatment is crucial in achieving control of the HIV pandemic, given the strong evidence that treatment is prevention [2]. Despite these marked advances, 28% of people living with HIV (PLHIV) are reported to be virally unsuppressed [1]. Viral non-suppression is associated with increased risk of progression to AIDS and portends poor outcomes for PLHIV [3,4]. Additionally, viral non-suppression increases the risk of onward transmission of HIV, reversing the gains made in combating the pandemic [3]. The risk of viral non-suppression is greater in certain groups. This Special Issue focuses on exploring HIV support, care, and treatment for vulnerable populations, or those at elevated risk of viral non-suppression and poor health outcomes.\r\nWe solicited articles on this topic and received submissions from diverse settings and authors of different backgrounds and training. The interest and importance of this topic are revealed in the diversity of articles that were submitted and the disciplines that showed interest. This Special Issue contains ten articles that advance our understanding of vulnerable populations, challenge the current thinking about vulnerable populations, and propose bold interventions to address the barriers to HIV care engagement throughout the cascade.\r\nThe articles in this Special Issue bring to the fore three critical questions about vulnerable groups: What makes one vulnerable? What are the threats to care engagement for vulnerable people? And what health care system changes are needed to accommodate vulnerable people? These questions must be addressed to improve outcomes among vulnerable groups, especially to design interventions that address their concerns.",
        "authors": [
            "James Ayieko",
            "Marguerite Thorp",
            "Musie Ghebremichael"
        ],
        "journal_conference_name": "Tropical Medicine and Infectious Disease",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157688",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Local geometry of NAE-SAT solutions in the condensation regime",
        "abstract": "The local behavior of typical solutions of random constraint satisfaction problems (csp) describes many important phenomena including clustering thresholds, decay of correlations, and the behavior of message passing algorithms. When the constraint density is low, studying the planted model is a powerful technique for determining this local behavior which in many examples has a simple Markovian structure. The work of Coja-Oghlan, Kapetanopoulos, Müller (Comb Prob Comput 29:346-422, 2020) showed that for a wide class of models, this description applies up to the so-called condensation threshold. Understanding the local behavior after the condensation threshold is more complex due to long-range correlations. In this work, we revisit the random regular nae-sat model in the condensation regime and determine the local weak limit which describes a random solution around a typical variable. This limit exhibits a complicated non-Markovian structure arising from the space of solutions being dominated by a small number of large clusters. This is the first description of the local weak limit in the condensation regime for any sparse random csps in the one-step replica symmetry breaking (1rsb) class. Our result is non-asymptotic and characterizes the tight fluctuation O ( n - 1 / 2 ) around the limit. Our proof is based on coupling the local neighborhoods of an infinite spin system, which encodes the structure of the clusters, to a broadcast model on trees whose channel is given by the 1rsb belief-propagation fixed point. We believe that our proof technique has broad applicability to random csps in the 1rsb class.",
        "authors": [
            "Allan Sly",
            "Youngtak Sohn"
        ],
        "journal_conference_name": "Probability Theory and Related Fields",
        "publisher": "Springer Berlin Heidelberg",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157559",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "FraudGT: A Simple, Effective, and Efficient Graph Transformer for Financial Fraud Detection",
        "abstract": "Fraud detection plays a crucial role in the financial industry, preventing significant financial losses. Traditional rule-based systems and manual audits often struggle with the evolving nature of fraud schemes and the vast volume of transactions. Recent advances in machine learning, particularly graph neural networks (GNNs), have shown promise in addressing these challenges. However, GNNs still face limitations in learning intricate patterns, effectively utilizing edge attributes, and maintaining efficiency on large financial graphs. To address these limitations, we introduce FraudGT, a simple, effective, and efficient graph transformer (GT) model specifically designed for fraud detection in financial transaction graphs. FraudGT leverages edge-based message passing gates and an edge attribute-based attention bias to enhance its ability to discern important transactional features and differentiate between normal and fraudulent transactions. Our model achieves state-of-the-art performance in detecting fraudulent activities while demonstrating high throughput and significantly lower latency compared to existing methods. We validate the effectiveness of FraudGT through extensive experiments on multiple large-scale synthetic financial datasets. FraudGT consistently outperforms other models, achieving 7.8–17.8% higher F1 scores, while delivering an average of 2.4 × greater throughput and reduced latency. Our code and datasets are available at https://github.com/junhongmit/FraudGT.",
        "authors": [
            "Junhong Lin",
            "Xiaojie Guo",
            "Yada Zhu",
            "Samuel Mitchell",
            "Erik Altman",
            "Julian Shun"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|5th ACM International Conference on AI in Finance",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157762",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "A 35-Year Analysis of Vegetation Cover in Rare-Earth Mining Areas Using Landsat Data",
        "abstract": "Fractional vegetation cover (FVC) plays a significant role in assessing ecological quality and protection, as well as soil and water conservation. As a typical rare-earth resource county in China, Dingnan County has experienced rapid development due to rare-earth mining, resulting in significant alterations to vegetation cover. To elucidate the spatio-temporal changes in vegetation within Dingnan County over the past 35 years and the effects of natural and human factors on these changes, the spatial and temporal variations in FVC were analyzed using Landsat-TM/OLI multispectral images taken in 1988, 1995, 1997, 2002, 2006, 2013, 2017, and 2023. The findings indicate that (1) vegetation coverage in Dingnan County decreased from 1988 to 2002, followed by a gradual increase; (2) high vegetation cover is predominantly found in forested areas that maintain their natural state, while the central town and mining areas exhibit generally low coverage; (3) there are regional differences in the relationship between vegetation cover and environmental factors in Dingnan County. This research facilitates the alignment of ion-type rare-earth mining with ecological protection, thereby promoting the sustainable development of the mining area and providing scientific guidance for local governments to formulate more effective management and protection strategies for the mining ecosystem. Additionally, this research offers a scientific foundation for mining areas globally to develop sustainable policies and informed decision-making regarding environmental protection and sustainable development.",
        "authors": [
            "Zhubin Zheng",
            "Yuqing Liu",
            "Na Chen",
            "Ge Liu",
            "Shaohua Lei",
            "Jie Xu",
            "Jianzhong Li",
            "Jingli Ren",
            "Chao Huang"
        ],
        "journal_conference_name": "Forests",
        "publisher": "Multidisciplinary Digital Publishing Institute",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157687",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Hydrodynamic forces on a side-by-side ellipse pair with and without relative motion",
        "abstract": "Motivated by flow interactions in schooling biological swimmers as well as in unmanned underwater vehicle fleets, we investigate the flow past two identical 6 : 1 ellipses using two-dimensional simulations at Reynolds numbers of  (103). When both ellipses move at the same velocity, overall drag reductions of 10 %–20 % can be achieved in staggered formations, with the strongest drag reductions occurring at the smallest lateral distances. In side-by-side configurations, the drag on both bodies increases by 10 %–20 %. Lift coefficients are repulsive and up to four times larger than the total drag coefficients. During overtaking manoeuvres, increasing the relative speed of the overtaking ellipse predominantly affects the forces on the overtaken ellipse. The mean drag force on the overtaken ellipse increases with increasing speed difference. Mean lift forces during the overtaking manoeuvre are repulsive for both bodies; as the speed difference increases, the repulsive force increases on the overtaken body and decreases on the overtaking body. Overall, these results highlight that the lateral forces in hydrodynamic interactions between bodies in formation dominate the hydrodynamic interactions. Further, the results indicate that future work is needed to investigate how viscous and three-dimensional effects change the lateral forces between side-by-side submerged bodies.",
        "authors": [
            "Preston Rhodes",
            "Wim M. van Rees"
        ],
        "journal_conference_name": "Flow",
        "publisher": "Cambridge University Press",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157676",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Hierarchically conductive electrodes unlock stable and scalable CO2 electrolysis",
        "abstract": "Electrochemical CO2 reduction has emerged as a promising CO2 utilization technology, with Gas Diffusion Electrodes becoming the predominant architecture to maximize performance. Such electrodes must maintain robust hydrophobicity to prevent flooding, while also ensuring high conductivity to minimize ohmic losses. Intrinsic material tradeoffs have led to two main architectures: carbon paper is highly conductive but floods easily; while expanded Polytetrafluoroethylene is flooding resistant but non-conductive, limiting electrode sizes to just 5 cm2. Here we demonstrate a hierarchically conductive electrode architecture which overcomes these scaling limitations by employing inter-woven microscale conductors within a hydrophobic expanded Polytetrafluoroethylene membrane. We develop a model which captures the spatial variability in voltage and product distribution on electrodes due to ohmic losses and use it to rationally design the hierarchical architecture which can be applied independent of catalyst chemistry or morphology. We demonstrate C2+ Faradaic efficiencies of ~75% and reduce cell voltage by as much as 0.9 V for electrodes as large as 50 cm2 by employing our hierarchically conductive electrode architecture.",
        "authors": [
            "Simon Rufer",
            "Michael P Nitzsche",
            "Sanjay Garimella",
            "Jack R Lake",
            "Kripa K Varanasi"
        ],
        "journal_conference_name": "Nature Communications",
        "publisher": "Springer Science and Business Media LLC",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/158170",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Theory of Quantum Anomalous Hall Phases in Pentalayer Rhombohedral Graphene Moiré Structures",
        "abstract": "Remarkable recent experiments on the moiré structure formed by pentalayer rhombohedral graphene aligned with a hexagonal boron nitride substrate report the discovery of a zero field fractional quantum Hall effect. These “(fractional) quantum anomalous Hall” [(F)QAH] phases occur for one sign of a perpendicular displacement field, and correspond, experimentally, to full or partial filling of a valley polarized Chern-1 band. Such a band is absent in the noninteracting band structure. Here we show that electron-electron interactions play a crucial role, and present microscopic theoretical calculations demonstrating the emergence of a nearly flat, isolated, Chern-1 band and FQAH phases in this system. We also study the four- and six-layer analogs and identify parameters where a nearly flat isolated Chern-1 band emerges which may be suitable to host FQAH physics.",
        "authors": [
            "Zhihuan Dong",
            "Adarsh S. Patri",
            "Todadri Senthil"
        ],
        "journal_conference_name": "Physical Review Letters",
        "publisher": "American Physical Society",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157541",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Bridging Dictionary: AI-Generated Dictionary of Partisan Language Use",
        "abstract": "Words often carry different meanings for people from diverse backgrounds. Today's era of social polarization demands that we choose words carefully to prevent miscommunication, especially in political communication and journalism. To address this issue, we introduce the Bridging Dictionary, an interactive tool designed to illuminate how words are perceived by people with different political views. The Bridging Dictionary includes a static, printable document featuring 796 terms with summaries generated by a large language model. These summaries highlight how the terms are used distinctively by Republicans and Democrats. Additionally, the Bridging Dictionary offers an interactive interface that lets users explore selected words, visualizing their frequency, sentiment, summaries, and examples across political divides. We present a use case for journalists and emphasize the importance of human agency and trust in further enhancing this tool. The deployed version of Bridging Dictionary is available at https://dictionary.ccc-mit.org/.",
        "authors": [
            "Hang Jiang",
            "Doug Beeferman",
            "William Brannon",
            "Andrew Heyward",
            "Deb Roy"
        ],
        "journal_conference_name": "No Journal/Conference",
        "publisher": "ACM|Companion of the 2024 Computer-Supported Cooperative Work and Social Computing",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157764",
        "group_name": "Cireng Crispy"
    },
    {
        "title": "Excess Mortality and its Determinants During the COVID-19 Pandemic in 21 Countries: An Ecological Study from the C-MOR Project, 2020 and 2021",
        "abstract": "Introduction The COVID-19 pandemic overwhelmed health systems, resulting in a surge in excess deaths. This study clustered countries based on excess mortality to understand their response to the pandemic and the influence of various factors on excess mortality within each cluster. Materials and Methods This ecological study is part of the COVID-19 MORtality (C-MOR) Consortium. Mortality data were gathered from 21 countries and were previously used to calculate weekly all-cause excess mortality. Thirty exposure variables were considered in five categories as factors potentially associated with excess mortality: population factors, health care resources, socioeconomic factors, air pollution, and COVID-19 policy. Estimation of Latent Class Linear Mixed Model (LCMM) was used to cluster countries based on response trajectory and Generalized Linear Mixture Model (GLMM) for each cluster was run separately. Results Using LCMM, two clusters were reached. Among 21 countries, Brazil, the USA, Georgia, and Poland were assigned to a separate cluster, with the mean of excess mortality z-score in 2020 and 2021 around 4.4, compared to 1.5 for all other countries assigned to the second cluster. In both clusters the population incidence of COVID-19 had the greatest positive relationship with excess mortality while interactions between the incidence of COVID-19, fully vaccinated people, and stringency index were negatively associated with excess mortality. Moreover, governmental variables (government revenue and government effectiveness) were the most protective against excess mortality. Conclusion This study highlighted that clustering countries based on excess mortality can provide insights to gain a broader understanding of countries' responses to the pandemic and their effectiveness.",
        "authors": [
            "Mohammad Reza Rahmanian Haghighi",
            "Chryso T. Pallari",
            "Souzana Achilleos",
            "Annalisa Quattrocchi",
            "John Gabel",
            "Andreas Artemiou",
            "Maria Athanasiadou",
            "Stefania Papatheodorou",
            "Tianyu Liu",
            "José Antonio Cernuda Martínez",
            "Gleb Denissov",
            "Błażej Łyszczarz",
            "Qian Huang",
            "Kostas Athanasakis",
            "Catherine M. Bennett"
        ],
        "journal_conference_name": "Journal of Epidemiology and Global Health",
        "publisher": "Springer Netherlands",
        "year": "2024",
        "doi": "https://hdl.handle.net/1721.1/157558",
        "group_name": "Cireng Crispy"
    }
]